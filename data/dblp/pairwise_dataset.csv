id,title,abstract,soft_labels_l1,relevant_children_l1,reasoning_l1,soft_labels_l2,relevant_children_l2,reasoning_l2,soft_labels_l3,relevant_children_l3,reasoning_l3
4216,Human actions recognition from streamed Motion Capture,"This paper introduces a new method for streamed action recognition using Motion Capture (MoCap) data. First, the histograms of action poses, extracted from MoCap data, are computed according to Hausdorf distance. Then, using a dynamic programming algorithm and an incremental histogram computation, our proposed solution recognizes actions in real time from streams of poses. The comparison of histograms for recognition was achieved using Bhattacharyya distance. Furthermore, the learning phase has remained very efficient with respect to both time and complexity. We have shown the effectiveness of our solution by testing it on large datasets, obtained from animation databases. In particular, we were able to achieve excellent recognition rates that have outperformed the existing methods.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.1,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.85,Applied computing:0.6,Social and professional topics:0.0",Computing methodologies,Computing methodologies: The paper presents algorithms for action recognition from MoCap data. Applied computing: Marginal relevance if applied to real-time systems. Other categories like 'Mathematics of computing' are less central as the focus is on algorithmic implementation.,"Artificial intelligence:0.2,Computer graphics:0.7,Concurrent computing methodologies:0.1,Distributed computing methodologies:0.1,Machine learning:0.6,Modeling and simulation:0.3,Parallel computing methodologies:0.1,Symbolic and algebraic manipulation:0.1","Computer graphics,Machine learning",Computer graphics is relevant as the method uses Motion Capture data for action recognition. Machine learning is central due to the real-time recognition algorithm. Modeling and simulation is secondary.,"Animation:0.5,Cross-validation:0.0,Graphics systems and interfaces:0.0,Image compression:0.0,Image manipulation:0.0,Learning paradigms:0.3,Learning settings:0.3,Machine learning algorithms:0.3,Machine learning approaches:1.0,Rendering:0.0,Shape modeling:0.0","Machine learning approaches,Animation",Machine learning approaches are central to the action recognition method. Animation is relevant due to MoCap data from animation databases. Other categories are less directly relevant.
1605,A parallel algorithm for compile-time scheduling of parallel programs on multiprocessors,"Proposes a parallel randomized algorithm, called PFAST (Parallel Fast Assignment using Search Technique), for scheduling parallel programs represented by directed acyclic graphs (DAGs) during compile-time. The PFAST algorithm has O(e) time complexity, where e is the number of edges in the DAG. This linear-time algorithm works by first generating an initial solution and then refining it using a parallel random search. Using a prototype computer-aided parallelization and scheduling tool called CASCH (Computer-Aided SCHeduling), the algorithm is found to outperform numerous previous algorithms while taking dramatically smaller execution times. The distinctive feature of this research is that, instead of simulations, our proposed algorithm is evaluated and compared with other algorithms using the CASCH tool with real applications running on an Intel Paragon. The PFAST algorithm is also evaluated with randomly generated DAGs for which optimal schedules are known. The algorithm generated optimal solutions for a majority of the test cases and close-to-optimal solutions for the others. The proposed algorithm is the fastest scheduling algorithm known to us and is an attractive choice for scheduling under running time constraints.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.95,Applied computing:0.1,Social and professional topics:0.1",Computing methodologies,Computing methodologies is highly relevant as the paper presents a parallel scheduling algorithm for multiprocessors with theoretical and experimental validation. Categories like Software or Networks are less aligned with the algorithmic contribution.,"Parallel computing methodologies:0.95,Modeling and simulation:0.8,Concurrent computing methodologies:0.3,Distributed computing methodologies:0.3,Machine learning:0.1,Artificial intelligence:0.1,Computer graphics:0.1,Symbolic and algebraic manipulation:0.1","Parallel computing methodologies,Modeling and simulation",Parallel computing methodologies is highly relevant for the PFAST algorithm. Modeling and simulation is relevant for the evaluation using real applications and generated DAGs. Other categories like Concurrent computing methodologies are less relevant as the focus is on scheduling rather than concurrency.,"Parallel algorithms:1,Simulation evaluation:0.9,Model development and analysis:0.7,Simulation types and techniques:0.4,Parallel programming languages:0.1","Parallel algorithms,Simulation evaluation",Parallel algorithms is relevant as the core contribution is a parallel scheduling algorithm. Simulation evaluation is relevant for the tool-based testing. Other categories like programming languages were rejected as the focus is on algorithm design.
925,Simple spatial processing for color mappings,"Color is commonly treated as an autonomous entity and all color processing and color mapping is predominantly done as a point operation, linking one color or color description to another. This point-wise approach clearly does not capture the spatial dependencies of color that we humans experience. However, spatial models are rather computationally extensive, leading to a continuous predominance of point-wise color processing methods. This paper examines what advantages can be gained from a most simplistic spatial approach.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:1.0,Applied computing:0.0,Social and professional topics:0.0",Computing methodologies,Computing methodologies is directly relevant as the paper introduces a new color processing technique. Other categories are irrelevant as the focus is on computational methods.,"Artificial intelligence:0.0,Computer graphics:1.0,Concurrent computing methodologies:0.0,Distributed computing methodologies:0.0,Machine learning:0.0,Modeling and simulation:0.75,Parallel computing methodologies:0.0,Symbolic and algebraic manipulation:0.0","Computer graphics,Modeling and simulation",Computer graphics: The paper discusses color processing in visual systems. Modeling and simulation: Spatial models are evaluated. Other categories like AI are irrelevant as the focus is on graphics rendering.,"Animation:0,Graphics systems and interfaces:0,Image compression:0,Image manipulation:1,Model development and analysis:0.5,Rendering:0.5,Shape modeling:0,Simulation evaluation:0,Simulation support systems:0,Simulation theory:0,Simulation types and techniques:0",Image manipulation,Image manipulation is relevant for spatial color processing. Other categories like Rendering are not the focus of the paper.
4698,A Multiresolution Approach Based on MRF and Bak-Sneppen Models for Image Segmentation,"The two major Markov Random Fields (MRF) based algorithms for image segmentation are the Simulated Annealing (SA) and Iterated Conditional Modes (ICM). In practice, compared to the SA, the ICM provides reasonable segmentation and shows robust behavior in most of the cases. However, the ICM strongly depends on the initialization phase. 
 
In this paper, we combine Bak-Sneppen model and Markov Random Fields to define a new image segmentation approach. We introduce a multiresolution technique in order to speed up the segmentation process and to improve the restoration process. Image pixels are viewed as lattice species of Bak-Sneppen model. The a-posteriori probability corresponds to a local fitness. At each cycle, some objectionable species are chosen for a random change in their fitness values. Furthermore, the change in the fitness of each species engenders fitness changes for its neighboring species. After a certain number of iteration, the system converges to a Maximum A Posteriori estimate. In this multireolution approach, we use a wavelet transform to reduce the size of the system.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.2,Networks:0.1,Software and its engineering:0.2,Theory of computation:0.3,Mathematics of computing:0.4,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.95,Applied computing:0.5,Social and professional topics:0.1",Computing methodologies,Computing methodologies (0.95): The paper presents a novel image segmentation algorithm combining MRF and Bak-Sneppen models with multiresolution techniques. Other categories like Mathematics of computing (0.4) or Theory of computation (0.3) are less relevant as the focus is on the application of models rather than mathematical theory or computational complexity.,"Artificial intelligence:0.8,Computer graphics:0.9,Concurrent computing methodologies:0.1,Distributed computing methodologies:0.1,Machine learning:0.6,Modeling and simulation:0.5,Parallel computing methodologies:0.1,Symbolic and algebraic manipulation:0.1","Computer graphics,Artificial intelligence",Computer graphics is highly relevant for image segmentation techniques. Artificial intelligence is relevant for computer vision applications. Modeling and simulation is secondary for the multiresolution approach.,"Animation:0.1,Computer vision:0.9,Control methods:0.1,Distributed artificial intelligence:0.1,Graphics systems and interfaces:0.2,Image compression:0.1,Image manipulation:0.6,Knowledge representation and reasoning:0.1,Natural language processing:0.1,Philosophical/theoretical foundations of artificial intelligence:0.1,Planning and scheduling:0.1,Rendering:0.1,Search methodologies:0.3,Shape modeling:0.1","Computer vision,Image manipulation",Computer vision: Paper presents a new image segmentation approach using MRF and Bak-Sneppen models. Image manipulation: Multiresolution technique using wavelet transforms is applied for segmentation optimization.
948,Rescoring effectiveness of language models using different levels of knowledge and their integration,"In this paper, we compare the efficacy of a variety of language models (LMs) for rescoring word graphs and N-best lists generated by a large vocabulary continuous speech recognizer. These LMs differ based on the level of knowledge used (word, lexical features, syntax) and the type of integration of that knowledge (tight or loose). The trigram LM incorporates word level information; our Part-of-Speech (POS) LM uses word and lexical class information in a tightly coupled way; our new SuperARV LM tightly integrates word, a richer set of lexical features than POS, and syntactic dependency information; and the Parser LM integrates some limited word information, POS, and syntactic information. We also investigate LMs created using a linear interpolation of LM pairs. When comparing each LM on the task of rescoring word graphs or N-best lists for the Wall Street Journal (WSJ) 5k- and 20k- vocabulary test sets, the SuperARV LM always achieves the greatest reduction in word error rate (WER) and the greatest increase in sentence accuracy (SAC). On the 5k test sets, the SuperARV LM obtains more than a 10% relative reduction in WER compared to the trigram LM, and on the 20k test set more than 2%. Additionally, the SuperARV LM performs comparably to or better than the interpolated LMs. Hence, we conclude that the tight coupling of knowledge from all three levels is an effective method of constructing high quality LMs.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.9,Applied computing:0.3,Social and professional topics:0.1",Computing methodologies,"Computing methodologies is relevant as the paper compares language model architectures for speech recognition, a machine learning methodology. Applied computing is less relevant as the focus is on model evaluation rather than real-world application.","Artificial intelligence:0.7,Computer graphics:0.1,Concurrent computing methodologies:0.1,Distributed computing methodologies:0.1,Machine learning:0.9,Modeling and simulation:0.3,Parallel computing methodologies:0.1,Symbolic and algebraic manipulation:0.1","Machine learning,Artificial intelligence",Machine learning is central to the development and comparison of language models. Artificial intelligence is relevant as these models are part of NLP within AI. Modeling and simulation is secondary to the core ML/AI focus.,"Natural language processing:1.0,Machine learning algorithms:0.8,Machine learning approaches:0.6,Knowledge representation and reasoning:0.4,Others:0.2","Natural language processing,Machine learning algorithms",Natural language processing is highly relevant for language model rescoring. Machine learning algorithms are relevant for model design. Other categories like knowledge representation are less central.
3735,An efficient java implementation of a GA-based miner for relational association rules with numerical attributes,"QuantMiner, proposed by Salleb-Aouissi et al., is one of the well-known systems for mining quantitative association rules using a genetic algorithm (GA). We have applied the GA-based methods of QuantMiner to multi-relational data mining (MRDM), where mining rules involves multiple relations from a relational database, and our preliminary experiments showed that a straightforward application of QuantMiner to MRDM has an efficiency problem. In this paper, we propose RelQM-J, a Java implementation of QuantMiner applied to multi-relational databases (MRDBs), with an aim to handle the computation of the supports of rules in an efficient way, by using a hash-based data structure. Our experimental results show the effectiveness of the proposed method, compared with the conventional method used in QuantMiner.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.8,Applied computing:0.1,Social and professional topics:0.1",Computing methodologies,"Computing methodologies is highly relevant because the paper proposes a GA-based relational association rule miner with efficiency improvements, a data mining methodology. Software and its engineering is less relevant as the focus is on the algorithm, not software design.","Artificial intelligence:0.8,Computer graphics:0.1,Concurrent computing methodologies:0.4,Distributed computing methodologies:0.5,Machine learning:1.0,Modeling and simulation:0.6,Parallel computing methodologies:0.3,Symbolic and algebraic manipulation:0.2",Machine learning,Machine learning (1.0) is directly relevant for the GA-based association rule mining. Artificial intelligence (0.8) has secondary relevance as genetic algorithms are an AI technique. Other categories like Distributed computing methodologies (0.5) are not strongly supported by the paper's content.,"Cross-validation:0.1,Learning paradigms:0.2,Learning settings:0.1,Machine learning algorithms:0.9,Machine learning approaches:0.8","Machine learning algorithms,Machine learning approaches",Machine learning algorithms is relevant as the paper proposes a hash-based optimization for genetic algorithms in association rule mining. Machine learning approaches is relevant because the study applies GA-based methods to multi-relational data mining. Other categories like 'Learning paradigms' are not the primary focus.
5645,Euclidean Reconstruction of Deformable Structure Using a Perspective Camera with Varying Intrinsic Parameters,"In this paper we present a novel approach for the 3D Euclidean reconstruction of deformable objects observed by a perspective camera with variable intrinsic parameters. We formulate the non-rigid shape and motion estimation problem as a non-linear optimization where the objective function to be minimised is the image reprojection error. Our approach is based on the observation that often some of the points on the observed object behave rigidly, while others deform from frame to frame. We propose to use the set of rigid points to obtain an initial estimate of the camera's varying internal parameters and the overall rigid motion. The prior information that some of the points in the object are rigid can also be added to the non-linear minimization scheme in order to avoid ambiguous configurations. Results on synthetic and real data prove the performance of our algorithm even when using a minimal set of rigid points and when varying the intrinsic camera parameters","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.9,Applied computing:0.3,Social and professional topics:0.1",Computing methodologies,Computing methodologies: The paper presents a novel 3D reconstruction algorithm with variable camera parameters. Applied computing has limited relevance compared to the core algorithmic contribution.,"Artificial intelligence:0.1,Computer graphics:0.95,Concurrent computing methodologies:0.05,Distributed computing methodologies:0.05,Machine learning:0.1,Modeling and simulation:0.75,Parallel computing methodologies:0.05,Symbolic and algebraic manipulation:0.05","Computer graphics,Modeling and simulation",Computer graphics: 3D reconstruction of deformable structures is central to computer graphics. Modeling and simulation: The paper involves simulation of object deformation and camera parameters. Other categories like AI are not directly relevant.,"Animation:0.1,Graphics systems and interfaces:0.1,Image compression:0.1,Image manipulation:0.3,Model development and analysis:1,Rendering:0.2,Shape modeling:0.8,Simulation evaluation:0.6,Simulation support systems:0.1,Simulation theory:0.4,Simulation types and techniques:0.3","Model development and analysis,Shape modeling",Model development and analysis: The paper presents a novel non-linear optimization model for 3D reconstruction. Shape modeling: The focus is on deformable object modeling. Simulation evaluation is secondary as the paper validates the algorithm rather than simulation systems.
2578,Cross-domain sentiment classification using a two-stage method,"In this paper, we give out a two-stage approach for domain adaptation problem in sentiment classification. In the first stage, based on our observation that customers often use different words to comment on the similar topics in the different domains, we regard these common topics as the bridge to link the different domain-specific features. We propose a novel topic model named Transfer-PLSA to extract the topic knowledge between different domains. Through these common topics, the features in the source domain are corresponded to the target features, so that those domain-specific knowledge can be transferred across different domains. In the second step, we use the classifier trained on the labeled examples in the source domain to pick up some informative examples in the target domain. Then we retrain the classifier on these selected examples, so that the classifier is adapted for the target domain. Experimental results on sentiment classification in four different domains indicate that our method outperforms other traditional methods.","General and reference:0,Hardware:0,Computer systems organization:0,Networks:0,Software and its engineering:0,Theory of computation:0,Mathematics of computing:0,Information systems:0,Security and privacy:0,Human-centered computing:0,Computing methodologies:1,Applied computing:0,Social and professional topics:0",Computing methodologies,Computing methodologies is relevant for the novel topic model (Transfer-PLSA) and domain adaptation techniques. Other categories are unrelated to the core focus on machine learning algorithms for sentiment classification.,"Artificial intelligence:0.7,Computer graphics:0.0,Concurrent computing methodologies:0.0,Distributed computing methodologies:0.0,Machine learning:0.9,Modeling and simulation:0.8,Parallel computing methodologies:0.0,Symbolic and algebraic manipulation:0.0","Machine learning,Modeling and simulation,Artificial intelligence",Machine learning: The paper uses domain adaptation via neural networks. Modeling and simulation: Simulations validate the two-stage method. Artificial intelligence: The approach is rooted in AI techniques. Categories like 'Computer graphics' or 'Parallel computing' are irrelevant.,"Computer vision:0,Control methods:0,Cross-validation:0,Distributed artificial intelligence:0,Knowledge representation and reasoning:0,Learning paradigms:0,Learning settings:0,Machine learning algorithms:1,Machine learning approaches:0,Model development and analysis:0,Natural language processing:1,Philosophical/theoretical foundations of artificial intelligence:0,Planning and scheduling:0,Search methodologies:0,Simulation evaluation:0,Simulation support systems:0,Simulation theory:0,Simulation types and techniques:0","Machine learning algorithms,Natural language processing",Machine learning algorithms: The paper presents a two-stage machine learning approach for domain adaptation. Natural language processing: Sentiment classification is a core NLP task. Other categories like Computer vision are not relevant as the focus is on text-based sentiment analysis.
1623,Automatic Generation of Heuristics for Scheduling,"This paper presents a technique, called GENH, that automatically generates search heuristics for scheduling problems. The impetus for developing this technique is the growing consensus that heuristics encode advice that is, at best, useful in solving most, or typical, problem instances, and, at worst, useful in solving only a narrowly defined set of instances. In either case, heuristic problem solvers, to be broadly applicable, should have a means of automatically adjusting to the idiosyncrasies of each problem instance. GENH generates a search heuristic for a given problem instance by hillclimbing in the space of possible multiattribute heuristics, where the evaluation of a candidate heuristic is based on the quality of the solution found under its guidance. We present empirical results obtained by applying GENH to the real world problem of telescope observation scheduling. These results demonstrate that GENH is a simple and effective way of improving the performance of an heuristic scheduler.","General and reference:0,Hardware:0,Computer systems organization:0,Networks:0,Software and its engineering:0,Theory of computation:0,Mathematics of computing:0,Information systems:0,Security and privacy:0,Human-centered computing:0,Computing methodologies:1,Applied computing:0.8,Social and professional topics:0",Computing methodologies,Computing methodologies: The paper introduces a methodology for generating heuristics for scheduling. Applied computing: The paper applies the methodology to telescope observation scheduling.,"Artificial intelligence:0.8,Computer graphics:0.1,Concurrent computing methodologies:0.2,Distributed computing methodologies:0.2,Machine learning:0.9,Modeling and simulation:0.4,Parallel computing methodologies:0.2,Symbolic and algebraic manipulation:0.1","Machine learning,Artificial intelligence",Machine learning is highly relevant as the paper introduces a hill-climbing-based algorithm for heuristic generation. Artificial intelligence is relevant due to the application in scheduling problems. Other categories like Modeling and simulation are secondary to the core contribution.,"Computer vision:0.0,Control methods:0.0,Cross-validation:0.0,Distributed artificial intelligence:0.0,Knowledge representation and reasoning:0.0,Learning paradigms:0.0,Learning settings:0.0,Machine learning algorithms:0.0,Machine learning approaches:0.0,Natural language processing:0.0,Philosophical/theoretical foundations of artificial intelligence:0.0,Planning and scheduling:1.0,Search methodologies:0.75","Planning and scheduling,Search methodologies",Planning and scheduling (1.0): The core contribution is a scheduling heuristic generation system. Search methodologies (0.75): The hillclimbing approach for heuristic optimization is a search method. Other categories are irrelevant as the paper does not address machine learning or NLP.
5732,High precision rotation angle estimation for rotated images,"The detection of image rotation and the estimation of rotation angle are of great help to “copy-paste” image forgery detection because the copy-pasted slice is often rotated to gain better visual effect. This paper proposes a novel blind image rotation detection algorithm with high precision rotation angle estimation. The proposed algorithm is based on the periodicity within pixel variances, which is introduced by the rotation interpolation. Experiment results show that this algorithm can estimate rotation angle precisely with a fine rotation angle resolution of 0.2 degrees for images of 1024 pixels in width.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.3,Theory of computation:0.2,Mathematics of computing:0.5,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:1.0,Applied computing:0.2,Social and professional topics:0.1",Computing methodologies,Computing methodologies is highly relevant for the image rotation detection algorithm. Mathematics of computing is secondary to the methodological focus. Networks and Applied computing are not central to the core contribution.,"Artificial intelligence:0.1,Computer graphics:0.8,Concurrent computing methodologies:0.1,Distributed computing methodologies:0.1,Machine learning:0.2,Modeling and simulation:0.3,Parallel computing methodologies:0.1,Symbolic and algebraic manipulation:0.1",Computer graphics,"Computer graphics: The paper proposes an image rotation detection algorithm based on pixel variance analysis, which is a core image processing task in computer graphics. Other categories like 'Machine learning' are less relevant as the method is deterministic and does not involve learning.","Animation:0,Graphics systems and interfaces:0,Image compression:0,Image manipulation:1,Rendering:0,Shape modeling:0",Image manipulation,Image manipulation is relevant as the paper presents a technique for detecting rotated images through pixel variance analysis. Other categories like Shape modeling are unrelated to rotation detection algorithms.
5798,Tree Edit Distance as a Baseline Approach for Paraphrase Representation,"Finding an adequate paraphrase representation formalism is a challenging issue in Natural Language Processing. In this paper, we analyse the performance of Tree Edit Distance as a paraphrase representation baseline. Our experiments using Edit Distance Textual Entailment Suite show that, as Tree Edit Distance consists of a purely syntactic approach, paraphrase alternations not based on structural reorganizations do not find an adequate representation. They also show that there is much scope for better modelling of the way trees are aligned.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:1.0,Applied computing:0.0,Social and professional topics:0.0",Computing methodologies,Computing methodologies is relevant for the algorithmic analysis of tree edit distance as a paraphrase representation technique. Other categories like 'Human-centered computing' are not central to the methodological focus.,"Artificial intelligence:0.8,Computer graphics:0.0,Concurrent computing methodologies:0.0,Distributed computing methodologies:0.0,Machine learning:0.1,Modeling and simulation:0.6,Parallel computing methodologies:0.0,Symbolic and algebraic manipulation:0.0","Artificial intelligence,Modeling and simulation",Artificial intelligence is highly relevant as the paper addresses natural language processing (paraphrase detection). Modeling and simulation is relevant for evaluating the tree edit distance approach. Other categories like Machine learning are not directly connected to the syntactic analysis focus.,"Computer vision:0,Control methods:0,Distributed artificial intelligence:0,Knowledge representation and reasoning:0.5,Model development and analysis:0,Natural language processing:1,Philosophical/theoretical foundations of artificial intelligence:0,Planning and scheduling:0,Search methodologies:0,Simulation evaluation:0,Simulation support systems:0,Simulation theory:0,Simulation types and techniques:0","Natural language processing,Knowledge representation and reasoning",Natural language processing is directly addressed as the paper evaluates a syntactic approach for paraphrase representation. Knowledge representation and reasoning is included as the study involves modeling structural relationships between paraphrases.
1028,A Coherent Grid Traversal Approach to Visualizing Particle-Based Simulation Data,We present an approach to visualizing particle-based simulation data using interactive ray tracing and describe an algorithmic enhancement that exploits the properties of these data sets to provide highly interactive performance and reduced storage requirements. This algorithm for fast packet-based ray tracing of multilevel grids enables the interactive visualization of large time-varying data sets with millions of particles and incorporates advanced features like soft shadows. We compare the performance of our approach with two recent particle visualization systems: one based on an optimized single ray grid traversal algorithm and the other on programmable graphics hardware. This comparison demonstrates that the new algorithm offers an attractive alternative for interactive particle visualization.,"General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.9,Applied computing:0.1,Social and professional topics:0.1",Computing methodologies,Computing methodologies is highly relevant for the algorithmic innovations in visualization. Other categories like Software and its engineering or Information systems are not as central to the core contribution of the methodological approach.,"Artificial intelligence:0.1,Computer graphics:0.8,Concurrent computing methodologies:0.1,Distributed computing methodologies:0.1,Machine learning:0.1,Modeling and simulation:0.1,Parallel computing methodologies:0.1,Symbolic and algebraic manipulation:0.1",Computer graphics,Computer graphics is highly relevant as the paper introduces a visualization technique for particle data. Other categories are not central to the core contribution of the paper.,"Animation:0.1,Graphics systems and interfaces:0.8,Image compression:0.1,Image manipulation:0.1,Rendering:0.9,Shape modeling:0.1","Graphics systems and interfaces,Rendering",Graphics systems and interfaces is relevant due to the focus on interactive visualization systems. Rendering is relevant as the paper describes ray tracing algorithms for particle data. Other categories like Image compression are not central to the contribution.
5630,Real-time salient object detection,"Salient object detection techniques have a variety of multimedia applications of broad interest. However, the detection must be fast to truly aid in these processes. There exist many robust algorithms tackling the salient object detection problem but most of them are computationally demanding. In this demonstration we show a fast salient object detection system implemented in a conventional PC environment. We examine the challenges faced in the design and development of a practical system that can achieve accurate detection in real-time.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.2,Networks:0.2,Software and its engineering:0.3,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.2,Computing methodologies:1.0,Applied computing:0.4,Social and professional topics:0.1",Computing methodologies,Computing methodologies is highly relevant as the paper presents real-time salient object detection algorithms. Applied computing is secondary as the focus is on methodological improvements rather than specific applications.,"Artificial intelligence:0.2,Computer graphics:0.8,Concurrent computing methodologies:0.1,Distributed computing methodologies:0.1,Machine learning:0.3,Modeling and simulation:0.2,Parallel computing methodologies:0.4,Symbolic and algebraic manipulation:0.1","Computer graphics,Parallel computing methodologies",Computer graphics is relevant for real-time object detection in multimedia. Parallel computing methodologies is relevant for achieving real-time performance. Other categories are rejected as the paper focuses on visualization algorithms rather than ML or symbolic methods.,"Animation:0.0,Graphics systems and interfaces:0.5,Image compression:0.0,Image manipulation:1.0,Parallel algorithms:1.0,Parallel programming languages:0.0,Rendering:0.0,Shape modeling:0.0","Image manipulation,Parallel algorithms",Image manipulation is directly relevant as the paper focuses on detecting salient objects in images. Parallel algorithms are relevant due to the emphasis on real-time processing requiring computational efficiency. Other categories like Graphics systems are only peripherally related to the core algorithmic contributions.
4945,Optimal Mass Transport for Shape Matching and Comparison,"Surface based 3D shape analysis plays a fundamental role in computer vision and medical imaging. This work proposes to use optimal mass transport map for shape matching and comparison, focusing on two important applications including surface registration and shape space. The computation of the optimal mass transport map is based on Monge-Brenier theory, in comparison to the conventional method based on Monge-Kantorovich theory, this method significantly improves the efficiency by reducing computational complexity from O(n2) to O(n). For surface registration problem, one commonly used approach is to use conformal map to convert the shapes into some canonical space. Although conformal mappings have small angle distortions, they may introduce large area distortions which are likely to cause numerical instability thus resulting failures of shape analysis. This work proposes to compose the conformal map with the optimal mass transport map to get the unique area-preserving map, which is intrinsic to the Riemannian metric, unique, and diffeomorphic. For shape space study, this work introduces a novel Riemannian framework, Conformal Wasserstein Shape Space, by combing conformal geometry and optimal mass transport theory. In our work, all metric surfaces with the disk topology are mapped to the unit planar disk by a conformal mapping, which pushes the area element on the surface to a probability measure on the disk. The optimal mass transport provides a map from the shape space of all topological disks with metrics to the Wasserstein space of the disk and the pullback Wasserstein metric equips the shape space with a Riemannian metric. We validate our work by numerous experiments and comparisons with prior approaches and the experimental results demonstrate the efficiency and efficacy of our proposed approach.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:1.0,Applied computing:0.5,Social and professional topics:0.1",Computing methodologies,Computing methodologies is directly relevant to the optimal mass transport algorithm development. Applied computing is secondary as the focus is on the method rather than specific medical applications.,"Artificial intelligence:0.5,Computer graphics:1.0,Concurrent computing methodologies:0.1,Distributed computing methodologies:0.1,Machine learning:0.3,Modeling and simulation:0.8,Parallel computing methodologies:0.1,Symbolic and algebraic manipulation:0.1","Computer graphics,Modeling and simulation","Computer graphics: The paper focuses on 3D shape analysis using optimal transport for computer vision. Modeling and simulation: The method involves shape registration and Riemannian frameworks. Other categories (e.g., Artificial intelligence) are secondary to the core computational geometry focus.","Animation:0.1,Graphics systems and interfaces:0.3,Image compression:0.1,Image manipulation:0.2,Model development and analysis:0.4,Rendering:0.1,Shape modeling:1.0,Simulation evaluation:0.1,Simulation support systems:0.1,Simulation theory:0.1,Simulation types and techniques:0.1",Shape modeling,Shape modeling is the primary domain for 3D shape analysis and optimal transport maps. Other options like graphics systems or rendering are secondary but less central than the core shape space framework.
4183,Fast Sweeping Methods for Eikonal Equations on Triangular Meshes,"The original fast sweeping method, which is an efficient iterative method for stationary Hamilton-Jacobi equations, relies on natural ordering provided by a rectangular mesh. We propose novel ordering strategies so that the fast sweeping method can be extended efficiently and easily to any unstructured mesh. To that end we introduce multiple reference points and order all the nodes according to their $l^p$-metrics to those reference points. We show that these orderings satisfy the two most important properties underlying the fast sweeping method: (1) these orderings can cover all directions of information propagating efficiently; (2) any characteristic can be decomposed into a finite number of pieces and each piece can be covered by one of the orderings. We prove the convergence of the new algorithm. The computational complexity of the algorithm is nearly optimal in the sense that the total computational cost consists of $O(M)$ flops for iteration steps and $O(M{\rm log}M)$ flops for sorting at the predetermined initialization step which can be efficiently optimized by adopting a linear time sorting method, where $M$ is the total number of mesh points. Extensive numerical examples demonstrate that the new algorithm converges in a finite number of iterations independent of mesh size.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:1.0,Applied computing:0.1,Social and professional topics:0.1",Computing methodologies,Computing methodologies is highly relevant as the paper introduces a numerical method for solving Eikonal equations. Other categories like Mathematics or Networks are less central.,"Modeling and simulation:1.0,Symbolic and algebraic manipulation:0.2,Artificial intelligence:0.1,Computer graphics:0.1,Concurrent computing methodologies:0.05,Distributed computing methodologies:0.05,Machine learning:0.05,Parallel computing methodologies:0.05",Modeling and simulation,"Modeling and simulation is directly relevant for numerical methods solving PDEs on triangular meshes. Symbolic manipulation is rejected as the focus is on numerical algorithms, not algebraic computation.","Model development and analysis:0.3,Simulation evaluation:0.7,Simulation support systems:0.2,Simulation theory:0.6,Simulation types and techniques:1.0","Simulation types and techniques,Simulation theory",Simulation types and techniques: The novel ordering strategies for triangular meshes are a key contribution. Simulation theory: The convergence proofs and complexity analysis are central. Other categories like Model development are secondary.
5581,Generating structure of latent variable models for nested data,"Probabilistic latent variable models have been successfully used to capture intrinsic characteristics of various data. However, it is nontrivial to design appropriate models for given data because it requires both machine learning and domain-specific knowledge. In this paper, we focus on data with nested structure and propose a method to automatically generate a latent variable model for the given nested data, with the proposed method, the model structure is adjustable by its structural parameters. Our model can represent a wide class of hierarchical and sequential latent variable models including mixture models, latent Dirichlet allocation, hidden Markov models and their combinations in multiple layers of the hierarchy. Even when deeply-nested data are given, where designing a proper model is difficult even for experts, our method generate an appropriate model by extracting the essential information. We present an efficient variational inference method for our model based on dynamic programming on the given data structure. We experimentally show that our method generates correct models from artificial datasets and demonstrate that models generated by our method can extract hidden structures of blog and news article datasets.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.5,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.9,Applied computing:0.0,Social and professional topics:0.0",Computing methodologies,Computing methodologies is highly relevant as the paper proposes a novel method for generating latent variable models in machine learning. Mathematics of computing (0.5) is secondary as the paper applies mathematical techniques to the model generation problem.,"Artificial intelligence:0.0,Computer graphics:0.0,Concurrent computing methodologies:0.0,Distributed computing methodologies:0.0,Machine learning:1.0,Modeling and simulation:0.8,Parallel computing methodologies:0.0,Symbolic and algebraic manipulation:0.0",Machine learning,"Machine learning: The paper introduces a method for generating latent variable models for nested data, a core ML contribution. Modeling and simulation is secondary, as the focus is on model generation rather than simulation techniques.","Machine learning algorithms:1.0,Machine learning approaches:0.9,Learning paradigms:0.8,Learning settings:0.4,Cross-validation:0.2","Machine learning algorithms,Machine learning approaches,Learning paradigms",Machine learning algorithms: Proposes algorithm for model generation. Approaches: Methodology for hierarchical models. Paradigms: Hierarchical and sequential paradigms.
4516,Precomputed Atmospheric Scattering,"We present a new and accurate method to render the atmosphere in real time from any viewpoint from ground level to outer space, while taking Rayleigh and Mie multiple scattering into account. Our method reproduces many effects of the scattering of light, such as the daylight and twilight sky color and aerial perspective for all view and light directions, or the Earth and mountain shadows (light shafts) inside the atmosphere. Our method is based on a formulation of the light transport equation that is precomputable for all view points, view directions and sun directions. We show how to store this data compactly and propose a GPU compliant algorithm to precompute it in a few seconds. This precomputed data allows us to evaluate at runtime the light transport equation in constant time, without any sampling, while taking into account the ground for shadows and light shafts.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:1.0,Applied computing:0.0,Social and professional topics:0.0",Computing methodologies,"Computing methodologies: The paper presents a real-time rendering method for atmospheric effects using precomputed data, which is a core topic in graphics and simulation methodologies.","Artificial intelligence:0.1,Computer graphics:0.9,Concurrent computing methodologies:0.1,Distributed computing methodologies:0.1,Machine learning:0.1,Modeling and simulation:0.7,Parallel computing methodologies:0.1,Symbolic and algebraic manipulation:0.1","Computer graphics,Modeling and simulation",Computer graphics is directly relevant as the paper presents a real-time atmospheric rendering method. Modeling and simulation receives moderate relevance due to the precomputed light transport model. Other categories are irrelevant as the focus is on real-time rendering techniques rather than AI or parallel computing.,"Animation:0.0,Graphics systems and interfaces:0.0,Image compression:0.0,Image manipulation:0.0,Model development and analysis:0.2,Rendering:1.0,Shape modeling:0.0,Simulation evaluation:0.0,Simulation support systems:0.0,Simulation theory:0.0,Simulation types and techniques:0.0",Rendering,Rendering: The paper presents a novel real-time rendering technique for atmospheric scattering. Other categories are less relevant as the focus is on rendering algorithms rather than simulation or animation.
1349,Optimal Dynamic Graphs for Video Content Analysis,"This study addresses the problem of learning the optimal structure of a dynamic graphical model for video content analysis given sparse data. We propose a Completed Likelihood AIC (CL-AIC) scoring function that differs from existing ones by optimising explicitly both the explanation and prediction capabilities of a model simultaneously. We demonstrate that CL-AIC is superior to existing scoring functions including BIC, AIC and ICL in building dynamic graph models for video content analysis.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.75,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:1.0,Applied computing:0.0,Social and professional topics:0.0",Computing methodologies,Computing methodologies is highly relevant as the paper introduces a novel dynamic graphical model scoring function (CL-AIC) for video content analysis. Other categories like Networks or Software and its engineering are not directly related to the core contribution of model optimization techniques.,"Artificial intelligence:0.0,Computer graphics:0.0,Concurrent computing methodologies:0.0,Distributed computing methodologies:0.0,Machine learning:1.0,Modeling and simulation:0.0,Parallel computing methodologies:0.0,Symbolic and algebraic manipulation:0.0",Machine learning,Machine learning is highly relevant as the core contribution is a novel scoring function for dynamic graphs in video analysis. Other categories like Computer graphics are not central to the methodological contribution.,"Cross-validation:0,Learning paradigms:0,Learning settings:0,Machine learning algorithms:1,Machine learning approaches:0.5",Machine learning algorithms,"Machine learning algorithms: The paper proposes a CL-AIC scoring function for dynamic graphs, a novel ML algorithm. Machine learning approaches is partially relevant but not as directly as the algorithm itself."
973,Break index labeling of mandarin text via syntactic-to-prosodic tree mapping,"In this study, we investigate the break index labeling problem with a syntactic-to-prosodic structure conversion. The statistical relationship between the mapped syntactic tree structure and prosodic tree structure of sentences in the training set is used to generate a Synchronous Tree Substitution Grammar (STSG) which can describe the probabilistic mapping (substitution) rules between them. For a given test sentence and the corresponding parsed syntactic tree structure, thus generated STSG can convert the syntactic tree to a prosodic tree statistically. We compare the labeling results with other approaches and show the probabilistic mapping can indeed benefit break index labeling performance.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.2,Theory of computation:0.2,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.9,Applied computing:0.2,Social and professional topics:0.1",Computing methodologies,Computing methodologies is highly relevant for the syntactic-to-prosodic tree mapping algorithm. Other categories like Human-centered computing are less directly related to the core computational method.,"Artificial intelligence:1.0,Computer graphics:0.0,Concurrent computing methodologies:0.0,Distributed computing methodologies:0.0,Machine learning:0.75,Modeling and simulation:0.0,Parallel computing methodologies:0.0,Symbolic and algebraic manipulation:0.0","Artificial intelligence,Machine learning","Artificial intelligence is relevant as the paper applies syntactic-to-prosodic structure conversion, a core NLP task. Machine learning is relevant for the probabilistic mapping in STSG. Other options are unrelated to the linguistic processing focus.","Computer vision:0,Control methods:0,Cross-validation:0,Distributed artificial intelligence:0,Knowledge representation and reasoning:0,Learning paradigms:0,Learning settings:0,Machine learning algorithms:1,Machine learning approaches:1,Natural language processing:1,Philosophical/theoretical foundations of artificial intelligence:0,Planning and scheduling:0,Search methodologies:0","Natural language processing,Machine learning approaches",Natural language processing is directly relevant for prosodic structure analysis. Machine learning approaches is relevant for probabilistic mapping in STSG.
4693,Bilingual FrameNet Dictionaries for Machine Translation,"This paper describes issues surrounding the planning and design of GermanFrameNet (GFN), a counterpart to the English-based FrameNet project. The goals of GFN are (a) to create lexical entries for German nouns, verbs, and adjectives that correspond to existing FrameNet entries, and (b) to link the parallel lexicon fragments by means of common semantic frames and numerical indexing mechanisms. GFN will take a fine-grained approach towards polysemy that seeks to split word senses based on the semantic frames that underlie their analysis. The parallel lexicon fragments represent an important step towards capturing valuable information about the different syntactic realizations of frame semantic concepts across languages, which is relevant for information retrieval, machine translation, and language generation.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.9,Applied computing:0.1,Social and professional topics:0.1",Computing methodologies,"Computing methodologies: The paper develops a FrameNet-based approach for machine translation, focusing on semantic modeling and topic modeling. Other categories like Applied Computing are irrelevant as the focus is on methodology rather than specific application domains.","Artificial intelligence:0.9,Computer graphics:0.1,Concurrent computing methodologies:0.1,Distributed computing methodologies:0.1,Machine learning:0.7,Modeling and simulation:0.2,Parallel computing methodologies:0.1,Symbolic and algebraic manipulation:0.1","Artificial intelligence,Machine learning",Artificial intelligence is highly relevant as FrameNet dictionaries are fundamental to natural language processing (NLP) in AI. Machine learning is relevant for the statistical alignment of semantic frames. Other fields like Computer graphics or Parallel computing are unrelated to lexical resource construction.,"Computer vision:1.0,Control methods:0.0,Cross-validation:0.2,Distributed artificial intelligence:0.0,Knowledge representation and reasoning:0.8,Learning paradigms:0.3,Learning settings:0.0,Machine learning algorithms:0.3,Machine learning approaches:0.6,Natural language processing:0.0,Philosophical/theoretical foundations of artificial intelligence:0.0,Planning and scheduling:0.0,Search methodologies:0.0","Computer vision,Knowledge representation and reasoning",Computer vision is relevant as the paper discusses FrameNet for semantic representation. Knowledge representation and reasoning is relevant due to the focus on semantic frames and cross-lingual alignment. Other categories like Machine learning approaches are less central.
82,Object-oriented Markov random model for classification of high resolution satellite imagery based on wavelet transform,"The high resolution satellite imagery (HRSI) have higher spatial resolution and less spectrum number, so there are some “object with different spectra, different objects with same spectrum” phenomena. The objective of this paper is to utilize the extracted features of high resolution satellite imagery (HRSI) obtained by the wavelet transform(WT) for segmentation. WT provides the spatial and spectral characteristics of a pixel along with its neighbors. The object-oriented Markov random Model in the wavelet domain is proposed in order to segment high resolution satellite imagery (HRSI). The proposed method is made up of three blocks: (1) WT-based feature extrcation.the aim of extraction of feature using WT for original spectral bands is to exploit the spatial and frequency information of the pixels; (2) over-segmentation object generation. Mean-Shift algorithm is employed to obtain over-segmentation objects; (3) classification based on Object-oriented Markov Random Model. Firstly the object adjacent graph (OAG) can be constructed on the over-segmentation objects. Secondly MRF model is easily defined on the OAG, in which WT-based feature of pixels are modeled in the feature field model and the neighbor system, potential cliques and energy functions of OAG are exploited in the labeling model. Experiments are conducted on one HRSI dataset-QuickBird images. We evaluate and compare the proposed approach with the well-known commercial software eCognition(object-based analysis approach) and Maximum Likelihood(ML) based pixels. Experimental results show that the proposed the method in this paper obviously outperforms the other methods.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.2,Networks:0.1,Software and its engineering:0.3,Theory of computation:0.1,Mathematics of computing:0.2,Information systems:0.3,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.9,Applied computing:0.4,Social and professional topics:0.1",Computing methodologies,Computing methodologies is relevant for wavelet-based image classification algorithms. Other categories are less relevant as the focus is not on hardware or security.,"Artificial intelligence:0.6,Computer graphics:0.8,Concurrent computing methodologies:0.1,Distributed computing methodologies:0.1,Machine learning:0.9,Modeling and simulation:0.4,Parallel computing methodologies:0.1,Symbolic and algebraic manipulation:0.1","Machine learning,Computer graphics",Machine learning is highly relevant due to the Markov random field classification approach. Computer graphics is relevant for the satellite imagery processing. Other categories like Artificial intelligence receive lower scores as the focus is on specific ML techniques rather than general AI.,"Animation:0.1,Cross-validation:0.2,Graphics systems and interfaces:0.3,Image compression:0.1,Image manipulation:0.8,Learning paradigms:0.4,Learning settings:0.3,Machine learning algorithms:0.7,Machine learning approaches:0.6,Rendering:0.2,Shape modeling:0.8","Image manipulation,Shape modeling",Image manipulation is central to the wavelet-based classification method. Shape modeling is directly addressed via the Markov random model. Machine learning algorithms are secondary due to the model's probabilistic nature.
2752,"Irrelevant Features, Class Separability, and Complexity of Classification Problems","In this paper, analysis of class separability measures is performed in attempt to relate their descriptive abilities to geometrical properties of classification problems in presence of irrelevant features. The study is performed on synthetic and benchmark data with known irrelevant features and other characteristics of interest, such as class boundaries, shapes, margins between classes, and density. The results have shown that some measures are individually informative, while others are less reliable and only can provide complimentary information. Classification problem complexity measurements on selected data sets are made to gain additional insights on the obtained results.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.9,Applied computing:0.0,Social and professional topics:0.0",Computing methodologies,Computing methodologies: The paper analyzes machine learning techniques for classification problems with irrelevant features. Other categories are rejected because the paper focuses on algorithmic analysis rather than implementation or application domains.,"Machine learning:1.0,Artificial intelligence:0.4,Other categories:0.0",Machine learning,Machine learning is directly relevant as the paper focuses on class separability measures and classification problem complexity. Artificial intelligence is less central to the core contribution.,"Cross-validation:0.0,Learning paradigms:0.0,Learning settings:0.0,Machine learning algorithms:0.5,Machine learning approaches:1.0",Machine learning approaches,Machine learning approaches is relevant for the analysis of class separability measures. Other categories are not central to the paper's focus.
741,A Segmentation Method for Bone Marrow Cavity Imaging Using Graph Cuts,"The improvement of bioimaging technologies enables the observation of cellular dynamics invivo. Some new bioimaging technologies are expected to contribute to the discovery of new drugs and mechanisms of disease. To improve the contributions of bioimaging, it is required to extract a particular region or to detect a particular cell's motion within bioimages. Moreover, automatic extraction and detection with image processing is also required because the accurate and uniformed processing of a massive number of images manually is unrealistic. To help automate this process, we introduce a bone marrow cavity segmentation method for two-photon excitation microscopy images. Specialists of cellular dynamics define regions of bone marrow cavity by considering several criteria, including characteristics of intensity and blood flow. We take those criteria into our method as the energy function of graph cuts. Results of evaluations and comparison with normal graph cuts show that our proposed method that does not use hard constraints achieved a performance better than normal graph cuts with hard constraints.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.8,Applied computing:0.2,Social and professional topics:0.1",Computing methodologies,Computing methodologies is relevant for graph cut-based image segmentation algorithms. Other categories like Applied computing are less relevant as the focus is on the method itself rather than biomedical application specifics.,"Artificial intelligence:0.75,Computer graphics:1.0,Concurrent computing methodologies:0.0,Distributed computing methodologies:0.0,Machine learning:0.75,Modeling and simulation:0.0,Parallel computing methodologies:0.0,Symbolic and algebraic manipulation:0.0","Computer graphics,Artificial intelligence,Machine learning",Computer graphics is relevant for image segmentation techniques. Artificial intelligence and Machine learning are relevant for the graph cuts optimization method. Other categories like Modeling and simulation are not central to the core contribution of segmentation.,"Animation:0,Computer vision:1,Control methods:0,Cross-validation:0,Distributed artificial intelligence:0,Graphics systems and interfaces:0,Image compression:0,Image manipulation:1,Knowledge representation and reasoning:0,Learning paradigms:0,Learning settings:0,Machine learning algorithms:0,Machine learning approaches:0,Natural language processing:0,Philosophical/theoretical foundations of artificial intelligence:0,Planning and scheduling:0,Rendering:0,Search methodologies:0,Shape modeling:0","Computer vision,Image manipulation",Computer vision is relevant for bone marrow cavity segmentation. Image manipulation applies to the graph cuts implementation. Other AI and graphics categories are not central to the method.
5405,Consistency Modeling,"SRI's consistency modeling project began in August 1992. The goal of the project is to develop consistency modeling technology. That is. we aim to reduce the number of improper independence assumptions used in traditional speech recognition algorithms so that the resulting speech recognition hypotheses are more self-consistent and, therefore, more accurate. Consistency is achieved by conditioning HMM output distributions on state and observations histories, P(x/s,H). The goal of the project is finding the proper form of the probability distribution P, the proper history vector. H, and the proper feature vector, x. and developing the infrastructure (e.g. efficient estimation and search techniques) so that consistency modeling can be effectively used.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.3,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.9,Applied computing:0.3,Social and professional topics:0.1",Computing methodologies,Computing methodologies is highly relevant as the paper presents a new approach to consistency modeling in speech recognition. Other categories like Applied computing receive lower scores because the focus is on methodology development rather than specific applications.,"Artificial intelligence:0.1,Computer graphics:0.1,Concurrent computing methodologies:0.1,Distributed computing methodologies:0.1,Machine learning:1.0,Modeling and simulation:0.5,Parallel computing methodologies:0.1,Symbolic and algebraic manipulation:0.1",Machine learning,Machine learning is highly relevant because the paper develops a consistency modeling technique for speech recognition. Modeling and simulation is moderately relevant due to the algorithmic evaluation.,"Cross-validation:0.3,Learning paradigms:0.5,Learning settings:0.4,Machine learning algorithms:1,Machine learning approaches:1","Machine learning algorithms,Machine learning approaches",Both categories directly address the consistency modeling method's algorithmic and approach innovations in speech recognition.
393,Computing the Hough Transform on a Scan Line Array Processor (Image Processing),"A parallel algorithm for a line-finding Hough transform that runs on a linearly connected, SIMD (single-instruction, multiple-data-stream) vector of processors is described. The authors show that a high-precision transform, usually considered to be an expensive global operation, can be performed efficiently, in two to three times real time, with only local, communication on a long vector. The algorithm also illustrates a decomposition principle that has wide application in algorithm design for large linear arrays. A review of straight-line Hough transform implementations is also presented. >","General and reference:0.1,Hardware:0.2,Computer systems organization:0.3,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.9,Applied computing:0.1,Social and professional topics:0.1",Computing methodologies,Computing methodologies is highly relevant as the paper presents a parallel algorithm for image processing. Hardware is secondary to the algorithmic focus.,"Artificial intelligence:0.25,Computer graphics:0.75,Concurrent computing methodologies:0.25,Distributed computing methodologies:0.25,Machine learning:0.25,Modeling and simulation:0.25,Parallel computing methodologies:1.0,Symbolic and algebraic manipulation:0.25","Parallel computing methodologies,Computer graphics",Parallel computing methodologies is directly relevant as the paper presents a parallel algorithm design. Computer graphics is relevant through the Hough Transform application. Other fields are not central to the contribution.,"Animation:0.1,Graphics systems and interfaces:0.1,Image compression:0.1,Image manipulation:0.3,Parallel algorithms:1.0,Parallel programming languages:0.2,Rendering:0.1,Shape modeling:0.1",Parallel algorithms,"Parallel algorithms are central to the Hough transform implementation on SIMD processors. Image manipulation has minor relevance as the application is image processing, but the core focus is algorithm design."
3484,Automatic Question Categorization: a New Approach for Text Elaboration,"Text adaptation is a normal activity of teachers to facilitate reading comprehension of specific contents; the general approaches for it are Text Simplification and Text Elaboration (TE). TE aims at clarifying, explaining information and making connections explicit in texts. In this paper, we present a new approach for TE: an automatic question categorization system which assigns wh-question labels to verbal arguments in a sentence. For example, in Mary danced yesterday. Who? is the label linking the verb danced to the argument Mary and When? links danced to the argument yesterday. This annotation is similar to semantic ro-le labeling, approached successfully via statistical language processing techniques. Specifically, we present experiments to build the system using a fine-grained question set in Portuguese lan-guage and address two key research questions: (1) Which machine-learning algorithm presents the best results? (2) Which problems this task presents and how to overcome them? Keywords: Text Elaboration, Semantic Role Labeling, Wh-question labels.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.9,Applied computing:0.1,Social and professional topics:0.1",Computing methodologies,Computing methodologies is highly relevant as the paper presents a novel NLP-based approach for text elaboration using machine learning and semantic role labeling. Other categories like Information systems are less relevant as the focus is on computational methods rather than data management.,"Artificial intelligence:0.2,Computer graphics:0.0,Concurrent computing methodologies:0.0,Distributed computing methodologies:0.0,Machine learning:1.0,Modeling and simulation:0.2,Parallel computing methodologies:0.0,Symbolic and algebraic manipulation:0.0",Machine learning,"Machine learning: The paper proposes a machine learning-based system for question categorization. Modeling and simulation is less relevant as the focus is on algorithm development, not simulation.","Cross-validation:0.0,Learning paradigms:0.0,Learning settings:0.0,Machine learning algorithms:1.0,Machine learning approaches:1.0","Machine learning algorithms,Machine learning approaches",Machine learning algorithms: The paper evaluates different algorithms for wh-question categorization. Machine learning approaches: The task is framed as a supervised learning problem. No other categories are relevant as the focus is on algorithmic design and ML techniques.
5667,Human Designed Vs. Genetically Programmed Differential Evolution Operators,"The hybridization and combination of different Evolutionary Algorithms to improve the quality of the solutions and to accelerate execution is a common research practice. In this paper, we utilize Genetic Programming to evolve novel Differential Evolution operators. The genetic evolution resulted in parameter free Differential Evolution operators. Our experimental results indicate that the performance of the genetically programmed operators is comparable and in some cases is considerably better than the already existing human designed ones.","General and reference:0.1,Hardware:0.2,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:1.0,Applied computing:0.2,Social and professional topics:0.2",Computing methodologies,Computing methodologies is relevant for the use of Genetic Programming to evolve Differential Evolution operators. Other categories like Hardware or Software are less relevant as the focus is on algorithmic methodology.,"Artificial intelligence:0.8,Computer graphics:0.1,Concurrent computing methodologies:0.1,Distributed computing methodologies:0.1,Machine learning:0.7,Modeling and simulation:0.1,Parallel computing methodologies:0.1,Symbolic and algebraic manipulation:0.1","Artificial intelligence,Machine learning","Artificial intelligence: Genetic Programming (GP) is a subfield of AI used to evolve DE operators. Machine learning (0.7): The evolved operators are parameter-free and compared to human-designed ones, aligning with algorithmic innovation in ML. Other categories like Computer graphics are irrelevant.","Computer vision:0.0,Control methods:0.0,Cross-validation:0.0,Distributed artificial intelligence:0.0,Knowledge representation and reasoning:0.0,Learning paradigms:1.0,Learning settings:0.0,Machine learning algorithms:1.0,Machine learning approaches:0.0,Natural language processing:0.0,Philosophical/theoretical foundations of artificial intelligence:0.0,Planning and scheduling:0.0,Search methodologies:1.0","Learning paradigms,Machine learning algorithms,Search methodologies",Learning paradigms is relevant for the genetic programming approach to evolve operators. Machine learning algorithms is relevant for differential evolution operators. Search methodologies is relevant for the evolutionary algorithm used. Other categories like Computer vision are irrelevant.
2480,Induction of Fine-Grained Part-of-Speech Taggers via Classifier Combination and Crosslingual Projection,"This paper presents an original approach to part-of-speech tagging of fine-grained features (such as case, aspect, and adjective person/number) in languages such as English where these properties are generally not morphologically marked. 
 
The goals of such rich lexical tagging in English are to provide additional features for word alignment models in bilingual corpora (for statistical machine translation), and to provide an information source for part-of-speech tagger induction in new languages via tag projection across bilingual corpora. 
 
First, we present a classifier-combination approach to tagging English bitext with very fine-grained part-of-speech tags necessary for annotating morphologically richer languages such as Czech and French, combining the extracted features of three major English parsers, and achieve fine-grained-tag-level syntactic analysis accuracy higher than any individual parser. 
 
Second, we present experimental results for the cross-language projection of part-of-speech taggers in Czech and French via word-aligned bitext, achieving successful fine-grained part-of-speech tagging of these languages without any Czech or French training data of any kind.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.9,Applied computing:0.3,Social and professional topics:0.1",Computing methodologies,"Computing methodologies is highly relevant as the paper presents novel algorithms for part-of-speech tagging using classifier combination and crosslingual projection. Applied computing is moderately relevant for NLP applications, but the core contribution is the methodology.","Artificial intelligence:0.8,Machine learning:0.9,Modeling and simulation:0.6,Concurrent computing methodologies:0.1,Distributed computing methodologies:0.1,Parallel computing methodologies:0.1,Symbolic and algebraic manipulation:0.1,Computer graphics:0.1","Machine learning,Artificial intelligence",Machine learning is highly relevant as the paper presents a classifier-combination approach for POS tagging. Artificial intelligence is relevant as part-of-speech tagging is a fundamental NLP task. Modeling and simulation receives moderate relevance due to the analytical evaluation of tagging accuracy. All other categories are irrelevant as they pertain to unrelated domains like graphics or parallel computing.,"Computer vision:0.1,Control methods:0.1,Cross-validation:0.4,Distributed artificial intelligence:0.1,Knowledge representation and reasoning:0.2,Learning paradigms:0.6,Learning settings:0.5,Machine learning algorithms:0.7,Machine learning approaches:1.0,Natural language processing:1.0,Philosophical/theoretical foundations of artificial intelligence:0.1,Planning and scheduling:0.1,Search methodologies:0.2","Natural language processing,Machine learning approaches",Natural language processing (part-of-speech tagging) and Machine learning approaches (classifier combination) are primary. Cross-validation is less relevant as the paper focuses on induction methods rather than evaluation.
5964,Improving of Initial Clusters Fitness in Genetic Guided-Clustering Ensembles,"The clustering ensemble is a new topic in machine learning. It can combine multiple partitions generated by different clustering algorithms into a single clustering solution. Genetic algorithms have been known as methods with high ability to find the solution of optimization problems like the clustering ensemble problem. So far, many contributions have been done to find consensus cluster partition by genetic algorithms; however there has been little discussion about the methods of carrying out the initialization population and generation of initial cluster partitions in the first phase of clustering ensembles. In this paper, we proposed a new algorithm that used by clustering ensembles which improve cluster partitions fitness. In addition, diversity clustering problem has been solved by used the proposed algorithm. We compared the fitness average among individuals generated by the proposed algorithm and other clustering algorithms which have been calculated by three different fitness functions. The obtained experimental results on several benchmark datasets have demonstrated the proposed algorithm improve cluster solutions fitness.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:1.0,Applied computing:0.1,Social and professional topics:0.1",Computing methodologies,Computing methodologies is directly relevant as the paper introduces a genetic algorithm-based clustering ensemble method. Other categories are irrelevant as the paper focuses on machine learning algorithms.,"Artificial intelligence:0.8,Computer graphics:0.1,Concurrent computing methodologies:0.1,Distributed computing methodologies:0.1,Machine learning:1.0,Modeling and simulation:0.1,Parallel computing methodologies:0.1,Symbolic and algebraic manipulation:0.1","Machine learning,Artificial intelligence",Machine learning is highly relevant as the paper proposes a genetic algorithm for clustering ensembles. Artificial intelligence is relevant due to the use of genetic algorithms. Other fields like Computer graphics are not directly related to the core contribution.,"Computer vision:0.8,Control methods:0.1,Cross-validation:0.1,Distributed artificial intelligence:0.1,Knowledge representation and reasoning:0.1,Learning paradigms:0.1,Learning settings:0.1,Machine learning algorithms:1.0,Machine learning approaches:0.7,Natural language processing:0.1,Philosophical/theoretical foundations of artificial intelligence:0.1,Planning and scheduling:0.1,Search methodologies:0.1","Machine learning algorithms,Computer vision",Machine learning algorithms: Uses genetic algorithms for clustering ensembles. Computer vision: The method is applied to action recognition. Other categories are irrelevant.
1402,ISCAS: A System for Chinese Word Sense Induction Based on K-means Algorithm,"This paper presents an unsupervised method for automatic Chinese word sense induction. The algorithm is based on clustering the similar words according to the contexts in which they occur. First, the target word which needs to be disambiguated is represented as the vector of its contexts. Then, reconstruct the matrix constituted by the vectors of target words through singular value decomposition (SVD) method, and use the vectors to cluster the similar words. Our system participants in CLP2010 back off task4-Chinese word sense induction.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.75,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:1.0,Applied computing:0.5,Social and professional topics:0.0",Computing methodologies,Computing methodologies: The paper presents a clustering algorithm (K-means with SVD) for word sense induction. Information systems: The application to Chinese language processing is secondary. Applied computing: The NLP task has some relevance but is not the core contribution.,"Artificial intelligence:0.8,Computer graphics:0.0,Concurrent computing methodologies:0.0,Distributed computing methodologies:0.0,Machine learning:0.9,Modeling and simulation:0.0,Parallel computing methodologies:0.0,Symbolic and algebraic manipulation:0.0","Machine learning,Artificial intelligence","Machine learning is highly relevant due to the paper's focus on K-means clustering for word sense induction. Artificial intelligence receives a high score because word sense disambiguation is a core NLP task in AI. Other options are irrelevant as the paper does not discuss graphics, simulation, or parallel computing.","Computer vision:0.1,Control methods:0.0,Cross-validation:0.0,Distributed artificial intelligence:0.1,Knowledge representation and reasoning:0.2,Learning paradigms:0.7,Learning settings:0.3,Machine learning algorithms:0.8,Machine learning approaches:0.9,Natural language processing:1.0,Philosophical/theoretical foundations of artificial intelligence:0.1,Planning and scheduling:0.1,Search methodologies:0.1","Natural language processing,Machine learning approaches",Natural language processing is primary as it's about Chinese word sense induction. Machine learning approaches is secondary for the K-means clustering method. Other options like CV or control methods are irrelevant.
5460,Select and Sample - A Model of Efficient Neural Inference and Learning,"An increasing number of experimental studies indicate that perception encodes a posterior probability distribution over possible causes of sensory stimuli, which is used to act close to optimally in the environment. One outstanding difficulty with this hypothesis is that the exact posterior will in general be too complex to be represented directly, and thus neurons will have to represent an approximation of this distribution. Two influential proposals of efficient posterior representation by neural populations are: 1) neural activity represents samples of the underlying distribution, or 2) they represent a parametric representation of a variational approximation of the posterior. We show that these approaches can be combined for an inference scheme that retains the advantages of both: it is able to represent multiple modes and arbitrary correlations, a feature of sampling methods, and it reduces the represented space to regions of high probability mass, a strength of variational approximations. Neurally, the combined method can be interpreted as a feed-forward preselection of the relevant state space, followed by a neural dynamics implementation of Markov Chain Monte Carlo (MCMC) to approximate the posterior over the relevant states. We demonstrate the effectiveness and efficiency of this approach on a sparse coding model. In numerical experiments on artificial data and image patches, we compare the performance of the algorithms to that of exact EM, variational state space selection alone, MCMC alone, and the combined select and sample approach. The select and sample approach integrates the advantages of the sampling and variational approximations, and forms a robust, neurally plausible, and very efficient model of processing and learning in cortical networks. For sparse coding we show applications easily exceeding a thousand observed and a thousand hidden dimensions.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:1.0,Applied computing:0.1,Social and professional topics:0.1",Computing methodologies,Computing methodologies is relevant as the paper proposes a neural inference and learning model combining sampling and variational methods. Other categories are not central to the core contribution.,"Artificial intelligence:0.9,Computer graphics:0.1,Concurrent computing methodologies:0.1,Distributed computing methodologies:0.1,Machine learning:0.9,Modeling and simulation:0.2,Parallel computing methodologies:0.1,Symbolic and algebraic manipulation:0.1","Machine learning,Artificial intelligence","Machine learning: Proposes a novel neural inference and learning model. Artificial intelligence: Addresses probabilistic reasoning and cortical network modeling. Other options like Computer graphics are irrelevant as the work is theoretical and algorithmic, not visual.","Computer vision:1,Control methods:0,Cross-validation:0,Distributed artificial intelligence:0,Knowledge representation and reasoning:0,Learning paradigms:0,Learning settings:0,Machine learning algorithms:0,Machine learning approaches:0,Natural language processing:0,Philosophical/theoretical foundations of artificial intelligence:0.5,Planning and scheduling:0,Search methodologies:0","Computer vision,Philosophical/theoretical foundations of artificial intelligence",Computer vision is the primary application. The theoretical foundations are relevant due to the probabilistic modeling approach.
1031,A MASDCW Model about Multi-agent Application,"To present, all previous studies about agent focus on describing multi-agent model, its communication mechanism, negotiation and cooperation process. There are also researches on multi-agent cooperation structure, dynamic group forming within multi-agent cooperation and task assignment for multi-agent respectively. But little is known about the application of multi-agent cooperation model, the formation of dynamic groups in multi-area synchronous dynamic cooperation work (MASDCW) environments and the task assignment algorithm for dynamic groups. The paper begins with an introduction of the status of current research on multi-agent cooperation, then presents a MASDCW model with multi-agent cooperation. Whereas, the main point is the algorithm of task agent selection, as well as the algorithm of task assignment based on a dynamic group.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.8,Applied computing:0.1,Social and professional topics:0.1",Computing methodologies,Computing methodologies is highly relevant for the multi-agent cooperation model and task assignment algorithms. Applied computing is less central to the core theoretical contribution.,"Artificial intelligence:0.7,Computer graphics:0.1,Concurrent computing methodologies:0.2,Distributed computing methodologies:0.7,Machine learning:0.1,Modeling and simulation:0.2,Parallel computing methodologies:0.1,Symbolic and algebraic manipulation:0.1","Artificial intelligence,Distributed computing methodologies",Artificial intelligence: Focuses on multi-agent cooperation models and algorithms. Distributed computing methodologies: Proposes MASDCW dynamic group task assignment. Other options like Computer graphics are irrelevant to the multi-agent system design.,"Computer vision:0,Control methods:0,Distributed algorithms:0.5,Distributed artificial intelligence:1,Distributed programming languages:0,Knowledge representation and reasoning:0,Natural language processing:0,Philosophical/theoretical foundations of artificial intelligence:0,Planning and scheduling:1,Search methodologies:0","Distributed artificial intelligence,Planning and scheduling",Distributed artificial intelligence is relevant due to the focus on multi-agent cooperation models and dynamic group formation. Planning and scheduling is relevant for the task assignment algorithms. Other categories like Control methods or Search methodologies are not directly addressed in the paper's core contribution.
485,Exploiting longer cycles for link prediction in signed networks,"We consider the problem of link prediction in signed networks. Such networks arise on the web in a variety of ways when users can implicitly or explicitly tag their relationship with other users as positive or negative. The signed links thus created reflect social attitudes of the users towards each other in terms of friendship or trust. Our first contribution is to show how any quantitative measure of social imbalance in a network can be used to derive a link prediction algorithm. Our framework allows us to reinterpret some existing algorithms as well as derive new ones. Second, we extend the approach of Leskovec et al. (2010) by presenting a supervised machine learning based link prediction method that uses features derived from longer cycles in the network. The supervised method outperforms all previous approaches on 3 networks drawn from sources such as Epinions, Slashdot and Wikipedia. The supervised approach easily scales to these networks, the largest of which has 132k nodes and 841k edges. Most real-world networks have an overwhelmingly large proportion of positive edges and it is therefore easy to get a high overall accuracy at the cost of a high false positive rate. We see that our supervised method not only achieves good accuracy for sign prediction but is also especially effective in lowering the false positive rate.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.3,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.9,Applied computing:0.2,Social and professional topics:0.1",Computing methodologies,Computing methodologies is highly relevant as the paper presents a novel machine learning approach for link prediction in signed networks. Other categories are less relevant because the focus is on algorithm development rather than systems implementation or human interaction.,"Artificial intelligence:0.0,Computer graphics:0.0,Concurrent computing methodologies:0.0,Distributed computing methodologies:0.0,Machine learning:1.0,Modeling and simulation:0.5,Parallel computing methodologies:0.0,Symbolic and algebraic manipulation:0.0","Machine learning,Modeling and simulation",Machine learning: The paper introduces a supervised machine learning approach for link prediction in signed networks. Modeling and simulation: The study involves modeling social imbalance and simulating network behavior. Other categories like artificial intelligence are not directly relevant as the focus is on specific machine learning techniques rather than general AI.,"Cross-validation:0,Learning paradigms:1,Learning settings:1,Machine learning algorithms:1,Machine learning approaches:1,Model development and analysis:0,Simulation evaluation:0,Simulation support systems:0,Simulation theory:0,Simulation types and techniques:0","Learning paradigms,Machine learning algorithms,Machine learning approaches","Learning paradigms are relevant as the paper uses supervised learning for link prediction. Machine learning algorithms are relevant due to the use of cycle-based features. Machine learning approaches are relevant as the framework derives new algorithms. Other categories like Simulation or Cross-validation are irrelevant as the focus is on algorithmic design, not simulation or validation techniques."
1376,Automated mitosis detection based on eXclusive Independent Component Analysis,"In this paper, we propose an approach for automated mitosis detection, which provides critical information during performing breast cancer prognosis. Essentially, the problem of mitotic detection involves irregular shape object classification. It is a very challenging task. In this paper, a novel algorithm, named eXclusive Independent Component Analysis (XICA) is proposed. The XICA is an extension of a generic ICA, but focusing the components of differences (called exclusive basis set) between two classes of training patterns rather than the major (independent) components. Based on the residuals obtained from the relative computing involving the exclusive basis set of the relative training patterns, the automated mitosis detection is performed. By computing the residual of the relative exclusive basis set, we are able to classify the given testing patterns. The proposed approach has been tested on a mitosis image set provided by a ICPR2012 contest. It contains 226 mitosis in 35 color images. It achieved accurate rate 100% in training patterns and 83.513% in testing patterns.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.9,Applied computing:0.2,Social and professional topics:0.1",Computing methodologies,Computing methodologies: The paper proposes a novel algorithm (XICA) for automated mitosis detection in medical images. Other categories are not as relevant as the focus is on developing a computational methodology for classification.,"Artificial intelligence:0.75,Computer graphics:0.0,Concurrent computing methodologies:0.0,Distributed computing methodologies:0.0,Machine learning:1.0,Modeling and simulation:0.25,Parallel computing methodologies:0.0,Symbolic and algebraic manipulation:0.0",Machine learning,"Machine learning is directly relevant as the paper proposes an extension of ICA and uses SVM for classification. Artificial intelligence receives partial relevance due to the algorithm's cognitive-like analysis, but the core contribution is machine learning. Other categories are not applicable.","Machine learning algorithms:1,Machine learning approaches:1,Cross-validation:1","Machine learning algorithms,Cross-validation","Machine learning algorithms: XICA is a novel algorithm for mitosis detection. Cross-validation: The method is evaluated using cross-validation for testing accuracy. Learning paradigms/settings (0) are irrelevant as the focus is on algorithm design, not learning frameworks."
3928,List-Based Simulated Annealing Algorithm for Traveling Salesman Problem,"Simulated annealing (SA) algorithm is a popular intelligent optimization algorithm which has been successfully applied in many fields. Parameters' setting is a key factor for its performance, but it is also a tedious work. To simplify parameters setting, we present a list-based simulated annealing (LBSA) algorithm to solve traveling salesman problem (TSP). LBSA algorithm uses a novel list-based cooling schedule to control the decrease of temperature. Specifically, a list of temperatures is created first, and then the maximum temperature in list is used by Metropolis acceptance criterion to decide whether to accept a candidate solution. The temperature list is adapted iteratively according to the topology of the solution space of the problem. The effectiveness and the parameter sensitivity of the list-based cooling schedule are illustrated through benchmark TSP problems. The LBSA algorithm, whose performance is robust on a wide range of parameter values, shows competitive performance compared with some other state-of-the-art algorithms.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.4,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.9,Applied computing:0.0,Social and professional topics:0.0",Computing methodologies,Computing methodologies is relevant due to the optimization algorithm design for TSP. Other categories do not focus on algorithmic methods.,"Artificial intelligence:0.2,Computer graphics:0.1,Concurrent computing methodologies:0.3,Distributed computing methodologies:0.2,Machine learning:0.1,Modeling and simulation:0.7,Parallel computing methodologies:0.3,Symbolic and algebraic manipulation:0.2",Modeling and simulation,"Modeling and simulation: The algorithm is evaluated through TSP benchmark simulations. Other categories like AI or ML are not core to this work, which focuses on optimization algorithm design rather than learning or symbolic methods.","Model development and analysis:0.3,Simulation evaluation:0.4,Simulation support systems:0.2,Simulation theory:1.0,Simulation types and techniques:1.0","Simulation theory,Simulation types and techniques",Simulation theory is relevant as the paper introduces a new cooling schedule for SA. Simulation types and techniques is relevant because the paper presents a novel simulation method. Model development and analysis is less relevant as the focus is on the algorithm rather than model development.
3679,Asymptotically exact inference in differentiable generative models,Many generative models can be expressed as a differentiable function of random inputs drawn from some simple probability density. This framework includes both deep generative architectures such as Variational Autoencoders and a large class of procedurally defined simulator models. We present a method for performing efficient MCMC inference in such models when conditioning on observations of the model output. For some models this offers an asymptotically exact inference method where Approximate Bayesian Computation might otherwise be employed. We use the intuition that inference corresponds to integrating a density across the manifold corresponding to the set of inputs consistent with the observed outputs. This motivates the use of a constrained variant of Hamiltonian Monte Carlo which leverages the smooth geometry of the manifold to coherently move between inputs exactly consistent with observations. We validate the method by performing inference tasks in a diverse set of models.,"General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.3,Theory of computation:0.2,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.9,Applied computing:0.1,Social and professional topics:0.1",Computing methodologies,Computing methodologies is central to MCMC inference in generative models. Other categories like Software engineering are not core to the methodological contribution.,"Artificial intelligence:0.3,Computer graphics:0.0,Concurrent computing methodologies:0.0,Distributed computing methodologies:0.0,Machine learning:0.9,Modeling and simulation:0.7,Parallel computing methodologies:0.0,Symbolic and algebraic manipulation:0.0","Machine learning,Modeling and simulation",Machine learning is highly relevant for MCMC inference in generative models. Modeling and simulation is relevant for Hamiltonian Monte Carlo methods in constrained manifolds. Artificial intelligence is less specific to the core contribution.,"Cross-validation:0.1,Learning paradigms:0.3,Learning settings:0.2,Machine learning algorithms:0.9,Machine learning approaches:0.7,Model development and analysis:0.8,Simulation evaluation:0.5,Simulation support systems:0.4,Simulation theory:0.6,Simulation types and techniques:0.7","Machine learning algorithms,Model development and analysis",Machine learning algorithms is relevant for the Hamiltonian Monte Carlo method. Model development and analysis is relevant for the generative model framework. Other categories like Simulation evaluation are rejected as the focus is on inference rather than simulation evaluation.
972,On the Role of Morphosyntactic Features in Hindi Dependency Parsing,"This paper analyzes the relative importance of different linguistic features for data-driven dependency parsing of Hindi, using a feature pool derived from two state-of-the-art parsers. The analysis shows that the greatest gain in accuracy comes from the addition of morpho-syntactic features related to case, tense, aspect and modality. Combining features from the two parsers, we achieve a labeled attachment score of 76.5%, which is 2 percentage points better than the previous state of the art. We finally provide a detailed error analysis and suggest possible improvements to the parsing scheme.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.2,Theory of computation:0.3,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.9,Applied computing:0.2,Social and professional topics:0.1",Computing methodologies,"Computing methodologies is highly relevant for the Hindi dependency parsing technique analysis. Theory of computation receives moderate relevance for algorithmic aspects, but the core contribution is methodological in NLP.","Artificial intelligence:1.0,Computer graphics:0.0,Concurrent computing methodologies:0.0,Distributed computing methodologies:0.0,Machine learning:0.75,Modeling and simulation:0.0,Parallel computing methodologies:0.0,Symbolic and algebraic manipulation:0.0","Artificial intelligence,Machine learning",Artificial intelligence is relevant as the paper addresses natural language processing in Hindi dependency parsing. Machine learning is relevant for the data-driven parsing approach. Other options are unrelated to the linguistic parsing task.,"Computer vision:0,Control methods:0,Cross-validation:0,Distributed artificial intelligence:0,Knowledge representation and reasoning:0,Learning paradigms:0,Learning settings:0,Machine learning algorithms:1,Machine learning approaches:1,Natural language processing:1,Philosophical/theoretical foundations of artificial intelligence:0,Planning and scheduling:0,Search methodologies:0","Natural language processing,Machine learning algorithms",Natural language processing is directly relevant for Hindi dependency parsing. Machine learning algorithms is relevant for the feature-based parsing approach.
5792,A Review on the Interpretability-Accuracy Trade-Off in Evolutionary Multi-Objective Fuzzy Systems (EMOFS),"Interpretability and accuracy are two important features of fuzzy systems which are conflicting in their nature. One can be improved at the cost of the other and this situation is identified as “Interpretability-Accuracy Trade-Off”. To deal with this trade-off Multi-Objective Evolutionary Algorithms (MOEA) are frequently applied in the design of fuzzy systems. Several novel MOEA have been proposed and invented for this purpose, more specifically, Non-Dominated Sorting Genetic Algorithms (NSGA-II), Strength Pareto Evolutionary Algorithm 2 (SPEA2), Fuzzy Genetics-Based Machine Learning (FGBML), (2 + 2) Pareto Archived Evolutionary Strategy ((2 + 2) PAES), (2 + 2) Memetic- Pareto Archived Evolutionary Strategy ((2 + 2) M-PAES), etc. This paper introduces and reviews the approaches to the issue of developing fuzzy systems using Evolutionary Multi-Objective Optimization (EMO) algorithms considering ‘Interpretability-Accuracy Trade-off’ and mainly focusing on the work in the last decade. Different research issues and challenges are also discussed.","General and reference:0.1,Hardware:0.2,Computer systems organization:0.3,Networks:0.1,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.2,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.3,Computing methodologies:0.9,Applied computing:0.5,Social and professional topics:0.1",Computing methodologies,Computing methodologies (0.9) for evolutionary multi-objective optimization techniques. Other categories like Applied Computing (0.5) have secondary relevance but lack the core algorithmic focus.,"Artificial intelligence:0.3,Computer graphics:0.0,Concurrent computing methodologies:0.0,Distributed computing methodologies:0.0,Machine learning:0.9,Modeling and simulation:0.8,Parallel computing methodologies:0.0,Symbolic and algebraic manipulation:0.1","Machine learning,Modeling and simulation","Machine learning is highly relevant as the paper focuses on evolutionary multi-objective optimization algorithms (e.g., NSGA-II, SPEA2) for fuzzy systems. Modeling and simulation is relevant due to the analysis of trade-offs in system design. Other categories like Artificial intelligence and Symbolic manipulation are less directly related to the core optimization techniques discussed.","Cross-validation:0,Learning paradigms:0.5,Learning settings:0,Machine learning algorithms:1,Machine learning approaches:1,Model development and analysis:0.8,Simulation evaluation:0.5,Simulation support systems:0,Simulation theory:0,Simulation types and techniques:0","Machine learning algorithms,Machine learning approaches",Machine learning algorithms: NSGA-II and SPEA2 are explicitly discussed as optimization algorithms. Machine learning approaches: EMOFS design using MOEAs is central. Rejected options like simulation evaluation are secondary to the core algorithmic focus.
5872,Classification of Electrocardiogram Signals with Deep Belief Networks,"This paper introduces an electrocardiogram beat classification method based on deep belief networks. This method includes two parts: feature extraction and classification. In the feature extraction part, features are extracted from the original electrocardiogram signal: including features extracted by deep belief networks and timing interval features. Several classifiers are selected to classify the electrocardiogram beat, and nonlinear support vector machine with Gaussian kernel achieves the best classification accuracy, reaching 98.49. Compared with other similar methods on electrocardiogram beat classification, our method can improve the recognition performance of some types of electrocardiogram beats. Keywords-Electrocardiogram; Arrhythmia; Deep Belief Networks; Artificial Neural Networks; Support Vector Machine","General and reference:0.2,Hardware:0.2,Computer systems organization:0.2,Networks:0.2,Software and its engineering:0.2,Theory of computation:0.2,Mathematics of computing:0.2,Information systems:0.7,Security and privacy:0.2,Human-centered computing:0.2,Computing methodologies:1.0,Applied computing:0.2,Social and professional topics:0.2",Computing methodologies,"Computing methodologies is highly relevant because the paper uses deep learning for classification. Information systems is moderately relevant, as it's about data analysis, but the focus is on the AI method.","Artificial intelligence:0.8,Computer graphics:0.1,Concurrent computing methodologies:0.1,Distributed computing methodologies:0.1,Machine learning:0.95,Modeling and simulation:0.1,Parallel computing methodologies:0.1,Symbolic and algebraic manipulation:0.1","Machine learning,Artificial intelligence",Machine learning is directly relevant as the paper uses deep belief networks for classification. Artificial intelligence is relevant as an overarching category. Other fields like graphics or distributed computing are not addressed.,"Computer vision:0.1,Control methods:0.1,Cross-validation:0.3,Distributed artificial intelligence:0.1,Knowledge representation and reasoning:0.1,Learning paradigms:0.5,Learning settings:0.4,Machine learning algorithms:0.9,Machine learning approaches:0.8,Natural language processing:0.1,Philosophical/theoretical foundations of artificial intelligence:0.1,Planning and scheduling:0.1,Search methodologies:0.1","Machine learning algorithms,Machine learning approaches,Learning paradigms",Machine learning algorithms is relevant for deep belief networks and SVM. Machine learning approaches is relevant for classification methods. Learning paradigms is relevant for feature extraction. Other categories like Cross-validation are mentioned but not central.
3600,Selecting models from videos for appearance-based face recognition,We propose an unsupervised approach to select representative face samples (models) from raw videos and build an appearance-based face recognition system. The approach is based on representing the face manifold in a low-dimensional space using the locally linear embedding (LLE) algorithm and then performing K-means clustering. We define the face models as the cluster centers. Our strategy is motivated by the efficiency of LLE to recover meaningful low-dimensional structures hidden in complex and high dimensional data such as face images. Two other well-known unsupervised learning algorithms (Isomap and SOM) are also considered. We compare and assess the efficiency of these different schemes on the CMU MoBo database which contains 96 face sequences of 24 subjects. The results clearly show significant performance enhancements over traditional methods such as the PCA-based one.,"General and reference:0.1,Hardware:0.2,Computer systems organization:0.3,Networks:0.2,Software and its engineering:0.3,Theory of computation:0.2,Mathematics of computing:0.75,Information systems:0.4,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:1.0,Applied computing:0.3,Social and professional topics:0.1",Computing methodologies,"Computing methodologies: The paper introduces unsupervised learning techniques (LLE, K-means) for face recognition, which are core to computing methodologies in machine learning and data analysis. 'Mathematics of computing' is relevant for the algorithms but is secondary to the primary focus on methodology. Other categories like 'Information systems' are less central.","Artificial intelligence:0.75,Computer graphics:0.1,Concurrent computing methodologies:0.05,Distributed computing methodologies:0.05,Machine learning:1.0,Modeling and simulation:0.2,Parallel computing methodologies:0.05,Symbolic and algebraic manipulation:0.05",Machine learning,"Machine learning is highly relevant as the paper focuses on unsupervised learning algorithms (LLE, K-means) for face recognition. Other fields like Artificial intelligence are peripherally relevant but not core. The remaining options are unrelated to the paper's primary domain.","Cross-validation:0.2,Learning paradigms:0.6,Learning settings:0.5,Machine learning algorithms:0.8,Machine learning approaches:0.7","Machine learning algorithms,Learning paradigms",Machine learning algorithms: The paper uses LLE and K-means for face recognition. Learning paradigms: The unsupervised approach aligns with learning paradigms. Other options like Cross-validation are not central to the methodology described.
917,Real time feature point tracking with automatic model selection,"We present an efficient and accurate algorithm for face tracking using a set of Active Appearance Models (AAMs). We observe that a single AAM, trained at a particular model resolution and a particular range of displacements, has a “sweet spot” - a range of displacements for which it is most accurate. A common approach to increasing the range of convergence is to use a multi-resolution model, or a sequence of AAMs trained on smaller and smaller displacements. However, during tracking it is inefficient to run the whole sequence at every frame. If there has been little movement since the previous frame, it is sufficient to only run one step of a single higher resolution AAM. In this paper we show that we can use a non-linear regressor to estimate the magnitude of the displacement from the optimal position in the current frame, and use this to select a model which has been tuned to work well at that displacement. This is significantly more efficient than running a complete sequence of models at every frame. We describe the method in detail and demonstrate its performance on several datasets.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.8,Applied computing:0.1,Social and professional topics:0.1",Computing methodologies,Computing methodologies: The study proposes a model selection technique for real-time tracking using statistical models. Other categories are not central to algorithm design for computer vision.,"Artificial intelligence:0.2,Computer graphics:0.6,Concurrent computing methodologies:0.1,Distributed computing methodologies:0.1,Machine learning:0.7,Modeling and simulation:0.5,Parallel computing methodologies:0.1,Symbolic and algebraic manipulation:0.1","Machine learning,Computer graphics,Modeling and simulation",Machine learning is relevant for the use of non-linear regressors in model selection. Computer graphics applies to the face tracking application. Modeling and simulation is included for the AAM-based tracking framework. Other categories like Artificial intelligence are less central to the specific tracking algorithm.,"Animation:0.0,Cross-validation:0.0,Graphics systems and interfaces:0.0,Image compression:0.0,Image manipulation:1.0,Learning paradigms:0.0,Learning settings:0.0,Machine learning algorithms:1.0,Machine learning approaches:0.0,Model development and analysis:0.0,Rendering:0.0,Shape modeling:0.0,Simulation evaluation:0.0,Simulation support systems:0.0,Simulation theory:0.0,Simulation types and techniques:0.0","Image manipulation,Machine learning algorithms","Image manipulation is relevant because the paper discusses face tracking using AAMs, which involves image processing. Machine learning algorithms is relevant as the method uses a non-linear regressor for model selection. Other categories like Simulation or Graphics systems are not directly relevant to the paper's focus."
2007,Salient Region Detection via Integrating Diffusion-Based Compactness and Local Contrast,"Salient region detection is a challenging problem and an important topic in computer vision. It has a wide range of applications, such as object recognition and segmentation. Many approaches have been proposed to detect salient regions using different visual cues, such as compactness, uniqueness, and objectness. However, each visual cue-based method has its own limitations. After analyzing the advantages and limitations of different visual cues, we found that compactness and local contrast are complementary to each other. In addition, local contrast can very effectively recover incorrectly suppressed salient regions using compactness cues. Motivated by this, we propose a bottom-up salient region detection method that integrates compactness and local contrast cues. Furthermore, to produce a pixel-accurate saliency map that more uniformly covers the salient objects, we propagate the saliency information using a diffusion process. Our experimental results on four benchmark data sets demonstrate the effectiveness of the proposed method. Our method produces more accurate saliency maps with better precision-recall curve and higher F-Measure than other 19 state-of-the-arts approaches on ASD, CSSD, and ECSSD data sets.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:1.0,Applied computing:0.2,Social and professional topics:0.0",Computing methodologies,"Computing methodologies: The paper presents a novel salient region detection method integrating compactness and local contrast cues, which is a core contribution in computer vision methodologies. Applied computing was considered but the paper's primary focus is on the methodological advancement rather than specific application domains.","Artificial intelligence:0.0,Computer graphics:0.0,Concurrent computing methodologies:0.0,Distributed computing methodologies:0.0,Machine learning:0.0,Modeling and simulation:1.0,Parallel computing methodologies:0.0,Symbolic and algebraic manipulation:0.0",Modeling and simulation,"Modeling and simulation is highly relevant as the paper integrates diffusion-based compactness and local contrast for salient region detection, which involves simulation techniques. Other categories like Computer graphics or Machine learning are not directly relevant to the method's focus on bottom-up saliency computation.","Model development and analysis:1.0,Simulation evaluation:0.8,Simulation support systems:0.1,Simulation theory:0.1,Simulation types and techniques:0.3","Model development and analysis,Simulation evaluation",Model development and analysis is core to the novel salient region detection method. Simulation evaluation is relevant due to experimental validation on benchmark datasets. Other simulation categories are less relevant as the focus is on model development rather than simulation frameworks.
3791,Score-Informed Leading Voice Separation from Monaural Audio,"Separating the leading voice from a musical recording seems to be natural to the human ear. Yet, it remains a difficult problem for automatic systems, in particular in the blind case, where no information is known about the signal. However, in the case where a musical score is available, one can take advantage of this additional information. In this paper, we present a novel application of this idea for leading voice separation exploiting a temporallyaligned MIDI Score. The model used is based on Nonnegative Matrix Factorization (NMF), whose solo part is represented by a sourcefilter model. We exploit the score information by constraining the source activations to conform to the aligned MIDI file. Experiments run on a database of real popular songs show that the use of these constraints can significantly improve the separation quality, in terms of both signal-based and perceptual evaluation metrics.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.3,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.9,Applied computing:0.5,Social and professional topics:0.1",Computing methodologies,Computing methodologies is relevant for NMF-based audio separation algorithms. Applied computing is secondary as the focus is on signal processing techniques rather than practical applications. Mathematics of computing is tangential as the paper emphasizes implementation over theoretical proofs.,"Artificial intelligence:0.8,Computer graphics:0.1,Concurrent computing methodologies:0.1,Distributed computing methodologies:0.1,Machine learning:0.7,Modeling and simulation:0.2,Parallel computing methodologies:0.1,Symbolic and algebraic manipulation:0.1","Artificial intelligence,Machine learning","Artificial intelligence: The paper applies NMF for audio source separation, an AI task. Machine learning: Uses constrained NMF, a ML technique. Modeling is secondary to the ML application.","Computer vision:0,Control methods:0,Cross-validation:0,Distributed artificial intelligence:0,Knowledge representation and reasoning:0,Learning paradigms:0,Learning settings:0,Machine learning algorithms:1,Machine learning approaches:0,Natural language processing:0,Philosophical/theoretical foundations of artificial intelligence:0,Planning and scheduling:0,Search methodologies:0",Machine learning algorithms,"Machine learning algorithms: The paper introduces a score-informed NMF model for voice separation, a machine learning algorithm. Other categories like computer vision are not relevant to audio processing."
1786,Detektion von Fahrzeugen in optischen Satellitenbildern,"In der vorliegenden Arbeit wird ein automatisches Verfahren zur Erkennung von Fahrzeugen und deren Bewegungszustand aus optischen Satellitenbilddaten vorgestellt. Im Gegensatz zu terrestrischen stationaren Erfassungsmethoden wie Induktionsschleifen und Bruckenkameras ist durch Satellitenbilddaten eine grosflachige Abbildung der Verkehrsituation moglich. Der entwickelte Ansatz berucksichtigt isoliert und in Gruppen auftretende Fahrzeuge. Isolierte Fahrzeuge werden durch Mustererkennung auf Basis von Trainingsdaten detektiert. Hypothesen fur Fahrzeugreihen werden durch differentialgeometrische Linienextraktion gebildet und anschliesend durch Verfahren der robusten Parameterschatzung in Einzelfahrzeuge zerlegt. Die geschatzten Parameter dienen zur Verifikation von Fahrzeugen. Fur Einzelfahrzeuge wird zusatzlich der Bewegungsstatus gepruft. Das Verfahren wurde mit QuickBird Bildern eines stadtischen Bereichs evaluiert. Wahrend die Vollstandigkeit der Ergebnisse bei 65% liegt, erreicht der Ansatz eine hohe Zuverlassigkeit von ca. 95%.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:1.0,Applied computing:0.5,Social and professional topics:0.1",Computing methodologies,Computing methodologies is highly relevant for the image processing and pattern recognition techniques. Applied computing is marginally relevant due to the traffic monitoring application but does not capture the core contribution.,"Artificial intelligence:1,Machine learning:1,Computer graphics:0.75,Modeling and simulation:0.25,Concurrent computing methodologies:0.1,Distributed computing methodologies:0.1,Parallel computing methodologies:0.1,Symbolic and algebraic manipulation:0.1","Artificial intelligence,Machine learning,Computer graphics",Artificial intelligence is relevant due to pattern recognition and hypothesis generation. Machine learning is central to the training data-driven detection approach. Computer graphics applies to image processing and visualization. Other categories are rejected for lacking direct algorithmic or data-driven focus.,"Animation:0,Computer vision:1,Control methods:0,Cross-validation:0,Distributed artificial intelligence:0,Graphics systems and interfaces:0,Image compression:0,Image manipulation:0.5,Knowledge representation and reasoning:0,Learning paradigms:0,Learning settings:0,Machine learning algorithms:0.5,Machine learning approaches:0.5,Natural language processing:0,Philosophical/theoretical foundations of artificial intelligence:0,Planning and scheduling:0,Rendering:0,Search methodologies:0,Shape modeling:0",Computer vision,"Computer vision: The paper presents an automated vehicle detection system using optical satellite imagery, a core computer vision task. Image manipulation and Machine learning algorithms are secondary but not central to the core contribution."
2355,A class-coded modulation method using orthogonal wavelets,"Researchers have studied two types of digital multi-carrier modulation, wavelet packet modulation (WPM), and fractal modulation. The advantage of WPM is that it allows signal time and frequency to be divided and multiplexed. Therefore the time and frequency of noise signals, e.g., tone and impulse noise, can be identified and effectively partitioned in packets. Using this advantage, we propose a class-coded modulation method for WPM that has good bit error rate (BER) performance when the length of the code constraint is extended and can select time and frequency noise signals simultaneously. This report describes the proposed method and the computer simulations carried out to test its effectiveness.","General and reference:0.2,Hardware:0.2,Computer systems organization:0.2,Networks:0.2,Software and its engineering:0.2,Theory of computation:0.2,Mathematics of computing:0.2,Information systems:0.2,Security and privacy:0.2,Human-centered computing:0.2,Computing methodologies:1.0,Applied computing:0.2,Social and professional topics:0.2",Computing methodologies,Computing methodologies is directly relevant as the paper introduces a novel wavelet-based modulation method. Other categories like Hardware or Networks are less central as the focus is on algorithmic innovation.,"Artificial intelligence:0.0,Computer graphics:0.0,Concurrent computing methodologies:0.0,Distributed computing methodologies:0.0,Machine learning:0.0,Modeling and simulation:0.7,Parallel computing methodologies:0.0,Symbolic and algebraic manipulation:0.0",Modeling and simulation,Modeling and simulation is relevant for the simulation of the modulation method. Other categories are not directly related to the core contribution.,"Model development and analysis:1,Simulation evaluation:1,Simulation support systems:0,Simulation theory:0,Simulation types and techniques:0","Model development and analysis,Simulation evaluation",Model development and analysis is relevant for the class-coded modulation method. Simulation evaluation is relevant for computer simulations testing effectiveness. Other children are irrelevant as the focus is on model and simulation evaluation.
1063,Word Translation Disambiguation Using Bilingual Bootstrapping,"This article proposes a new method for word translation disambiguation, one that uses a machine-learning technique called bilingual bootstrapping. In learning to disambiguate words to be translated, bilingual bootstrapping makes use of a small amount of classified data and a large amount of unclassified data in both the source and the target languages. It repeatedly constructs classifiers in the two languages in parallel and boosts the performance of the classifiers by classifying unclassified data in the two languages and by exchanging information regarding classified data between the two languages. Experimental results indicate that word translation disambiguation based on bilingual bootstrapping consistently and significantly outperforms existing methods that are based on monolingual bootstrapping.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.6,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.95,Applied computing:0.1,Social and professional topics:0.1",Computing methodologies,"Computing methodologies is highly relevant as the paper introduces a novel machine learning technique (bilingual bootstrapping) for translation disambiguation. Theory of computation has moderate relevance due to algorithmic aspects, but the core contribution lies in applied machine learning methodologies.","Machine learning:0.95,Artificial intelligence:0.8,Modeling and simulation:0.2,Computer graphics:0.05,Concurrent computing methodologies:0.05,Distributed computing methodologies:0.05,Parallel computing methodologies:0.05,Symbolic and algebraic manipulation:0.05","Machine learning,Artificial intelligence",Machine learning: Proposes a semi-supervised ensemble method. Artificial intelligence: Involves natural language processing for translation. Other categories: Less central to the core contribution.,"Computer vision:0,Control methods:0,Cross-validation:0,Distributed artificial intelligence:0,Knowledge representation and reasoning:0,Learning paradigms:0.5,Learning settings:0.5,Machine learning algorithms:1,Machine learning approaches:1,Natural language processing:1,Philosophical/theoretical foundations of artificial intelligence:0,Planning and scheduling:0,Search methodologies:0","Machine learning algorithms,Natural language processing","Machine learning algorithms: The paper introduces a novel machine learning technique (bilingual bootstrapping) for translation disambiguation. Natural language processing: The core application domain is word translation, a central NLP task. Other children are irrelevant as the paper does not discuss computer vision, control methods, or philosophical AI foundations."
989,A New Individual-Decision Cognitive Learning Factor Selection Strategy,"As an important parameter, up to day, many strategies for cognitive coefficient have been proposed. However, there is still some work need to do. Since each particle maintains different living experience e.g. feeding, nursing baby and so on. Thus different individual will make a different decision. However, this decision mechanism is not included in the improved particle swarm optimization (PSO). Therefore, with the assistant of mature individual decision way and mechanism, this paper dynamically adjusts cognitive coefficient by the change ratio of historical fitness value. Using several test functions to simulation, Simulation results show that its performance is superior to other two variants.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.9,Applied computing:0.1,Social and professional topics:0.1",Computing methodologies,Computing methodologies is highly relevant for the novel PSO cognitive learning strategy. Other categories like Applied computing are less directly tied to the core contribution.,"Artificial intelligence:0.8,Computer graphics:0.0,Concurrent computing methodologies:0.0,Distributed computing methodologies:0.0,Machine learning:0.7,Modeling and simulation:0.6,Parallel computing methodologies:0.0,Symbolic and algebraic manipulation:0.0","Artificial intelligence,Machine learning",Artificial intelligence is relevant for the PSO algorithm adaptation. Machine learning is relevant due to the cognitive learning factor strategy. Modeling and simulation is secondary as the focus is on optimization rather than simulation.,"Computer vision:0.2,Control methods:0.3,Cross-validation:0.3,Distributed artificial intelligence:0.2,Knowledge representation and reasoning:0.2,Learning paradigms:1.0,Learning settings:0.4,Machine learning algorithms:1.0,Machine learning approaches:0.6,Natural language processing:0.1,Philosophical/theoretical foundations of artificial intelligence:0.3,Planning and scheduling:0.2,Search methodologies:1.0","Machine learning algorithms,Learning paradigms,Search methodologies",Machine learning algorithms is highly relevant as the paper discusses particle swarm optimization. Learning paradigms is relevant as it addresses cognitive learning strategies. Search methodologies is relevant since PSO is an optimization algorithm. Computer vision and control methods are less relevant as the paper focuses on learning algorithms.
408,Beyond trees: common-factor models for 2D human pose recovery,"Tree structured models have been widely used for determining the pose of a human body, from either 2D or 3D data. While such models can effectively represent the kinematic constraints of the skeletal structure, they do not capture additional constraints such as coordination of the limbs. Tree structured models thus miss an important source of information about human body pose, as limb coordination is necessary for balance while standing, walking, or running, as well as being evident in other activities such as dancing and throwing. In this paper, we consider the use of undirected graphical models that augment a tree structure with latent variables in order to account for coordination between limbs. We refer to these as common-factor models, since they are constructed by using factor analysis to identify additional correlations in limb position that are not accounted for by the kinematic tree structure. These common-factor models have an underlying tree structure and thus a variant of the standard Viterbi algorithm for a tree can be applied for efficient estimation. We present some experimental results contrasting common-factor models with tree models, and quantify the improvement in pose estimation for 2D image data.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.3,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:1.0,Applied computing:0.2,Social and professional topics:0.1",Computing methodologies,"Computing methodologies is highly relevant because the paper introduces common-factor graphical models for human pose estimation, a machine learning method. 'Information systems' is marginally relevant if data processing is considered, but the core contribution is the model. 'Applied computing' is secondary as the application is vision, not direct societal use.","Artificial intelligence:1.0,Computer graphics:0.8,Concurrent computing methodologies:0.2,Distributed computing methodologies:0.2,Machine learning:1.0,Modeling and simulation:0.7,Parallel computing methodologies:0.2,Symbolic and algebraic manipulation:0.2","Artificial intelligence,Machine learning,Computer graphics,Modeling and simulation",Artificial intelligence is relevant for AI applications in computer vision. Machine learning is relevant for model design. Computer graphics is relevant for modeling human poses. Modeling and simulation is relevant for pose modeling. Other categories are rejected as they do not align with the paper's focus on pose recovery models.,"Animation:0.0,Computer vision:1.0,Control methods:0.0,Cross-validation:0.0,Distributed artificial intelligence:0.0,Graphics systems and interfaces:0.0,Image compression:0.0,Image manipulation:0.0,Knowledge representation and reasoning:0.0,Learning paradigms:0.5,Learning settings:0.0,Machine learning algorithms:0.0,Machine learning approaches:0.8,Model development and analysis:0.0,Natural language processing:0.0,Philosophical/theoretical foundations of artificial intelligence:0.0,Planning and scheduling:0.0,Rendering:0.0,Search methodologies:0.0,Shape modeling:0.0,Simulation evaluation:0.0,Simulation support systems:0.0,Simulation theory:0.0,Simulation types and techniques:0.0","Computer vision,Machine learning approaches",Computer vision is directly relevant as the paper introduces a method for 2D human pose recovery. Machine learning approaches are relevant due to the use of common-factor models and probabilistic techniques. Other options like Animation or Image compression are unrelated to the core contribution of pose estimation.
3828,Quality guided handbag segmentation,"In this paper, we address the problem of handbag segmentation, which is a challenging while important pre-processing for fashion related applications such as handbag tagging and search. Inaccurate segmentation will easily lead to other descriptions of color and shape of the handbag. We first design and extract a set of features for measuring the quality of the handbag segmentation based on some prior knowledge of handbag images. The quality of the handbag segmentation is then measured based on the weighted combination of these features. Guided by such quality measurement, we propose to segment the handbag image by a bottom-up super-pixel fusion. We conduct the experiment on a newly built handbag dataset as well as an existing branded handbag dataset. The results show that our segmentation algorithm performs favorably for handbags. The performance of handbag tagging and recognition is shown to be improved by incorporating such algorithm as pre-processing.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.2,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.85,Applied computing:0.1,Social and professional topics:0.1",Computing methodologies,Computing methodologies: The paper presents image segmentation techniques for fashion applications. Other fields like information systems are secondary to the algorithmic contribution.,"Artificial intelligence:0.8,Computer graphics:0.7,Concurrent computing methodologies:0.0,Distributed computing methodologies:0.0,Machine learning:0.6,Modeling and simulation:0.0,Parallel computing methodologies:0.0,Symbolic and algebraic manipulation:0.0","Artificial intelligence,Computer graphics,Machine learning",Artificial intelligence is highly relevant as the paper discusses image segmentation algorithms. Computer graphics is relevant due to the focus on visual processing. Machine learning is relevant for the feature-based segmentation approach. Other categories like Symbolic and algebraic manipulation are not directly relevant.,"Animation:0,Computer vision:1,Control methods:0,Cross-validation:0,Distributed artificial intelligence:0,Graphics systems and interfaces:0,Image compression:0,Image manipulation:0.5,Knowledge representation and reasoning:0,Learning paradigms:0,Learning settings:0,Machine learning algorithms:0,Machine learning approaches:0,Natural language processing:0,Philosophical/theoretical foundations of artificial intelligence:0,Planning and scheduling:0,Rendering:0,Search methodologies:0,Shape modeling:0",Computer vision,"Computer vision is relevant due to the focus on handbag segmentation as a pre-processing step for fashion applications. Image manipulation is secondary but less central than core computer vision tasks. Other options are irrelevant as the paper does not address AI theory, rendering, or language processing."
3857,Robust wavelet zerotree image compression with fixed-length packetization,"We present a novel robust image compression algorithm in which the output of a wavelet zerotree-style coder is manipulated into fixed-length segments. The segments are independently decodable, and errors occurring in one segment do not propagate into any other. The method provides both excellent compression performance and graceful degradation under increasing packet losses. We extend the basic scheme to perform region-based compression, in which specified portions of the image are coded to higher quality with little or no side information required by the decoder.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.8,Applied computing:0.2,Social and professional topics:0.1",Computing methodologies,"Computing methodologies: The paper presents a novel wavelet-based image compression algorithm with fixed-length packetization, which is a core contribution in computing methodologies. Applied computing is marginally relevant but secondary to the algorithmic innovation.","Artificial intelligence:0.2,Computer graphics:1.0,Concurrent computing methodologies:0.3,Distributed computing methodologies:0.3,Machine learning:0.3,Modeling and simulation:0.6,Parallel computing methodologies:0.3,Symbolic and algebraic manipulation:0.2","Computer graphics,Modeling and simulation",Computer graphics is central to image compression techniques. Modeling and simulation is relevant for evaluating compression performance under packet losses. Other fields like Machine learning are not directly addressed.,"Animation:0,Graphics systems and interfaces:0,Image compression:1,Image manipulation:0,Model development and analysis:0,Rendering:0,Shape modeling:0,Simulation evaluation:0,Simulation support systems:0,Simulation theory:0,Simulation types and techniques:0",Image compression,Image compression is directly mentioned in the title and abstract. Other options like Image manipulation or Graphics systems are not discussed as core contributions.
3076,Regularisation in Sequential Learning Algorithms,"In this paper, we discuss regularisation in online/sequential learning algorithms. In environments where data arrives sequentially, techniques such as cross-validation to achieve regularisation or model selection are not possible. Further, bootstrapping to determine a confidence level is not practical. To surmount these problems, a minimum variance estimation approach that makes use of the extended Kalman algorithm for training multi-layer perceptrons is employed. The novel contribution of this paper is to show the theoretical links between extended Kalman filtering, Sutton's variable learning rate algorithms and Mackay's Bayesian estimation framework. In doing so, we propose algorithms to overcome the need for heuristic choices of the initial conditions and noise covariance matrices in the Kalman approach.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.2,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.9,Applied computing:0.1,Social and professional topics:0.1",Computing methodologies,Computing methodologies is highly relevant as the paper presents novel regularization techniques for sequential learning algorithms. Other categories like 'Mathematics of computing' are less relevant as the focus is on algorithmic development rather than pure mathematical theory.,"Artificial intelligence:0.3,Computer graphics:0.2,Concurrent computing methodologies:0.1,Distributed computing methodologies:0.2,Machine learning:1.0,Modeling and simulation:0.6,Parallel computing methodologies:0.1,Symbolic and algebraic manipulation:0.1",Machine learning,"Machine learning is highly relevant as the paper addresses regularization in sequential learning algorithms. Modeling and simulation receives moderate relevance for theoretical framework analysis, but the core contribution focuses on machine learning methodology.","Cross-validation:0.1,Learning paradigms:0.3,Learning settings:0.3,Machine learning algorithms:0.9,Machine learning approaches:0.8","Machine learning algorithms,Machine learning approaches",Machine learning algorithms is relevant for the Kalman filter-based regularization technique. Machine learning approaches is relevant for the Bayesian estimation framework. Other options like Learning paradigms are less central to the core contribution.
1522,Statistical Approaches to Identifying Androgen Response Elements,"DNA-binding transcription factors play an integral role in regulating gene expression. Transcription factor binding sites (TFBS) in the gene promoter regions can be predicted by using computational methods, such as Support Vector Machine (SVM), Hidden Markov Model (HMM), and Random Forest (RF), all of which summarize sequence patterns of experimentally determined TFBSs. Androgen receptor (AR), a ligand-dependent transcription factor, plays an important role in male reproductive functions by regulating gene transcription through directly binding to androgen response elements (ARE) in target gene promoters. The aim of this study is to use data mining tools to identify and characterize AREs based on sequence information. Three statistical methods were explored to strengthen the prediction of putative AREs in the human genome. Cross-validation results indicated that all of the three models provided good sensitivity and specificity in identifying AREs, with an accuracy of at least 80%. It is the first time that HMM, SVM and RF have all been applied to constructing ARE prediction models.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.9,Applied computing:0.1,Social and professional topics:0.1",Computing methodologies,"Computing methodologies: The paper uses statistical methods (SVM, HMM, RF) for bioinformatics tasks, which are algorithmic and methodological approaches. Other categories are rejected because the focus is on data mining techniques, not hardware or software engineering.","Artificial intelligence:0.8,Machine learning:0.95,Modeling and simulation:0.75,Computer graphics:0.1,Concurrent computing methodologies:0.1,Distributed computing methodologies:0.1,Parallel computing methodologies:0.1,Symbolic and algebraic manipulation:0.1","Machine learning,Artificial intelligence,Modeling and simulation","Machine learning is highly relevant as the paper explicitly uses SVM, HMM, and RF for ARE prediction. Artificial intelligence is relevant as these methods fall under AI. Modeling and simulation are relevant for the computational approaches used. Other categories like computer graphics or parallel computing are not related to the core contribution of statistical sequence analysis.","Computer vision:0,Control methods:0,Cross-validation:0.5,Distributed artificial intelligence:0,Knowledge representation and reasoning:0,Learning paradigms:0,Learning settings:0,Machine learning algorithms:1,Machine learning approaches:1,Model development and analysis:0,Natural language processing:0,Philosophical/theoretical foundations of artificial intelligence:0,Planning and scheduling:0,Search methodologies:0,Simulation evaluation:0,Simulation support systems:0,Simulation theory:0,Simulation types and techniques:0","Machine learning algorithms,Machine learning approaches,Cross-validation","Machine learning algorithms: The paper employs HMM, SVM, and RF, which are ML algorithms. Machine learning approaches: Data mining tools are used, which are ML approaches. Cross-validation: The evaluation method is mentioned. Other categories like computer vision or NLP are not relevant."
99,Speech pause detection for noise spectrum estimation by tracking power envelope dynamics,"A speech pause detection algorithm is an important and sensitive part of most single-microphone noise reduction schemes for enhancement of speech signals corrupted by additive noise as an estimate of the background noise is usually determined when speech is absent. An algorithm is proposed which detects speech pauses by adaptively tracking minima in a noisy signal's power envelope both for the broadband signal and for the high-pass and low-pass filtered signal. In poor signal-to-noise ratios (SNRs), the proposed algorithm maintains a low false-alarm rate in the detection of speech pauses while the standardized algorithm of ITU G.729 shows an increasing false-alarm rate in unfavorable situations. These characteristics are found with different types of noise and indicate that the proposed algorithm is better suited to be used for noise estimation in noise reduction algorithms, as speech deterioration may thus be kept at a low level. It is shown that in connection with the Ephraim-Malah (1984) noise reduction scheme, the speech pause detection performance can even be further increased by using the noise-reduced signal instead of the noisy signal as input for the speech pause decision unit.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.8,Applied computing:0.7,Social and professional topics:0.1",Computing methodologies,Computing methodologies is relevant for signal processing algorithms. Applied computing is secondary for audio application. Other categories do not address speech processing techniques.,"Artificial intelligence:0.6,Computer graphics:0.1,Concurrent computing methodologies:0.2,Distributed computing methodologies:0.2,Machine learning:0.5,Modeling and simulation:0.8,Parallel computing methodologies:0.2,Symbolic and algebraic manipulation:0.2",Modeling and simulation,Modeling and simulation is highly relevant for the signal processing algorithm. Other AI/ML categories are less central as the paper focuses on signal processing rather than ML model development.,"Model development and analysis:1,Simulation evaluation:1,Simulation support systems:0,Simulation theory:0,Simulation types and techniques:0","Model development and analysis,Simulation evaluation",Model development and analysis: The paper proposes a speech pause detection algorithm. Simulation evaluation: The algorithm's performance is validated through experiments. Other options like Simulation theory are irrelevant.
5016,A New Method of Image Compression using Irreducible Covers of Maximum Rectangles,"The binary-image-compression problem is analyzed using irreducible cover of maximal rectangles. A bound on the minimum-rectangular-cover problem for image compression is given under certain conditions that previously have not been analyzed. It is demonstrated that for a simply connected image, the irreducible cover proposed uses less than four times the number of the rectangles in a minimum cover. With n pixels in a square, the parallel algorithm for obtaining the irreducible cover uses (n/log n) concurrent-read-exclusive write (CREW) processors in O(log n) time. >","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:1.0,Applied computing:0.0,Social and professional topics:0.0",Computing methodologies,Computing methodologies is highly relevant as the paper introduces a novel image compression algorithm using mathematical optimization techniques.,"Artificial intelligence:0.2,Computer graphics:0.7,Concurrent computing methodologies:0.3,Distributed computing methodologies:0.2,Machine learning:0.2,Modeling and simulation:0.4,Parallel computing methodologies:0.8,Symbolic and algebraic manipulation:0.1","Computer graphics,Parallel computing methodologies",Computer graphics: Image compression using geometric partitioning. Parallel computing methodologies: CREW processors for parallel algorithms. Other categories: Not focused on modeling or simulation.,"Animation:0.1,Graphics systems and interfaces:0.2,Image compression:1.0,Image manipulation:0.3,Parallel algorithms:1.0,Parallel programming languages:0.1,Rendering:0.1,Shape modeling:0.1","Image compression,Parallel algorithms",Image compression is directly addressed in the title and abstract. Parallel algorithms are core to the CREW processor implementation. Other fields like Graphics systems or Rendering are not central to the paper's contributions.
199,Visualizing vector field topology in fluid flows,"Methods for automating the analysis and display of vector field topology in general, and flow topology in particular, are described. By using techniques to extract and visualize topological information, it is possible to combine the simplicity of schematic depictions with the quantitative accuracy of curves and surfaces computed directly from the data. Two-dimensional vector field topology is discussed, covering critical points and time-dependent flows, to provide a basis for the examination of topology in three-dimensional separated flows. Surface topology and separation structures in three-dimensional flows are then addressed. The construction of representations of tangent surfaces that are accurate, as well as efficient to compute and display, is examined, covering tessellation, clipping, and refinement. Locating, characterizing, and displaying three-dimensional critical points are considered.<<ETX>>","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.9,Applied computing:0.0,Social and professional topics:0.0",Computing methodologies,"Computing methodologies: The paper presents algorithms for visualizing vector field topology, a core methodological contribution. Other fields like Applied Computing are irrelevant as the focus is on computational techniques rather than real-world applications.","Artificial intelligence:0.0,Computer graphics:1.0,Concurrent computing methodologies:0.0,Distributed computing methodologies:0.0,Machine learning:0.0,Modeling and simulation:1.0,Parallel computing methodologies:0.0,Symbolic and algebraic manipulation:0.0","Computer graphics,Modeling and simulation",Computer graphics is relevant for visualizing vector field topology. Modeling and simulation is relevant for analyzing fluid flow dynamics. Other fields like AI or machine learning are not central to this paper.,"Animation:0,Graphics systems and interfaces:0,Image compression:0,Image manipulation:0,Model development and analysis:1,Rendering:0,Shape modeling:0,Simulation evaluation:1,Simulation support systems:0.5,Simulation theory:0.5,Simulation types and techniques:0.5","Model development and analysis,Simulation evaluation",Model development and analysis is relevant for the vector field topology methods. Simulation evaluation is relevant for the data analysis in fluid flows. Other simulation categories are moderately relevant.
5787,Evaluating the Divergent Auto-Encoder (DIVA) as a Machine Learning Algorithm,"Evaluating the Divergent Auto-Encoder (DIVA) as a Machine Learning Algorithm Kenneth Kurtz Binghamton University (SUNY) Xavier Oyarzabal Binghamton University (SUNY) Abstract: The divergent auto-encoder (Kurtz, 2007) offers an alternative to the multi-layer perceptron (MLP) for classification learning via back-propagation. The artificial neural network classifies based on its success reconstructing the input features (from shared, reduced dimensionality recodings) in terms of a generative model of each category. Successful simulations of rapid human learning of elemental, non-linearly separable category structures suggest potential in machine learning. In a series of simulation studies using benchmarks problems from the UCI database, the divergent autoencoder showed learning and generalization performance comparable to state-of-the-art algorithms with several major advantages: no evidence of overfitting, low sensitivity to parameter settings, and fast runtimes. Discussion focuses on three issues: (1) for what types of problems is the divergent autoencoder better or worse than leading algorithms; (2) comparison with MLP as the default architecture for classification learning with artificial neural networks; (3) comparison with other (Bayesian) generative methods for classification learning.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.9,Applied computing:0.2,Social and professional topics:0.1",Computing methodologies,Computing methodologies is relevant for machine learning algorithm evaluation. Other categories are less relevant to neural network methodology analysis.,"Artificial intelligence:0.7,Computer graphics:0.1,Concurrent computing methodologies:0.1,Distributed computing methodologies:0.1,Machine learning:1.0,Modeling and simulation:0.3,Parallel computing methodologies:0.1,Symbolic and algebraic manipulation:0.1",Machine learning,Machine learning is directly relevant as the paper evaluates a novel auto-encoder algorithm for classification tasks. Artificial intelligence is a secondary relevance due to the broader context of neural networks. Other categories like computer graphics are not relevant.,"Cross-validation:1,Learning paradigms:0.5,Learning settings:0.5,Machine learning algorithms:1,Machine learning approaches:1","Machine learning algorithms,Machine learning approaches,Cross-validation",Machine learning algorithms and approaches are central to the paper's evaluation of the DIVA architecture. Cross-validation is relevant as the study includes benchmark comparisons. Learning paradigms/settings receive moderate scores for their indirect relevance to ML methodology.
1491,Hierarchical dynamic neighborhood based Particle Swarm Optimization for global optimization,"Particle Swarm Optimization (PSO) is arguably one of the most popular nature-inspired algorithms for real parameter optimization at present. In this article, we introduce a new variant of PSO referred to as Hierarchical D-LPSO (Dynamic Local Neighborhood based Particle Swarm Optimization). In this new variant of PSO the particles are arranged following a dynamic hierarchy. Within each hierarchy the particles search for better solution using dynamically varying sub-swarms i.e. these sub-swarms are regrouped frequently and information is exchanged among them. Whether a particle will move up or down the hierarchy depends on the quality of its so-far best-found result. The swarm is largely influenced by the good particles that move up in the hierarchy. The performance of Hierarchical D-LPSO is tested on the set of 25 numerical benchmark functions taken from the competition and special session on real parameter optimization held under IEEE Congress on Evolutionary Computation (CEC) 2005. The results have been compared to those obtained with a few best-known variants of PSO as well as a few significant existing evolutionary algorithms.","General and reference:0.1,Hardware:0.05,Computer systems organization:0.05,Networks:0.1,Software and its engineering:0.2,Theory of computation:0.2,Mathematics of computing:0.1,Information systems:0.3,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.9,Applied computing:0.2,Social and professional topics:0.1",Computing methodologies,Computing methodologies is highly relevant as the paper introduces a novel PSO algorithm for optimization. Information systems is less relevant. Other categories like Theory of computation are not central to the core contribution of algorithm design.,"Artificial intelligence:0.85,Machine learning:0.8,Modeling and simulation:0.75,Concurrent computing methodologies:0.2,Distributed computing methodologies:0.2,Parallel computing methodologies:0.3,Symbolic and algebraic manipulation:0.2","Artificial intelligence,Machine learning,Modeling and simulation",Artificial intelligence is relevant for optimization algorithms. Machine learning is relevant as PSO is a metaheuristic used in ML. Modeling and simulation is relevant for testing on benchmark functions. Other categories are less relevant as the focus is not on concurrency or symbolic methods.,"Computer vision:0.2,Control methods:0.4,Cross-validation:0.3,Distributed artificial intelligence:0.4,Knowledge representation and reasoning:0.3,Learning paradigms:0.5,Learning settings:0.4,Machine learning algorithms:1.0,Machine learning approaches:0.8,Model development and analysis:0.7,Natural language processing:0.2,Philosophical/theoretical foundations of artificial intelligence:0.3,Planning and scheduling:0.3,Search methodologies:0.9,Simulation evaluation:0.8,Simulation support systems:0.4,Simulation theory:0.3,Simulation types and techniques:0.5","Machine learning algorithms,Search methodologies",Machine learning algorithms: Introduces PSO variant for optimization. Search methodologies: Optimization algorithm focus. Other options: Simulation aspects are secondary.
3,Two Phases of V1 Activity for Visual Recognition of Natural Images,"Present theories of visual recognition emphasize the role of interactive processing across populations of neurons within a given network, but the nature of these interactions remains unresolved. In particular, data describing the sufficiency of feedforward algorithms for conscious vision and studies revealing the functional relevance of feedback connections to the striate cortex seem to offer contradictory accounts of visual information processing. TMS is a good method to experimentally address this issue, given its excellent temporal resolution and its capacity to establish causal relations between brain function and behavior. We studied 20 healthy volunteers in a visual recognition task. Subjects were briefly presented with images of animals (birds or mammals) in natural scenes and were asked to indicate the animal category. MRI-guided stereotaxic single TMS pulses were used to transiently disrupt striate cortex function at different times after image onset (SOA). Visual recognition was significantly impaired when TMS was applied over the occipital pole at SOAs of 100 and 220 msec. The first interval has consistently been described in previous TMS studies and is explained as the interruption of the feedforward volley of activity. Given the late latency and discrete nature of the second peak, we hypothesize that it represents the disruption of a feedback projection to V1, probably from other areas in the visual network. These results provide causal evidence for the necessity of recurrent interactive processing, through feedforward and feedback connections, in visual recognition of natural complex images.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.2,Networks:0.1,Software and its engineering:0.3,Theory of computation:0.2,Mathematics of computing:0.3,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.2,Computing methodologies:0.7,Applied computing:0.4,Social and professional topics:0.1",Computing methodologies,Computing methodologies is relevant for the computational modeling of visual recognition processes. Other categories are less relevant as the paper primarily focuses on biological mechanisms rather than computer science techniques.,"Artificial intelligence:0.0,Computer graphics:0.0,Concurrent computing methodologies:0.0,Distributed computing methodologies:0.0,Machine learning:0.0,Modeling and simulation:1.0,Parallel computing methodologies:0.0,Symbolic and algebraic manipulation:0.0",Modeling and simulation,Modeling and simulation is relevant for simulating brain activity through TMS studies. Other categories like AI are not relevant as the focus is on neuroscience research.,"Model development and analysis:0.75,Simulation evaluation:0.25,Simulation support systems:0.25,Simulation theory:0.75,Simulation types and techniques:0.25","Model development and analysis,Simulation theory",Model development and analysis is relevant for creating and testing neural activity models. Simulation theory is relevant for the causal simulation of brain processes. Other simulation categories are not central to the methodology.
236,Designing a resource-allocating codebook for patch-based visual object recognition,"The state-of-the-art approach in visual object recognition is the use of local information extracted at several points or image patches from an image. Local information at specific points can deal with object shape variability and partial occlusions. The underlying idea is that, in different images, the statistical distribution of the patches is different, which can be effectively exploited for recognition. In such a patch-based object recognition system, the key role of a visual codebook is to provide a way to map the low-level features into a fixed-length vector in histogram space to which standard classifiers can be directly applied. The discriminative power of a visual codebook determines the quality of the codebook model, whereas the size of the codebook controls the complexity of the model. Thus, the construction of a codebook plays a central role that affects the model’s complexity. The construction of a codebook is an important step which is usually done by cluster analysis. However, clustering is a process that retains regions of high density in a distribution and it follows that the resulting codebook need not have discriminant properties. This is also recognised as a computational bottleneck of such systems. This thesis demonstrates a novel approach, that we call resource-allocating codebook (RAC), to constructing a discriminant codebook in a one-pass design procedure inspired by the resource-allocation network family of algorithms. The RAC approach slightly outperforms more traditional approaches due to its tendency to spread out the cluster centres over a broader range of the feature space thereby including rare low-level features in the codebook than density-preserving clustering-based codebooks. Our algorithm achieves this performance at drastically reduced computing times, because apart from an initial scan through a small subset to determine length scales, each data item is processed only once. We illustrate some properties of our method and compare it to a closely related approach known as the mean-shift clustering technique. A pruning strategy has been employed to tackle a few outliers when assigning each feature in images to the closest codeword to create a histogram representation for each image. Features whose distance from the closest codeword exceeds an empirical distance maximum are neglected. A recognition system that learns incrementally with training images and the output classifier accounting for class-specific discriminant features is also presented. Furthermore, we address an approach which, instead of clustering, adaptively constructs a codebook by computing Fisher scores between the classes of interest. This thesis also demonstrates a novel sequential hierarchical clustering technique that initially builds a hierarchical tree from a small subset of the data, while the remaining data are processed sequentially and the tree adapted constructively. Evaluations performed with this approach show that the performance is comparable while reducing the computational needs. Finally, during the process of classification, we demonstrate a new learning architecture for multi-class classification tasks using support vector machines. This technique is faster in testing compared to directed acyclic graph (DAG) SVMs, while maintaining comparable performance to the standard multi-class classification techniques.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.0,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.0,Human-centered computing:0.2,Computing methodologies:0.9,Applied computing:0.3,Social and professional topics:0.1",Computing methodologies,"Computing methodologies is highly relevant as the paper presents a novel codebook design algorithm for object recognition. Applied computing (0.3) receives a moderate score for the computer vision application, but the core contribution is methodological.","Artificial intelligence:0.9,Machine learning:1.0,Computer graphics:0.2,Concurrent computing methodologies:0.1,Distributed computing methodologies:0.1,Modeling and simulation:0.3,Parallel computing methodologies:0.1,Symbolic and algebraic manipulation:0.1","Machine learning,Artificial intelligence","Machine learning: The paper introduces a novel algorithm (RAC) for codebook construction, a core machine learning problem. Artificial intelligence: The focus on visual object recognition aligns with AI applications. Computer graphics is irrelevant as the work focuses on algorithm design rather than rendering. Other categories relate to system architecture or symbolic methods, which are not central here.","Computer vision:1,Control methods:0,Cross-validation:0,Distributed artificial intelligence:0,Knowledge representation and reasoning:0,Learning paradigms:0,Learning settings:0,Machine learning algorithms:1,Machine learning approaches:0,Natural language processing:0,Philosophical/theoretical foundations of artificial intelligence:0,Planning and scheduling:0,Search methodologies:0","Computer vision,Machine learning algorithms",Computer vision is relevant as the paper addresses visual object recognition. Machine learning algorithms are relevant for the resource-allocating codebook method. Other categories like control methods or NLP are not discussed.
3610,Structured Relation Discovery using Generative Models,"We explore unsupervised approaches to relation extraction between two named entities; for instance, the semantic bornIn relation between a person and location entity. Concretely, we propose a series of generative probabilistic models, broadly similar to topic models, each which generates a corpus of observed triples of entity mention pairs and the surface syntactic dependency path between them. The output of each model is a clustering of observed relation tuples and their associated textual expressions to underlying semantic relation types. Our proposed models exploit entity type constraints within a relation as well as features on the dependency path between entity mentions. We examine effectiveness of our approach via multiple evaluations and demonstrate 12% error reduction in precision over a state-of-the-art weakly supervised baseline.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:1.0,Applied computing:0.0,Social and professional topics:0.0",Computing methodologies,Computing methodologies is highly relevant for the probabilistic generative models in NLP. Other categories like Information systems are less relevant as the focus is on model development rather than data management.,"Artificial intelligence:0.8,Computer graphics:0.1,Concurrent computing methodologies:0.1,Distributed computing methodologies:0.1,Machine learning:0.9,Modeling and simulation:0.7,Parallel computing methodologies:0.1,Symbolic and algebraic manipulation:0.1","Machine learning,Artificial intelligence",Machine learning is central to the probabilistic generative models for relation extraction. Artificial intelligence is relevant due to the focus on semantic relation discovery. Other categories are not core to the paper's methodology.,"Computer vision:0.2,Control methods:0.1,Cross-validation:0.3,Distributed artificial intelligence:0.2,Knowledge representation and reasoning:1,Learning paradigms:0.7,Learning settings:0.5,Machine learning algorithms:0.8,Machine learning approaches:1,Natural language processing:0.4,Philosophical/theoretical foundations of artificial intelligence:0.1,Planning and scheduling:0.1,Search methodologies:0.3","Knowledge representation and reasoning,Machine learning approaches",Knowledge representation and reasoning is relevant for relation type clustering. Machine learning approaches is relevant for generative probabilistic models. Other fields like NLP and Computer Vision are less relevant as the focus is on relation extraction rather than language processing.
5650,A Best-First Anagram Hashing Filter for Approximate String Matching with Generalized Edit Distance,"This paper presents an efficient method for approximate string matching against a lexicon. We define a filter that for each source word selects a small set of target lexical entries, from which the best match is then selected using generalized edit distance, where edit operations can be assigned an arbitrary weight. The filter combines a specialized hash function with best-first search. Our work extends and improves upon a previously proposed hash-based filter, developed for matching with uniform-weight edit distance. We evaluate an approximate matching system implemented with the new best-first filter, by conducting several experiments on a historical corpus and a set of weighted rules taken from the literature. We present running times and discuss how performance varies using different stopping criteria and target lexica. The results show that the filter is suitable for large rule sets and million word corpora, and encourage further development.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.5,Theory of computation:0.3,Mathematics of computing:0.4,Information systems:0.2,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:1.0,Applied computing:0.3,Social and professional topics:0.1",Computing methodologies,Computing methodologies is highly relevant as the paper introduces an algorithm for approximate string matching. Other categories like Software and its engineering are less relevant as the focus is on the method rather than implementation.,"Artificial intelligence:0.4,Computer graphics:0.3,Concurrent computing methodologies:0.2,Distributed computing methodologies:0.2,Machine learning:0.3,Modeling and simulation:0.2,Parallel computing methodologies:0.2,Symbolic and algebraic manipulation:0.8",Symbolic and algebraic manipulation,Symbolic and algebraic manipulation is highly relevant as the paper introduces a hash-based filter for symbolic string matching. Other categories like Artificial intelligence or Machine learning are not directly addressed.,"Computer algebra systems:0.1,Representation of mathematical objects:0.2,Symbolic and algebraic algorithms:1.0",Symbolic and algebraic algorithms,Symbolic and algebraic algorithms (hash-based approximate string matching with edit distance). No other categories align with algorithmic contributions in string matching.
744,Applying Static and Dynamic Weight Measures in Ensemble Systems,"It is well known that the use of ensemble systems usually increases the accuracy rate of individual machine learning systems. A way of improving the accuracy of these systems even further is through the use of weight measures. This paper analyzes the influence of the use of static and dynamic weights in the accuracy of two structures (homogeneous and heterogeneous) of ensemble systems. Furthermore, it investigates the relation between diversity and the use weights in ensemble system.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:1.0,Applied computing:0.1,Social and professional topics:0.1",Computing methodologies,Computing methodologies is highly relevant as the paper focuses on ensemble learning techniques with weight measures. Applied computing is irrelevant as the focus is on theoretical analysis rather than practical application.,"Artificial intelligence:0.5,Computer graphics:0.0,Concurrent computing methodologies:0.0,Distributed computing methodologies:0.0,Machine learning:1.0,Modeling and simulation:0.0,Parallel computing methodologies:0.0,Symbolic and algebraic manipulation:0.0",Machine learning,Machine learning is directly relevant for ensemble systems with weighted aggregation. Artificial intelligence is marginally relevant but not the primary focus.,"Cross-validation:0.2,Learning paradigms:0.4,Learning settings:0.3,Machine learning algorithms:0.3,Machine learning approaches:1","Machine learning approaches,Learning paradigms","Machine learning approaches: The paper focuses on ensemble systems and weight measures as a methodological approach. Learning paradigms: Discusses static/dynamic weights, touching on learning paradigms. Other categories like Cross-validation are not central to the core contribution."
4739,A Perceptual Image Sharpness Metric Based on Local Edge Gradient Analysis,"In this letter, a no-reference perceptual sharpness metric based on a statistical analysis of local edge gradients is presented. The method takes properties of the human visual system into account. Based on perceptual properties, a relationship between the extracted statistical features and the metric score is established to form a Perceptual Sharpness Index (PSI). A comparison with state-of-the-art metrics shows that the proposed method correlates highly with human perception and exhibits low computational complexity. In contrast to existing metrics, the PSI performs well for a wide range of blurriness and shows a high degree of invariance for different image contents.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:1.0,Applied computing:0.2,Social and professional topics:0.1",Computing methodologies,"Computing methodologies is directly relevant as the paper introduces a perceptual image sharpness metric based on edge gradient analysis, a core image processing task. Other fields are irrelevant as the focus is on the method, not applications.","Artificial intelligence:0.75,Computer graphics:1.0,Concurrent computing methodologies:0.1,Distributed computing methodologies:0.1,Machine learning:0.2,Modeling and simulation:0.3,Parallel computing methodologies:0.1,Symbolic and algebraic manipulation:0.1",Computer graphics,Computer graphics is directly relevant for image sharpness metric development. AI is secondary for perceptual modeling.,"Animation:0.1,Graphics systems and interfaces:0.2,Image compression:0.1,Image manipulation:0.8,Rendering:0.1,Shape modeling:0.3",Image manipulation,Image manipulation: The paper presents a sharpness metric used to evaluate and manipulate image quality. Other options like shape modeling are only tangentially related to edge gradient analysis.
6060,EBEK: Exemplar-Based Kernel Preserving Embedding,"With the rapid increase in the available data, it becomes computationally harder to extract useful information. Thus, several techniques like PCA were proposed to embed high-dimensional data into low-dimensional latent space. However, these techniques don't take the data relations into account. This motivated the development of other techniques like MDS and LLE which preserve the relations between the data instances. Nonetheless, all these techniques still use latent features, which are difficult for data analysts to understand and grasp the information encoded in them. In this work, a new embedding technique is proposed to mitigate the previous problems by projecting the data to a space described by few points (i.e, exemplars) which preserves the relations between the data points. The proposed method Exemplar-based Kernel Preserving (EBEK) embedding is shown theoretically to achieve the lowest reconstruction error of the kernel matrix. Using EBEK in approximate nearest neighbor task shows its ability to outperform related work by up to 60% in the recall while maintaining a good running time. In addition, our interpretability experiments show that EBEK's selected basis are more understandable than the latent basis in images datasets.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.9,Applied computing:0.2,Social and professional topics:0.1",Computing methodologies,Computing methodologies is relevant as the paper introduces a novel data embedding technique (EBEK). Other categories are less relevant as the focus is on algorithm design for dimensionality reduction rather than application domains or hardware.,"Artificial intelligence:0.2,Computer graphics:0.05,Concurrent computing methodologies:0.05,Distributed computing methodologies:0.05,Machine learning:1.0,Modeling and simulation:0.3,Parallel computing methodologies:0.05,Symbolic and algebraic manipulation:0.05",Machine learning,Machine learning: The paper introduces a novel ML-based embedding technique (EBEK). Modeling and simulation is secondary due to algorithmic analysis. Other categories are irrelevant to the core contribution.,"Cross-validation:0.3,Learning paradigms:0.4,Learning settings:0.3,Machine learning algorithms:1.0,Machine learning approaches:1.0","Machine learning algorithms,Machine learning approaches",Machine learning algorithms and approaches are central to the proposed EBEK embedding technique. Cross-validation is secondary for evaluation.
2603,A layered method for determining manga text bubble reading order,"Comic books of all cultures are an active research area as digitizing content for mobile and web is becoming more common. Past research on comics has largely concentrated on text extraction, panel segmentation and document analysis, while the utilisation of the extracted data has had less attention. In this paper we present a method to automatically determine the reading order of Japanese manga text bubbles using only text bubble position and image data. Our method classifies and orders page and text position information on three layers, which are hierarchically sorted to obtain the final ordering. The method is evaluated on a data set of 1769 manga pages with 14726 manually annotated text positions and correct ordering. Evaluation shows the method has over 95% transition accuracies and vastly outperforms a naive implementation.","General and reference:0,Hardware:0,Computer systems organization:0,Networks:0,Software and its engineering:0,Theory of computation:0,Mathematics of computing:0.2,Information systems:0.3,Security and privacy:0,Human-centered computing:0,Computing methodologies:1,Applied computing:0.1,Social and professional topics:0",Computing methodologies,"Computing methodologies is directly relevant as the paper presents a layered algorithm for text bubble ordering. Information systems receives partial relevance for document analysis, but the core contribution is algorithmic.","Artificial intelligence:0.1,Computer graphics:1.0,Concurrent computing methodologies:0.1,Distributed computing methodologies:0.1,Machine learning:0.3,Modeling and simulation:0.2,Parallel computing methodologies:0.1,Symbolic and algebraic manipulation:0.1",Computer graphics,Computer graphics is highly relevant as the paper focuses on manga text bubble layout analysis. Other categories like machine learning are less central to the image-based reading order determination.,"Animation:0.0,Graphics systems and interfaces:0.3,Image compression:0.0,Image manipulation:0.8,Rendering:0.2,Shape modeling:0.1","Image manipulation,Graphics systems and interfaces","Image manipulation (text bubble position analysis), Graphics systems and interfaces (layered hierarchical sorting). Rendering and shape modeling are less relevant."
4085,Complex curve tracing based on a minimum spanning tree model and regularized fuzzy clustering,"The fuzzy curve-tracing (FCT) algorithm can be used to extract a smooth curve from unordered noisy data. However, the model produces good results only if the curve shape is either opened or closed. In this paper, we propose several techniques to generalize the FCT algorithm for tracing complicated curves. We develop a modified clustering algorithm that can produce cluster centers less dependent on the pre-specified number of clusters, which makes the reordering of cluster centers easier. We make use of the Eikonal equation and the Prim's algorithm to form the initial curve, which may contain sharp corners and intersections. We also introduce a more powerful curve smoothing method. Our generalized FCT algorithm is able to trace a wide range of complicated curves, such as handwritten Chinese characters.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.2,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.9,Applied computing:0.0,Social and professional topics:0.0",Computing methodologies,"Computing methodologies is highly relevant because the paper introduces algorithms for complex curve tracing, including clustering and smoothing techniques. Mathematics of computing is less relevant as the focus is on computational methods for data processing.","Artificial intelligence:0.0,Computer graphics:1.0,Concurrent computing methodologies:0.0,Distributed computing methodologies:0.0,Machine learning:1.0,Modeling and simulation:0.5,Parallel computing methodologies:0.0,Symbolic and algebraic manipulation:0.0","Computer graphics,Machine learning",Computer graphics is directly relevant for curve-tracing algorithms. Machine learning is relevant for the clustering techniques used. Modeling and simulation receives a moderate score as the process involves algorithmic modeling but is not the primary focus.,"Animation:0,Cross-validation:0,Graphics systems and interfaces:0,Image compression:0,Image manipulation:1,Learning paradigms:0,Learning settings:0,Machine learning algorithms:1,Machine learning approaches:1,Rendering:0,Shape modeling:1","Image manipulation,Machine learning algorithms,Shape modeling",Image manipulation: Curve extraction from noisy data is a core task. Machine learning algorithms: Clustering and smoothing techniques are used. Shape modeling: The algorithm handles complex curve topologies. Other categories like 'Animation' are irrelevant to static image processing.
1848,Unstructured Overlapping Mesh Distribution in Parallel,"We present a simple mathematical framework and API for parallel mesh and data distribution, load balancing, and overlap generation. It relies on viewing the mesh as a Hasse diagram, abstracting away information such as cell shape, dimension, and coordinates. The high level of abstraction makes our interface both concise and powerful, as the same algorithm applies to any representable mesh, such as hybrid meshes, meshes embedded in higher dimension, and overlapped meshes in parallel. We present evidence, both theoretical and experimental, that the algorithms are scalable and efficient. A working implementation can be found in the latest release of the PETSc libraries.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.2,Theory of computation:0.3,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.9,Applied computing:0.2,Social and professional topics:0.1",Computing methodologies,Computing methodologies: The paper introduces a novel framework for parallel mesh distribution and load balancing using abstract mathematical models. Theory of computation (0.3) is secondary as the focus is on algorithmic methodology rather than theoretical foundations.,"Artificial intelligence:0.1,Computer graphics:0.1,Concurrent computing methodologies:0.1,Distributed computing methodologies:0.7,Machine learning:0.1,Modeling and simulation:0.2,Parallel computing methodologies:0.85,Symbolic and algebraic manipulation:0.1","Distributed computing methodologies,Parallel computing methodologies",Distributed computing methodologies is relevant for the parallel mesh distribution framework. Parallel computing methodologies is highly relevant since the paper introduces scalable algorithms for parallel computing environments. Other categories like modeling and simulation are less central to the core contribution of the work.,"Distributed algorithms:1,Distributed programming languages:0.2,Parallel algorithms:1,Parallel programming languages:0.3","Distributed algorithms,Parallel algorithms",Distributed algorithms and Parallel algorithms are both central to the mesh distribution framework. Language-specific categories are less relevant as the focus is on algorithmic design.
5180,Combinatorial Multi-Armed Bandit with General Reward Functions,"In this paper, we study the stochastic combinatorial multi-armed bandit (CMAB) framework that allows a general nonlinear reward function, whose expected value may not depend only on the means of the input random variables but possibly on the entire distributions of these variables. Our framework enables a much larger class of reward functions such as the max() function and nonlinear utility functions. Existing techniques relying on accurate estimations of the means of random variables, such as the upper confidence bound (UCB) technique, do not work directly on these functions. We propose a new algorithm called stochastically dominant confidence bound (SDCB), which estimates the distributions of underlying random variables and their stochastically dominant confidence bounds. We prove that SDCB can achieve O(log T) distribution-dependent regret and O(√T) distribution-independent regret, where T is the time horizon. We apply our results to the K-MAX problem and expected utility maximization problems. In particular, for K-MAX, we provide the first polynomial-time approximation scheme (PTAS) for its offline problem, and give the first O(√T) bound on the (1 — e)-approximation regret of its online problem, for any e > 0.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.3,Mathematics of computing:0.6,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:1.0,Applied computing:0.0,Social and professional topics:0.0",Computing methodologies,Computing methodologies: The paper presents a new algorithm (SDCB) for multi-armed bandit problems. Theory of computation (0.3) and Mathematics of computing (0.6) are secondary due to the algorithm's theoretical analysis.,"Artificial intelligence:0.75,Computer graphics:0.0,Concurrent computing methodologies:0.0,Distributed computing methodologies:0.0,Machine learning:1.0,Modeling and simulation:0.2,Parallel computing methodologies:0.0,Symbolic and algebraic manipulation:0.0","Machine learning,Artificial intelligence","Machine learning: The paper introduces a novel algorithm (SDCB) for combinatorial multi-armed bandits, a core topic in reinforcement learning. Artificial intelligence: Bandit algorithms are foundational to AI decision-making under uncertainty. Other fields: The work has no connection to graphics, concurrency, or symbolic computation.","Computer vision:0.1,Control methods:0.3,Cross-validation:0.2,Distributed artificial intelligence:0.1,Knowledge representation and reasoning:0.2,Learning paradigms:0.3,Learning settings:0.2,Machine learning algorithms:1.0,Machine learning approaches:1.0,Natural language processing:0.1,Philosophical/theoretical foundations of artificial intelligence:0.2,Planning and scheduling:0.1,Search methodologies:0.3","Machine learning algorithms,Machine learning approaches",Machine learning algorithms is relevant as the paper introduces the SDCB algorithm. Machine learning approaches is relevant due to the framework for general reward functions. Other categories are less central to the paper's focus.
4916,A parallel architecture for quadtree-based fractal image coding,"This paper proposes a parallel architecture for quadtree-based fractal image coding. This architecture is capable of performing the fractal image coding based on quadtree partitioning without the external memory for the fixed domain pool. Since a large domain block consists of small domain blocks, the calculations of distortion for all kinds of domain blocks are performed by the summation of the distortions for the maximum-depth domain pool which is extracted from the smallest range blocks of the neighbor processors. Fast comparison module is proposed for this architecture. This module can compute the distortions between range blocks and their eight isometric transformations by one full rotation around the center.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:1.0,Applied computing:0.1,Social and professional topics:0.1",Computing methodologies,"Computing methodologies: The paper proposes a parallel architecture for fractal image coding using quadtree partitioning. Other categories are irrelevant as the focus is on algorithm design and architecture optimization, not hardware or applications.","Artificial intelligence:0.1,Computer graphics:0.3,Concurrent computing methodologies:0.2,Distributed computing methodologies:0.2,Machine learning:0.1,Modeling and simulation:0.2,Parallel computing methodologies:1.0,Symbolic and algebraic manipulation:0.4",Parallel computing methodologies,Parallel computing methodologies is highly relevant as the paper's core contribution is a novel parallel architecture for fractal image coding. Other categories like Computer graphics are secondary to the parallel computing focus.,"Parallel algorithms:1.0,Parallel programming languages:0.2",Parallel algorithms,"Parallel algorithms is highly relevant for the proposed architecture and its computational approach. Parallel programming languages is not discussed in the paper, which focuses on hardware architecture rather than language design."
353,Realtime Aesthetic Image Retargeting,"Humans have always sought to achieve aesthetics in art. In this paper, we present a novel approach for retargeting images to different aspect ratios while improving the composition aesthetics of the results. A simpler computational aesthetic energy is proposed and used to drive the salient objects and prominent lines to move towards their corresponding optimal positions. A mesh-based warping scheme is presented to transform the images while protecting the visual appearance of salient objects. The objective function is quadratic and thus it can be quickly minimized by solving a sparse linear system. The retargeting results are generated in realtime while the user changes the aspect ratios of the target images. A variety of experiments have shown the applicability and effectiveness of our algorithm.","General and reference:0,Hardware:0,Computer systems organization:0,Networks:0,Software and its engineering:0,Theory of computation:0,Mathematics of computing:0.5,Information systems:0,Security and privacy:0,Human-centered computing:0,Computing methodologies:1,Applied computing:0,Social and professional topics:0",Computing methodologies,"Computing methodologies is highly relevant because the paper presents a novel image retargeting algorithm using mesh-based warping and quadratic optimization, which are core computer graphics techniques. Other categories like Mathematics of computing are only peripherally relevant as the focus is on algorithmic application, not mathematical theory.","Computer graphics:1.0,Artificial intelligence:0.7,Machine learning:0.6,Modeling and simulation:0.5,Concurrent computing methodologies:0.1,Distributed computing methodologies:0.1,Parallel computing methodologies:0.1,Symbolic and algebraic manipulation:0.1","Computer graphics,Artificial intelligence",Computer graphics is core as the paper introduces an image retargeting algorithm. Artificial intelligence is relevant due to the aesthetic energy formulation for composition optimization. Machine learning is somewhat relevant but less central than AI. Other options are unrelated to the visual processing focus.,"Animation:0,Computer vision:1,Control methods:0,Distributed artificial intelligence:0,Graphics systems and interfaces:0.3,Image compression:0,Image manipulation:1,Knowledge representation and reasoning:0,Natural language processing:0,Philosophical/theoretical foundations of artificial intelligence:0,Planning and scheduling:0,Rendering:0.2,Search methodologies:0,Shape modeling:0","Image manipulation,Computer vision",Image manipulation: The paper proposes a novel image retargeting technique using mesh-based warping. Computer vision: Aesthetic energy and salient object detection involve computer vision principles. 'Graphics systems and interfaces' is rejected as the focus is on image content rather than rendering systems.
2477,Incorporating Rule-based and Statistic-based Techniques for Coreference Resolution,"This paper describes a coreference resolution system for CONLL 2012 shared task developed by HLT_HITSZ group, which incorporates rule-based and statistic-based techniques. The system performs coreference resolution through the mention pair classification and linking. For each detected mention pairs in the text, a Decision Tree (DT) based binary classifier is applied to determine whether they form a coreference. This classifier incorporates 51 and 61 selected features for English and Chinese, respectively. Meanwhile, a rule-based classifier is applied to recognize some specific types of coreference, especially the ones with long distances. The outputs of these two classifiers are merged. Next, the recognized coreferences are linked to generate the final coreference chain. This system is evaluated on English and Chinese sides (Closed Track), respectively. It achieves 0.5861 and 0.6003 F1 score on the development data of English and Chinese, respectively. As for the test dataset, the achieved F1 scores are 0.5749 and 0.6508, respectively. This encouraging performance shows the effectiveness of our proposed coreference resolution system.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:1.0,Applied computing:0.1,Social and professional topics:0.1",Computing methodologies,Computing methodologies is highly relevant as the paper presents coreference resolution techniques combining decision trees and rule-based methods. Other fields like Information systems are irrelevant as the focus is on NLP algorithms rather than data management systems.,"Artificial intelligence:0.8,Computer graphics:0.0,Concurrent computing methodologies:0.0,Distributed computing methodologies:0.0,Machine learning:1.0,Modeling and simulation:0.2,Parallel computing methodologies:0.0,Symbolic and algebraic manipulation:0.0","Machine learning,Artificial intelligence",Machine learning: The paper uses Decision Tree classifiers for coreference resolution. Artificial intelligence: Coreference resolution is a natural language processing task in AI. Other categories like Modeling and simulation are not relevant.,"Computer vision:0.1,Control methods:0.1,Cross-validation:0.3,Distributed artificial intelligence:0.1,Knowledge representation and reasoning:0.4,Learning paradigms:0.3,Learning settings:0.3,Machine learning algorithms:1.0,Machine learning approaches:1.0,Natural language processing:1.0,Philosophical/theoretical foundations of artificial intelligence:0.2,Planning and scheduling:0.1,Search methodologies:0.3","Natural language processing,Machine learning algorithms",Natural language processing: Coreference resolution is a fundamental NLP task. Machine learning algorithms: The paper combines decision tree classifiers with rule-based methods. 'Machine learning approaches' received similar relevance for the hybrid methodology.
2335,"Flocks, herds and schools: A distributed behavioral model","The aggregate motion of a flock of birds, a herd of land animals, or a school of fish is a beautiful and familiar part of the natural world. But this type of complex motion is rarely seen in computer animation. This paper explores an approach based on simulation as an alternative to scripting the paths of each bird individually. The simulated flock is an elaboration of a particle systems, with the simulated birds being the particles. The aggregate motion of the simulated flock is created by a distributed behavioral model much like that at work in a natural flock; the birds choose their own course. Each simulated bird is implemented as an independent actor that navigates according to its local perception of the dynamic environment, the laws of simulated physics that rule its motion, and a set of behaviors programmed into it by the animator. The aggregate motion of the simulated flock is the result of the dense interaction of the relatively simple behaviors of the individual simulated birds.""","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:1.0,Applied computing:0.75,Social and professional topics:0.0",Computing methodologies,Computing methodologies is highly relevant for the distributed behavioral simulation model. Applied computing (0.75) is relevant for the animation application but is secondary to the methodological contribution. Other categories like Theory of computation are less directly related.,"Computer graphics:1.0,Distributed computing methodologies:0.8,Artificial intelligence:0.6,Modeling and simulation:0.5,Concurrent computing methodologies:0.3,Machine learning:0.2,Parallel computing methodologies:0.2,Symbolic and algebraic manipulation:0.1","Computer graphics,Distributed computing methodologies",Computer graphics is directly relevant for simulating flocking behavior in animation. Distributed computing methodologies are relevant for the decentralized behavioral model. Artificial intelligence is secondary as the focus is on simulation rather than AI-specific techniques.,"Animation:1,Distributed algorithms:0.8,Distributed programming languages:0,Graphics systems and interfaces:0,Image compression:0,Image manipulation:0,Rendering:0,Shape modeling:0","Animation,Distributed algorithms",Animation is directly relevant as the paper presents a distributed behavioral model for simulating flocking behavior. Distributed algorithms receive moderate relevance due to the decentralized decision-making process in the model. Other graphics-related categories are irrelevant as the focus is on animation techniques rather than rendering or image processing.
3416,Fast edge-preserving gravity-like image interpolation,"In this paper we propose a novel image interpolation algorithm which preserves edges and keeps a natural texture of interpolated images. The algorithm is based on an idea that only pixels that belong to the same side of an edge should be used in interpolation of pixels that belong to an edge. Beside similarity-based separation of known interpolation pixels a gravity-like interpolation coefficient set is also introduced in order to support different number of interpolation pixels and their location in two dimensional plane. The algorithm also applies arbitrary scaling factors, thus offering a broader scope of applications. Use of a local set of interpolating points makes the proposed algorithm suitable for applications on resource-limited platforms. The edge performance is demonstrated for structured geometric forms, while a general interpolated image quality is evaluated using objective measures and subjective comparisons. A comparison with some relevant interpolation algorithms shows the desirable tradeoff between image quality (sharpness and texture) and requested computing power (run-time).","General and reference:0,Hardware:0,Computer systems organization:0,Networks:0,Software and its engineering:0.3,Theory of computation:0,Mathematics of computing:0.2,Information systems:0.1,Security and privacy:0,Human-centered computing:0,Computing methodologies:1,Applied computing:0.6,Social and professional topics:0",Computing methodologies,Computing methodologies is highly relevant for the edge-preserving image interpolation algorithm. Applied computing is marginally relevant for resource-limited platform applications. Other categories are irrelevant as the focus is on image processing algorithms.,"Artificial intelligence:0.2,Computer graphics:0.9,Concurrent computing methodologies:0.2,Distributed computing methodologies:0.2,Machine learning:0.2,Modeling and simulation:0.2,Parallel computing methodologies:0.2,Symbolic and algebraic manipulation:0.2",Computer graphics,"Computer graphics: The paper proposes an image interpolation algorithm for edge preservation, a core graphics problem. Other categories like AI or symbolic manipulation are not central to the interpolation method.","Animation:0,Graphics systems and interfaces:0,Image compression:0,Image manipulation:1,Rendering:0,Shape modeling:0",Image manipulation,Image manipulation is relevant because the paper introduces a new image interpolation algorithm. Other children like Graphics systems and interfaces are irrelevant since the paper focuses on interpolation techniques rather than graphics systems.
504,A Bootstrapping Approach to Named Entity Classification Using Successive Learners,"This paper presents a new bootstrapping approach to named entity (NE) classification. This approach only requires a few common noun/pronoun seeds that correspond to the concept for the target NE type, e.g. he/she/man/woman for PERSON NE. The entire bootstrapping procedure is implemented as training two successive learners: (i) a decision list is used to learn the parsing-based high precision NE rules; (ii) a Hidden Markov Model is then trained to learn string sequence-based NE patterns. The second learner uses the training corpus automatically tagged by the first learner. The resulting NE system approaches supervised NE performance for some NE types. The system also demonstrates intuitive support for tagging user-defined NE types. The differences of this approach from the co-training-based NE bootstrapping are also discussed.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.2,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:1.0,Applied computing:0.1,Social and professional topics:0.1",Computing methodologies,The paper presents a novel machine learning methodology for named entity classification using decision lists and HMMs. Computing methodologies is directly relevant as it describes the algorithmic approach. Other fields like Networks or Security are not discussed in the context of this work.,"Artificial intelligence:0.7,Computer graphics:0.1,Concurrent computing methodologies:0.1,Distributed computing methodologies:0.1,Machine learning:1.0,Modeling and simulation:0.2,Parallel computing methodologies:0.1,Symbolic and algebraic manipulation:0.1","Machine learning,Artificial intelligence",Machine learning is the primary category as the paper uses decision lists and HMMs for NE classification. Artificial intelligence is relevant due to the NLP application. Categories like 'Computer graphics' are less relevant as the focus is on text classification rather than visual algorithms.,"Computer vision:0.0,Control methods:0.0,Cross-validation:0.2,Distributed artificial intelligence:0.0,Knowledge representation and reasoning:0.1,Learning paradigms:0.1,Learning settings:0.1,Machine learning algorithms:0.3,Machine learning approaches:1.0,Natural language processing:1.0,Philosophical/theoretical foundations of artificial intelligence:0.0,Planning and scheduling:0.0,Search methodologies:0.0","Natural language processing,Machine learning approaches",Natural language processing is the primary domain as the paper focuses on named entity classification. Machine learning approaches is relevant because the paper uses a successive learner strategy combining decision lists and HMMs. Other options like 'Machine learning algorithms' are less relevant as the focus is on the overall approach rather than specific algorithms.
1997,A Faster Patch Ordering Method for Image Denoising,"Among the patch-based image denoising processing methods, smooth ordering of local patches (patch ordering) has been shown to give state-of-art results. For image denoising the patch ordering method forms two large TSPs (Traveling Salesman Problem) comprised of nodes in N-dimensional space. Ten approximate solutions of the two large TSPs are then used in a filtering process to form the reconstructed image. Use of large TSPs makes patch ordering a computationally intensive method. A modified patch ordering method for image denoising is proposed. In the proposed method, several smaller-sized TSPs are formed and the filtering process varied to work with solutions of these smaller TSPs. In terms of PSNR, denoising results of the proposed method differed by 0.032 dB to 0.016 dB on average. In original method, solving TSPs was observed to consume 85% of execution time. In proposed method, the time for solving TSPs can be reduced to half of the time required in original method. The proposed method can denoise images in 40% less time.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.8,Applied computing:0.1,Social and professional topics:0.1",Computing methodologies,Computing methodologies: The paper introduces a faster patch ordering method for image denoising using computational techniques. Other categories like Information systems are less relevant as the focus is on algorithmic improvements for image processing.,"Artificial intelligence:0.1,Computer graphics:0.9,Concurrent computing methodologies:0.1,Distributed computing methodologies:0.1,Machine learning:0.1,Modeling and simulation:0.1,Parallel computing methodologies:0.1,Symbolic and algebraic manipulation:0.1",Computer graphics,"Computer graphics: The paper optimizes an image denoising method, which is a core topic in computer graphics. Other options like Parallel computing are not the primary focus of the algorithmic improvement described.","Image manipulation:1,Image compression:0.3,Rendering:0.2",Image manipulation,"Image manipulation is relevant as the paper improves image denoising techniques. Image compression is only peripherally relevant as the focus is on denoising, not compression. Rendering is not relevant to this work."
4390,Unsupervised Multi-class Joint Image Segmentation,"Joint segmentation of image sets is a challenging problem, especially when there are multiple objects with variable appearance shared among the images in the collection and the set of objects present in each particular image is itself varying and unknown. In this paper, we present a novel method to jointly segment a set of images containing objects from multiple classes. We first establish consistent functional maps across the input images, and introduce a formulation that explicitly models partial similarity across images instead of global consistency. Given the optimized maps between pairs of images, multiple groups of consistent segmentation functions are found such that they align with segmentation cues in the images, agree with the functional maps, and are mutually exclusive. The proposed fully unsupervised approach exhibits a significant improvement over the state-of-the-art methods, as shown on the co-segmentation data sets MSRC, Flickr, and PASCAL.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.8,Applied computing:0.1,Social and professional topics:0.1",Computing methodologies,Computing methodologies (unsupervised image segmentation algorithms) is the core contribution. Software and its engineering is secondary as the focus is on algorithm development rather than system design.,"Artificial intelligence:0.6,Computer graphics:0.1,Concurrent computing methodologies:0.1,Distributed computing methodologies:0.1,Machine learning:0.7,Modeling and simulation:0.2,Parallel computing methodologies:0.1,Symbolic and algebraic manipulation:0.1","Artificial intelligence,Machine learning",Artificial intelligence and Machine learning are relevant for unsupervised image segmentation. Other categories are rejected as they focus on graphics or unrelated methodologies.,"Computer vision:1.0,Control methods:0.2,Cross-validation:0.3,Distributed artificial intelligence:0.3,Knowledge representation and reasoning:0.2,Learning paradigms:0.5,Learning settings:0.4,Machine learning algorithms:1.0,Machine learning approaches:0.7,Natural language processing:0.1,Philosophical/theoretical foundations of artificial intelligence:0.2,Planning and scheduling:0.2,Search methodologies:0.3","Computer vision,Machine learning algorithms",Computer vision is directly relevant as the paper focuses on image segmentation. Machine learning algorithms is relevant for the unsupervised learning approach. 'Learning paradigms' was rejected as the focus is on algorithmic implementation rather than paradigms.
1329,A recursive approach to reconstruction of sparse signals,"Compressive Sensing (CS) theory details how a sparsely represented signal in a known basis can be reconstructed using less number of measurements. In many practical systems, the observation signal has a sparse representation in a continuous parameter space. This situation rises the possibility of use of the CS reconstruction techniques in the practical problems. In order to utilize CS techniques, the continuous parameter space have to be discretized. This discritization brings the well-known off-grid problem. To prevent the off-grid problem, this study offers a recursive approach which discritizes the parameter space in an adaptive manner. The simulations show that the proposed approach can estimate the parameters with a high accuracy even if targets are closely spaced.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.75,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:1.0,Applied computing:0.1,Social and professional topics:0.1",Computing methodologies,"Computing methodologies is highly relevant as the paper introduces a recursive algorithm for compressive sensing, a core methodological contribution in signal processing and computational techniques. Other categories are irrelevant as the paper does not address hardware, systems, theory, or applied domains.","Artificial intelligence:0.1,Computer graphics:0.2,Concurrent computing methodologies:0.1,Distributed computing methodologies:0.1,Machine learning:0.1,Modeling and simulation:0.8,Parallel computing methodologies:0.1,Symbolic and algebraic manipulation:0.1",Modeling and simulation,Modeling and simulation is relevant because the paper presents a recursive approach to sparse signal reconstruction validated through simulations. Other fields like AI or ML are not central to the method's core contribution.,"Model development and analysis:1.0,Simulation evaluation:0.8,Simulation support systems:0.4,Simulation theory:0.5,Simulation types and techniques:0.6","Model development and analysis,Simulation evaluation","Model development and analysis: The paper introduces a recursive approach for sparse signal reconstruction, a core model development contribution. Simulation evaluation: Simulations validate the method's accuracy. Categories like Simulation theory were rejected because the paper focuses on application rather than theoretical simulation frameworks."
3854,An Algorithm for Anaphora Resolution in Spanish Texts,"This paper presents an algorithm for identifying noun phrase antecedents of third person personal pronouns, demonstrative pronouns, reflexive pronouns, and omitted pronouns (zero pronouns) in unrestricted Spanish texts. We define a list of constraints and preferences for different types of pronominal expressions, and we document in detail the importance of each kind of knowledge (lexical, morphological, syntactic, and statistical) in anaphora resolution for Spanish. The paper also provides a definition for syntactic conditions on Spanish NP-pronoun noncoreference using partial parsing. The algorithm has been evaluated on a corpus of 1,677 pronouns and achieved a success rate of 76.8. We have also implemented four competitive algorithms and tested their performance in a blind evaluation on the same test corpus. This new approach could easily be extended to other languages such as English, Portuguese, Italian, or Japanese.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.9,Applied computing:0.1,Social and professional topics:0.1",Computing methodologies,"Computing methodologies: The paper presents an anaphora resolution algorithm, a core NLP/ML methodology contribution. Other categories like Information systems are less directly relevant.","Artificial intelligence:0.9,Computer graphics:0.2,Concurrent computing methodologies:0.3,Distributed computing methodologies:0.4,Machine learning:0.7,Modeling and simulation:0.5,Parallel computing methodologies:0.3,Symbolic and algebraic manipulation:0.2","Artificial intelligence,Machine learning",Artificial intelligence is central as the paper presents an NLP algorithm for anaphora resolution. Machine learning is secondary due to the use of statistical methods. Other categories like Computer graphics are irrelevant.,"Computer vision:0.6,Control methods:0.1,Cross-validation:0.7,Distributed artificial intelligence:0.2,Knowledge representation and reasoning:0.3,Learning paradigms:0.3,Learning settings:0.3,Machine learning algorithms:0.8,Machine learning approaches:0.7,Natural language processing:0.9,Philosophical/theoretical foundations of artificial intelligence:0.1,Planning and scheduling:0.1,Search methodologies:0.2","Natural language processing,Machine learning algorithms",Natural language processing (0.9): The paper presents an anaphora resolution algorithm for Spanish texts. Machine learning algorithms (0.8): The approach uses statistical methods and algorithmic constraints for NLP tasks.
2871,Online Real-Time Multiple Spatiotemporal Action Localisation and Prediction,"We present a deep-learning framework for real-time multiple spatio-temporal (S/T) action localisation and classification. Current state-of-the-art approaches work offline, and are too slow to be useful in real-world settings. To overcome their limitations we introduce two major developments. Firstly, we adopt real-time SSD (Single Shot Multi-Box Detector) CNNs to regress and classify detection boxes in each video frame potentially containing an action of interest. Secondly, we design an original and efficient online algorithm to incrementally construct and label ‘action tubes’ from the SSD frame level detections. As a result, our system is not only capable of performing S/T detection in real time, but can also perform early action prediction in an online fashion. We achieve new state-of-the-art results in both S/T action localisation and early action prediction on the challenging UCF101-24 and J-HMDB-21 benchmarks, even when compared to the top offline competitors. To the best of our knowledge, ours is the first real-time (up to 40fps) system able to perform online S/T action localisation on the untrimmed videos of UCF101-24.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.2,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.8,Applied computing:0.2,Social and professional topics:0.1",Computing methodologies,Computing methodologies: Develops a deep-learning framework for real-time spatiotemporal action detection. Other fields are irrelevant as the focus is on machine learning algorithms and real-time processing.,"Artificial intelligence:0.75,Computer graphics:0.1,Concurrent computing methodologies:0.1,Distributed computing methodologies:0.1,Machine learning:0.8,Modeling and simulation:0.2,Parallel computing methodologies:0.1,Symbolic and algebraic manipulation:0.1","Artificial intelligence,Machine learning",Artificial intelligence: Real-time action localization and prediction are AI tasks. Machine learning: The framework uses deep learning (SSD CNNs). Modeling is secondary. Other categories like graphics are not central.,"Computer vision:0.9,Control methods:0.1,Cross-validation:0.1,Distributed artificial intelligence:0.1,Knowledge representation and reasoning:0.1,Learning paradigms:0.6,Learning settings:0.5,Machine learning algorithms:0.8,Machine learning approaches:0.3,Natural language processing:0.1,Philosophical/theoretical foundations of artificial intelligence:0.1,Planning and scheduling:0.1,Search methodologies:0.1","Computer vision,Machine learning algorithms",Computer vision is the primary domain as the paper introduces a real-time action localization system. Machine learning algorithms are relevant due to the use of SSD CNNs for detection. Learning paradigms are secondary because the paper emphasizes online learning techniques. Other categories like Control methods or Planning are not central to the paper's contribution.
3661,Mining Actor-level Structural and Neighborhood Evolution for Link Prediction in Dynamic Networks,"Link prediction problem in network science has experienced extensive methodological improvements and simultaneously, spawned over numerous applications. In relation to evolutionary network analysis, different dynamic link prediction methods in network science not only support the prediction of future links but also assist in modelling network dynamics. The concept of constructing dynamic similarity metrics by considering the actor-level evolution of network structure and associated neighborhoods has been widely ignored for the purpose of dynamic link prediction. This study attempts to propose two dynamic similarity metrics for the purpose of dynamic link prediction in longitudinal networks through mining evolutionary information. These metrics consider the similarity between network structural and neighborhood changes over time incident to non-connected actor pairs. These metrics are then used as dynamic features in supervised link prediction model and performances are compared against two baseline static similarity metrics (i.e., AdamicAdar and Katz). Higher performance scores achieved by these features, examined in this study, exemplifies them as prospective candidates not only for dynamic link prediction task but also in understanding the growth pattern of dynamic networks.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.9,Applied computing:0.1,Social and professional topics:0.1",Computing methodologies,Computing methodologies is highly relevant as the paper presents novel dynamic similarity metrics for link prediction in networks. Other fields are irrelevant as the focus is on algorithmic methodology for network analysis.,"Artificial intelligence:0.2,Computer graphics:0.2,Concurrent computing methodologies:0.1,Distributed computing methodologies:0.2,Machine learning:0.8,Modeling and simulation:0.75,Parallel computing methodologies:0.1,Symbolic and algebraic manipulation:0.1","Machine learning,Modeling and simulation",Machine learning is highly relevant as the paper proposes supervised link prediction models using dynamic features. Modeling and simulation is relevant for analyzing network evolution patterns. Other fields like computer graphics or parallel computing are not core to this network analysis work.,"Cross-validation:0.7,Learning paradigms:1,Learning settings:0.8,Machine learning algorithms:1,Machine learning approaches:1,Model development and analysis:0.9,Simulation evaluation:0.6,Simulation support systems:0.4,Simulation theory:0.3,Simulation types and techniques:0.5","Machine learning algorithms,Learning paradigms,Model development and analysis",Machine learning algorithms is core to the paper's dynamic link prediction methods. Learning paradigms is relevant for the approach to dynamic similarity metrics. Model development and analysis is included due to the design and evaluation of the proposed metrics. Other options like simulation evaluation are secondary as the focus is on algorithm design rather than simulation methodology.
289,Triangle order optimization for graphics hardware computation culling,"We describe an automatic preprocessing algorithm that reorders triangles in a mesh so as to enable the graphics hardware to efficiently cull vertex and pixel processing at rendering time.Our method starts by dividing the mesh into planar clusters which are subsequently sorted into a view-independent order which greatly reduces overdraw. The result is an increase in the opportunities for early Z-culling, reducing pixel processing time. The clusters are then optimized for mesh locality. This produces high rates of vertex cache hits, reducing vertex processing time.We have found that our method brings the overdraw rates of a wide range of models close to that of front-to-back order, while preserving state of the art vertex cache performance. This results in higher frame rates for pixel-bound applications with no penalty to vertex-bound applications.","General and reference:0,Hardware:0.25,Computer systems organization:0.5,Networks:0,Software and its engineering:0,Theory of computation:0,Mathematics of computing:0,Information systems:0,Security and privacy:0,Human-centered computing:0,Computing methodologies:1,Applied computing:0.25,Social and professional topics:0",Computing methodologies,"Computing methodologies: The paper introduces a novel triangle reordering algorithm for graphics hardware optimization, which is a core contribution in rendering methodologies. Other categories like Computer systems organization receive lower scores because the focus is on algorithmic optimization rather than hardware architecture.","Computer graphics:0.95,Modeling and simulation:0.8,Parallel computing methodologies:0.6,Artificial intelligence:0.1,Concurrent computing methodologies:0.1,Distributed computing methodologies:0.1,Machine learning:0.1,Symbolic and algebraic manipulation:0.1","Computer graphics,Modeling and simulation",Computer graphics: The paper directly addresses GPU-based rendering optimization. Modeling and simulation: Involves mesh processing and rendering pipeline simulation. Artificial intelligence is irrelevant as the focus is on rendering algorithms rather than AI techniques.,"Animation:0.0,Graphics systems and interfaces:1.0,Image compression:0.0,Image manipulation:0.5,Model development and analysis:0.0,Rendering:1.0,Shape modeling:0.0,Simulation evaluation:0.0,Simulation support systems:0.0,Simulation theory:0.0,Simulation types and techniques:0.0","Rendering,Graphics systems and interfaces",Rendering: Triangle reordering for culling directly impacts rendering efficiency. Graphics systems and interfaces: Method leverages graphics hardware computation. Image manipulation is secondary (0.5) as preprocessing affects rendering rather than direct image alteration.
573,Fast gradual matching measure for image retrieval based on visual similarity and spatial relations,"In this article, we propose a new method to retrieve images containing a request set of regions. The user is asked to specify a set of regions belonging to a single image. Then this request set of regions is compared to the sets of the regions of the images in the database. We propose a comparison measure that not only evaluates the similarity of regions one to the other, but that also takes into account the spatial configuration of the regions. The spatial structure of the regions is represented by means of fuzzy spatial relations, like horizontal and vertical disposal and connexity. © 2006 Wiley Periodicals, Inc. Int J Int Syst 21: 711–723, 2006.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.3,Theory of computation:0.2,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.9,Applied computing:0.3,Social and professional topics:0.1",Computing methodologies,Computing methodologies (0.9): The paper presents a novel image retrieval algorithm using spatial relations and visual similarity. Other categories like Applied computing are less relevant as the focus is on the algorithmic approach rather than specific applications.,"Computer graphics:1.0,Artificial intelligence:0.7,Machine learning:0.3,Modeling and simulation:0.4,Other categories:0.1",Computer graphics,Computer graphics is directly relevant for image retrieval techniques. Artificial intelligence is secondary if fuzzy relations are considered AI.,"Animation:0.0,Graphics systems and interfaces:0.0,Image compression:0.0,Image manipulation:1.0,Rendering:0.0,Shape modeling:0.0",Image manipulation,Image manipulation is relevant for the region comparison and spatial relation algorithms. Other categories like image compression or shape modeling are not addressed.
1146,Decision tree based state tying for speech recognition using DNN derived embeddings,"Recently, context dependent (CD)-deep neural network (DNN)-hidden Markov model (HMM) obtains significant improvements in many automatic speech recognition (ASR) tasks. In the standard training procedure for CD-DNN-HMM, the Gaussian mixture models (GMM) based ASR system has to be firstly built to pre-segment the training data and to define the CD states as the targets for DNN. In this paper, we propose a novel decision tree based state tying procedure, in which, the state embeddings derived from DNN are used and clustered to minimize the sum-of-squared error. Thus, the GMM is not a necessary part to define the targets for CD-DNN. Besides, we introduce a training procedure for CD-DNN-HMM, where, the forward backward algorithm is used for context independent (CI) DNN-HMM training, and the proposed state tying approach is applied to define the CD-DNN targets. Experiments were conducted on a 30-hour Chinese broadcast news speech database and the results demonstrate that the proposed DNN based state tying approach yielded comparable performance to the GMM based one.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.8,Applied computing:0.2,Social and professional topics:0.1",Computing methodologies,"Computing methodologies is highly relevant as the paper introduces a decision tree-based state tying method for speech recognition. Applied computing (0.2) is marginally relevant due to the application in speech recognition, but the core is the methodological contribution.","Artificial intelligence:0.7,Computer graphics:0.1,Concurrent computing methodologies:0.2,Distributed computing methodologies:0.2,Machine learning:0.9,Modeling and simulation:0.4,Parallel computing methodologies:0.2,Symbolic and algebraic manipulation:0.1",Machine learning,Machine learning is directly relevant for the DNN-based state tying approach. Artificial intelligence is secondary due to the speech recognition application context.,"Cross-validation:0.1,Learning paradigms:0.2,Learning settings:0.1,Machine learning algorithms:0.9,Machine learning approaches:0.7","Machine learning algorithms,Machine learning approaches",Machine learning algorithms: The paper introduces a decision tree-based state tying algorithm for speech recognition. Machine learning approaches: The approach combines DNN-derived embeddings with decision trees for state clustering. Other children like Cross-validation are not discussed in this context.
6022,Recent developments in granular computing: A bibliometrics study,"This is a follow-up of the paper ldquoA ten-year review of granular computingrdquo published in 2007. We continue to examine the most influential papers in granular computing. Based on the analysis of the impact papers, a list of key issues of granular computing research is given. We also summarize recent developments of research in granular computing.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.9,Applied computing:0.1,Social and professional topics:0.1",Computing methodologies,Computing methodologies is highly relevant as the paper focuses on granular computing techniques and their development. Other categories like 'Social and professional topics' are less aligned with the methodological analysis.,"Artificial intelligence:1.0,Computer graphics:0.0,Concurrent computing methodologies:0.0,Distributed computing methodologies:0.0,Machine learning:0.0,Modeling and simulation:0.0,Parallel computing methodologies:0.0,Symbolic and algebraic manipulation:0.0",Artificial intelligence,Artificial intelligence is highly relevant as granular computing is a subfield of AI. Other categories like Computer graphics or Parallel computing are unrelated to the paper’s focus on theoretical developments in granular computing.,"Computer vision:0,Control methods:0,Distributed artificial intelligence:0,Knowledge representation and reasoning:0.5,Natural language processing:0,Philosophical/theoretical foundations of artificial intelligence:1,Planning and scheduling:0,Search methodologies:0","Philosophical/theoretical foundations of artificial intelligence,Knowledge representation and reasoning",Philosophical/theoretical foundations of artificial intelligence is relevant because the paper reviews foundational aspects of granular computing. Knowledge representation and reasoning is moderately relevant due to the focus on structuring and analyzing knowledge in granular computing. Other categories lack direct relevance to the paper's theoretical and bibliometric focus.
1236,Non-Linear Dynamic Texture Analysis and Synthesis Using Constrained Gaussian Process Latent Variable Model,"Linear dynamic system (LDS) has been proposed to model dynamic texture. However, the temporal evolution of dynamic texture is non-linear in general and is not fully captured by the linear model. In this paper, we formulate the dynamic texture learning and synthesis via nonlinear approach. Assuming that dynamic texture is sampled from a low dimensional manifold, the constrained Gaussian process latent variable model (CGPLVM) is proposed to model the dynamic texture as a set of latent states. The essence of dynamic texture is captured as the spatial relationship within the latent states. Moreover, Metropolis-Hastings sampling method is used to sample new states, which hold the spatio-temporal statistics of dynamic texture. Experimental results demonstrate that our approach can produce dynamic texture sequences with promising visual quality.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.5,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:1.0,Applied computing:0.0,Social and professional topics:0.0",Computing methodologies,Computing methodologies: The paper introduces a constrained Gaussian Process Latent Variable Model for dynamic texture analysis. Mathematics of computing receives a lower score because the core contribution is the application of a computational model to a specific problem domain (image processing).,"Artificial intelligence:0.3,Computer graphics:0.8,Concurrent computing methodologies:0.1,Distributed computing methodologies:0.1,Machine learning:0.9,Modeling and simulation:0.6,Parallel computing methodologies:0.1,Symbolic and algebraic manipulation:0.1","Machine learning,Computer graphics,Modeling and simulation",Machine learning: Uses constrained Gaussian process latent variable models. Computer graphics: Focuses on dynamic texture synthesis. Modeling and simulation: Involves sampling methods for texture generation. Other fields are less relevant as the focus is on probabilistic modeling for visual synthesis.,"Animation:0.2,Cross-validation:0.1,Graphics systems and interfaces:0.1,Image compression:0.1,Image manipulation:0.1,Learning paradigms:0.3,Learning settings:0.2,Machine learning algorithms:1.0,Machine learning approaches:0.5,Model development and analysis:0.6,Rendering:0.1,Shape modeling:0.1,Simulation evaluation:0.2,Simulation support systems:0.1,Simulation theory:0.2,Simulation types and techniques:0.1","Machine learning algorithms,Model development and analysis",Machine learning algorithms: The constrained Gaussian Process Latent Variable Model is a novel ML approach for dynamic texture. Model development and analysis: The paper analyzes the model's properties and performance. Other graphics-related categories are not central to the ML methodology.
1127,"Models, Abstractions and Phases in Multi-Agent Based Simulation","This paper introduces some considerations about simulation practice and a schema of the models that are implicitly and explicitly involved in a Multi-Agent Based Simulation (MABS). The aim of this work is to set simulation inside scientific framework. In order to do that we give an interpretation of the levels that compound a simulation and that constitute different kinds of abstraction. A clear awareness of the relations that exist between these levels and the corresponding steps, in fact, it is necessary if MABS wants to be adopted as a scientific investigation method. Our opinion is that this analysis suggests some answers to the objections that are often directed towards the use of simulation in scientific practice but also underlines some criticalities in this process.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:1.0,Applied computing:0.0,Social and professional topics:0.0",Computing methodologies,"Computing methodologies (1.0): The paper introduces theoretical models for multi-agent simulations. Others: No focus on hardware, systems, or applications.","Artificial intelligence:0.2,Computer graphics:0.2,Concurrent computing methodologies:0.1,Distributed computing methodologies:0.1,Machine learning:0.1,Modeling and simulation:0.9,Parallel computing methodologies:0.1,Symbolic and algebraic manipulation:0.1",Modeling and simulation,"Modeling and simulation is highly relevant as the paper introduces a schema of models and abstractions in Multi-Agent Based Simulation. Other categories like Artificial intelligence or Machine learning are less relevant as the core contribution focuses on simulation frameworks, not AI techniques or learning algorithms.","Model development and analysis:1.0,Simulation evaluation:0.5,Simulation support systems:0.0,Simulation theory:1.0,Simulation types and techniques:1.0","Model development and analysis,Simulation theory,Simulation types and techniques",Model development and analysis is central to the paper's framework. Simulation theory and types/techniques address the abstraction and methodological structure. Simulation evaluation is secondary as the focus is on conceptual frameworks.
749,A genetic algorithm with exon shuffling crossover for hard bin packing problems,"A novel evolutionary approach for the bin packing problem (BPP) is presented. A simple steady-state genetic algorithm is developed that produces results comparable to other approaches in the literature, without the need for any additional heuristics. The algorithm's design makes maximum use of the principle of natural selection to evolve valid solutions without the explicit need to verify constraint violations. Our algorithm is based upon a biologically inspired group encoding which allows for a modularisation of the search space in which individual sub-solutions may be assigned independent cost values. These values are subsequently utilised in a crossover event modelled on the theory of exon shuffling to produce a single offspring that inherits the most promising segments from its parents. The algorithm is tested on a set of hard benchmark problems and the results indicate that the method has a very high degree of accuracy and reliability compared to other approaches in the literature.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:1.0,Applied computing:0.1,Social and professional topics:0.1",Computing methodologies,Computing methodologies is highly relevant for the genetic algorithm approach. Applied computing is irrelevant as the focus is on the bin packing algorithm itself. Other categories like Theory of computation are not directly addressed.,"Artificial intelligence:0.0,Computer graphics:0.0,Concurrent computing methodologies:0.0,Distributed computing methodologies:0.0,Machine learning:1.0,Modeling and simulation:0.0,Parallel computing methodologies:0.0,Symbolic and algebraic manipulation:0.0",Machine learning,"Machine learning is relevant because the paper introduces a novel genetic algorithm approach to bin packing problems, which is a subfield of evolutionary algorithms in machine learning. Other categories like Artificial intelligence are not explicitly addressed as the core contribution.","Cross-validation:0.1,Learning paradigms:0.3,Learning settings:0.2,Machine learning algorithms:1,Machine learning approaches:0.6","Machine learning algorithms,Machine learning approaches",Machine learning algorithms: The paper presents a genetic algorithm with a novel crossover method. Machine learning approaches: Discusses evolutionary approaches to bin packing. Other categories like Cross-validation are not central.
1689,Models of Collective Motion in Systems with Self-propelled Objects,"Vicsek model • Vicsek model (VM), Tamás Vicsek et al, “Novel type of phase transition in a system of self-driven particles”, Phys. Rev. Lett. 75 (1995) 1226. • definition of VM: – N point particles, off-lattice – state described by z = (r1, . . . , rN ;v1, . . .vN ). – |vi| = v0 fix – direction vi/v0 of particle i motion depends on average velocity if all particles j in neighborhood of i (with radius R) • implementation of VM: – choose discrete time step ∆t – simple algorithm","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.5,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:1.0,Applied computing:0.25,Social and professional topics:0.1",Computing methodologies,Computing methodologies is highly relevant because the paper introduces the Vicsek model for simulating collective motion. Applied computing receives low relevance as the focus is on theoretical modeling rather than real-world applications. Other categories are rejected due to lack of connection.,"Modeling and simulation:1.0,Distributed computing methodologies:0.3,Artificial intelligence:0.2,Machine learning:0.1,Concurrent computing methodologies:0.1,Parallel computing methodologies:0.1,Symbolic and algebraic manipulation:0.1",Modeling and simulation,Modeling and simulation is directly relevant as the paper presents the Vicsek model for simulating collective motion. Other categories like Distributed computing methodologies are less relevant as the focus is on theoretical modeling rather than distributed systems.,"Model development and analysis:1.0,Simulation evaluation:0.7,Simulation support systems:0.0,Simulation theory:0.5,Simulation types and techniques:1.0","Model development and analysis,Simulation types and techniques",Model development and simulation techniques are core to analyzing self-propelled particle motion. Simulation theory is secondary. Other categories like Support systems are irrelevant.
338,Extracting buildings from aerial topographic maps,"The recovery of 2D information from intensity images, and that of 3D information from range images are the major issues in 3D objects recognition from sensory data. The analysis and interpretation of remote sensing aerial images have important applications. This paper presents an efficient method for the analysis and modeling of such scenes based on range sensory data. Unlike methods using 2D intensity images, we exploit the rich 3D data. We extract symbolic information from the 3D triangular mesh models. These are used to recognize buildings using symbolic reasoning and generic object models.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:1.0,Applied computing:0.0,Social and professional topics:0.0",Computing methodologies,Computing methodologies: The paper presents 3D data processing methods for building recognition. Other categories are irrelevant.,"Artificial intelligence:0.1,Computer graphics:1.0,Concurrent computing methodologies:0.05,Distributed computing methodologies:0.05,Machine learning:0.05,Modeling and simulation:0.75,Parallel computing methodologies:0.05,Symbolic and algebraic manipulation:0.1","Computer graphics,Modeling and simulation",Computer graphics: The paper uses 3D range data for building extraction. Modeling and simulation: Symbolic reasoning for 3D object modeling. Other categories like AI are less relevant as the focus is on geometric modeling.,"Animation:0.1,Graphics systems and interfaces:0.2,Image compression:0.2,Image manipulation:0.3,Model development and analysis:1.0,Rendering:0.2,Shape modeling:1.0,Simulation evaluation:0.1,Simulation support systems:0.1,Simulation theory:0.1,Simulation types and techniques:0.1","Model development and analysis,Shape modeling","The paper presents a 3D modeling method for building extraction, directly aligning with 'Model development and analysis' and 'Shape modeling'. Other graphics-related categories like image manipulation are secondary."
229,Make my day - high-fidelity color denoising with Near-Infrared,"We address the task of restoring RGB images taken under low illumination (e.g. night time), when an aligned near infrared (NIR or simply N) image taken under stronger NIR illumination is available. Such restoration holds the promise that algorithms designed to work under daylight conditions could be used around the clock. Increasingly, RGBN cameras are becoming available, as car cameras tend to include a Near-Infrared (N) band, next to R, G, and B bands, and NIR artificial lighting is applied. Under low lighting conditions, the NIR band is less noisy than the others and this is all the more the case if stronger illumination is only available in the NIR band. We address the task of restoring the R, G, and B bands on the basis of the NIR band in such cases. Even if the NIR band is less strongly correlated with the R, G, and B bands than these bands are mutually, there is sufficient such correlation to pick up important textural and gradient information in the NIR band and inject it into the others. The algorithm that we propose - coined `Make My Day' or MMD for short - is akin to the previously published BM3D denoising algorithm. MMD denoises the three (visible - NIR) differential images to then add back the original NIR image. It not only effectively reduces the noise but also includes the texture and edge information in the high spatial frequency range. MMD outperforms other state-of-art denoising methods in terms of PSNR, texture quality, and color fidelity. We publish our codes and images.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.2,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.9,Applied computing:0.3,Social and professional topics:0.1",Computing methodologies,Computing methodologies is highly relevant for the image denoising algorithm. Other categories like Human-centered computing are less relevant as the focus is on algorithmic development.,"Artificial intelligence:0.2,Computer graphics:1.0,Concurrent computing methodologies:0.1,Distributed computing methodologies:0.1,Machine learning:0.3,Modeling and simulation:0.2,Parallel computing methodologies:0.1,Symbolic and algebraic manipulation:0.1",Computer graphics,"Computer graphics is highly relevant as the paper introduces a novel image denoising algorithm (MMD) that improves RGB image restoration using NIR data, a core topic in computer graphics. Other categories like Machine learning are only peripherally relevant since the method builds on traditional image processing (BM3D), not learning-based approaches.","Animation:0,Graphics systems and interfaces:0,Image compression:0,Image manipulation:1,Rendering:0,Shape modeling:0",Image manipulation,Image manipulation is relevant for RGB-NIR denoising techniques. Other graphics-related categories are not central to the restoration process.
853,Joint Contour Nets,"Contour Trees and Reeb Graphs are firmly embedded in scientific visualization for analysing univariate (scalar) fields. We generalize this analysis to multivariate fields with a data structure called the Joint Contour Net that quantizes the variation of multiple variables simultaneously. We report the first algorithm for constructing the Joint Contour Net, and demonstrate some of the properties that make it practically useful for visualisation, including accelerating computation by exploiting a relationship with rasterisation in the range of the function.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:1.0,Applied computing:0.0,Social and professional topics:0.0",Computing methodologies,Computing methodologies is highly relevant because the paper introduces a new data structure (Joint Contour Net) for scientific visualization of multivariate data. Other categories like Applied computing receive low scores because the focus is on the algorithmic contribution rather than domain-specific applications.,"Artificial intelligence:0.1,Computer graphics:0.9,Concurrent computing methodologies:0.1,Distributed computing methodologies:0.1,Machine learning:0.1,Modeling and simulation:0.6,Parallel computing methodologies:0.1,Symbolic and algebraic manipulation:0.1","Computer graphics,Modeling and simulation",Computer graphics is highly relevant as the paper introduces a visualization data structure for multivariate fields. Modeling and simulation is relevant for algorithmic efficiency analysis. Other categories are not central to the visualization focus.,"Model development and analysis:1,Rendering:0.6,Shape modeling:0.4,Simulation evaluation:0.2",Model development and analysis,Model development and analysis: Introduces Joint Contour Nets for multivariate data analysis. Rendering is secondary if the paper discusses visualization techniques. Other categories like Shape modeling are less relevant to the core contribution.
2997,An Image Denoising Algorithm with an Adaptive Window,"Mihcak et al. proposed a low complexity but powerful image denoising algorithm LAWML based on the decimated wavelet transform (DWT). The shortcoming of LAWML is to determine the global optimal neighboring window size by experimenting. We improve on LAWML using Stein's unbiased risk estimate(SURE). Our method can automatically estimate an optimal neighboring window for every wavelet subband. Its denoising performance also surpasses LAWML because the subband adaptive window is superior to the global window. Furthermore, our method on the DWT is extended to on the dual-tree complex wavelet transform (DT-CWT). Experimental results indicate that our method (DT-CWT) delivers the comparable or better performance than some of the already published state-of-the-art denoising algorithms.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.2,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.9,Applied computing:0.3,Social and professional topics:0.1",Computing methodologies,"Computing methodologies: The paper develops a new image denoising algorithm using adaptive windowing in wavelet transforms, focusing on computational methods for signal processing. Applied computing was rejected because the primary contribution is the algorithm itself, not its application to a specific domain.","Artificial intelligence:0.2,Computer graphics:0.9,Concurrent computing methodologies:0.1,Distributed computing methodologies:0.1,Machine learning:0.3,Modeling and simulation:0.1,Parallel computing methodologies:0.1,Symbolic and algebraic manipulation:0.1",Computer graphics,Computer graphics is highly relevant as the paper focuses on image denoising algorithm improvement. Other categories lack direct connection to core image processing contributions.,"Animation:0.5,Graphics systems and interfaces:0,Image compression:0.5,Image manipulation:1,Rendering:0,Shape modeling:0",Image manipulation,Image manipulation is highly relevant as the paper focuses on denoising algorithms. Image compression is moderately relevant due to wavelet-based methods but is secondary. Animation is not discussed.
2986,Speech synthesis by phonological structure matching,"This paper presents a new technique for speech synthesis by unit selection. The technique works by specifying the synthesis target and the speech database as phonological trees, and using a selection algorithm which finds the largest parts of trees in the database which match parts of the target tree. The technique avoids many of the errors made by prosody generation modules by incorporating their operation in the selection implicitly. A technique for using signal processing only when it is needed most is also described. The technique produces better quality speech than previous approaches and is also significantly faster.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.9,Applied computing:0.1,Social and professional topics:0.1",Computing methodologies,"Computing methodologies: The paper proposes a phonological tree-matching algorithm for speech synthesis, emphasizing novel unit selection techniques. Other categories like 'Applied computing' are less relevant as the focus is on the algorithmic innovation itself.","Artificial intelligence:0.9,Computer graphics:0.2,Concurrent computing methodologies:0.2,Distributed computing methodologies:0.2,Machine learning:0.3,Modeling and simulation:0.6,Parallel computing methodologies:0.2,Symbolic and algebraic manipulation:0.2","Artificial intelligence,Modeling and simulation",Artificial intelligence is relevant for speech synthesis techniques. Modeling and simulation is moderately relevant for algorithm design. Other children are unrelated to the core topic.,"Computer vision:0.0,Control methods:0.0,Distributed artificial intelligence:0.0,Knowledge representation and reasoning:0.0,Model development and analysis:0.5,Natural language processing:0.9,Philosophical/theoretical foundations of artificial intelligence:0.0,Planning and scheduling:0.0,Search methodologies:0.0,Simulation evaluation:0.0,Simulation support systems:0.0,Simulation theory:0.0,Simulation types and techniques:0.0","Natural language processing,Model development and analysis",Natural language processing: Speech synthesis is a core NLP task. Model development and analysis: The paper introduces a novel algorithmic model for unit selection. Other categories like Computer vision or Planning and scheduling are unrelated to the paper's focus on speech synthesis and model design.
396,Solving multi-objective permutation flowshop scheduling problem using CUDA,"In this paper we propose a parallel tabu search algorithm for the bi-criteria scheduling problem implemented on CUDA platform. The idea presented in this paper, refers to bi-criteria permutation flow shop case: minimization of total completion time (makespan) and total flow time. Proposed parallel Tabu Search algorithm uses multi-start with varying criteria weights in order to improve algorithms effectiveness. For the set of common benchmarks, proposed approach finds superior approximation of the Pareto front to other methods and obtains it in significantly shorter computation time compared to sequential methods.","General and reference:0.1,Hardware:0.2,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.9,Applied computing:0.2,Social and professional topics:0.1",Computing methodologies,Computing methodologies is highly relevant as the paper presents parallel algorithm design for scheduling problems. Applied computing is secondary to the methodological focus.,"Parallel computing methodologies:1.0,Concurrent computing methodologies:0.7,Modeling and simulation:0.6,Artificial intelligence:0.3,Distributed computing methodologies:0.4,Machine learning:0.2,Computer graphics:0.1,Symbolic and algebraic manipulation:0.1",Parallel computing methodologies,Parallel computing methodologies is highly relevant as the paper proposes a CUDA-based algorithm. Concurrent computing is secondary but related. Other categories like modeling and simulation are less central as the focus is on algorithm implementation.,"Parallel algorithms:1.0,Parallel programming languages:0.3",Parallel algorithms,"Parallel algorithms are the focus of the CUDA-based tabu search. Parallel programming languages are not directly relevant as the paper emphasizes algorithm design, not language-specific features."
1092,State of the Art in Ray Tracing Animated Scenes,"Ray tracing has long been a method of choice for off‐line rendering, but traditionally was too slow for interactive use. With faster hardware and algorithmic improvements this has recently changed, and real‐time ray tracing is finally within reach. However, real‐time capability also opens up new problems that do not exist in an off‐line environment. In particular real‐time ray tracing offers the opportunity to interactively ray trace moving/animated scene content.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.9,Applied computing:0.1,Social and professional topics:0.1",Computing methodologies,"Computing methodologies: The paper discusses ray tracing algorithms for animated scenes, which is a core computing methodology in computer graphics. Other categories are not directly relevant to the algorithmic contributions.","Artificial intelligence:0.1,Computer graphics:0.9,Concurrent computing methodologies:0.1,Distributed computing methodologies:0.1,Machine learning:0.1,Modeling and simulation:0.1,Parallel computing methodologies:0.1,Symbolic and algebraic manipulation:0.1",Computer graphics,"Computer graphics: The paper discusses ray tracing techniques for animated scenes. Other children are irrelevant as the focus is on rendering algorithms rather than AI, distributed computing, or symbolic methods.","Animation:1.0,Graphics systems and interfaces:0.0,Image compression:0.0,Image manipulation:0.0,Rendering:1.0,Shape modeling:0.0","Animation,Rendering",Animation and Rendering are relevant because the paper discusses real-time ray tracing techniques for animated scenes. Other categories like Image compression or Shape modeling are not central to the core contribution.
3835,A constraint language approach to matchmaking,"The need to discover and select entities that match specified requirements arises in many contexts in distributed systems. Meeting this need is complicated by the fact that not only may the potential consumer specify constraints on resources, but the owner of the entity in question may specify constraints on the consumer. This observation has motivated Raman et al. to propose that discovery and selection be implemented as symmetric matching process, an approach they take in their ClassAds system. We present here a new approach to symmetric matching that achieves significant advances in expressivities relative to the current ClassAds - for example, allowing for multi-way matches, expression and location of resource with negotiable capability. The key to our approach is that we reinterpret matching as a constraint problem and exploit constraint-solving technologies to implement matching operations. We have prototyped a matchmaking mechanism, named Redline, and used it to model and solve several challenging matching problems.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.95,Applied computing:0.1,Social and professional topics:0.1",Computing methodologies,Computing methodologies is highly relevant due to the paper's focus on constraint-solving approaches for matchmaking. Other categories lack direct connection to the algorithmic methodology development.,"Symbolic and algebraic manipulation:1.0,Distributed computing methodologies:0.7,Concurrent computing methodologies:0.6,Artificial intelligence:0.3,Computer graphics:0.1,Machine learning:0.2,Modeling and simulation:0.4,Parallel computing methodologies:0.3","Symbolic and algebraic manipulation,Distributed computing methodologies",Symbolic manipulation is core as the paper uses constraint solving for matching. Distributed methodologies are relevant due to application in distributed systems. Other categories lack direct connection.,"Computer algebra systems:0.1,Distributed algorithms:0.9,Distributed programming languages:0.8,Representation of mathematical objects:0.2,Symbolic and algebraic algorithms:0.3","Distributed algorithms,Distributed programming languages",Distributed algorithms are core to the symmetric matching approach. Distributed programming languages relate to the constraint language implementation. Other categories like Computer algebra systems are irrelevant to the paper's focus on matchmaking.
2984,A Subband-Based SVM Front-End for Robust ASR,This work proposes a novel support vector machine (SVM) based robust automatic speech recognition (ASR) front-end that operates on an ensemble of the subband components of high-dimensional acoustic waveforms. The key issues of selecting the appropriate SVM kernels for classification in frequency subbands and the combination of individual subband classifiers using ensemble methods are addressed. The proposed front-end is compared with state-of-the-art ASR front-ends in terms of robustness to additive noise and linear filtering. Experiments performed on the TIMIT phoneme classification task demonstrate the benefits of the proposed subband based SVM front-end: it outperforms the standard cepstral front-end in the presence of noise and linear filtering for signal-to-noise ratio (SNR) below 12-dB. A combination of the proposed front-end with a conventional front-end such as MFCC yields further improvements over the individual front ends across the full range of noise levels.,"General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.95,Applied computing:0.1,Social and professional topics:0.1",Computing methodologies,"Computing methodologies: The paper introduces a subband-based SVM ensemble method for robust ASR, focusing on algorithmic design and evaluation. Other categories like 'Hardware' or 'Information systems' are not central to the computational methodology contribution.","Artificial intelligence:1.0,Computer graphics:0.2,Concurrent computing methodologies:0.3,Distributed computing methodologies:0.2,Machine learning:1.0,Modeling and simulation:0.4,Parallel computing methodologies:0.2,Symbolic and algebraic manipulation:0.2","Artificial intelligence,Machine learning","Artificial intelligence and Machine learning are highly relevant as the paper proposes an SVM-based ASR front-end, a core ML technique for speech processing. Other children are irrelevant to the core ML focus.","Computer vision:0.0,Control methods:0.0,Cross-validation:0.1,Distributed artificial intelligence:0.0,Knowledge representation and reasoning:0.0,Learning paradigms:0.2,Learning settings:0.1,Machine learning algorithms:0.9,Machine learning approaches:0.7,Natural language processing:0.8,Philosophical/theoretical foundations of artificial intelligence:0.0,Planning and scheduling:0.0,Search methodologies:0.1","Machine learning algorithms,Natural language processing","Machine learning algorithms: The paper introduces an SVM-based front-end for ASR, a core ML algorithm. Natural language processing: ASR is a classic NLP task. Other categories like Control methods or Search methodologies are irrelevant to the paper's focus on ML and speech processing."
2083,Actin Automata: Phenomenology and Localizations,"Actin is a globular protein which forms long filaments in the eukaryotic cytoskeleton, whose roles in cell function include structural support, contractile activity to intracellular signalling. We model actin filaments as two chains of one-dimensional binary-state semi-totalistic automaton arrays to describe hypothetical signalling events therein. Each node of the actin automaton takes state `0' (resting) or `1' (excited) and updates its state in discrete time depending on its neighbour's states. We analyse the complete rule space of actin automata using integral characteristics of space-time configurations generated by these rules and compute state transition rules that support travelling and mobile localizations. Approaches towards selection of the localisation supporting rules using the global characteristics are outlined. We find that some properties of actin automata rules may be predicted using Shannon entropy, activity and incoherence of excitation between the polymer chains. We also show that it is possible to infer whether a given rule supports travelling or stationary localizations by looking at ratios of excited neighbours are essential for generations of the localizations. We conclude by applying biomolecular hypotheses to this model and discuss the significance of our findings in context with cell signalling and emergent behaviour in cellular computation.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.4,Mathematics of computing:0.3,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.9,Applied computing:0.5,Social and professional topics:0.1",Computing methodologies,Computing methodologies is directly relevant for developing and analyzing automata models. Applied computing is somewhat relevant for biological applications but less central than the core computational model.,"Artificial intelligence:0.0,Computer graphics:0.0,Concurrent computing methodologies:0.0,Distributed computing methodologies:0.0,Machine learning:0.0,Modeling and simulation:0.9,Parallel computing methodologies:0.0,Symbolic and algebraic manipulation:0.0",Modeling and simulation,Modeling and simulation is relevant for automaton-based cellular process modeling. Other fields lack direct connection to computational modeling.,"Model development and analysis:0.9,Simulation evaluation:0.3,Simulation support systems:0.2,Simulation theory:0.8,Simulation types and techniques:0.4","Model development and analysis,Simulation theory",Model development and analysis: The paper develops actin automata models and analyzes their behavior. Simulation theory: The paper discusses the theoretical framework for modeling actin filaments. Other categories like Simulation types are less relevant as the focus is on model development rather than simulation techniques.
5965,Real-time Action Recognition by Spatiotemporal Semantic and Structural Forests,"This paper presents a novel real-time action recogniser by utilising both local appearance and structural information. Our method is able to recognise actions continuously in real-time while achieving comparably high accuracy over state-of-the-arts. Run-time speed is of vital importance in real-world action recognition systems, but existing methods seldom take computational complexity into full consideration. A class label is assigned after an entire query video is analysed, or a large lookahead is required to recognise an action. In addition, the “bag of words”(BOW) has proven effective for action recognition [5]. However, the standard BOW model ignores the spatiotemporal relationships among feature descriptors, which are useful for describing actions. Addressing these challenges, we present a novel approach for action recognition. The major contributions include the followings: Efficient Spatiotemporal Codebook Learning: We extend the use of semantic texton forests [6] (STFs) from 2D image segmentation to spatiotemporal analysis. As well as being much faster than a traditional flat codebook such as k-means clustering, STFs achieve high accuracy comparable to that of existing approaches. STFs are ensembles of random decision trees that textonise input video patches into semantic textons. Since only a small number of simple features are used to traverse the trees, STFs are extremely fast to evaluate. They also serve a powerful discriminative codebook by multiple decision trees. Figure 1 illustrates how visual codewords are generated using STFs in the proposed method. Combined Structural and Appearance Information: We propose a richer description of features, hence actions can be classified in very short video sequences. Based on [3], we introduce the pyramidal spatiotemporal relationship match (PSRM) to encapsulate both local appearance and structural information efficiently. Subsequences are sampled from an input video in short intervals (e.g. ≤ 10 frames). After spatiotemporal interest points are localised, the trained STFs assign visual codewords to the features. A set of pairwise spatiotemporal associations are designed to capture the structural relationships among features (i.e. pairwise distances along space-time axes). All possible pairs in the bag of features are analysed by the association rules and stored in the 3-D histogram. PSRM leverages the properties of semantic trees and pyramidal match kernels. Multiple pyramidal histograms are then combined to classify a query video. Figure 2 illustrates how the relationship histograms are constructed and matched using PSRM. For each tree in STFs, the threedimensional histogram is constructed according to their spatiotemporal structures (see figure 2 (left)). Its hierarchical structure offers a time efficient way to perform the pyramid match kernel [1] for codeword matching (figure 2 (right)). Enhanced Efficiency and Combined Classification: Several techniques are employed to improve the recognition speed and accuracy. A novel spatiotemporal interest point detector, called V-FAST, is designed based on the FAST 2D corners [2]. The recognition accuracy is enhanced by adaptively combining PSRM and the bag of semantic texton (BOST) method [6]: the k-means forest classifier is learned using PSRM as a matching kernel. The task of action recognition is performed separately Spatiotemporal Relationship Match of visual codewords from Semantic Texton Forest Pyramid Match Kernel is utilised to match the histograms Feature Extraction Feature Matching","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:1.0,Applied computing:0.1,Social and professional topics:0.1",Computing methodologies,Computing methodologies is directly relevant as the paper presents novel machine learning techniques for real-time action recognition. Other categories are irrelevant as the focus is on algorithmic development.,"Artificial intelligence:0.9,Computer graphics:0.1,Concurrent computing methodologies:0.1,Distributed computing methodologies:0.1,Machine learning:1.0,Modeling and simulation:0.1,Parallel computing methodologies:0.1,Symbolic and algebraic manipulation:0.1","Machine learning,Artificial intelligence",Machine learning is highly relevant as the paper presents a real-time action recognition system using spatiotemporal forests. Artificial intelligence is relevant due to the use of semantic and structural forests. Other fields like Computer graphics are not directly related to the core contribution.,"Computer vision:1.0,Control methods:0.1,Cross-validation:0.1,Distributed artificial intelligence:0.1,Knowledge representation and reasoning:0.1,Learning paradigms:0.1,Learning settings:0.1,Machine learning algorithms:0.8,Machine learning approaches:0.7,Natural language processing:0.1,Philosophical/theoretical foundations of artificial intelligence:0.1,Planning and scheduling:0.1,Search methodologies:0.1","Computer vision,Machine learning approaches",Computer vision: Focuses on real-time action recognition. Machine learning approaches: Uses spatiotemporal semantic forests. Other AI categories are not central to the method.
1175,APT: Action localization proposals from dense trajectories,"This paper is on action localization in video with the aid of spatio-temporal proposals. To alleviate the computational expensive segmentation step of existing proposals, we propose bypassing the segmentations completely by generating proposals directly from the dense trajectories used to represent videos during classification. Our Action localization Proposals from dense Trajectories (APT) use an efficient proposal generation algorithm to handle the high number of trajectories in a video. Our spatio-temporal proposals are faster than current methods and outperform the localization and classification accuracy of current proposals on the UCF Sports, UCF 101, and MSR-II video datasets. Corrected version: we fixed a mistake in our UCF-101 ground truth. Numbers are different; conclusions are unchanged","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.2,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.8,Applied computing:0.2,Social and professional topics:0.1",Computing methodologies,"Computing methodologies is relevant for action localization algorithms in video processing. Other categories are less relevant as the paper focuses on computer vision methodology rather than hardware, theory, or applied domains.","Artificial intelligence:0.85,Computer graphics:0.3,Concurrent computing methodologies:0.1,Distributed computing methodologies:0.2,Machine learning:0.9,Modeling and simulation:0.4,Parallel computing methodologies:0.2,Symbolic and algebraic manipulation:0.1","Artificial intelligence,Machine learning",Artificial intelligence/Machine learning (action localization using dense trajectories in videos). Other children like Computer graphics or Modeling are secondary to the core ML/AI contribution of spatio-temporal proposal generation.,"Computer vision:1.0,Control methods:0.1,Cross-validation:0.1,Distributed artificial intelligence:0.1,Knowledge representation and reasoning:0.1,Learning paradigms:0.3,Learning settings:0.1,Machine learning algorithms:0.5,Machine learning approaches:0.1,Natural language processing:0.1,Philosophical/theoretical foundations of artificial intelligence:0.1,Planning and scheduling:0.1,Search methodologies:0.1","Computer vision,Machine learning algorithms",Computer vision is highly relevant as the paper focuses on action localization in videos. Machine learning algorithms is relevant because the paper discusses classification accuracy. Other categories like Learning paradigms have lower relevance.
770,Reconfiguration of the Air Control System of a Bark Boiler,"In this paper, the problem of reconfiguration of controllers in the air control system of a bark boiler is addressed. The air control system of a bark boiler is considered as an interconnected multivariable system and the configuration of the controllers is adapted with the perspective to improve the closed-loop performance of the system. This paper proposes a generic procedure for the reconfiguration of controllers using a combination of established theories together with rather recently proposed methods. The procedure is applied to a model of the air flow system, which is derived in this paper. The results indicate that a significant performance improvement can be achieved by reconfiguring the control system instead of redesigning the process. The improvement is quantified through theoretical assessment, simulation studies, and performance assessment of pilot tests at the bark boiler at SCA Obbola AB, Sweden. It can be concluded that the proposed procedure is both efficient and easy to apply.","General and reference:0,Hardware:0,Computer systems organization:0.75,Networks:0,Software and its engineering:0,Theory of computation:0,Mathematics of computing:0,Information systems:0,Security and privacy:0,Human-centered computing:0,Computing methodologies:0,Applied computing:1,Social and professional topics:0",Applied computing,Applied computing is highly relevant for industrial control system optimization. Computer systems organization receives moderate relevance for control architecture considerations. Other categories like Software and its engineering are irrelevant as the focus is on system performance rather than software implementation.,"Arts and humanities:0.0,Computer forensics:0.0,Computers in other domains:0.0,Document management and text processing:0.0,Education:0.0,Electronic commerce:0.0,Enterprise computing:0.0,Law, social and behavioral sciences:0.0,Life and medical sciences:0.0,Operations research:0.0,Physical sciences and engineering:1.0",Physical sciences and engineering,Physical sciences and engineering is directly relevant for the control system reconfiguration in industrial boiler systems. All other categories are irrelevant to the engineering control focus.,"Aerospace:0,Archaeology:0,Astronomy:0,Chemistry:0,Earth and atmospheric sciences:0,Electronics:0,Engineering:1,Mathematics and statistics:0.5,Physics:0,Telecommunications:0",Engineering,Engineering is directly relevant as the paper discusses control system reconfiguration for a bark boiler. Mathematics and statistics is secondary because the paper mentions theoretical assessment but focuses on practical control system improvement rather than mathematical theory.
2492,Compensation of physiological motion using linear predictive force control,"This paper proposes a new approach to compensate the physiological motion, induced by respiration and heart beating, for robotized minimally invasive cardiac surgery. The control algorithm, based on a linear predictive control, uses the effort information applied on the heart by the instrument.","General and reference:0.0,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.3,Theory of computation:0.1,Mathematics of computing:0.2,Information systems:0.1,Security and privacy:0.0,Human-centered computing:0.1,Computing methodologies:0.5,Applied computing:1.0,Social and professional topics:0.0",Applied computing,"Applied computing is central to the medical robotics application. Computing methodologies is relevant for the control algorithm, but Applied computing is the primary domain. Other categories are irrelevant to the healthcare context.","Arts and humanities:0.1,Computer forensics:0.1,Computers in other domains:0.2,Document management and text processing:0.1,Education:0.1,Electronic commerce:0.1,Enterprise computing:0.1,Law, social and behavioral sciences:0.1,Life and medical sciences:0.9,Operations research:0.2,Physical sciences and engineering:0.2",Life and medical sciences,Life and medical sciences is highly relevant as the paper presents a robotic control solution for cardiac surgery. Other categories like physical sciences and engineering are less relevant as the focus is on medical application rather than general engineering principles.,"Bioinformatics:0.0,Computational biology:0.0,Consumer health:0.0,Genetics:0.0,Genomics:0.0,Health care information systems:0.0,Health informatics:0.0,Metabolomics / metabonomics:0.0,Systems biology:0.0",,"All options are irrelevant. The paper focuses on robotic control for cardiac surgery, which is more related to robotics and control systems than health informatics or biology."
1625,Long Term Evaluation of the CancerNet WWW Service,"To improve the quality of healthcare, we offer access to high quality guidelines to physicians as well as information harmonized with these guidelines to concerned patients via internet. We launched this service in 1994. Since then, we have been offering user-friendly access to the NCI's CancerNet via WWW (http://www.meb.uni-bonn.de/). Among other information, CancerNet by the National Cancer Institute contains up-to-date summaries on the prognosis, staging, and treatment of more than 80 major tumor types. To obtain information about how users navigate through this service, all users' activities are logged by using cookies for tracking them without injuring privacy. To get additional information about our users and their interests regarding our service, we performed user surveys in 1996 and 1997. The analysis of 538 valid answers in 1996 and 1001 in 1997 show that the attempt to bring high quality information and guidelines to physicians and patients was successful. About 95% of our users rated our service excellent or good"".""""""","General and reference:0,Hardware:0,Computer systems organization:0,Networks:0,Software and its engineering:1,Theory of computation:0,Mathematics of computing:0,Information systems:0.8,Security and privacy:0,Human-centered computing:0.5,Computing methodologies:0,Applied computing:1,Social and professional topics:0",Applied computing,"Applied computing: The paper evaluates a healthcare information service. Information systems: The paper discusses WWW service and user tracking. Human-centered computing: Only marginally relevant as the focus is on service evaluation, not user interaction design.","Arts and humanities:0.1,Computer forensics:0.1,Computers in other domains:0.9,Document management and text processing:0.3,Education:0.2,Electronic commerce:0.1,Enterprise computing:0.3,Law, social and behavioral sciences:0.1,Life and medical sciences:0.8,Operations research:0.1,Physical sciences and engineering:0.1","Computers in other domains,Life and medical sciences",Computers in other domains is highly relevant as the paper discusses healthcare information systems. Life and medical sciences is relevant due to the focus on cancer information. Other categories like Education or Enterprise computing are secondary to the core contribution.,"Agriculture:0.0,Bioinformatics:0.0,Cartography:0.0,Computational biology:0.0,Computing in government:0.0,Consumer health:0.5,Digital libraries and archives:0.75,Genetics:0.0,Genomics:0.0,Health care information systems:1.0,Health informatics:1.0,Metabolomics / metabonomics:0.0,Military:0.0,Personal computers and PC applications:0.0,Publishing:0.0,Systems biology:0.0","Health care information systems,Health informatics",Health care information systems (1.0): The paper evaluates a healthcare information delivery system. Health informatics (1.0): Focuses on information systems for clinicians and patients. Digital libraries (0.75): The service functions as a digital library for medical guidelines.
2698,SuperCloud: economical cloud service on multiple vendors,"Today, Infrastructure-as-a-Service (IaaS) cloud providers such as Amazon's Elastic Compute Engine (EC2), Google's Compute Engine, and Microsoft's Azure offer elastic and isolated compute resources via virtualization and users often choose one of these providers based on price, locality, performance, and features. Typically, a user will choose the same provider for computation and storage to minimize latency and networking costs. Unfortunately, it can be difficult to switch providers once one is selected due to vendor lock-in [2].","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.3,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.2,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.2,Computing methodologies:0.2,Applied computing:0.9,Social and professional topics:0.1",Applied computing,"Applied computing: The paper addresses cloud service management across vendors, a practical application of distributed systems. Other categories like Networks are secondary as the focus is on service economics and vendor lock-in, not low-level infrastructure.","Arts and humanities:0.0,Computer forensics:0.0,Computers in other domains:0.0,Document management and text processing:0.0,Education:0.0,Electronic commerce:0.5,Enterprise computing:1.0,Law, social and behavioral sciences:0.0,Life and medical sciences:0.0,Operations research:0.0,Physical sciences and engineering:0.0",Enterprise computing,Enterprise computing is relevant for the discussion of cloud services and vendor lock-in. Electronic commerce is less relevant as the focus is on cloud services rather than commerce. Other fields are not directly relevant.,"Business process management:0.2,Business rules:0.1,Business-IT alignment:0.3,Enterprise architectures:0.4,Enterprise computing infrastructures:0.8,Enterprise data management:0.2,Enterprise information systems:0.3,Enterprise interoperability:0.4,Enterprise modeling:0.2,Enterprise ontologies, taxonomies and vocabularies:0.1,Event-driven architectures:0.1,IT architectures:0.6,IT governance:0.2,Reference models:0.3,Service-oriented architectures:0.9","Enterprise computing infrastructures,Service-oriented architectures","Enterprise computing infrastructures: The paper addresses multi-vendor cloud service integration, a core infrastructure challenge. Service-oriented architectures: SuperCloud leverages SOA principles to abstract and manage cross-vendor services."
4407,Functional annotation of proteomic sequences based on consensus of sequence and structural analysis,"To maximise the assignment of function of the proteins encoded by a genome and to aid the search for novel drug targets, there is an emerging need for sensitive methods of predicting protein function on a genome-wide basis. GeneAtlas is an automated, high-throughput pipeline for the prediction of protein structure and function using sequence similarity detection, homology modelling and fold recognition methods. GeneAtlas is described in detail here. To test GeneAtlas, a 'virtual' genome was used, a subset of PDB structures from the SCOP database, in which the functional relationships are known. GeneAtlas detects additional relationships by building 3D models in comparison with the sequence searching method PSI-BLAST. Functionally related proteins with sequence identity below the twilight zone can be recognised correctly.","General and reference:0,Hardware:0,Computer systems organization:0,Networks:0,Software and its engineering:0,Theory of computation:0,Mathematics of computing:0,Information systems:0,Security and privacy:0,Human-centered computing:0,Computing methodologies:0,Applied computing:1,Social and professional topics:0",Applied computing,Applied computing is relevant because the paper presents a computational pipeline (GeneAtlas) for predicting protein function in biological contexts. Other categories like 'Software and its engineering' or 'Mathematics of computing' are not directly relevant to the core contribution of applying computational methods to biological data.,"Arts and humanities:0.0,Computer forensics:0.0,Computers in other domains:0.0,Document management and text processing:0.0,Education:0.0,Electronic commerce:0.0,Enterprise computing:0.0,Law, social and behavioral sciences:0.0,Life and medical sciences:0.9,Operations research:0.0,Physical sciences and engineering:0.0",Life and medical sciences,Life and medical sciences is highly relevant for the proteomic sequence annotation in drug target discovery. Other categories like 'Computers in other domains' are less directly aligned with the core bioinformatics application.,"Bioinformatics:1.0,Computational biology:1.0,Consumer health:0.4,Genetics:0.7,Genomics:0.7,Health care information systems:0.2,Health informatics:0.3,Metabolomics / metabonomics:0.2,Systems biology:0.9","Bioinformatics,Computational biology,Systems biology",Bioinformatics and Computational biology are central to protein function prediction. Systems biology is secondary. Other categories are less relevant.
2914,Road slope and vehicle mass estimation for light commercial vehicle using linear Kalman filter and RLS with forgetting factor integrated approach,"This paper explains application of Kalman filter theory and recursive least squares algorithm with forgetting factor on real time estimation problem of light commercial vehicle mass and road grade on which motor vehicle moves. After a brief survey on mass and slope estimating in literature, there are proposed algorithms theoretical approaches and implementations on a real-time ECU. The test data are obtained from urban, extra-urban and highway experiments with prototypal vehicles.","General and reference:0.05,Hardware:0.05,Computer systems organization:0.05,Networks:0.05,Software and its engineering:0.05,Theory of computation:0.05,Mathematics of computing:0.15,Information systems:0.05,Security and privacy:0.05,Human-centered computing:0.05,Computing methodologies:0.15,Applied computing:0.8,Social and professional topics:0.05",Applied computing,"Applied computing is relevant for engineering applications in vehicle systems and real-time control. Mathematics of computing applies to the algorithms used (Kalman filter, RLS), but the primary domain is engineering systems.","Arts and humanities:0.1,Computer forensics:0.1,Computers in other domains:0.1,Document management and text processing:0.1,Education:0.1,Electronic commerce:0.1,Enterprise computing:0.1,Law, social and behavioral sciences:0.1,Life and medical sciences:0.1,Operations research:0.1,Physical sciences and engineering:0.9",Physical sciences and engineering,"Physical sciences and engineering: The paper presents a vehicle dynamics estimation system using Kalman filters and RLS algorithms, which are core to engineering applications. Other categories are irrelevant as the focus is on control systems and real-time estimation, not data management or AI.","Aerospace:0.1,Archaeology:0.1,Astronomy:0.1,Chemistry:0.1,Earth and atmospheric sciences:0.1,Electronics:1.0,Engineering:1.0,Mathematics and statistics:0.3,Physics:0.1,Telecommunications:0.1","Engineering,Electronics",Engineering: Vehicle mass and slope estimation is an engineering problem. Electronics: Kalman filter implementation on ECU (electronic control unit) is an electronics application. Other categories like 'Mathematics and statistics' are secondary as the focus is on real-world implementation.
4774,Metrological Performances of a Plantar Pressure Measurement System,"Plantar pressure measurements provide useful information to diagnose a diverse range of foot disorders; unfortunately, the commercially available measurement systems are undesirably sensitive to several disturbances, but this aspect is mostly neglected in the literature. This paper describes the results of an experimental campaign aiming at the identification of pressure measuring system metrological performances, at system modeling, and at the implementation of correction procedures. The sensor model was implemented using the results of static and dynamic tests performed on a pedar-X plantar pressure measurement system. The static calibration was performed by analyzing the effect of temperature, single sensor coverage area, local curvature, tangential forces, long-term stability (creep), and sensor crosstalk on the system performances. The dynamic calibration was performed on an electrodynamic shaker, identifying the single sensor frequency response function and the hysteresis under different average loads. The dynamic sensor model is based on the Kelvin-Voigt model, which is representative of the viscoelastic behavior of the material. The model allowed us to compensate both the creep (i.e., the behavior under static loads) and the nonunitary frequency response function. A deconvolution-based algorithm has been proposed to compensate the sensor crosstalk effects, although its implementation requires additional investigations. Experimental results of bobbing and gait tests showed that, with the adoption of the proposed compensation algorithms, the force and center of pressure errors could be reduced by more than 50% of their initial values.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.65,Applied computing:0.8,Social and professional topics:0.0",Applied computing,"Applied computing is relevant due to the practical focus on medical sensor systems and their correction algorithms. Computing methodologies received a moderate score for the algorithmic modeling techniques used, but the primary domain is applied.","Arts and humanities:0.1,Computer forensics:0.1,Computers in other domains:0.1,Document management and text processing:0.1,Education:0.1,Electronic commerce:0.1,Enterprise computing:0.1,Law, social and behavioral sciences:0.1,Life and medical sciences:0.8,Operations research:0.1,Physical sciences and engineering:0.6","Life and medical sciences,Physical sciences and engineering",Life and medical sciences is relevant for the application in foot disorder diagnosis. Physical sciences and engineering applies to sensor metrology. Other categories like Education are irrelevant.,"Aerospace:0,Archaeology:0,Astronomy:0,Bioinformatics:0,Chemistry:0,Computational biology:0,Consumer health:1,Earth and atmospheric sciences:0,Electronics:0,Engineering:1,Genetics:0,Genomics:0,Health care information systems:0,Health informatics:1,Mathematics and statistics:0,Metabolomics / metabonomics:0,Physics:0,Systems biology:0,Telecommunications:0","Consumer health,Engineering,Health informatics",Consumer health is relevant as the paper addresses plantar pressure measurement for diagnosing foot disorders. Engineering aligns with the sensor system's design and calibration. Health informatics is relevant due to the focus on medical metrology. Other children like bioinformatics or genomics are not central to the paper's contribution.
1079,Modeling and Simulation of DIGSILENT-based Micro-grid System,"The accurate modeling of micro-grid access to power system planning and design stage needs is the primary problem to solve. This paper modeled the micro grid photovoltaic power generation system ,including silicon solar cell, photovoltaic inverters, battery energy storage system, and the micro power distribution system .The use of power system analysis software (DIGSILENT) of actual power system simulation, the simulation results verify the model's correctness. In the power grid fault disturbance, the light intensity of disturbance and the load disturbances, the simulation results show that the optical storage combined with micro network has fast dynamic response characteristics, and its network of grid-connected voltage influenced by the changes of the light and load is little, while more affected by the network fault influence. DOI :  http://dx.doi.org/10.11591/telkomnika.v12i6.5471 Full Text: PDF","General and reference:0.1,Hardware:0.2,Computer systems organization:0.1,Networks:0.3,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.3,Applied computing:0.8,Social and professional topics:0.1",Applied computing,"Applied computing is relevant as the paper focuses on power system simulation for micro-grid design. Networks and Computing methodologies are less relevant since the core contribution is applied to power engineering, not general networking or algorithmic methodologies.","Arts and humanities:0.0,Computer forensics:0.0,Computers in other domains:0.25,Document management and text processing:0.0,Education:0.0,Electronic commerce:0.0,Enterprise computing:0.0,Law, social and behavioral sciences:0.0,Life and medical sciences:0.0,Operations research:0.0,Physical sciences and engineering:1.0",Physical sciences and engineering,"Physical sciences and engineering: The paper presents a power system simulation model for micro-grid systems, which falls under engineering applications in the physical sciences domain.","Aerospace:0,Archaeology:0,Astronomy:0,Chemistry:0,Earth and atmospheric sciences:0,Electronics:1,Engineering:1,Mathematics and statistics:0,Physics:0,Telecommunications:0","Engineering,Electronics",Engineering is relevant for system modeling and simulation of micro-grids. Electronics is relevant for photovoltaic and power electronics components. Telecommunications and other fields are not mentioned in the context.
3422,"Research Commentary - Cooperation, Coordination, and Governance in Multisourcing: An Agenda for Analytical and Empirical Research","Multisourcing, the practice of stitching together best-of-breed IT services from multiple, geographically dispersed service providers, represents the leading edge of modern organizational forms. While major strides have been achieved in the last decade in the information systems (IS) and strategic management literature in improving our understanding of outsourcing, the focus has been on a dyadic relationship between a client and a vendor. We demonstrate that a straightforward extrapolation of such a dyadic relationship falls short of addressing the nuanced incentive-effort-output linkages that arise when multiple vendors, who are competitors, have to cooperate and coordinate to achieve the client's business objectives. We suggest that when multiple vendors have to work together to deliver end-to-end services to a client, the choice of formal incentives and relational governance mechanisms depends on the degree of interdependence between the various tasks as well as the observability and verifiability of output. With respect to cooperation, we find that a vendor must not only put effort in a “primary” task it is responsible for but also cooperate through “helping” effort in enabling other vendors perform their primary tasks. In the context of coordination, we find that task redesign for modularity, OLAs, and governance structures such as the guardian vendor model represent important avenues for further research. Based on the analysis of actual multisourcing contract details over the last decade, interviews with leading practitioners, and a review of the single-sourcing literature, we lay a foundation for normative theories of multisourcing and present a research agenda in this domain.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:1.0,Social and professional topics:0.1",Applied computing,"The paper addresses IT service governance and organizational strategies for multisourcing, fitting Applied computing. Social and professional topics is only relevant for minor managerial aspects.","Enterprise computing:0.8,Operations research:0.65,Law, social and behavioral sciences:0.3,Education:0.2,Electronic commerce:0.2,Document management and text processing:0.1,Life and medical sciences:0.1,Physical sciences and engineering:0.1,Arts and humanities:0.1,Computer forensics:0.1,Computers in other domains:0.1","Enterprise computing,Operations research",Enterprise computing is directly relevant to multisourcing in IT services. Operations research connects to the coordination models discussed. Social sciences and other categories are less central to the technical governance analysis.,"Business process management:0.7,Business rules:0.3,Business-IT alignment:0.5,Computer-aided manufacturing:0.1,Consumer products:0.1,Decision analysis:0.4,Enterprise architectures:0.5,Enterprise computing infrastructures:0.3,Enterprise data management:0.4,Enterprise information systems:0.8,Enterprise interoperability:0.6,Enterprise modeling:0.5,Enterprise ontologies, taxonomies and vocabularies:0.3,Event-driven architectures:0.2,Forecasting:0.1,IT architectures:0.4,IT governance:1,Industry and manufacturing:0.2,Marketing:0.1,Reference models:0.3,Service-oriented architectures:0.4,Transportation:0.1","Enterprise information systems,IT governance",Enterprise information systems is highly relevant as the paper discusses multisourcing in IT services. IT governance is directly relevant for the analysis of governance mechanisms. Business process management is moderately relevant for coordination challenges. Other categories are rejected as the focus is on governance rather than service architectures or modeling.
5470,A regulation strategy for virtual power plant,"A large number of batteries and gas generators in a virtual power plant (VPP) can achieve high profits by providing high-quality frequency regulation services. Therefore, the virtual power plant will participate in the regulation market for those batteries and gas generators. However, uncertainty of intermittent energy in VPP will damage the performance score in settlement and decrease the benefit of VPP. In order to give full play to the high-quality regulation services of VPP, this paper introduces a regulation mechanism for VPP, and we propose a calculation method of VPP's frequency performance. To reduce the influence of fluctuation of intermittent power and load on VPP's regulation performance, an improved VPP regulation control strategy is proposed. Finally, the results show the effectiveness of the control strategy.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.75,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:1.0,Social and professional topics:0.1",Applied computing,Applied computing is directly relevant as the paper focuses on practical regulation strategies for virtual power plants in energy markets. Other fields like Mathematics of computing are only tangentially related to the optimization methods used.,"Arts and humanities:0.1,Computer forensics:0.1,Computers in other domains:0.6,Document management and text processing:0.1,Education:0.1,Electronic commerce:0.2,Enterprise computing:0.5,Law, social and behavioral sciences:0.1,Life and medical sciences:0.1,Operations research:1.0,Physical sciences and engineering:0.8","Operations research,Computers in other domains,Physical sciences and engineering",Operations research is highly relevant for the optimization of VPP regulation strategies. Computers in other domains is relevant for applying computational methods to energy systems. Physical sciences and engineering is relevant for the technical implementation of power systems. Other categories like Education or Law are irrelevant as the paper focuses on technical optimization rather than social or educational aspects.,"Aerospace:0,Agriculture:0,Archaeology:0,Astronomy:0,Cartography:0,Chemistry:0,Computer-aided manufacturing:0,Computing in government:0,Consumer products:0,Decision analysis:0,Digital libraries and archives:0,Earth and atmospheric sciences:0,Electronics:0,Engineering:1,Forecasting:0.5,Industry and manufacturing:1,Marketing:0,Mathematics and statistics:0,Military:0,Personal computers and PC applications:0,Physics:0,Publishing:0,Telecommunications:0,Transportation:0","Engineering,Industry and manufacturing","Engineering: The paper addresses control strategies for energy systems in virtual power plants. Industry and manufacturing: VPPs are industrial systems for energy management. 'Forecasting' is only moderately relevant if the control strategy uses predictive models, but the paper focuses on regulation rather than prediction."
4045,Human Dorsal Striatum Encodes Prediction Errors during Observational Learning of Instrumental Actions,"The dorsal striatum plays a key role in the learning and expression of instrumental reward associations that are acquired through direct experience. However, not all learning about instrumental actions require direct experience. Instead, humans and other animals are also capable of acquiring instrumental actions by observing the experiences of others. In this study, we investigated the extent to which human dorsal striatum is involved in observational as well as experiential instrumental reward learning. Human participants were scanned with fMRI while they observed a confederate over a live video performing an instrumental conditioning task to obtain liquid juice rewards. Participants also performed a similar instrumental task for their own rewards. Using a computational model-based analysis, we found reward prediction errors in the dorsal striatum not only during the experiential learning condition but also during observational learning. These results suggest a key role for the dorsal striatum in learning instrumental associations, even when those associations are acquired purely by observing others.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:1.0,Social and professional topics:0.0",Applied computing,Applied computing is relevant because the paper applies computational modeling techniques to neuroscience research. The study itself is primarily about human learning mechanisms rather than core computer science topics like algorithms or systems.,"Arts and humanities:0.1,Computer forensics:0.1,Computers in other domains:0.1,Document management and text processing:0.1,Education:0.1,Electronic commerce:0.1,Enterprise computing:0.1,Law, social and behavioral sciences:0.1,Life and medical sciences:0.9,Operations research:0.1,Physical sciences and engineering:0.1",Life and medical sciences,Life and medical sciences is relevant as the study investigates neurological processes in the human dorsal striatum. Other categories like Computer forensics are entirely unrelated to neuroscience research.,"Bioinformatics:0.1,Computational biology:0.1,Genetics:0.1,Health informatics:0.1",,None of the options are relevant: The paper focuses on neuroscience (dorsal striatum) rather than computational methods in biology or health informatics.
6057,Multi-AUV Target Search Based on Bioinspired Neurodynamics Model in 3-D Underwater Environments,"Target search in 3-D underwater environments is a challenge in multiple autonomous underwater vehicles (multi-AUVs) exploration. This paper focuses on an effective strategy for multi-AUV target search in the 3-D underwater environments with obstacles. First, the Dempster-Shafer theory of evidence is applied to extract information of environment from the sonar data to build a grid map of the underwater environments. Second, a topologically organized bioinspired neurodynamics model based on the grid map is constructed to represent the dynamic environment. The target globally attracts the AUVs through the dynamic neural activity landscape of the model, while the obstacles locally push the AUVs away to avoid collision. Finally, the AUVs plan their search path to the targets autonomously by a steepest gradient descent rule. The proposed algorithm deals with various situations, such as static targets search, dynamic targets search, and one or several AUVs break down in the 3-D underwater environments with obstacles. The simulation results show that the proposed algorithm is capable of guiding multi-AUV to achieve search task of multiple targets with higher efficiency and adaptability compared with other algorithms.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.3,Theory of computation:0.2,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.4,Applied computing:0.9,Social and professional topics:0.1",Applied computing,Applied computing is relevant as the paper presents an algorithm for multi-AUV target search in underwater environments. Other categories are less relevant as the focus is on real-world robotic applications rather than theoretical computing or software engineering.,"Arts and humanities:0.05,Computer forensics:0.05,Computers in other domains:0.7,Document management and text processing:0.05,Education:0.05,Electronic commerce:0.05,Enterprise computing:0.05,Law, social and behavioral sciences:0.05,Life and medical sciences:0.05,Operations research:0.1,Physical sciences and engineering:0.6,Symbolic and algebraic manipulation:0.05","Computers in other domains,Physical sciences and engineering",Computers in other domains: The underwater AUV application is a domain-specific use case. Physical sciences and engineering: The paper focuses on 3D underwater navigation and obstacle avoidance. Other categories are irrelevant to the robotics context.,"Aerospace:0.0,Agriculture:0.0,Archaeology:0.0,Astronomy:0.0,Cartography:0.0,Chemistry:0.0,Computing in government:0.0,Digital libraries and archives:0.0,Earth and atmospheric sciences:0.0,Electronics:1.0,Engineering:1.0,Mathematics and statistics:0.2,Military:0.0,Personal computers and PC applications:0.0,Physics:0.0,Publishing:0.0,Telecommunications:0.0","Engineering,Electronics",Engineering is relevant for multi-AUV system design. Electronics is central for CMOS filter implementation. Other application domains are not addressed.
5159,Creating Hybrid Instruction: A Lens for Defining Exemplary Teaching in Distance Learning,"The Military Career Transition Program (MCTP) at Old Dominion University (Virginia) is the primary alternative certification program for teacher licensure targeting military personnel who are transitioning into education careers. Beginning in the fall of 1998, the MCTP added distance learning to its existing course offerings at 5 of its 15 sites. These courses follow a hybrid instructional model that includes VTEL and World Wide Web assisted instruction. Broadcast instructors and instructional facilitators at each distance learning site are utilized to facilitate collaborative, problem-based learning. This paper describes the evolution of the MCTP distance learning effort and examines the progression of strategic decision making involving the issues of course integrity, exemplary instructional methodology, and student accountability. (Author/MES) Reproductions supplied by EDRS are the best that can be made from the original document. Creating Hybrid Instruction: A Lens for Defining Exemplary Teaching in Distance Learning Alexandria Forte-Turner Military Career Transition Program, Darden College of Education, Old Dominion University, Norfolk, Virginia, USA. E-mail: aforte@odu.edu Lynn Schultz Educational Curriculum and Instruction, Darden College of Education, Old Dominion University, Norfolk, Virginia, USA. E-mail: llschult@odu.edu Linda Miller-Dunleavy Early Childhood/Speech and Language Pathology/Special Education, Darden College of Education, Old Dominion University, Norfolk, Virginia, USA. E-mail: lmillerd@odu.edu Abstract: The Military Career Transition Program (MCTP) at Old Dominion University is the primary alternative certification program for teacher licensure targeting military personnel who are transitioning into education careers. Beginning Fall 1998, the MCTP added distance learning to its existing course offerings at five of its fifteen sites. These courses follow a hybrid instructional model, which include VTEL and web assisted instruction. Broadcast instructors and instructional facilitators at each distance learning site are utilized to facilitate collaborative; problem-based learning. This paper describes the evolution of the MCTP distance learning effort and examines the progression of strategic decision making involving the issues of course integrity, exemplary instructional methodology, and student accountability. The Military Career Transition Program (MCTP) at Old Dominion University is the primary alternative certification program for teacher licensure targeting military personnel who are transitioning into education careers. Beginning Fall 1998, the MCTP added distance learning to its existing course offerings at five of its fifteen sites. These courses follow a hybrid instructional model, which include VTEL and web assisted instruction. Broadcast instructors and instructional facilitators at each distance learning site are utilized to facilitate collaborative; problem-based learning. This paper describes the evolution of the MCTP distance learning effort and examines the progression of strategic decision making involving the issues of course integrity, exemplary instructional methodology, and student accountability.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:1.0,Social and professional topics:0.5",Applied computing,Applied computing is directly relevant for distance learning systems in education. 'Social and professional topics' is only marginally relevant for pedagogical discussions.,"Arts and humanities:0.2,Computer forensics:0.2,Computers in other domains:0.5,Document management and text processing:0.2,Education:1.0,Electronic commerce:0.2,Enterprise computing:0.3,Law, social and behavioral sciences:0.4,Life and medical sciences:0.2,Operations research:0.3,Physical sciences and engineering:0.2",Education,Education is the primary domain as the paper focuses on hybrid instructional models for teacher training. Other categories like computers in other domains are less directly relevant as the paper emphasizes pedagogical methodology over technical systems.,"Collaborative learning:0.8,Computer-assisted instruction:0.6,Computer-managed instruction:0.4,Digital libraries and archives:0.2,Distance learning:1.0,E-learning:0.7,Interactive learning environments:0.5,Learning management systems:0.3","Distance learning,Collaborative learning",Distance learning is directly addressed as the primary focus of the paper. Collaborative learning is relevant due to the use of broadcast instructors and facilitators for problem-based learning. Other children like E-learning or Learning management systems are too specific or less central.
1769,Smart Gateway Grid: A DG-Based Residential Electric Power Supply System,"This paper presents a smart residential electric power supply system, which is named smart gateway grid (SGG). It can enable residential distributed generations (DGs) and energy storage system (ESS) to participate in system operation, which will improve the reliability of power supply and help households secure a high quality service while reducing the cost of consumption. The SGG operation modes and associated transition strategies are analyzed in the paper. And the control strategy is developed to guarantee the whole system operating satisfactorily. To meet the demand of the typical U.S. household, a low cost and high efficiency SGG configuration is designed as a case study. Finally, analysis, simulation, and experimental results are presented and they show that the proposed SGG has the attractive features.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:1.0,Social and professional topics:0.0",Applied computing,Applied computing is highly relevant as the paper focuses on a smart grid system for residential power supply. Other categories are not central to the paper's core contribution.,"Arts and humanities:0.0,Computer forensics:0.0,Computers in other domains:1.0,Document management and text processing:0.0,Education:0.0,Electronic commerce:0.0,Enterprise computing:0.0,Law, social and behavioral sciences:0.0,Life and medical sciences:0.0,Operations research:0.0,Physical sciences and engineering:0.5","Computers in other domains,Physical sciences and engineering",Computers in other domains is directly relevant as the paper applies computer-based control strategies to smart grid systems. Physical sciences and engineering is moderately relevant due to the focus on power systems. Other categories like Education or Enterprise computing are not mentioned.,"Aerospace:0.1,Agriculture:0.1,Archaeology:0.1,Astronomy:0.1,Cartography:0.1,Chemistry:0.1,Computing in government:0.1,Digital libraries and archives:0.1,Earth and atmospheric sciences:0.1,Electronics:0.5,Engineering:0.8,Mathematics and statistics:0.3,Military:0.1,Personal computers and PC applications:0.2,Physics:0.2,Publishing:0.1,Telecommunications:0.3",Engineering,Engineering: The paper presents a smart grid system design and analysis. Electronics has moderate relevance due to power system components. Other fields like Mathematics or Physics are not primary domains of the work.
3690,The TactaPack: A Wireless Sensor/Actuator Package for Physical Therapy Applications,"In this paper, we present preliminary work we have done on designing the TactaPack, a wearable sensor/actuator device that uses a Bluetooth wireless connection to return sensor data to a host, and to receive commands to initiate expressive vibrotactile stimuli. We present our work in the context of a physical therapy application designed to provide more autonomy for patients when performing rehabilitative exercises. This assistive technology has the potential to reduce injuries during therapy due to improper patient joint movement, and decrease the workload of physical therapists, thereby reducing healthcare costs. Though still in the early stages of design, we believe the TactaPack can be used to produce systems that are less cumbersome than current, wired solutions, and simplify the creation of high-level applications by offloading from the CPU to the device the process of sensing, testing against threshold values, and actuation.","General and reference:0.1,Hardware:0.3,Computer systems organization:0.1,Networks:0.3,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:1.0,Social and professional topics:0.1",Applied computing,Applied computing is highly relevant as the paper describes a wearable sensor/actuator device for physical therapy applications. Other categories like Hardware and Networks were considered but rejected as the focus is on the application rather than the underlying technology.,"Arts and humanities:0.1,Computer forensics:0.1,Computers in other domains:0.1,Document management and text processing:0.1,Education:0.1,Electronic commerce:0.1,Enterprise computing:0.1,Law, social and behavioral sciences:0.1,Life and medical sciences:0.9,Operations research:0.1,Physical sciences and engineering:0.85,Software creation and management:0.1,Software notations and tools:0.1,Software organization and properties:0.1","Life and medical sciences,Physical sciences and engineering",Life and medical sciences applies to physical therapy context. Physical sciences and engineering relates to device design. Other categories like Education are not central.,"Aerospace:0,Archaeology:0,Astronomy:0,Bioinformatics:0,Chemistry:0,Computational biology:0,Consumer health:1,Earth and atmospheric sciences:0,Electronics:0,Engineering:0,Genetics:0,Genomics:0,Health care information systems:1,Health informatics:0,Mathematics and statistics:0,Metabolomics / metabonomics:0,Physics:0,Systems biology:0,Telecommunications:0","Consumer health,Health care information systems",Consumer health: The TactaPack is designed for patient autonomy in physical therapy. Health care information systems: The paper addresses wearable technology integration into healthcare workflows. Other categories are irrelevant to the application domain.
4400,Exploration of structural stability: A computer implementation,"In this paper, the education of structural stability is viewed from the perspective of a range of analysis techniques emphasizing the exploratory nature of not only instability phenomenon, but also engineering problems, in general. The purpose of this research was to develop a computational‐based educational platform upon which to perform “experimental” tests. Three aspects of this work are discussed: the philosophy of the platform, the design concepts behind its implementation, and the programme itself. © 2001 John Wiley & Sons, Inc. Comput Appl Eng Educ 9: 122–135, 2001","General and reference:0.1,Hardware:0.1,Computer systems organization:0.2,Networks:0.1,Software and its engineering:0.3,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.2,Computing methodologies:0.2,Applied computing:0.8,Social and professional topics:0.1",Applied computing,"Applied computing: The paper describes a computational platform for structural stability education, directly addressing applied engineering problems. Other categories are rejected as the focus is not on hardware design, network systems, or theoretical mathematics.","Arts and humanities:0.0,Computer forensics:0.0,Computers in other domains:0.1,Document management and text processing:0.0,Education:0.9,Electronic commerce:0.0,Enterprise computing:0.0,Law, social and behavioral sciences:0.0,Life and medical sciences:0.0,Operations research:0.0,Physical sciences and engineering:0.1",Education,Education is highly relevant as the paper focuses on developing a computational educational platform for structural stability. Other categories like 'Computers in other domains' and 'Physical sciences and engineering' are less directly related to the primary educational focus.,"Collaborative learning:0.5,Computer-assisted instruction:1.0,Computer-managed instruction:0.3,Digital libraries and archives:0.1,Distance learning:0.4,E-learning:0.6,Interactive learning environments:1.0,Learning management systems:0.2","Computer-assisted instruction,Interactive learning environments",Computer-assisted instruction is relevant for the computational-based educational platform. Interactive learning environments align with the exploratory experimental tests. Other options like Collaborative learning are mentioned only peripherally.
5140,Invited: A protein domain-centric approach to translational bioinformatics,"Identifying the functional context for key molecular disruptions in complex diseases is a major goal of modern medicine that will lead to improved preventive clinical approaches, earlier diagnosis and more effective personalized disease therapies. Most available resources for visualization and analysis of disease mutations centered on gene analysis and do not leverage information about similarity on the functional context of the mutation. In addition, gene-centric approaches are confounded because genes may share some functional sub-units, or protein domains, but not others. We have built a resource for domain mapping of disease mutations, DMDM, a protein domain database in which each disease mutation can be displayed by its protein domain location. DMDM provides a unique domain-level view where human coding mutations are mapped to protein domains by highlighting molecular relationships among mutations from different diseases that might not have been discovered with traditional gene-centric visualization tools. We have also developed a statistical method, the domain significance scores (DSScores), to assess the significance of disease mutation clusters on protein domains. When we applied the DS-Scores to human data and identified domain hotspots in oncogenes, tumor suppressors, and genes associated with Mendelian diseases. Since most proteins need to interact to perform their function, the identification domains of clinical relevance needs to be complemented by the identification of domain-domain interactions to provide a better understanding about the molecular underpinning of disease. Computational tools to predict domain-domain interactions provide a detailed molecular view of the protein interactions and complements expensive and laborious experimental techniques to identify such interactions. The evolutionary distances of interacting proteins often display a higher level of similarity than those of non-interacting proteins. This finding indicates that interacting proteins are subject to common evolutionary constraints and constitute the basis of a method to predict protein interactions known as mirrortree. It has been difficult, however, to identify the direct cause of the observed similarities between evolutionary trees. One possible explanation is the existence of compensatory mutations between partners binding sites to maintain proper binding. This explanation, however, has been recently challenged. It has been suggested that the signal of correlated evolution uncovered by the mirrortree method is unrelated to any correlated evolution between binding sites. We have addressed this controversial debate in the field by studying the contribution of binding sites to the correlation between evolutionary trees of interacting domains. We showed that binding neighborhoods of interacting proteins have, on average, higher co-evolutionary signal compared to the regions outside binding sites; although when the binding neighborhood was removed, the remaining domain sequence still contained some co-evolutionary signal. These results provide evidence of the role of compensatory mutations in protein co-evolution and contribute to our understanding of co-evolution of interacting proteins. Our domain-centric methods have the potential to be incorporated into translational bioinformatics tools for functional characterization of rare and common human variants from large-scale sequencing studies.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.3,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.9,Social and professional topics:0.1",Applied computing,Applied computing is highly relevant due to the focus on translational bioinformatics and computational tools for disease mutation analysis. Computing methodologies is less directly relevant as the paper emphasizes application rather than novel methodologies.,"Arts and humanities:0.1,Computer forensics:0.1,Computers in other domains:0.1,Document management and text processing:0.1,Education:0.1,Electronic commerce:0.1,Enterprise computing:0.1,Law, social and behavioral sciences:0.1,Life and medical sciences:1.0,Operations research:0.1,Physical sciences and engineering:0.1",Life and medical sciences,"Life and medical sciences is directly relevant as the paper focuses on translational bioinformatics, protein domains, and disease mutation analysis. Other fields like computer forensics or enterprise computing are not related to the core contribution.","Bioinformatics:1,Computational biology:0.7,Consumer health:0.3,Genetics:0.6,Genomics:0.8,Health care information systems:0.4,Health informatics:0.5,Metabolomics / metabonomics:0.2,Systems biology:0.6","Bioinformatics,Genomics",Bioinformatics is highly relevant as the paper focuses on protein domain analysis for disease mutations. Genomics is also relevant due to the study of human coding mutations. Other categories like Systems biology are secondary to the core focus.
3073,InSAR for estimation of changes in snow water equivalent of dry snow,"This study describes the theoretical relation between interferometric phase and changes in dry snow Snow Water Equivalent (SWE) or snow depth. Results from experiments using three ERS-1/2 tandem datasets from Norway obtained in middle of March (dry snow), late May (wet snow) and late July (snow free) are also reported. Presented theory and results implies that even small changes in snow properties between InSAR acquisitions introduces a significant error in DEM derived from glaciers, ice sheets or ground, even in the case of high coherence. Accordingly, the height error (or range displacement) can also be used to estimate changes in dry snow SWE or depth. A snow density of 0.3 g/cm/sup 3/ at 23/spl deg/ incidence angle gives phase wrapping for changes in snow depth of 10 cm, equal to a SWE of 3,0 cm.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.2,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.8,Social and professional topics:0.1",Applied computing,Applied computing is highly relevant as the paper uses InSAR technology for environmental monitoring of snow properties. Other categories like 'Networks' are less relevant as the focus is on remote sensing applications rather than network infrastructure.,"Arts and humanities:0.0,Computer forensics:0.0,Computers in other domains:0.8,Document management and text processing:0.0,Education:0.0,Electronic commerce:0.0,Enterprise computing:0.0,Law, social and behavioral sciences:0.0,Life and medical sciences:0.1,Operations research:0.0,Physical sciences and engineering:0.75","Computers in other domains,Physical sciences and engineering",Computers in other domains: Uses InSAR for environmental monitoring. Physical sciences and engineering: Applies the technique to snow property estimation. Life and medical sciences is less relevant as the focus is on environmental rather than biological systems.,"Aerospace:0.2,Agriculture:0.1,Archaeology:0.1,Astronomy:0.2,Cartography:0.3,Chemistry:0.1,Computing in government:0.1,Digital libraries and archives:0.1,Earth and atmospheric sciences:0.9,Electronics:0.2,Engineering:0.2,Mathematics and statistics:0.2,Military:0.1,Personal computers and PC applications:0.1,Physics:0.3,Publishing:0.1,Telecommunications:0.2",Earth and atmospheric sciences,Earth and atmospheric sciences is highly relevant as the paper focuses on environmental monitoring of snow properties using InSAR. Other categories like Cartography/Telecommunications are less relevant as the paper addresses environmental science applications.
2292,Optimal Synthesis for Nonholonomic Vehicles With Constrained Side Sensors,"We present a complete characterization of shortest paths to a goal position for a vehicle with unicycle kinematics and a limited range sensor, constantly keeping a given landmark in sight. Previous work on this subject studied the optimal paths in case of a frontal, symmetrically limited Field‐Of‐View (FOV). In this paper we provide a generalization to the case of arbitrary FOVs, including the case that the direction of motion is not an axis of symmetry for the FOV, and even that it is not contained in the FOV. The provided solution is of particular relevance to applications using side-scanning, such as e.g. in underwater sonar-based surveying and navigation.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.9,Social and professional topics:0.1",Applied computing,Applied computing is relevant because the paper addresses real-world robotic path planning for vehicles with sensor constraints. Categories like Computer systems organization are not central to the core contribution of applied control theory.,"Physical sciences and engineering:1.0,Computers in other domains:0.2,Operations research:0.2,Arts and humanities:0.2,Computer forensics:0.2,Education:0.2,Electronic commerce:0.2,Enterprise computing:0.2,Law, social and behavioral sciences:0.2,Life and medical sciences:0.2",Physical sciences and engineering,Physical sciences and engineering is the only relevant category as the paper presents optimal synthesis for nonholonomic vehicles with sensor constraints. Other options are unrelated to the robotics and control systems focus.,"Aerospace:0.2,Archaeology:0.1,Astronomy:0.1,Chemistry:0.1,Earth and atmospheric sciences:0.1,Electronics:0.3,Engineering:1.0,Mathematics and statistics:0.8,Physics:0.2,Telecommunications:0.1","Engineering,Mathematics and statistics",Engineering is core to vehicle kinematics and optimal path synthesis. Mathematics and statistics are relevant for analytical methods. Other categories like Electronics or Physics are only marginally relevant.
1340,Analysis of geosar dual-band InSAR data for peruvian forest,At present there is no consensus as to which remote sensing technologies are appropriate for tropical forest biomass estimation. Cloud cover in the tropics and biomass saturation suggest that a combination of low-frequency SAR and interferometry (either PolInSAR or dual-band interferometric SAR - DBInSAR) could provide a solution.,"General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.8,Social and professional topics:0.1",Applied computing,Applied computing is relevant for applying remote sensing to environmental analysis. Other categories are not central to the domain application.,"Arts and humanities:0.1,Computer forensics:0.1,Computers in other domains:0.5,Document management and text processing:0.1,Education:0.1,Electronic commerce:0.1,Enterprise computing:0.1,Law, social and behavioral sciences:0.1,Life and medical sciences:0.2,Operations research:0.1,Physical sciences and engineering:0.6","Computers in other domains,Physical sciences and engineering",Computers in other domains is relevant for remote sensing application. Physical sciences and engineering is relevant for SAR technology analysis. Other categories are less relevant as they don't address remote sensing or engineering applications.,"Aerospace:0.2,Agriculture:0.3,Archaeology:0.2,Astronomy:0.2,Cartography:0.3,Chemistry:0.2,Computing in government:0.5,Digital libraries and archives:0.3,Earth and atmospheric sciences:1.0,Electronics:0.2,Engineering:0.4,Mathematics and statistics:0.3,Military:0.2,Personal computers and PC applications:0.2,Physics:0.2,Publishing:0.2,Telecommunications:0.3",Earth and atmospheric sciences,"Earth and atmospheric sciences: The paper deals with remote sensing data analysis for forest monitoring, a core application in Earth sciences. Other children like cartography or telecommunications are less relevant as the focus is on data analysis rather than mapping or communication systems."
5020,Muscle strength and Mass Distribution Identification toward subject-specific musculoskeletal modeling,"In current biomechanics approach, the assumptions are commonly used in body-segment parameters and muscle strength parameters due to the difficulty in accessing those subject-specific values. Especially in the rehabilitation and sports science where each subject can easily have quite different anthropometry and muscle condition due to disease, age or training history, it would be important to identify those parameters to take benefits correctly from the recent advances in computational musculoskeletal modeling. In this paper, Mass Distribution Identification to improve the joint torque estimation and Muscle Strength Identification to improve the muscle force estimation were performed combined with previously proposed methods in muscle tension optimization. This first result highlights that the reliable muscle force estimation could be extracted after these identifications. The proposed framework toward subject-specific musculoskeletal modeling would contribute to a patient-oriented computational rehabilitation.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.3,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.8,Social and professional topics:0.1",Applied computing,Applied computing: The paper applies computational methods to biomechanics for personalized modeling. Other categories lack direct connection to application-driven computing in biological systems.,"Arts and humanities:0.0,Computer forensics:0.0,Computers in other domains:0.0,Document management and text processing:0.0,Education:0.0,Electronic commerce:0.0,Enterprise computing:0.0,Law, social and behavioral sciences:0.0,Life and medical sciences:1.0,Operations research:0.0,Physical sciences and engineering:0.0",Life and medical sciences,Life and medical sciences is highly relevant as the paper discusses musculoskeletal modeling for rehabilitation. Other categories are not directly related to this biomedical application.,"Bioinformatics:0.1,Computational biology:0.2,Consumer health:0.1,Genetics:0.1,Genomics:0.1,Health care information systems:0.2,Health informatics:1.0,Metabolomics / metabonomics:0.1,Systems biology:0.1",Health informatics,Health informatics is directly relevant to subject-specific musculoskeletal modeling for rehabilitation. Computational biology is mentioned but is secondary to health applications.
6040,A comparative study of two control schemes for anti-lock braking systems,"The commercial Bang-Bang anti-lock braking system has several short-comings. Therefore, there is the need for continual improvement on the ABS control strategy. One of the problems associated with these short-comings includes the physical shock experienced by drivers through the brake petal pulsation, when the system is activated. A non-linear and a neural network-based alternative ABS control schemes are proposed and evaluated. The main goal is for the ABS controller to maintain optimal system performance in terms of slip regulation and minimising the vehicle stopping distance during hard-braking. A comparative analysis of the two ABS controllers based on simulation results, showed the neural network-based controller to be superior.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.2,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.9,Social and professional topics:0.1",Applied computing,Applied computing is relevant as the paper presents an applied control system improvement for anti-lock braking systems. Other categories like Computer systems organization or Hardware are less relevant as the focus is on control strategy rather than hardware design.,"Arts and humanities:0.1,Computer forensics:0.1,Computers in other domains:0.3,Document management and text processing:0.1,Education:0.1,Electronic commerce:0.1,Enterprise computing:0.2,Law, social and behavioral sciences:0.1,Life and medical sciences:0.1,Operations research:0.6,Physical sciences and engineering:0.9","Physical sciences and engineering,Operations research",Physical sciences and engineering is highly relevant as the paper addresses automotive control systems. Operations research is relevant due to the optimization of control strategies. Other fields like Life and medical sciences are not relevant as the focus is on mechanical engineering applications.,"Aerospace:0.1,Archaeology:0.1,Astronomy:0.1,Chemistry:0.1,Computer-aided manufacturing:0.1,Consumer products:0.1,Decision analysis:0.1,Earth and atmospheric sciences:0.1,Electronics:0.2,Engineering:1.0,Forecasting:0.1,Industry and manufacturing:0.3,Marketing:0.1,Mathematics and statistics:0.5,Physics:0.1,Telecommunications:0.1,Transportation:1.0","Engineering,Transportation","Engineering is relevant because the paper focuses on improving ABS control systems, an engineering application. Transportation is relevant as the study addresses vehicle braking systems. Other categories like Mathematics and statistics are only tangentially relevant to the control theory used."
1095,Two Mitigation Strategies for Motion System Limits in Driving and Flight Simulators,"Limited workspace is a challenge for all motion-based simulators, whether they are large excursion systems like the National Advanced Driving Simulator or smaller simulators utilizing only Stewart Platforms. Two approaches for addressing this challenge are nonlinear washout scaling and software displacement limiting. This paper presents new algorithms developed for these approaches. The nonlinear scaling method uses the cubic Hermite interpolation polynomial to smooth the corner in the scaled output at the limit. A software displacement limiting method that generates control signals in the table frame of reference is introduced. As a result, unwanted acceleration artifacts caused by unbalanced limiting of actuators are avoided. The methods are described, and offline simulation results using the new displacement limiting method are presented.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.8,Social and professional topics:0.1",Applied computing,"Applied computing: The paper addresses simulator workspace limitations in driving/flight systems, which is an applied computing problem. Other categories like computer systems organization are less relevant compared to the domain-specific application focus.","Arts and humanities:0.1,Computer forensics:0.1,Computers in other domains:0.8,Document management and text processing:0.1,Education:0.1,Electronic commerce:0.1,Enterprise computing:0.1,Law, social and behavioral sciences:0.1,Life and medical sciences:0.1,Operations research:0.1,Physical sciences and engineering:0.8","Computers in other domains,Physical sciences and engineering",Computers in other domains: The paper addresses simulation technologies for driving and flight simulators. Physical sciences and engineering: The work involves engineering solutions for motion system limitations. Other children are irrelevant as the focus is on simulator technologies rather than education or commerce.,"Aerospace:1,Engineering:1,Agriculture:0,Archaeology:0,Astronomy:0,Cartography:0,Chemistry:0,Computing in government:0,Digital libraries and archives:0,Earth and atmospheric sciences:0,Electronics:0,Mathematics and statistics:0,Military:0,Personal computers and PC applications:0,Physics:0,Publishing:0,Telecommunications:0","Aerospace,Engineering",Aerospace is relevant due to flight simulator applications. Engineering is relevant for technical solutions to motion system challenges. Other fields like Agriculture or Chemistry are unrelated to motion simulator algorithms.
1029,A brain machine interface control algorithm designed from a feedback control perspective,"We present a novel brain machine interface (BMI) control algorithm, the recalibrated feedback intention-trained Kalman filter (ReFIT-KF). The design of ReFIT-KF is motivated from a feedback control perspective applied to existing BMI control algorithms. The result is two design innovations that alter the modeling assumptions made by these algorithms and the methods by which these algorithms are trained. In online neural control experiments recording from a 96-electrode array implanted in M1 of a macaque monkey, the ReFIT-KF control algorithm demonstrates large performance improvements over the current state of the art velocity Kalman filter, reducing target acquisition time by a factor of two, while maintaining a 500 ms hold period, thereby increasing the clinical viability of BMI systems.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.3,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.8,Social and professional topics:0.1",Applied computing,Applied computing is highly relevant for the BMI system application in clinical contexts. Software and its engineering is secondary for algorithm implementation. Other categories like Human-centered computing are less central to the core contribution.,"Arts and humanities:0.1,Computer forensics:0.1,Computers in other domains:0.2,Document management and text processing:0.1,Education:0.1,Electronic commerce:0.1,Enterprise computing:0.1,Law, social and behavioral sciences:0.1,Life and medical sciences:0.8,Operations research:0.1,Physical sciences and engineering:0.7","Life and medical sciences,Physical sciences and engineering",Life and medical sciences: The paper presents a BMI system for neural control with clinical applications. Physical sciences and engineering: The feedback control algorithm is an engineering approach to BMI systems. Other options like Arts and humanities or Education are irrelevant as the focus is on medical and engineering contributions.,"Aerospace:0.1,Archaeology:0.1,Astronomy:0.1,Bioinformatics:0.1,Chemistry:0.1,Computational biology:0.1,Consumer health:0.1,Earth and atmospheric sciences:0.1,Electronics:0.1,Engineering:0.1,Genetics:0.1,Genomics:0.1,Health care information systems:0.1,Health informatics:0.8,Mathematics and statistics:0.1,Metabolomics / metabonomics:0.1,Physics:0.1,Systems biology:0.2,Telecommunications:0.1",Health informatics,"Health informatics is relevant because the paper addresses BMI systems, which are critical in healthcare applications. Other categories like Systems biology are only moderately relevant but not as central as the primary domain."
4142,Public participation based interactive decision-making model for brownfield redevelopment projects (BRPs),"The BRPs require huge investment and have the characteristic of high public welfare. The lack of public participation in decision-making can easily cause the group events. To achieve the maximum public interest, this paper will introduce public participation into BRPs decision-making process. Using intuitionistic fuzzy evaluation, similarity degree method, cluster analysis establish the BRPs interactive decision-making model based on public participation. Finally, a numerical case is given to illustrate the feasibility of the model. The establishment of BRPs decision-making model with public participation has not been published in China. The paper can provide a certain theoretical guiding significance for public participation in BRPs.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.2,Mathematics of computing:0.2,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.3,Applied computing:0.9,Social and professional topics:0.3",Applied computing,Applied computing is directly relevant as the paper applies computational models to urban planning and public participation. Social and professional topics are secondary. Other categories like Information systems are not central to the core focus.,"Arts and humanities:0.0,Computer forensics:0.0,Computers in other domains:0.25,Document management and text processing:0.0,Education:0.0,Electronic commerce:0.0,Enterprise computing:0.0,Law, social and behavioral sciences:0.5,Life and medical sciences:0.0,Operations research:1.0,Physical sciences and engineering:0.0",Operations research,"Operations research is highly relevant for the public participation decision-making model. Law, social and behavioral sciences receives a lower score due to the focus on computational methods rather than social science theory.","Decision analysis:1,Forecasting:0.3,Computer-aided manufacturing:0.2,Consumer products:0.2,Industry and manufacturing:0.2,Marketing:0.2,Transportation:0.2",Decision analysis,"Decision analysis: The paper proposes an interactive decision-making model for public participation. Other categories like 'Forecasting' were rejected because the focus is on decision-making processes, not predictive modeling."
124,Extremum Seeking-Based Optimization of High Voltage Converter Modulator Rise-Time,"We digitally implement an extremum seeking (ES) algorithm, which optimizes the rise time of the output voltage of a high voltage converter modulator (HVCM) at the Los Alamos Neutron Science Center by iteratively, simultaneously tuning the first eight switching edges of each of the three-phase drive waveforms (24 variables total). We achieve a 50 μs rise time, which is reduction in half, compared to the 100 μs achieved at the Spallation Neutron Source at Oak Ridge National Laboratory. Considering that HVCMs typically operate with an output voltage of 100 kV, with a 60-Hz repetition rate, the 50 μs rise time reduction will result in very significant energy savings. The ES algorithm will prove successful, despite the noisy measurements and cost calculations, confirming the theoretical results that the algorithm is not affected by noise whose frequency components are independent of the perturbing frequencies.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.95,Social and professional topics:0.1",Applied computing,Applied computing is central for optimizing high voltage hardware systems using digital algorithms. Other categories lack direct connection to the engineering application focus.,"Arts and humanities:0.2,Computer forensics:0.2,Computers in other domains:0.2,Document management and text processing:0.2,Education:0.2,Electronic commerce:0.2,Enterprise computing:0.2,Law, social and behavioral sciences:0.2,Life and medical sciences:0.2,Operations research:0.8,Physical sciences and engineering:1.0","Physical sciences and engineering,Operations research","Physical sciences and engineering: The paper focuses on high-voltage converter modulator optimization in a scientific context. Operations research: Extremum seeking optimization is a method from operations research. Other children are irrelevant as the study is applied to engineering systems, not commerce or medical fields.","Aerospace:0.0,Archaeology:0.0,Astronomy:0.0,Chemistry:0.0,Computer-aided manufacturing:0.1,Consumer products:0.0,Decision analysis:0.1,Earth and atmospheric sciences:0.0,Electronics:1.0,Engineering:1.0,Forecasting:0.1,Industry and manufacturing:0.1,Marketing:0.0,Mathematics and statistics:0.2,Physics:0.1,Telecommunications:0.0,Transportation:0.1","Electronics,Engineering",Electronics and Engineering are relevant for optimizing a high voltage converter modulator. Mathematics and statistics is partially relevant for algorithm design but not the primary focus. Other categories like Physics are not central to the applied engineering optimization context.
648,"Three-dimensional modeling for functional analysis of cardiac images, a review","Three-dimensional (3-D) imaging of the heart is a rapidly developing area of research in medical imaging. Advances in hardware and methods for fast spatio-temporal cardiac imaging are extending the frontiers of clinical diagnosis and research on cardiovascular diseases. In the last few years, many approaches have been proposed to analyze images and extract parameters of cardiac shape and function from a variety of cardiac imaging modalities. In particular, techniques based on spatio-temporal geometric models have received considerable attention. This paper surveys the literature of two decades of research on cardiac modeling. The contribution of the paper is three-fold: (1) to serve as a tutorial of the field for both clinicians and technologists, (2) to provide an extensive account of modeling techniques in a comprehensive and systematic manner, and (3) to critically review these approaches in terms of their performance and degree of clinical evaluation with respect to the final goal of cardiac functional analysis. From this review it is concluded that whereas 3-D model-based approaches have the capability to improve the diagnostic value of cardiac images, issues as robustness, 3-D interaction, computational complexity and clinical validation still require significant attention.","General and reference:0.1,Hardware:0.2,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.2,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.95,Social and professional topics:0.1",Applied computing,Applied computing is relevant as the paper discusses medical imaging techniques for cardiac functional analysis. Other categories like Information systems are less relevant as the focus is on modeling rather than data management.,"Arts and humanities:0.1,Computer forensics:0.1,Computers in other domains:0.1,Document management and text processing:0.1,Education:0.1,Electronic commerce:0.1,Enterprise computing:0.1,Law, social and behavioral sciences:0.1,Life and medical sciences:0.9,Operations research:0.1,Physical sciences and engineering:0.1",Life and medical sciences,Life and medical sciences is highly relevant as the paper focuses on 3D cardiac image analysis for clinical diagnosis. Other options like Education or Law are irrelevant as the paper is about medical imaging research rather than educational applications or legal issues.,"Bioinformatics:0.5,Computational biology:1.0,Consumer health:0.0,Genetics:0.0,Genomics:0.0,Health care information systems:0.0,Health informatics:0.0,Metabolomics / metabonomics:0.0,Systems biology:0.0",Computational biology,"Computational biology is highly relevant as the paper focuses on spatio-temporal modeling techniques for cardiac function analysis. Bioinformatics receives partial relevance due to computational methods in biology, but the core focus is on modeling rather than data analysis."
4460,Collision avoidance and trajectory tracking control based on approximations of the maximum function,"In this paper, convergent over approximations of the maximum function (previously introduced and used for designing strategies for multiple players in pursuit-evasion games) are used to accomplish two objectives for multiple vehicles scenarios: collision avoidance and trajectory tracking. The fact that these approximations are upper approximations of the maximum functions allows us to establish guarantees on the performance (in terms of achieving the objectives) of the vehicles. We combine the approximations of the maximum function with the receding horizon control approach to design control laws. Finally, the method is illustrated by a representative example, where the control laws are designed for nonholonomic vehicles with actuator constraints.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.9,Social and professional topics:0.1",Applied computing,Applied computing: The paper presents control algorithms for real-world vehicle systems. Other categories: The work is not about pure computing methodologies or theory.,"Physical sciences and engineering:0.9,Operations research:0.7,Arts and humanities:0.1,Computer forensics:0.1,Computers in other domains:0.1,Document management and text processing:0.1,Education:0.1,Electronic commerce:0.1,Enterprise computing:0.1,Law, social and behavioral sciences:0.1,Life and medical sciences:0.1",Physical sciences and engineering,"Physical sciences and engineering is relevant for the control system design for vehicles. Operations research is partially relevant for optimization concepts, but the paper's primary domain is physical system control.","Aerospace:0.3,Archaeology:0.1,Astronomy:0.1,Chemistry:0.1,Earth and atmospheric sciences:0.1,Electronics:0.1,Engineering:1,Mathematics and statistics:0.8,Physics:0.1,Telecommunications:0.1","Engineering,Mathematics and statistics",Engineering: Involves vehicle control systems. Mathematics and statistics: Uses mathematical approximations for control theory. Other children have minimal relevance to the technical content.
2383,An age estimation method using brain local features for T1-weighted images,"Previous statistical analysis studies using large-scale brain magnetic resonance (MR) image databases have examined that brain tissues have age-related morphological changes. This fact indicates that one can estimate the age of a subject from his/her brain MR image by evaluating morphological changes with healthy aging. This paper proposes an age estimation method using local features extracted from T1-weighted MR images. The brain local features are defined by volumes of brain tissues parcellated into local regions defined by the automated anatomical labeling atlas. The proposed method selects optimal local regions to improve the performance of age estimation. We evaluate performance of the proposed method using 1,146 T1-weighted images from a Japanese MR image database. We also discuss the medical implication of selected optimal local regions.","General and reference:0.1,Hardware:0.2,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.3,Theory of computation:0.1,Mathematics of computing:0.4,Information systems:0.2,Security and privacy:0.1,Human-centered computing:0.2,Computing methodologies:0.5,Applied computing:0.9,Social and professional topics:0.1",Applied computing,"Applied computing: The paper applies machine learning techniques to medical imaging for age estimation. Computing methodologies is partially relevant for the method itself, but the primary domain is medical application. Other categories like Software engineering are less relevant as the focus is on the application rather than software development.","Arts and humanities:0.1,Computer forensics:0.1,Computers in other domains:0.2,Document management and text processing:0.1,Education:0.1,Electronic commerce:0.1,Enterprise computing:0.1,Law, social and behavioral sciences:0.1,Life and medical sciences:1.0,Operations research:0.5,Physical sciences and engineering:0.2","Life and medical sciences,Operations research",Life and medical sciences is directly relevant as the paper focuses on age estimation using brain MR images. Operations research is relevant for the optimization of local features. Other categories like Computer forensics are unrelated to the medical application.,"Bioinformatics:1,Computational biology:1,Consumer health:0.5,Health informatics:0.5,Metabolomics / metabonomics:0","Bioinformatics,Computational biology","Bioinformatics and Computational biology are relevant as the paper analyzes brain MR images for age estimation. Consumer health and Health informatics are secondary due to broader applications, while Metabolomics is irrelevant."
690,Peer to PCAST: What does open video have to do with open government?,"The Obama Administration has outlined a set of principles and practices to support Open Government in which citizens can collaborate with the government to solve problems. The Administration is using technology, especially web-based technology, to support Open Government in practice. Many of the government's websites include video. We examine the website built to support the President's Council of Advisers on Science and Technology (PCAST). We critique it and argue that a number of important design decisions made for the current site should be changed to better support Open Government. Key to our argument is what has come to be known as Open Video, an application of the ideals of Open Source Software to video. Our critique is followed by a discussion of a prototype system we have built to demonstrate an alternative to the current PCAST site. Our prototype is called Peer-to-PCAST to call attention to the similarities between our proposals and Peer-to-Patent, the first Open Government system built for a different context, the US Patent and Trademark Office [34].","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:1.0,Social and professional topics:0.1",Applied computing,Applied computing is directly relevant for the open government technology application. Other categories are rejected because the paper focuses on real-world government applications rather than pure software/hardware.,"Arts and humanities:0.2,Computer forensics:0.2,Computers in other domains:0.8,Document management and text processing:0.3,Education:0.2,Electronic commerce:0.2,Enterprise computing:0.2,Law, social and behavioral sciences:0.2,Life and medical sciences:0.2,Operations research:0.2,Physical sciences and engineering:0.2",Computers in other domains,"Computers in other domains is relevant as the paper applies web-based technology to open government. Other categories are irrelevant as the focus is not on art, forensics, or specific technical domains like commerce or education.","Agriculture:0.2,Cartography:0.2,Computing in government:1,Digital libraries and archives:0.2,Military:0.2,Personal computers and PC applications:0.2,Publishing:0.2",Computing in government,Computing in government: Focuses on open government initiatives using technology. Other options are unrelated to the context of government and open video.
2462,Urban Versus Rural: The Decrease of Agricultural Areas and the Development of Urban Zones Analyzed with Spatial Statistics,"Until a few decades ago it was very easy to distinguish between city and country: in most cases the edge was defined by defensive barriers. In recent times, the relationships between urban and rural areas completely changed, placing the country in a subordinate position. Consequently, many terms have been coined in order to describe the new phenomena taking place between city and country. The term adopted, â€œperiurban areaâ€ , despite its large use, does not have a clear and unambiguous definition. Such various approaches are due to the complexity of the phenomenon to be analyzed and to the huge variety of territorial contexts in which it may reveal. The phenomenon is characterized by urban growth with soil consumption generating loss of competitiveness for agricultural activities. This paper defines more precise rules in order to describe the periurban phenomenon, using techniques of spatial statistic and point pattern analysis. This approach has been tested in the case of study of Potenza municipality. Interest in this area comes after the earthquake of 1980, when a large migration of inhabitants began towards the countryside around Potenza.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.8,Social and professional topics:0.1",Applied computing,Applied computing is highly relevant for spatial statistics in urban planning. Other categories like Mathematics of computing are secondary to the applied domain focus.,"Arts and humanities:0.2,Computer forensics:0.2,Computers in other domains:0.75,Document management and text processing:0.2,Education:0.2,Electronic commerce:0.2,Enterprise computing:0.2,Law, social and behavioral sciences:0.75,Life and medical sciences:0.2,Operations research:0.2,Physical sciences and engineering:0.75","Computers in other domains,Law, social and behavioral sciences,Physical sciences and engineering","Computers in other domains applies to urban analysis. Law, social and behavioral sciences relates to urban-rural dynamics. Physical sciences and engineering is relevant for spatial statistics. Other options are not core.","Aerospace:0,Agriculture:1,Archaeology:0,Astronomy:0,Cartography:0,Chemistry:0,Computing in government:0,Digital libraries and archives:0,Earth and atmospheric sciences:0,Electronics:0,Engineering:0,Mathematics and statistics:1,Military:0,Personal computers and PC applications:0,Physics:0,Publishing:0,Telecommunications:0","Agriculture,Mathematics and statistics",Agriculture is relevant due to the focus on agricultural area reduction and periurban dynamics. Mathematics and statistics is relevant for the use of spatial statistics and point pattern analysis. Other fields like Aerospace or Physics are unrelated to the paper's focus on territorial changes and statistical modeling.
1407,Hierarchical Protein Structure Superposition Using Both Secondary Structure and Atomic Representations,"The structural comparison of proteins has become increasingly important as a means to identify protein motifs and fold families. In this paper we present a new algorithm for the comparison of proteins based on a hierarchy of structural representations, from the secondary structure level to the atomic level. Our technique represents alpha-helices and beta-strands as vectors and uses a set of seven scoring functions to compare pairs of vectors from different proteins. The scores obtained are used in a dynamic programming algorithm that finds the best local alignment of the two sets of vectors. The second step in our algorithm is based on the atomic coordinates of the protein structures and improves the initial vector alignment by iteratively minimizing the RMSD between pairs of nearest atoms from the two proteins. We refine the final alignment by determining a core of well aligned atoms and minimizing the RMSD of this core. In a comparison of our method to Holm and Sander's DALI algorithm, our program was able to detect structural similarity at the same level as DALI. We also performed searches of a representative set of the Protein Data Bank (PDB) using our program and detected structurally similarity between several distantly related proteins.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.75,Applied computing:1.0,Social and professional topics:0.0",Applied computing,Applied computing: The paper presents a bioinformatics method for protein structure comparison. Computing methodologies: The algorithmic aspects are secondary to the biological application.,"Arts and humanities:0.1,Computer forensics:0.05,Computers in other domains:0.2,Document management and text processing:0.05,Education:0.1,Electronic commerce:0.05,Enterprise computing:0.1,Law, social and behavioral sciences:0.1,Life and medical sciences:0.9,Operations research:0.8,Physical sciences and engineering:0.2","Life and medical sciences,Operations research",Life and medical sciences is directly relevant for protein structure analysis in biological contexts. Operations research applies to the optimization algorithms used in structural comparison. Other domains are unrelated to the paper's focus on bioinformatics and computational biology.,"Bioinformatics:1.0,Computational biology:1.0,Computer-aided manufacturing:0.0,Consumer health:0.0,Consumer products:0.0,Decision analysis:0.0,Forecasting:0.0,Genetics:0.0,Genomics:0.0,Health care information systems:0.0,Health informatics:0.0,Industry and manufacturing:0.0,Marketing:0.0,Metabolomics / metabonomics:0.0,Systems biology:0.0,Transportation:0.0","Bioinformatics,Computational biology",Bioinformatics and Computational biology are both highly relevant as the paper introduces a novel protein structure comparison algorithm central to bioinformatics research. All other categories are unrelated to structural biology or computational methods.
3061,A Framework for Clinical Decision Making and Medical Experience Storing,"Different models of Decision Support Systems (DSS) are used in medicine to help physicians in disease diagnosis, prognosis evaluation and therapy prescription. The DSS models rely on mathematical or computer theories. Each of them offers advantages and drawbacks. We propose a DSS system architecture which tries to integrate the different kinds of decision models and to use them to deal with the successive clinical decision steps. Besides, the decision process includes patient data, experts' knowledge, statistical and epidemiological data, experience. The Case Based Reasoning (CBR) approach is used to store and retrieve the previous clinical cases. The object case components are clustered, indexed and stored in an object data base. To sum up, we propose a framework for clinical decision making and experience storing based on the main DSS models. Each step of the decision process is supervised by a finite state automaton which triggers the appropriate module and knowledge or data sources. At last, we illustrate our approach through an example : the epilepsy diagnosis and therapy.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.3,Information systems:0.5,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:1.0,Social and professional topics:0.1",Applied computing,Applied computing (1.0) is highly relevant as the paper proposes a DSS framework for clinical decision-making in healthcare. Information systems (0.5) is moderately relevant due to the system architecture. Other categories are irrelevant as the paper focuses on medical application rather than computational theory.,"Arts and humanities:0.1,Computer forensics:0.1,Computers in other domains:0.9,Document management and text processing:0.2,Education:0.1,Electronic commerce:0.1,Enterprise computing:0.3,Law, social and behavioral sciences:0.1,Life and medical sciences:0.85,Operations research:0.1","Life and medical sciences,Computers in other domains",Life and medical sciences is relevant for clinical decision support in epilepsy. Computers in other domains is relevant for DSS application in medicine. Enterprise computing is not central to the framework.,"Agriculture:0,Bioinformatics:0,Cartography:0,Computational biology:0,Computing in government:0,Consumer health:0,Digital libraries and archives:0,Genetics:0,Genomics:0,Health care information systems:1,Health informatics:1,Metabolomics / metabonomics:0,Military:0,Personal computers and PC applications:0,Publishing:0,Systems biology:0","Health care information systems,Health informatics",Health care information systems and Health informatics are both highly relevant as the paper presents a clinical decision support framework integrating case-based reasoning and finite state automata for medical applications.
5097,Model predictive control architectures with force feedback for robotic-assisted beating heart surgery,"Minimally invasive surgery (MIS) offers considerable advantages for patients, lowering infection risks, and reducing trauma and convalescence times. Dedicated surgical robots significantly improve surgeon's skills especially for tasks requiring high precision such as lead placement and suturing. However, these robotic setups do not allow yet beating heart surgery with motion compensation functionalities. This paper tackles autonomous heart motion compensation with force feedback. We propose a cascade model predictive control (MPC) architecture with a Kalman Active Observer (AOB) in the loop, and compare it with the classical MPC approach. The cascade MPC-AOB control architecture has two loops. The inner one performs model-reference adaptive control, guaranteeing a desired force tracking dynamics. The outer one generates control actions to compensate physiological motions. Both MPC-based architectures are analyzed and experimentally evaluated. Two robots are used. A lightweight 4-DoF surgical robot generates desired surgical forces and a 3-DoF robot equipped with an ex vivo heart at the end-effector reproduces realistic heart motions.","General and reference:0.1,Hardware:0.2,Computer systems organization:0.3,Networks:0.1,Software and its engineering:0.4,Theory of computation:0.1,Mathematics of computing:0.3,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.8,Social and professional topics:0.1",Applied computing,Applied computing is highly relevant as the paper presents a robotic control system for surgical applications. Software and its engineering has moderate relevance for the control algorithm implementation.,"Arts and humanities:0.1,Computer forensics:0.0,Computers in other domains:0.7,Document management and text processing:0.0,Education:0.0,Electronic commerce:0.0,Enterprise computing:0.0,Law, social and behavioral sciences:0.0,Life and medical sciences:0.8,Operations research:0.1,Physical sciences and engineering:0.7","Life and medical sciences,Computers in other domains",Life and medical sciences: The paper applies control theory to robotic-assisted heart surgery. Computers in other domains: The control architecture is a novel application of computing in medical robotics. Physical sciences and engineering receives partial credit for the control methodology.,"Agriculture:0.0,Bioinformatics:0.1,Cartography:0.0,Computational biology:0.1,Computing in government:0.0,Consumer health:0.2,Digital libraries and archives:0.0,Genetics:0.0,Genomics:0.0,Health care information systems:0.0,Health informatics:0.6,Metabolomics / metabonomics:0.0,Military:0.0,Personal computers and PC applications:0.0,Publishing:0.0,Systems biology:0.0",Health informatics,Health informatics is the only relevant category as the paper addresses a medical application (beating heart surgery) with control systems. Other options like Bioinformatics or Genomics are irrelevant to the control algorithm focus.
5762,An adaptive compliant multi-finger approach-to-grasp strategy for objects with position uncertainties,"This paper presents an adaptive and compliant approach-to-grasp strategy for multi-finger robotic hands, to improve the performance of autonomous grasping when encountering object position uncertainties. With the proposed approach-to-grasp strategy, the first robot finger to experience unexpected impact would pause its movement in a compliant manner, and remains in contact with the object to minimize the unplanned motion of the target object. At the same time, the remainder of the fingers continuously, adaptively move toward re-adjusted grasping positions with respect to the first finger in contact with the object, without the need for on-line re-planning or re-grasping. An adaptive grasp control strategy based on spatial virtual spring framework is proposed to achieve local (e.g. not resorting to the robotic arm) in-hand adjustments of the fingers not yet in contact. As such, these fingers can be adaptively driven to the adjusted desired position to accomplish the grasp. Experimental results demonstrate that significantly larger position errors with respect to the hand workspace can be accommodated with the proposed adaptive compliant grasp control strategy. As much as 391% increase in position error area coverage has been achieved. Finally, beyond the quantitative analysis, additional observations during the extensive experiment trials are discussed qualitatively, to help examine several open issues, and further understand the approach-to-grasp phases of the robot hand tasks.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.9,Social and professional topics:0.1",Applied computing,Applied computing is highly relevant for robotics applications in autonomous grasping systems. Other categories lack direct connection to physical systems and control algorithms.,"Arts and humanities:0.1,Computer forensics:0.1,Computers in other domains:1.0,Document management and text processing:0.1,Education:0.2,Electronic commerce:0.1,Enterprise computing:0.1,Law, social and behavioral sciences:0.1,Life and medical sciences:0.3,Operations research:0.2,Physical sciences and engineering:0.5",Computers in other domains,Computers in other domains is highly relevant for robotics and control strategies. Physical sciences and engineering are secondary due to mechanical applications.,"Agriculture:0.2,Cartography:0.3,Computing in government:0.3,Digital libraries and archives:0.3,Military:0.2,Personal computers and PC applications:0.4,Publishing:0.2",,None of the provided categories directly relate to the paper's focus on robotics and grasp control strategies. The application areas listed are too broad and not aligned with the technical content of the paper.
85,Self-redundancy in Music,"Where a structural analysis can be produced for a musical artefact, variants of the artefact can often be obtained by `inverting' the analysis, in much the same way we produce novel sentences from a grammar. The paper describes use of information theory for purposes of deriving structural analyses of sequences, and shows how the method can be used with musical data, for purposes of generating novel musical patterns.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.3,Information systems:0.2,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.4,Applied computing:0.7,Social and professional topics:0.1",Applied computing,Applied computing is relevant for musical pattern generation using information theory. Other categories are less relevant as the focus is not on algorithms or security.,"Arts and humanities:0.9,Computer forensics:0.0,Computers in other domains:0.7,Document management and text processing:0.1,Education:0.0,Electronic commerce:0.0,Enterprise computing:0.0,Law, social and behavioral sciences:0.0,Life and medical sciences:0.0,Operations research:0.0,Physical sciences and engineering:0.0",Arts and humanities,Arts and humanities is highly relevant as the paper focuses on musical structure analysis. Computers in other domains receives a mid score due to computational methods applied to music.,"Architecture (buildings):0.1,Fine arts:0.3,Language translation:0.1,Media arts:0.2,Performing arts:0.4,Sound and music computing:0.9",Sound and music computing,Sound and music computing is directly relevant as the paper uses information theory for musical pattern generation. Performing arts is less specific to the computational methods described.
2533,Computational Strategies for Analyzing Data in Gene Expression Microarray Experiments,"Microarray analysis has become a widely used method for generating gene expression data on a genomic scale. Microarrays have been enthusiastically applied in many fields of biological research, even though several open questions remain about the analysis of such data. A wide range of approaches are available for computational analysis, but no general consensus exists as to standard for microarray data analysis protocol. Consequently, the choice of data analysis technique is a crucial element depending both on the data and on the goals of the experiment. Therefore, basic understanding of bioinformatics is required for optimal experimental design and meaningful interpretation of the results. This review summarizes some of the common themes in DNA microarray data analysis, including data normalization and detection of differential expression. Algorithms are demonstrated by analyzing cDNA microarray data from an experiment monitoring gene expression in T helper cells. Several computational biology strategies, along with their relative merits, are overviewed and potential areas for additional research discussed. The goal of the review is to provide a computational framework for applying and evaluating such bioinformatics strategies. Solid knowledge of microarray informatics contributes to the implementation of more efficient computational protocols for the given data obtained through microarray experiments.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.9,Social and professional topics:0.1",Applied computing,Applied computing is highly relevant as the paper discusses computational strategies for gene expression microarray analysis. Other categories like Information systems are irrelevant as the focus is on biological data analysis rather than information management.,"Arts and humanities:0.1,Computer forensics:0.1,Computers in other domains:0.8,Document management and text processing:0.1,Education:0.1,Electronic commerce:0.1,Enterprise computing:0.1,Law, social and behavioral sciences:0.1,Life and medical sciences:0.9,Operations research:0.1,Physical sciences and engineering:0.1",Life and medical sciences,"Life and medical sciences: The paper analyzes gene expression data using computational biology strategies. Other children: Computers in other domains is low as the focus is on biological applications, not general computational systems.","Bioinformatics:1.0,Computational biology:1.0,Consumer health:0.0,Genetics:0.5,Genomics:1.0,Health care information systems:0.0,Health informatics:0.0,Metabolomics / metabonomics:0.0,Systems biology:0.0","Bioinformatics,Computational biology,Genomics",Bioinformatics: Reviews computational strategies for microarray analysis. Computational biology: Focuses on gene expression data processing. Genomics: Microarray data is analyzed on a genomic scale. Other options like Health informatics are irrelevant.
1397,A federation object coordinator for simulation based control and analysis,"This paper presents an architecture and a design for a federation object coordinator (FOC) for simulation based control and analysis. This research focuses on developing a methodology for implementing a distributed simulation control mechanism which can be adopted to virtual manufacturing or virtual enterprises. In this method, distributed fast or real time simulation models interact with low level controllers and among themselves to actively control a system. The timing and coordination requirements of the simulation models to interact with the MRP systems and control systems as well as the interaction among the distributed simulation models are discussed in this paper.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:1.0,Social and professional topics:0.0",Applied computing,Applied computing is relevant for virtual manufacturing and enterprise simulation. Other categories lack connection to industrial simulation and control systems.,"Arts and humanities:0.0,Computer forensics:0.0,Computers in other domains:0.0,Document management and text processing:0.0,Education:0.0,Electronic commerce:0.0,Enterprise computing:0.9,Law, social and behavioral sciences:0.0,Life and medical sciences:0.0,Operations research:0.3,Physical sciences and engineering:0.5","Enterprise computing,Modeling and simulation",Enterprise computing is relevant because the paper discusses distributed simulation control for virtual enterprises and manufacturing systems. Modeling and simulation receives a high score due to the focus on simulation model coordination and interaction. Other options like Operations research and Physical sciences and engineering receive lower scores because the paper focuses on simulation methodology rather than optimization techniques or physical system modeling.,"Business process management:0.3,Business rules:0,Business-IT alignment:0.2,Enterprise architectures:0.4,Enterprise computing infrastructures:1,Enterprise data management:0.1,Enterprise information systems:0.3,Enterprise interoperability:0.2,Enterprise modeling:0.1,Enterprise ontologies, taxonomies and vocabularies:0,Event-driven architectures:0.2,IT architectures:0.3,IT governance:0.1,Model development and analysis:0.5,Reference models:0,Service-oriented architectures:0.2,Simulation evaluation:0.6,Simulation support systems:1,Simulation theory:0.7,Simulation types and techniques:1","Enterprise computing infrastructures,Simulation support systems,Simulation types and techniques",Enterprise computing infrastructures is relevant as the paper discusses distributed simulation for enterprise systems. Simulation support systems is relevant for the FOC architecture. Simulation types and techniques is relevant for the control and coordination methodology. Business process management is less relevant as the focus is on technical coordination rather than business processes.
2532,Quantitative Comparison of Catalytic Mechanisms and Overall Reactions in Convergently Evolved Enzymes: Implications for Classification of Enzyme Function,"Functionally analogous enzymes are those that catalyze similar reactions on similar substrates but do not share common ancestry, providing a window on the different structural strategies nature has used to evolve required catalysts. Identification and use of this information to improve reaction classification and computational annotation of enzymes newly discovered in the genome projects would benefit from systematic determination of reaction similarities. Here, we quantified similarity in bond changes for overall reactions and catalytic mechanisms for 95 pairs of functionally analogous enzymes (non-homologous enzymes with identical first three numbers of their EC codes) from the MACiE database. Similarity of overall reactions was computed by comparing the sets of bond changes in the transformations from substrates to products. For similarity of mechanisms, sets of bond changes occurring in each mechanistic step were compared; these similarities were then used to guide global and local alignments of mechanistic steps. Using this metric, only 44% of pairs of functionally analogous enzymes in the dataset had significantly similar overall reactions. For these enzymes, convergence to the same mechanism occurred in 33% of cases, with most pairs having at least one identical mechanistic step. Using our metric, overall reaction similarity serves as an upper bound for mechanistic similarity in functional analogs. For example, the four carbon-oxygen lyases acting on phosphates (EC 4.2.3) show neither significant overall reaction similarity nor significant mechanistic similarity. By contrast, the three carboxylic-ester hydrolases (EC 3.1.1) catalyze overall reactions with identical bond changes and have converged to almost identical mechanisms. The large proportion of enzyme pairs that do not show significant overall reaction similarity (56%) suggests that at least for the functionally analogous enzymes studied here, more stringent criteria could be used to refine definitions of EC sub-subclasses for improved discrimination in their classification of enzyme reactions. The results also indicate that mechanistic convergence of reaction steps is widespread, suggesting that quantitative measurement of mechanistic similarity can inform approaches for functional annotation.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.9,Social and professional topics:0.1",Applied computing,Applied computing is highly relevant as the paper focuses on computational methods for enzyme function classification and bioinformatics. Other categories like Theory of computation are irrelevant as the paper addresses biological applications rather than foundational algorithms.,"Arts and humanities:0.1,Computer forensics:0.1,Computers in other domains:0.8,Document management and text processing:0.1,Education:0.1,Electronic commerce:0.1,Enterprise computing:0.1,Law, social and behavioral sciences:0.1,Life and medical sciences:0.9,Operations research:0.1,Physical sciences and engineering:0.1",Life and medical sciences,Life and medical sciences: The study focuses on enzyme catalytic mechanisms and biochemical classification. Other children: Computers in other domains is low as the paper is purely biological with no computational system focus.,"Bioinformatics:1.0,Computational biology:1.0,Consumer health:0.0,Genetics:0.3,Genomics:0.7,Health care information systems:0.0,Health informatics:0.0,Metabolomics / metabonomics:0.0,Systems biology:0.3","Bioinformatics,Computational biology",Bioinformatics: Analyzes enzyme function using computational methods. Computational biology: Studies catalytic mechanisms and reaction classification. Other options like Genomics are less central to the enzyme mechanism focus.
2510,Robust Model Predictive Control for humanoids standing balancing,"This paper presents the implementations of Model Predictive Control for the standing balance control of a humanoid to reject external disturbances. The strategies allow the robot to have a compliant behaviour against external forces resulting in a stable and smooth response. The first, ZMP based controller, compensates for the center of mass deviation while the second, attitude controller, regulates the orientation of the body to counterbalance the external disturbances. These two control strategies are combined as an integrated stabilizer, which further increases the effectiveness. Simulation studies on the COMAN humanoid are presented and the data are analysed. The simulations show significant improvements in rejection of external disturbances compared to an existing compliant stabilizer.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.2,Applied computing:1.0,Social and professional topics:0.0",Applied computing,"Applied computing: The paper presents a robotics application of Model Predictive Control for humanoid balance. Computing methodologies is marginally relevant for the algorithmic approach, but the primary focus is on real-world robotic control systems.","Arts and humanities:0.1,Computer forensics:0.1,Computers in other domains:0.2,Document management and text processing:0.1,Education:0.1,Electronic commerce:0.1,Enterprise computing:0.1,Law, social and behavioral sciences:0.1,Life and medical sciences:0.1,Operations research:0.1,Physical sciences and engineering:1.0",Physical sciences and engineering,Physical sciences and engineering is highly relevant as the paper focuses on humanoid robot control systems in robotics. Other categories are not applicable since the work is engineering-focused rather than computational or social.,"Aerospace:0.1,Archaeology:0.0,Astronomy:0.0,Chemistry:0.0,Earth and atmospheric sciences:0.0,Electronics:0.0,Engineering:1.0,Mathematics and statistics:1.0,Physics:0.0,Telecommunications:0.0","Engineering,Mathematics and statistics",Engineering is relevant due to the implementation of Model Predictive Control for humanoid robotics. Mathematics and statistics are relevant for the algorithmic foundations of the control strategies. Other fields lack direct relevance to the paper's focus on control systems and simulation.
695,Iterative Linear Quadratic Regulator Design for Nonlinear Biological Movement Systems,"This paper presents an Iterative Linear Quadratic Regulator (ILQR) method for locally-optimal feedback control of nonlinear dynamical systems. The method is applied to a musculo-skeletal arm model with 10 state dimensions and 6 controls, and is used to compute energy-optimal reaching movements. Numerical comparisons with three existing methods demonstrate that the new method converges substantially faster and finds slightly better solutions.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.9,Social and professional topics:0.1",Applied computing,Applied computing is directly relevant as the method is applied to biological movement systems. Other categories are irrelevant since the focus is on control theory application in biomechanics rather than core computing systems.,"Arts and humanities:0.1,Computer forensics:0.1,Computers in other domains:0.1,Document management and text processing:0.1,Education:0.1,Electronic commerce:0.1,Enterprise computing:0.1,Law, social and behavioral sciences:0.1,Life and medical sciences:0.85,Operations research:0.3,Physical sciences and engineering:0.2",Life and medical sciences,"Life and medical sciences is relevant because the paper applies the ILQR method to a musculo-skeletal arm model for movement control, directly relating to biomedical applications. Operations research receives a moderate score due to optimization aspects, but the primary focus is on life sciences applications.","Bioinformatics:0.3,Computational biology:1.0,Consumer health:0.2,Genetics:0.1,Genomics:0.1,Health care information systems:0.2,Health informatics:0.2,Metabolomics / metabonomics:0.1,Systems biology:1.0","Computational biology,Systems biology","Computational biology: The paper applies ILQR to a musculo-skeletal model, a computational method for biological movement systems. Systems biology: The work models biological systems (arm movement) as dynamical systems, aligning with systems biology. Other options (e.g., Genetics) are irrelevant as the paper focuses on control theory and biomechanics, not genetic or genomic data."
2627,Distributed storage for intermittent energy sources: Control design and performance limits,"One of the most important challenges in the integration of renewable energy sources into the power grid lies in their ‘intermittent’ nature. The power output of sources like wind and solar varies with time and location due to factors that cannot be controlled by the provider. Two strategies have been proposed to hedge against this variability: 1) use energy storage systems to effectively average the produced power over time; 2) exploit distributed generation to effectively average production over location. We introduce a network model to study the optimal use of storage and transmission resources in the presence of random energy sources. We propose a Linear-Quadratic based methodology to design control strategies, and show that these strategies are asymptotically optimal for some simple network topologies. For these topologies, the dependence of optimal performance on storage and transmission capacity is explicitly quantified.","General and reference:0,Hardware:0.2,Computer systems organization:0.1,Networks:0.3,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.2,Information systems:0,Security and privacy:0,Human-centered computing:0,Computing methodologies:0.1,Applied computing:1,Social and professional topics:0",Applied computing,"Applied computing is highly relevant as the paper applies control theory to energy storage for renewable energy systems. Networks and Hardware are less directly relevant as the focus is on energy system modeling, not communication or physical devices.","Arts and humanities:0.1,Computer forensics:0.1,Computers in other domains:0.1,Document management and text processing:0.1,Education:0.1,Electronic commerce:0.1,Enterprise computing:0.1,Law, social and behavioral sciences:0.1,Life and medical sciences:0.1,Operations research:0.8,Physical sciences and engineering:0.7","Physical sciences and engineering,Operations research",Physical sciences and engineering: The paper addresses energy storage and transmission in renewable systems. Operations research: It formulates and solves an optimization problem for resource allocation. Other categories like Education are irrelevant as the focus is on technical energy systems.,"Aerospace:0.1,Archaeology:0.1,Astronomy:0.1,Chemistry:0.1,Computer-aided manufacturing:0.1,Consumer products:0.1,Decision analysis:0.3,Earth and atmospheric sciences:0.1,Electronics:0.1,Engineering:0.2,Forecasting:0.1,Industry and manufacturing:0.1,Marketing:0.1,Mathematics and statistics:0.7,Physics:0.1,Telecommunications:0.1,Transportation:0.1",Mathematics and statistics,Mathematics and statistics: The paper presents a mathematical framework for optimizing storage in energy systems. Other categories like 'Engineering' are too broad and do not capture the mathematical modeling focus.
5427,Agent-Based Modelling of Primitive Human Communities,"The task of tracing what underlies the transition from kinship tribes to nation states continues to remain an important challenge for interdisciplinary study. Multi-agent simulation can shed light on these evolutionary processes by examining the emergent social behaviour that arises from individual agent interactions. In this paper, we build a model based on existing observational and simulation studies of pre-contact Pacific Island hunter-gatherer societies. Starting with the simple tribal structure, we examine how different societies' structures were affected by various characteristics and strategies of their chiefs. Our model represents the influence of societies' structure on how agents fulfil their basic needs and the consequences of an agent's action on both short term and long term society's survival. The evolving societal structures of the model have long-term effects on wealth inequality and whether the society grows or collapses. The results of an extensive model exploration encourage the idea that significantly different outcomes in social welfare do not necessarily require massive changes to all the agents, but can be achieved by relatively moderate modifications in social structure and the governance of societies.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.2,Computing methodologies:0.2,Applied computing:0.9,Social and professional topics:0.1",Applied computing,"Applied computing is relevant because the paper applies agent-based modeling to study prehistoric societal evolution, an interdisciplinary application in social sciences. Computing methodologies could be secondary if the focus were on simulation techniques, but the primary domain is the application to social theory.","Arts and humanities:0.0,Computer forensics:0.0,Computers in other domains:0.0,Document management and text processing:0.0,Education:0.0,Electronic commerce:0.0,Enterprise computing:0.0,Law, social and behavioral sciences:1.0,Life and medical sciences:0.0,Operations research:0.0,Physical sciences and engineering:0.0","Law, social and behavioral sciences","Law, social and behavioral sciences: The paper models primitive human communities using multi-agent systems to study social evolution, which is a core topic in social sciences. Other categories (e.g., computers in other domains) are irrelevant as the focus is on social modeling, not computational tools.",,,All categories are irrelevant as the input provides no possible children for classification.
391,A Universal Control Approach for a Family of Uncertain Nonlinear Systems,"We study the problem of global adaptive stabilization by output feedback for nonlinear systems with unknown parameters. The class of uncertain systems under consideration is assumed to be dominated by a bounding system which is linear growth in the unmeasurable states but can be a polynomial function of the system output, with unknown growth rate. To achieve global stabilization in the presence of parametric uncertainty, we propose a non-identifier based output feedback control law using the idea of universal control integrated with the linear-like output feedback control scheme proposed recently. In particular, we explicitly design a universal output feedback controller which globally regulates all the states of the uncertain system while maintaining global boundedness of the closed-loop system.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.3,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.9,Social and professional topics:0.1",Applied computing,Applied computing is highly relevant as the paper addresses control systems for nonlinear processes with real-world applications. Mathematics of computing is secondary to the applied focus.,"Arts and humanities:0.25,Computer forensics:0.25,Computers in other domains:0.25,Document management and text processing:0.25,Education:0.25,Electronic commerce:0.25,Enterprise computing:0.25,Law, social and behavioral sciences:0.25,Life and medical sciences:0.25,Operations research:0.25,Physical sciences and engineering:1.0",Physical sciences and engineering,Physical sciences and engineering is directly relevant as the paper presents control theory for nonlinear systems. Other categories are not mentioned in the abstract or title.,"Engineering:1,Mathematics and statistics:1,Physics:0.6,Aerospace:0.3,Chemistry:0.2,Earth and atmospheric sciences:0.2,Electronics:0.4,Archaeology:0.1,Astronomy:0.2,Telecommunications:0.3","Engineering,Mathematics and statistics",Engineering is relevant as the paper addresses control systems design. Mathematics and statistics is relevant due to the use of mathematical models for stabilization. Other categories like 'Physics' or 'Electronics' are not central to the paper's focus.
1606,Real-Time Monitoring and Economic Dispatch of Smart Distribution Grids: High Performance Algorithms for DMS Applications,"Power distribution networks are changing its traditional operation paradigm, due to the introduction of the smart grid concept, which encompasses, among others, continuous demand variations, as well as continuous dispatch variations of the distributed generation units. The latter may be controllable or noncontrollable. This paradigm shift means to switch from a fixed radial operative topology network to a more dynamical configuration, which changes during the day and hour by hour. In this sense, the massive incorporation of new telecontrol and telecommunication technologies allows a dynamic operation in real-time (RT) of distribution networks. In order to achieve an optimal network operation, it is required a continuous RT monitoring, control and economic dispatch by means of a smart distribution management system (DMS). The named system must count on high performance algorithms and RT information systems. This paper presents three very efficient algorithms (in terms of speed, robustness and accuracy), which are: load estimation, ac power flow, and optimal reconfiguration for loss minimization. Furthermore, these algorithms have been tested in many real and large scale distribution networks for several utilities and have presented remarkable features for its implementation in a DMS for smart distribution grids (SDGs).","General and reference:0.2,Hardware:0.1,Computer systems organization:0.1,Networks:0.3,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.9,Social and professional topics:0.1",Applied computing,Applied computing is relevant because the paper focuses on applying algorithms to smart distribution grid management. Other categories like Networks and Hardware are less relevant as the core contribution is about algorithmic solutions for power systems.,"Operations research:0.85,Computers in other domains:0.8,Physical sciences and engineering:0.3,Arts and humanities:0.1,Computer forensics:0.1,Document management and text processing:0.1,Education:0.1,Electronic commerce:0.1,Enterprise computing:0.1,Law, social and behavioral sciences:0.1,Life and medical sciences:0.1","Operations research,Computers in other domains",Operations research is highly relevant for the economic dispatch and optimization algorithms. Computers in other domains is relevant for the application in smart grid management. Other categories like Physical sciences and engineering are less relevant as the focus is on algorithmic optimization rather than engineering.,"Agriculture:0,Cartography:0,Computer-aided manufacturing:0.1,Computing in government:0.3,Consumer products:0,Decision analysis:0.6,Digital libraries and archives:0,Forecasting:0.4,Industry and manufacturing:0.9,Marketing:0,Military:0,Personal computers and PC applications:0,Publishing:0,Transportation:0","Industry and manufacturing,Decision analysis,Computing in government",Industry and manufacturing is highly relevant due to the focus on smart grid algorithms for power distribution optimization. Decision analysis is relevant for economic dispatch and real-time monitoring. Computing in government is secondary as it involves public infrastructure management. Other categories like Agriculture or Military are irrelevant to the paper's core focus on grid operations.
4942,ACM international workshop on cloud-based multimedia applications and services for e-health(CBMAS-EH 2012),"Cloud-Based Multimedia services and technologies are emerging as an innovative means of accessing and delivering e-health resources and services in the years to come. The research in multimedia cloud for E-health is still in its infancy, and several technical issues remain open. Prior to its general use and adoption, careful consideration and evaluation are required. This article provides a summary and overview of the First International ACM workshop on Cloud-Based Multimedia Applications and Services for E-Health.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.3,Networks:0.5,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.5,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:1.0,Social and professional topics:0.1",Applied computing,Applied computing is directly relevant as the paper focuses on cloud-based multimedia applications in e-health. Other categories like Networks/Information systems are only tangentially relevant to infrastructure considerations. Core contribution is about e-health applications.,"Arts and humanities:0.1,Computer forensics:0.1,Computers in other domains:0.1,Document management and text processing:0.1,Education:0.1,Electronic commerce:0.1,Enterprise computing:0.3,Law, social and behavioral sciences:0.1,Life and medical sciences:1.0,Operations research:0.1,Physical sciences and engineering:0.1",Life and medical sciences,"Life and medical sciences: The workshop focuses on e-health applications of cloud-based multimedia. Other categories (e.g., Enterprise computing) are secondary to the primary e-health context.","Bioinformatics:0,Computational biology:0,Consumer health:0.5,Genetics:0,Genomics:0,Health care information systems:1,Health informatics:1,Metabolomics / metabonomics:0,Systems biology:0","Health care information systems,Health informatics",Health care information systems and health informatics align with cloud-based e-health services. Consumer health is marginally relevant.
1925,Effects of vessel geometry and catheter position on dose delivery in intracoronary brachytherapy,"In-stent restenosis is commonly observed in coronary arteries after intervention. Intravascular brachytherapy has been found effective in reducing the recurrence of restenosis after stent placement. Conventional dosing models for brachytherapy with beta (/spl beta/) radiation neglect vessel geometry as well as the position of the delivery catheter. This paper demonstrates in computer simulations on phantoms and on in vivo patient data that the estimated dose distribution varies substantially in curved vessels. In simulated phantoms of 50-mm length with a shape corresponding to a 60/spl deg/-180/spl deg/ segment of a respectively sized torus, the average dose in 2-mm depth was decreased by 2.70%-7.48% at the outer curvature and increased by 2.95%-9.70% at the inner curvature as compared with a straight phantom. In vivo data were represented in a geometrically correct three-dimensional model that was derived by fusion of intravascular ultrasound (IVUS) and biplane angiography. These data were compared with a simplified tubular model reflecting common assumptions of conventional dosing schemes. The simplified model yielded significantly lower estimates of the delivered radiation and the dose variability as compared with a geometrically correct model (p < 0.001). The estimated dose in ten vessel segments of eight patients was on average 8.76% lower at the lumen/plaque and 6.52% lower at the media/adventitia interfaces (simplified tubular model relative to geometrically correct model). The differences in dose estimates between the two models were significantly higher in the right coronary artery as compared with the left coronary artery (p < 0.001).","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.8,Social and professional topics:0.1",Applied computing,"Applied computing is highly relevant as the paper uses computer simulations for biomedical applications (intracoronary brachytherapy). Other categories like Software and its engineering (0.2) and Computing methodologies (0.1) receive low scores because the focus is on application of computing to medical problems, not core CS theory or software design.","Arts and humanities:0.0,Computer forensics:0.0,Computers in other domains:1.0,Document management and text processing:0.0,Education:0.0,Electronic commerce:0.0,Enterprise computing:0.0,Law, social and behavioral sciences:0.0,Life and medical sciences:1.0,Operations research:0.0,Physical sciences and engineering:0.3","Life and medical sciences,Computers in other domains",Life and medical sciences is highly relevant because the paper addresses intracoronary brachytherapy for coronary restenosis. Computers in other domains is relevant as computational models are applied to medical problems. Physical sciences and engineering is less central to the application focus.,"Health informatics:1,Computational biology:1,Health care information systems:0.8,Bioinformatics:0.5,Systems biology:0.4,Consumer health:0.3,Digital libraries and archives:0.2,Genomics:0.2","Health informatics,Computational biology",Health informatics is relevant due to the medical application context. Computational biology is relevant as the paper uses simulations for dose delivery analysis. Other categories like bioinformatics are less relevant as the focus is on medical physics rather than biological data analysis.
4773,Faraday Rotation Correction for the SMAP Radiometer,"Faraday rotation is an important issue for remote sensing of parameters such as soil moisture and ocean salinity, which are best done at low microwave frequency (e.g., L-band). Modern instruments such as the radiometer on the Soil Moisture and Ocean Salinity (SMOS) satellite and the Aquarius radiometers include polarimetric radiometer channels specifically to implement a correction for Faraday rotation. This works well over ocean, but it is known that over inhomogeneous scenes, such as a land/water mixture, significant errors can occur. This is a particularly important issue for the newest L-band sensor in space, the radiometer on the Soil Moisture Active Passive (SMAP) satellite, where the goal is remote sensing over land (soil moisture) and where the conical scan induces rapid variation in Faraday rotation. Analysis is presented here of the issues associated with retrieving Faraday rotation using the SMAP geometry and antenna pattern. It is shown that, in addition to scenes with a mixture of land and water, scenes with significant vegetation canopy are also associated with large errors in the retrieved Faraday rotation. Examples from the SMAP radiometer support the analysis.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.9,Social and professional topics:0.0",Applied computing,Applied computing is highly relevant because the paper addresses real-world remote sensing challenges in satellite systems. Other categories like Hardware or Networks are less relevant as the focus is on algorithmic correction rather than infrastructure.,"Arts and humanities:0.1,Computer forensics:0.1,Computers in other domains:0.1,Document management and text processing:0.1,Education:0.1,Electronic commerce:0.1,Enterprise computing:0.1,Law, social and behavioral sciences:0.1,Life and medical sciences:0.2,Operations research:0.1,Physical sciences and engineering:0.9",Physical sciences and engineering,"Physical sciences and engineering is highly relevant as the paper addresses Faraday rotation correction in a satellite radiometer, a physical phenomenon in remote sensing. Other categories like Life and medical sciences are marginally relevant due to soil moisture applications but not central.","Aerospace:0,Archaeology:0,Astronomy:0,Chemistry:0,Earth and atmospheric sciences:1,Electronics:0,Engineering:1,Mathematics and statistics:0,Physics:1,Telecommunications:0","Earth and atmospheric sciences,Physics",Earth and atmospheric sciences are relevant as the paper focuses on remote sensing for soil moisture and salinity. Physics is central to the discussion of Faraday rotation. Engineering is moderately relevant but not as core as the other two. Other children like aerospace or chemistry are not directly addressed.
2014,Stochastic Optimization of Grid to Vehicle Frequency Regulation Capacity Bids,This paper investigates the application of stochastic dynamic programming to the optimization of charging and frequency regulation capacity bids of an electric vehicle (EV) in a smart electric grid environment. We formulate a Markov decision problem to minimize an EV's expected cost over a fixed charging horizon. We account for both Markov random prices and a Markov random regulation signal. We also propose an enhancement to the classical discrete stochastic dynamic programming method. This enhancement allows optimization over a continuous space of decision variables via linear programming at each state. Simple stochastic process models are built from real data and used to simulate the implementation of the proposed method. The proposed method is shown to outperform deterministic model predictive control in terms of average EV charging cost.,"General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.2,Applied computing:1.0,Social and professional topics:0.0",Applied computing,"Applied computing: The paper applies stochastic optimization techniques to smart grid EV charging, which is a direct application to real-world energy systems. Computing methodologies received partial score for the optimization algorithms but applied computing is the primary domain.","Arts and humanities:0.1,Computer forensics:0.1,Computers in other domains:0.3,Document management and text processing:0.1,Education:0.1,Electronic commerce:0.1,Enterprise computing:0.1,Law, social and behavioral sciences:0.1,Life and medical sciences:0.1,Operations research:0.9,Physical sciences and engineering:0.3",Operations research,"Operations research: The paper uses stochastic dynamic programming and Markov decision processes for optimization, which are core operations research techniques. Other categories like 'Computers in other domains' (0.3) are marginally relevant due to smart grid application but not primary focus.","Computer-aided manufacturing:0.1,Consumer products:0.1,Decision analysis:0.9,Forecasting:0.7,Industry and manufacturing:0.2,Marketing:0.1,Transportation:0.6","Decision analysis,Forecasting,Transportation",Decision analysis is relevant because the paper uses stochastic optimization for decision-making under uncertainty. Forecasting is relevant due to modeling of random prices and regulation signals. Transportation is relevant as it involves electric vehicles in grid environments. Other categories lack direct connection to the paper's core focus on optimization and EV systems.
3984,Exploring reconfiguration alternatives in self-organising evolvable production systems through simulation,Simulation has played an important role along the years to predict systems' behaviour before their deployment. In the case of self-organising mechatronic systems simulation tools can help researchers and practitioners understanding the full potential of the solution as well as its underlying limitations. Self-organising mechatronic systems have passed a feasibility study and presented promising results. However they are rarely explored in industry in part due to the lack of methods to support their design and configuration and the difficulty to predict the systems' behaviour before their deployment. Given the cost and development time associated with building self-organising mechatronic systems this research problem has been left quite unattended. In this article we present a tool that enables the creation and simulation of Evolvable Production Systems and their self-organising behaviour. The generated operational results can posteriorly be used to analyse the suitability of different design and configuration alternatives for different product types and volumes.,"General and reference:0.1,Hardware:0.1,Computer systems organization:0.2,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:1.0,Social and professional topics:0.1",Applied computing,Applied computing is relevant because the paper focuses on simulation tools for analyzing evolvable production systems in industrial contexts. Other categories like 'Computer systems organization' or 'Networks' are less directly related to the core application domain.,"Arts and humanities:0.1,Computer forensics:0.1,Computers in other domains:1.0,Document management and text processing:0.2,Education:0.1,Electronic commerce:0.1,Enterprise computing:0.3,Law, social and behavioral sciences:0.1,Life and medical sciences:0.1,Operations research:0.3,Physical sciences and engineering:0.2",Computers in other domains,Computers in other domains is relevant for applying simulation in production systems. Categories like 'Enterprise computing' are less relevant as the focus is on simulation tools rather than enterprise systems.,"Agriculture:0.1,Cartography:0.1,Computing in government:0.2,Digital libraries and archives:0.1,Military:0.1,Personal computers and PC applications:0.1,Publishing:0.1",,"None of the options directly relate to self-organizing production system simulation. Computing in government is the closest, but the paper doesn't specify public sector applications. All categories receive low scores due to poor alignment with the simulation tool for industrial systems."
293,"Adaptive control for takeoff, hovering, and landing of a robotic fly","Challenges for controlled flight of a robotic insect are due to the inherent instability of the system, complex fluid-structure interactions, and the general lack of a complete system model. In this paper, we propose theoretical models of the system based on the limited information available from previous work and a comprehensive adaptive flight controller that is capable of coping with uncertainties in the system. We have demonstrated that the proposed methods enable the robot to achieve sustained hovering flights with relatively small errors compared to a similar but non-adaptive approach. Furthermore, vertical takeoff and landing flights are also shown to illustrate the fidelity of the flight controller.","General and reference:0,Hardware:0.5,Computer systems organization:0.5,Networks:0,Software and its engineering:0.5,Theory of computation:0.5,Mathematics of computing:0,Information systems:0,Security and privacy:0,Human-centered computing:0,Computing methodologies:0,Applied computing:1,Social and professional topics:0",Applied computing,"Applied computing: The paper develops adaptive control for robotic flight, a practical application of control theory in robotics. Hardware and Software and its engineering receive lower scores as the focus is on the application rather than hardware/software design.","Arts and humanities:0.0,Computer forensics:0.0,Computers in other domains:0.0,Document management and text processing:0.0,Education:0.0,Electronic commerce:0.0,Enterprise computing:0.0,Law, social and behavioral sciences:0.0,Life and medical sciences:0.0,Operations research:0.0,Physical sciences and engineering:1.0",Physical sciences and engineering,Physical sciences and engineering is highly relevant as the paper addresses robotics and adaptive control systems for a robotic fly. No other domains are directly addressed in the core contribution.,"Aerospace:1.0,Archaeology:0.0,Astronomy:0.0,Chemistry:0.0,Earth and atmospheric sciences:0.0,Electronics:0.0,Engineering:1.0,Mathematics and statistics:0.0,Physics:0.0,Telecommunications:0.0","Aerospace,Engineering",Aerospace: Flight control for a robotic fly. Engineering: Adaptive control systems design. Other STEM fields are not directly related to the control methodology.
3189,WormNet v3: a network-assisted hypothesis-generating server for Caenorhabditis elegans,"High-throughput experimental technologies gradually shift the paradigm of biological research from hypothesis-validation toward hypothesis-generation science. Translating diverse types of large-scale experimental data into testable hypotheses, however, remains a daunting task. We previously demonstrated that heterogeneous genomics data can be integrated into a single genome-scale gene network with high prediction power for ribonucleic acid interference (RNAi) phenotypes in Caenorhabditis elegans, a popular metazoan model in the study of developmental biology, neurobiology and genetics. Here, we present WormNet version 3 (v3), which is a new network-assisted hypothesis-generating server for C. elegans. WormNet v3 includes major updates to the base gene network, which substantially improved predictions of RNAi phenotypes. The server generates various gene network-based hypotheses using three complementary network methods: (i) a phenotype-centric approach to ‘find new members for a pathway’; (ii) a gene-centric approach to ‘infer functions from network neighbors’ and (iii) a context-centric approach to ‘find context-associated hub genes’, which is a new method to identify key genes that mediate physiology within a specific context. For example, we demonstrated that the context-centric approach can be used to identify potential molecular targets of toxic chemicals. WormNet v3 is freely accessible at http://www.inetbio.org/wormnet.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.2,Software and its engineering:0.2,Theory of computation:0.3,Mathematics of computing:0.2,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.4,Applied computing:1.0,Social and professional topics:0.1",Applied computing,Applied computing is highly relevant as the paper presents a biological application (gene network for C. elegans). Computing methodologies receives a moderate score for network-based methods. Other categories are less relevant as the focus is on life sciences applications.,"Arts and humanities:0.1,Computer forensics:0.1,Computers in other domains:0.9,Document management and text processing:0.1,Education:0.1,Electronic commerce:0.1,Enterprise computing:0.1,Law, social and behavioral sciences:0.1,Life and medical sciences:0.8,Operations research:0.3,Physical sciences and engineering:0.2","Life and medical sciences,Computers in other domains",Life and medical sciences is directly relevant for the C. elegans gene network analysis. Computers in other domains applies to the biological application domain. Other fields are not relevant to the core contribution.,"Agriculture:0,Bioinformatics:0.8,Cartography:0,Computational biology:0.9,Computing in government:0.1,Consumer health:0.3,Digital libraries and archives:0.2,Genetics:0.6,Genomics:0.5,Health care information systems:0.3,Health informatics:0.4,Metabolomics / metabonomics:0.2,Military:0.1,Personal computers and PC applications:0,Publishing:0.1,Systems biology:0.7","Computational biology,Bioinformatics",Computational biology is central to the gene network-based hypothesis generation. Bioinformatics is relevant for integrating heterogeneous genomics data. Systems biology and genetics are secondary as the focus is on network-based analysis.
5275,"Six Sigma and simulation, so what's the correlation?","This paper explores the fundamental relationships between Six Sigma and simulation. A basic overview of Six Sigma includes: 1. Six Sigma philosophy, 2. Basic tools, 3. Theory of variation, 4. SPC, 5. Process capability, 6. Six Sigma infrastructure, and 7. DMAIC and DFSS processes. Simulation is applied to the appropriate areas of the overview. Improvement in the robustness of the Six Sigma methodology is discussed and the strengths of simulation are presented as capable and preferable enhancements to the Six Sigma processes. Quotes from Six Sigma and industry leaders are presented. Simulation is presented as an innovation tool enhancing the Six Sigma DMAIC and DFSS processes.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.3,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.4,Applied computing:0.8,Social and professional topics:0.1",Applied computing,Applied computing is highly relevant for industrial applications of simulation in Six Sigma. Computing methodologies receives moderate relevance for the methodological analysis. Theory of computation is less relevant as the focus is on application rather than algorithmic development.,"Arts and humanities:0.0,Computer forensics:0.0,Computers in other domains:0.75,Document management and text processing:0.0,Education:0.0,Electronic commerce:0.0,Enterprise computing:0.0,Law, social and behavioral sciences:0.0,Life and medical sciences:0.0,Operations research:1.0,Physical sciences and engineering:0.0","Operations research,Computers in other domains",Operations research is directly relevant as the paper discusses Six Sigma and simulation for process optimization. Computers in other domains is secondary as the study applies simulation to management methodologies.,"Agriculture:0.1,Cartography:0.1,Computer-aided manufacturing:0.2,Computing in government:0.1,Consumer products:0.1,Decision analysis:0.7,Digital libraries and archives:0.1,Forecasting:0.3,Industry and manufacturing:0.8,Marketing:0.1,Military:0.1,Personal computers and PC applications:0.1,Publishing:0.1,Transportation:0.1","Industry and manufacturing,Decision analysis",Industry and manufacturing is relevant because Six Sigma is an industry methodology. Decision analysis is relevant as the paper discusses simulation applications for decision-making. Other categories like Forecasting are less directly addressed.
5652,Modeling and Analysis of Surgery Patient Identification Using RFID,"This article proposes a workflow and reliability model for surgery patient identification using RFID (Radio Frequency Identification). Certain types of mistakes may be prevented by automatically identifying the patient before surgery. The proposed workflow is designed to ensure that both the correct site and patient are engaged in the surgical process. The reliability model can be used to assess improvements in patientsâ€™ safety during this process. A proof-of-concept system is developed to understand the information flow and to use information in RFID-based patient identification. Reliability model indicates the occurrences of patient identification error can be reduced from 90 to as low as 0.89 per 10,000 surgeries using the proposed RFID based workflow.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.3,Theory of computation:0.1,Mathematics of computing:0.2,Information systems:0.2,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:1.0,Social and professional topics:0.1",Applied computing,Applied computing is highly relevant as the paper applies RFID technology to healthcare. Other categories like Information systems are less relevant as the focus is on application rather than data management.,"Arts and humanities:0.2,Computer forensics:0.2,Computers in other domains:0.9,Document management and text processing:0.3,Education:0.2,Electronic commerce:0.2,Enterprise computing:0.3,Law, social and behavioral sciences:0.2,Life and medical sciences:0.4,Operations research:0.3,Physical sciences and engineering:0.2",Computers in other domains,Computers in other domains is highly relevant as the paper applies RFID to healthcare workflows. Life and medical sciences are less central compared to the computational focus.,"Agriculture:0.1,Cartography:0.3,Computing in government:0.5,Digital libraries and archives:0.2,Military:0.1,Personal computers and PC applications:0.2,Publishing:0.1",Computing in government,Computing in government (healthcare safety applications for surgery workflows). Other categories are unrelated to healthcare or public safety contexts.
2490,Fast and Accurate 3-D Registration of HR-pQCT Images,"High-resolution peripheral quantitative computed tomography (HR-pQCT) is a new noninvasive bone imaging technology that generates high-resolution 3-D images for quantitatively analysis of the bone microarchitecture in human. To enable quantitative evaluation of bone changes, either bone gain or loss, accurate alignment between the baseline and follow-up scans of the same individual is necessary. The major difficulties in achieving efficient and automatic registration of the HR-pQCT data are the large data size, deformations in the nonskeletal structures, and the complexity of the trabecular bone geometry. In this paper, we propose an automatic surface-based approach for fast and accurate registration of the HR-pQCT data, where the rigid registration is applied on the surfaces of the bony structures extracted from the grayscale HR-pQCT. The bony structure segmentation is performed via an automatic method that can adaptively determine the thresholds for separating the bony structure from the background and nonskeletal tissues. Experimental results performed on ten pairs of baseline and follow-up wrist scans of five adolescents and five elderly patients with osteoporosis showed the advantage of the proposed method in the high degree of automation, while the resultant parameters describing bone mineral density and trabecular architecture after registration were comparable with the outputs of the scanner's software. This automatic and accurate matching procedure may contribute to the clinical application and research of HR-pQCT.","General and reference:0.0,Hardware:0.2,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.3,Theory of computation:0.1,Mathematics of computing:0.4,Information systems:0.1,Security and privacy:0.0,Human-centered computing:0.1,Computing methodologies:0.6,Applied computing:1.0,Social and professional topics:0.0",Applied computing,"Applied computing is central to the medical imaging application. Computing methodologies is relevant for the registration algorithm, but Applied computing is the primary domain. Other categories are irrelevant to the healthcare application.","Arts and humanities:0.1,Computer forensics:0.1,Computers in other domains:0.2,Document management and text processing:0.1,Education:0.1,Electronic commerce:0.1,Enterprise computing:0.1,Law, social and behavioral sciences:0.1,Life and medical sciences:0.9,Operations research:0.2,Physical sciences and engineering:0.2",Life and medical sciences,Life and medical sciences is highly relevant as the paper presents a bone imaging analysis method for clinical applications. Other categories like operations research are less relevant as the focus is on medical image registration rather than optimization techniques.,"Bioinformatics:0.0,Computational biology:0.0,Consumer health:0.0,Genetics:0.0,Genomics:0.0,Health care information systems:0.9,Health informatics:1.0,Metabolomics / metabonomics:0.0,Systems biology:0.0","Health informatics,Health care information systems","Health informatics: The paper addresses medical imaging and data analysis for bone microarchitecture, aligning with health data processing. Health care information systems: The focus on 3D registration for clinical applications fits this category. Other options are irrelevant as the paper does not discuss genomics or systems biology."
809,A simulation test bed for computational modeling of business processes,"Business process reengineering has been the focus of attention of businesses since the early 1990s (Hammer and Champy 1993). To an extent, existing business process modeling tools (Spurr et al. 1994) provide features to analyze business processes. However, the decision support provided by these tools is often too limited (Yu et al. 1996). For instance, many modeling tools based on IDEF, DFD, etc. lack the necessary means to support the representation of time and resource constraints and most ignore the human and behavioral aspects of reengineering (Davenport and Stoddard 1994).","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.7,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:1.0,Social and professional topics:0.0",Applied computing,Applied computing is relevant as the paper focuses on computational modeling of business processes as an applied domain. Other categories are irrelevant as the paper does not address software engineering directly or other technical fields.,"Arts and humanities:0.1,Computer forensics:0.1,Computers in other domains:0.8,Document management and text processing:0.2,Education:0.1,Electronic commerce:0.1,Enterprise computing:0.7,Law, social and behavioral sciences:0.2,Life and medical sciences:0.1,Operations research:0.6,Physical sciences and engineering:0.3","Computers in other domains,Enterprise computing,Operations research",Computers in other domains: The simulation testbed applies computing to business process modeling. Enterprise computing: Focus on business process reengineering. Operations research: Involves optimization and simulation. Other categories like Physical sciences lack direct relevance.,"Agriculture:0,Business process management:1,Business rules:0,Business-IT alignment:0,Cartography:0,Computer-aided manufacturing:0,Computing in government:0,Consumer products:0,Decision analysis:1,Digital libraries and archives:0,Enterprise architectures:0,Enterprise computing infrastructures:0,Enterprise data management:0,Enterprise information systems:0,Enterprise interoperability:0,Enterprise modeling:1,Enterprise ontologies, taxonomies and vocabularies:0,Event-driven architectures:0,Forecasting:0,IT architectures:0,IT governance:0,Industry and manufacturing:0,Marketing:0,Military:0,Personal computers and PC applications:0,Publishing:0,Reference models:0,Service-oriented architectures:0,Transportation:0","Business process management,Enterprise modeling",Business process management: The paper addresses business process reengineering and modeling tools. Enterprise modeling: The focus on simulation and modeling of business processes aligns here. Decision analysis is secondary but relevant for the evaluation aspect.
4082,A Telemetry System Embedded in Clothes for Indoor Localization and Elderly Health Monitoring,"This paper presents a telemetry system used in a combined trilateration method for the precise indoor localization of the elderly who need health monitoring. The system is based on the association of two wireless technologies: ultrasonic and 802.15.4. The use of the 802.15.4 RF signal gives the reference starting time of the ultrasonic emission (time difference of arrival method). A time of flight measurement of the ultrasonic pulses provides the distances between the mobile node and three anchor points. These distance measurements are then used to locate the mobile node using the trilateration method with an accuracy of a few centimetres. The originality of our work lies in embedding the mobile node in clothes. The system is embedded in clothes in two ways: on a shoe in order to form a “smart” shoe and in a hat in order to form a “smart” hat. Both accessories allow movements, gait speed and distance covered to be monitored for health applications. Experiments in a test room are presented to show the effectiveness of our system.","General and reference:0.0,Hardware:0.1,Computer systems organization:0.0,Networks:0.2,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.1,Applied computing:0.9,Social and professional topics:0.0",Applied computing,Applied computing is highly relevant because the paper presents a real-world health monitoring system for elderly care. Networks and Hardware are less relevant as the core contribution is the application of the system to healthcare rather than network protocols or hardware design.,"Arts and humanities:0.0,Computer forensics:0.0,Computers in other domains:0.5,Document management and text processing:0.0,Education:0.0,Electronic commerce:0.0,Enterprise computing:0.0,Law, social and behavioral sciences:0.0,Life and medical sciences:1.0,Operations research:0.0,Physical sciences and engineering:0.0",Life and medical sciences,Life and medical sciences is directly relevant for elderly health monitoring. Computers in other domains receives a low score as the system is specifically applied to medical contexts rather than general 'other domains' applications.,"Bioinformatics:0,Computational biology:0,Consumer health:1,Genetics:0,Genomics:0,Health care information systems:0,Health informatics:1,Metabolomics / metabonomics:0,Systems biology:0","Consumer health,Health informatics",Consumer health: The system directly addresses elderly health monitoring through activity tracking. Health informatics: The paper focuses on integrating wireless telemetry with health data collection. Other categories like 'Genomics' are irrelevant as the paper does not involve biological data analysis.
3057,Audio and vision-based evaluation of parkinson~s disease from discourse video,"Parkinson's disease (PD) belongs to a class of neurodegenerative diseases that affect both the patient's speech and motor capabilities. To date, PD diagnosis and the determination of disease progress and treatment efficacy is based entirely on the subjective observation of a trained physician. We present the results of a pilot study of two Idiopathic PD patients who have undergone Lee Silverman Voice Treatment (LSVT). It has been observed subjectively that gestural performance of patients improve in tandem with speech improvements after LSVT. It is hypothesized that these improvements are taking place at a neurological level. Measurements of speech and gesture suggest that LSVT improves the quality of both gesticulation and speech.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.3,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.5,Applied computing:1.0,Social and professional topics:0.1",Applied computing,"Applied computing (1.0) is highly relevant as the paper applies audio-visual analysis to Parkinson's disease evaluation. Computing methodologies (0.5) is moderately relevant due to the use of signal processing techniques. Other categories are irrelevant as the paper does not focus on system design, algorithms, or social theory.","Arts and humanities:0.1,Computer forensics:0.1,Computers in other domains:0.1,Document management and text processing:0.1,Education:0.1,Electronic commerce:0.1,Enterprise computing:0.1,Law, social and behavioral sciences:0.1,Life and medical sciences:1.0,Operations research:0.1,Physical sciences and engineering:0.1",Life and medical sciences,Medical application for Parkinson's evaluation.,"Bioinformatics:0,Computational biology:0,Consumer health:0.5,Genetics:0,Genomics:0,Health care information systems:1,Health informatics:1,Metabolomics / metabonomics:0,Systems biology:0","Health care information systems,Health informatics","Health care information systems and Health informatics are both highly relevant as the paper presents an audio/visual system for evaluating Parkinson's disease, which falls under healthcare technology and data-driven clinical evaluation."
4455,Methodological Approach for Integrated Grid and Market Simulation of Coherent Distribution and Transmission Systems,"The analysis of distributed generation systems and their influence on the different voltage levels of the power grid involves a detailed modelling of all observed units and the connecting distribution and overlaying transmission networks. In return this generation has increasing effects on the European transmission system, whose entire simulation with the same detailing as single distributed generation systems is not feasible, because of the complexity of the therefor needed simulation system. Thus a concept has been developed to fulfil both a high level of detail in distribution networks for a multi-agent system based control algorithm and the analysis of the European transmission system. This was achieved by aggregating and transferring the information and behaviour of single distribution networks to the ultra-high voltage nodes of the transmission system in separated, but coordinated joint simulations.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.9,Social and professional topics:0.1",Applied computing,Applied computing: The paper describes power grid simulation techniques for real-world energy systems. Other categories: The work is not about pure computing methodologies or theoretical CS.,"Arts and humanities:0.0,Computer forensics:0.0,Computers in other domains:0.0,Document management and text processing:0.0,Education:0.0,Electronic commerce:0.0,Enterprise computing:0.0,Law, social and behavioral sciences:0.0,Life and medical sciences:0.0,Operations research:0.0,Physical sciences and engineering:1.0",Physical sciences and engineering,Physical sciences and engineering is highly relevant as the paper discusses power grid simulation and control algorithms. The other options are not relevant to this engineering-focused simulation research.,"Aerospace:0.1,Archaeology:0.1,Astronomy:0.1,Chemistry:0.1,Earth and atmospheric sciences:0.2,Electronics:1.0,Engineering:1.0,Mathematics and statistics:0.3,Physics:0.2,Telecommunications:0.1","Electronics,Engineering",Electronics is central to the grid simulation methodology. Engineering applies to the system-level integration. Other categories like Physics are only tangential.
901,Application of Radial Basis Network Model for HIV/AIDs Regimen Specifications,"HIV/AIDs Regimen specification one of many problems for which bioinformaticians have implemented and trained machine learning methods such as neural networks. Predicting HIV resistance would be much easier, but unfortunately we rarely have enough structural information available to train a neural network. To network model designed to predict how long the HIV patient can prolong his/her life time with certain regimen specification. To learn this model 300 patient's details have taken as a training set to train the network and 100 patients medical history has taken to test this model. This network model is trained using MAT lab implementation. Resources: RBN algorithm implemented in MAT Lab, data from a ART centre in Tamil Nadu.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.3,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:1.0,Social and professional topics:0.1",Applied computing,Applied computing is highly relevant as the paper applies machine learning to HIV/AIDS regimen prediction. Mathematics of computing is marginally relevant due to the use of mathematical models but not the primary focus.,"Arts and humanities:0.1,Computer forensics:0.1,Computers in other domains:0.1,Document management and text processing:0.1,Education:0.1,Electronic commerce:0.1,Enterprise computing:0.1,Law, social and behavioral sciences:0.1,Life and medical sciences:0.8,Operations research:0.1,Physical sciences and engineering:0.1",Life and medical sciences,Life and medical sciences is directly relevant for HIV regimen prediction. Other categories are less relevant as the paper focuses on medical applications.,"Bioinformatics:0.6,Computational biology:0.5,Consumer health:0.3,Genetics:0.2,Genomics:0.2,Health care information systems:0.4,Health informatics:1.0,Metabolomics / metabonomics:0.1,Systems biology:0.3","Health informatics,Bioinformatics",Health informatics is highly relevant for the medical prediction model. Bioinformatics is relevant for applying machine learning to HIV data. Other categories like genomics are less directly relevant.
296,Multiagent based distributed control for state-of-charge balance of distributed energy storage in DC microgrids,"In this paper, a distributed multiagent based algorithm is proposed to achieve SoC balance for DES in the DC microgrid by means of voltage scheduling. Reference voltage given is adjusted instead of droop gain. Dynamic average consensus algorithm is explored in each agent to get the required information for scheduling voltage autonomously. State-space analysis on a single energy storage unit and simulation verification shows that the proposed method has two advantages. Firstly, modifying the reference voltage given has less impact on system stability compared to gain scheduling. Secondly, by adopting multiagent methodology, the proposed distributed control has less communication dependence and more reliable during communication topology changes.","General and reference:0,Hardware:0.25,Computer systems organization:0.25,Networks:0,Software and its engineering:0.5,Theory of computation:0.25,Mathematics of computing:0.5,Information systems:0,Security and privacy:0,Human-centered computing:0,Computing methodologies:0.25,Applied computing:1,Social and professional topics:0",Applied computing,"Applied computing: The paper applies multiagent systems to energy storage control in microgrids, a practical application of distributed algorithms. Software and its engineering receives a lower score as the focus is on the control system application rather than software development.","Arts and humanities:0.0,Computer forensics:0.0,Computers in other domains:0.0,Document management and text processing:0.0,Education:0.0,Electronic commerce:0.0,Enterprise computing:0.0,Law, social and behavioral sciences:0.0,Life and medical sciences:0.0,Operations research:0.0,Physical sciences and engineering:1.0",Physical sciences and engineering,Physical sciences and engineering is highly relevant as the paper presents a multiagent-based control algorithm for DC microgrid energy storage. No other domains are directly addressed in the core contribution.,"Aerospace:0.0,Archaeology:0.0,Astronomy:0.0,Chemistry:0.0,Earth and atmospheric sciences:0.0,Electronics:1.0,Engineering:1.0,Mathematics and statistics:0.2,Physics:0.0,Telecommunications:0.0","Electronics,Engineering","Electronics: The paper discusses voltage scheduling and control in DC microgrids, which are core topics in electronics. Engineering: The distributed control system for energy storage is a clear engineering application. Other fields like Mathematics and statistics are only peripherally relevant for the dynamic average consensus algorithm."
1602,Multistable Decision Switches for Flexible Control of Epigenetic Differentiation,"It is now recognized that molecular circuits with positive feedback can induce two different gene expression states (bistability) under the very same cellular conditions. Whether, and how, cells make use of the coexistence of a larger number of stable states (multistability) is however largely unknown. Here, we first examine how autoregulation, a common attribute of genetic master regulators, facilitates multistability in two-component circuits. A systematic exploration of these modules' parameter space reveals two classes of molecular switches, involving transitions in bistable (progression switches) or multistable (decision switches) regimes. We demonstrate the potential of decision switches for multifaceted stimulus processing, including strength, duration, and flexible discrimination. These tasks enhance response specificity, help to store short-term memories of recent signaling events, stabilize transient gene expression, and enable stochastic fate commitment. The relevance of these circuits is further supported by biological data, because we find them in numerous developmental scenarios. Indeed, many of the presented information-processing features of decision switches could ultimately demonstrate a more flexible control of epigenetic differentiation.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.95,Social and professional topics:0.1",Applied computing,Applied computing is highly relevant as the paper applies computational models to biological systems for epigenetic differentiation. Categories like Computing Methodologies or Theory of Computation are less aligned with the domain-specific application.,"Life and medical sciences:0.85,Computers in other domains:0.8,Physical sciences and engineering:0.2,Arts and humanities:0.1,Computer forensics:0.1,Document management and text processing:0.1,Education:0.1,Electronic commerce:0.1,Enterprise computing:0.1,Law, social and behavioral sciences:0.1,Operations research:0.1","Life and medical sciences,Computers in other domains",Life and medical sciences is highly relevant for the biological application of epigenetic differentiation. Computers in other domains is relevant for the computational modeling of molecular circuits. Other categories like Physical sciences and engineering are less relevant as the focus is on biological systems rather than engineering.,"Systems biology:1,Computational biology:0.9,Genetics:0.7,Bioinformatics:0.5,Genomics:0.4,Metabolomics / metabonomics:0.1","Systems biology,Computational biology",Systems biology is relevant as the paper models gene regulatory circuits. Computational biology is relevant for the theoretical analysis of genetic switches. Other categories like genomics were rejected as the focus is on circuit behavior rather than genomic data.
3096,Utilizing performance monitor counters to effectively guide windows and SQL server tuning efforts,"Window and the SQL Server database management system provide many performance counters. However, the vast number of counters and the fact that numerous explanations provided via the Windows Performance Monitor simply restate the name of the counter make resolving performance problems in this environment a daunting task. This paper discusses methodologies, metrics, and techniques that can be utilized during a performance crisis to ascertain the cause(s) of the problem and determine an appropriate solution.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.9,Social and professional topics:0.1",Applied computing,Applied computing is directly relevant as the paper focuses on performance tuning for database systems. Other categories are rejected because the focus is on applying computing to system performance analysis rather than theoretical computation or software engineering.,"Arts and humanities:0.1,Computer forensics:0.1,Computers in other domains:0.2,Document management and text processing:0.1,Education:0.1,Electronic commerce:0.1,Enterprise computing:0.85,Law, social and behavioral sciences:0.1,Life and medical sciences:0.1,Operations research:0.3,Physical sciences and engineering:0.1",Enterprise computing,Enterprise computing is highly relevant for Windows and SQL Server performance optimization in enterprise environments. Other children like Computers in other domains are less specific to the enterprise systems focus.,"Business process management:0,Business rules:0,Business-IT alignment:0,Enterprise architectures:0,Enterprise computing infrastructures:1,Enterprise data management:1,Enterprise information systems:0.5,Enterprise interoperability:0.3,Enterprise modeling:0.2,Enterprise ontologies, taxonomies and vocabularies:0.2,Event-driven architectures:0.1,IT architectures:0.7,IT governance:0.3,Reference models:0.2,Service-oriented architectures:0.4","Enterprise computing infrastructures,Enterprise data management",Enterprise computing infrastructures and Enterprise data management are directly relevant as the paper addresses performance tuning of Windows and SQL Server systems. IT architectures is secondary due to the focus on infrastructure-level performance monitoring. Other business/enterprise categories are less relevant as the focus is on technical system optimization.
224,Research of CO2 welding inverter power source under current waveform control,"STT (surface tension transition) waveform control method is efficient method to realize no splash transition and improve appearance of welding in CO2 welding process. This system directly controls welding current, and given value is the STT current waveform of the short circuit, burning arc stage which can meet to realize without splash transition and improve the appearance of welding. It designs the corresponding fuzzy PI controller, constitutes current waveform control system of CO2 welding inverter power source, and series resistance is added in welding circuit at the end of the short circuit stage to accelerate rate of descending of loop discharge current to reduce the splash and improve the appearance of welding. The simulation results show that the designed controller system can make the welding current rapidly and accurately track the given current waveform, and welding current can meet the conditions of the surface tension transition.","General and reference:0.1,Hardware:0.2,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.4,Theory of computation:0.1,Mathematics of computing:0.2,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.7,Social and professional topics:0.1",Applied computing,Applied computing is relevant for the control system application in welding. Other categories like Software and Hardware are less relevant as the focus is on applied engineering methods.,"Physical sciences and engineering:1.0,Electronic commerce:0.1,Operations research:0.2,Life and medical sciences:0.1",Physical sciences and engineering,"Physical sciences and engineering is the only relevant category as the paper focuses on CO2 welding process optimization, a domain-specific engineering application. Other categories are entirely unrelated to the topic.","Aerospace:0,Archaeology:0,Astronomy:0,Chemistry:0,Earth and atmospheric sciences:0,Electronics:1,Engineering:1,Mathematics and statistics:0,Physics:0,Telecommunications:0","Electronics,Engineering",Electronics is relevant due to the focus on inverter power source control systems. Engineering is relevant as the paper addresses welding process optimization through engineering design. Other fields like Chemistry or Physics are not directly addressed.
5279,A practical approach to robotic design for the DARPA Urban Challenge,"This article presents a practical approach to engineering a robot to effectively navigate in an urban environment. Inherent in this approach is the use of relatively simple sensors, actuators, and processors to generate robot vision, intelligence, and planning. Sensor data are fused from multiple low‐cost, two‐dimensional laser scanners with an innovative rotational mount to provide three‐dimensional coverage with image processing using both range and intensity data. Information is combined with Doppler radar returns to yield a world view processed by a context‐based reasoning control system to yield tactical mission commands forwarded to traditional proportional‐integral‐derivative (PID) control loops. As an example of simplicity and robustness, steering control successfully utilized a relatively simple follow‐the‐carrot guidance approach that has been successfully demonstrated at speeds of 60 mph (97 km/h). The approach yielded a robot that reached the finals of the Urban Challenge and completed approximately 2 h of the event before being forced to withdraw as a result of a global positioning system data failure. © 2008 Wiley Periodicals, Inc.","General and reference:0.1,Hardware:0.3,Computer systems organization:0.2,Networks:0.1,Software and its engineering:0.4,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.2,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.9,Social and professional topics:0.1",Applied computing,Applied computing is relevant for the practical robotic navigation system in urban environments. Other categories like Hardware or Software and its engineering are rejected because the core contribution is the application of computing to real-world robotics challenges rather than component design or general software engineering techniques.,"Arts and humanities:0.0,Computer forensics:0.0,Computers in other domains:0.75,Document management and text processing:0.0,Education:0.0,Electronic commerce:0.0,Enterprise computing:0.0,Law, social and behavioral sciences:0.0,Life and medical sciences:0.0,Operations research:0.0,Physical sciences and engineering:1.0","Physical sciences and engineering,Computers in other domains",Physical sciences and engineering is directly relevant as the paper discusses robotic design for the DARPA Urban Challenge. Computers in other domains is secondary as the application is in robotics.,"Aerospace:0,Agriculture:0,Archaeology:0,Astronomy:0,Cartography:0,Chemistry:0,Computing in government:0,Digital libraries and archives:0,Earth and atmospheric sciences:0,Electronics:0,Engineering:1,Mathematics and statistics:0,Military:0,Personal computers and PC applications:0,Physics:0,Publishing:0,Telecommunications:0",Engineering,"Engineering is directly relevant as the paper discusses robotic design, sensor integration, and control systems for urban navigation. All other options are unrelated to robotics or urban challenge engineering."
1388,Stereoscopic media editing based on 3D cinematography principles,"Developments of stereoscopic displays and binocular cameras have made capture and display of stereoscopic media easy. There will be strong needs for stereoscopic media processing to crop, make transitions, resize or stabilize stereoscopic imagery as for conventional 2D media. However, despite of fast progresses on hardware, few have been made towards the stereoscopic media processing side, especially for stereoscopic media captured by consumers. This paper introduces 3D cinematography principles and applies them to stereoscopic media processing to maintain viewing comfort and pleasure. Stereoscopic video stabilization and stereoscopic photo slideshows are used as examples to demonstrate these principles' usage in stereoscopic media processing.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.8,Social and professional topics:0.1",Applied computing,"Applied computing is relevant as the paper applies 3D cinematography principles to stereoscopic media processing, addressing real-world media editing challenges. Computing methodologies is less relevant because the focus is on application rather than algorithmic innovation.","Arts and humanities:0.1,Computer forensics:0.05,Computers in other domains:0.1,Document management and text processing:0.05,Education:0.05,Electronic commerce:0.05,Enterprise computing:0.05,Law, social and behavioral sciences:0.05,Life and medical sciences:0.15,Operations research:0.05,Physical sciences and engineering:0.1",Computer graphics,Computer graphics is directly relevant to stereoscopic media processing. Other categories like Life and medical sciences are not central to the core contribution.,"Image manipulation:1,Graphics systems and interfaces:1,Rendering:0.4","Image manipulation,Graphics systems and interfaces","Image manipulation is primary due to techniques for stereoscopic media editing. Graphics systems apply to the application of 3D cinematography principles. Rendering is less relevant as the focus is on editing, not rendering processes."
5772,Hydraulic Splines: A Hybrid Approach to Modeling River Channel Geometries,"The hydraulic spline algorithm generates irregular 2D channel grids from highly accurate cross-sectional survey data at any desired resolution, facilitating its integration with high-density light detection and ranging (lidar) data.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.9,Social and professional topics:0.1",Applied computing,Applied computing: The paper applies computational methods to model river channels for environmental/hydraulic engineering. Other categories are less relevant as the focus is on real-world application rather than theoretical or algorithmic contributions.,"Arts and humanities:0.1,Computer forensics:0.1,Computers in other domains:0.8,Document management and text processing:0.2,Education:0.1,Electronic commerce:0.1,Enterprise computing:0.2,Law, social and behavioral sciences:0.1,Life and medical sciences:0.2,Operations research:0.3,Physical sciences and engineering:0.7","Computers in other domains,Physical sciences and engineering",Computers in other domains applies to engineering applications in river modeling. Physical sciences and engineering is relevant to hydraulic modeling. Other fields are unrelated to the domain-specific application.,"Aerospace:0.1,Agriculture:0.1,Archaeology:0.1,Astronomy:0.1,Cartography:0.2,Chemistry:0.1,Computing in government:0.1,Digital libraries and archives:0.1,Earth and atmospheric sciences:0.9,Electronics:0.1,Engineering:0.1,Mathematics and statistics:0.2,Military:0.1,Personal computers and PC applications:0.1,Physics:0.1,Publishing:0.1,Telecommunications:0.1",Earth and atmospheric sciences,Earth and atmospheric sciences is relevant due to the application in river channel geometry modeling. Other categories like Cartography (0.2) are less relevant as the focus is on computational modeling rather than mapping.
5098,Robust Control using Recursive Design Method for Flexible Joint Robot Manipulator,"Flexible joint robot manipulators can be decomposed into two cascaded subsystems, a series connection of robot link dynamics and joint dynamics. For these flexible manipulators, we propose the robust controller using a recursive design method. The recursive design procedures are constructive and contain two steps. First, a fictitious robust controller for the robot link dynamics is designed as if the link dynamics had an independent control. As the fictitious control, a nonlinear Hinfin control using the energy dissipation is designed in the sense of L2-gain attenuation from the disturbance caused by uncertainties to performance. Second, a real robust control is designed recursively by using a Lyapunov's second method. The designed robust control is applied to a 2 DOF robot manipulator with joint flexibilities.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.3,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.8,Social and professional topics:0.1",Applied computing,Applied computing is highly relevant as the paper presents a control system design for robotic manipulators. Mathematics of computing has moderate relevance for the theoretical analysis.,"Arts and humanities:0.0,Computer forensics:0.0,Computers in other domains:0.7,Document management and text processing:0.0,Education:0.0,Electronic commerce:0.0,Enterprise computing:0.0,Law, social and behavioral sciences:0.0,Life and medical sciences:0.0,Operations research:0.0,Physical sciences and engineering:0.8",Physical sciences and engineering,Physical sciences and engineering: The paper presents a robust control methodology for flexible joint robotics. Computers in other domains is partially relevant for the application context but secondary to the engineering focus.,"Aerospace:0.1,Archaeology:0.0,Astronomy:0.0,Chemistry:0.0,Earth and atmospheric sciences:0.0,Electronics:0.2,Engineering:0.9,Mathematics and statistics:0.5,Physics:0.1,Telecommunications:0.0",Engineering,Engineering is highly relevant as the paper presents a robust control method for flexible joint robots. Other categories like Mathematics or Physics are only tangentially related to the core control system design.
4033,Simulating airspace redesign for arrivals to Detroit-Wayne County Airport (DTW),"In 2001, Detroit Metropolitan Wayne County Airport (DTW) opened a new runway parallel to three existing runways. While this increases DTW's runway capacity, the airport is served by an airspace (routes, procedures, and controller assignments) that was designed only for a three-runway airport. To increase the airport's effective capacity, the Detroit-area terminal radar approach control facility (D21 TRACON) and nearby air route traffic control centers (ARTCC) are redesigning their airspace. This paper describes the simulation modeling effort to estimate delay and cost benefits of the ARTCC redesign for arrival traffic. The model, written in the SLX simulation language, represents miles-in-trail (MIT) restrictions, as well as air traffic controllers' ability to direct flights to different paths dynamically, based on predicted demand downstream. The redesign work is part of the Federal Aviation Administration's Midwest airspace capacity enhancement (MACE) project.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.9,Social and professional topics:0.1",Applied computing,Applied computing: The paper uses simulation to analyze real-world airspace redesign for an airport. Other categories like Networks are secondary as the focus is on applying computational methods to air traffic management.,"Arts and humanities:0,Computer forensics:0,Computers in other domains:0.9,Document management and text processing:0.1,Education:0,Electronic commerce:0,Enterprise computing:0.2,Law, social and behavioral sciences:0.1,Life and medical sciences:0,Operations research:0.3,Physical sciences and engineering:0.4",Computers in other domains,"Computers in other domains is relevant as the paper applies simulation to aviation. Operations research is secondary due to the optimization focus, but the primary domain is aviation.","Agriculture:0.0,Cartography:0.0,Computing in government:1.0,Digital libraries and archives:0.0,Military:0.0,Personal computers and PC applications:0.0,Publishing:0.0","Computing in government,Simulation types and techniques",Computing in government is relevant as the simulation models airspace redesign for a public infrastructure project. Simulation types and techniques is relevant due to the focus on simulation modeling for estimating benefits. Other categories like Cartography or Agriculture are unrelated to the paper's focus on government-led simulation for airport capacity.
1566,Recognising the kind of cloud using a new fuzzy knowledge-based system,"Nowadays, expert systems play a major role in better doing of complex tasks and giving advice to the experts because expertism is a specialised knowledge. Overall, expert systems are used to solve the problems for which there is not an accurate knowledge and a particular algorithm. Understanding the atmospheric phenomena and their role in human life are the most important and affecting issues in human societies. In meteorology, it is important to identify the type of clouds. By monitoring from the Earth's surface (seeing bottom view of the cloud) and using satellites (seeing top view of the cloud), we can identify the variety of clouds. A fuzzy inference system with the specialists' knowledge of meteorology is designed in this paper and its aims are detection of the cloud type through extracting knowledge from satellite images of the cloud upper portions. The used data are extracted from the reputable website of UCI called cloud dataset. This dataset is gathered by Philip Collard in two ranges of IR and VISIBLE. Using the experts' knowledge, this system determines the type of cloud with an accuracy level of 88.25% ± 0.5 and according to experts' opinion; the results are suitable and acceptable.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.2,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:1.0,Social and professional topics:0.0",Applied computing,"Applied computing is highly relevant as the paper applies computational methods to meteorological cloud classification. Software and its engineering is marginally relevant due to system implementation, but the core focus is on real-world application.","Arts and humanities:0.0,Computer forensics:0.0,Computers in other domains:0.75,Document management and text processing:0.0,Education:0.0,Electronic commerce:0.0,Enterprise computing:0.0,Law, social and behavioral sciences:0.0,Life and medical sciences:0.0,Operations research:0.25,Physical sciences and engineering:0.0",Computers in other domains,Computers in other domains is relevant as the paper applies computer science to meteorological cloud classification. Operations research is less relevant as the focus is on fuzzy systems rather than optimization techniques.,"Agriculture:0.0,Cartography:0.0,Computing in government:0.0,Digital libraries and archives:0.0,Military:0.0,Personal computers and PC applications:0.0,Publishing:0.0",,"The paper focuses on a fuzzy knowledge-based system for cloud type recognition in meteorology. None of the provided options (Agriculture, Cartography, etc.) align with the computational meteorology or AI domains."
367,Resource Management with RFID Technology in Automatic Warehouse System,"Warehouse operation has become a critical activity in supply chain to outperform competitors on customer service, lead times and costs. However, managers often meet difficulties in formulating good resource utilization plan due to shortage of comprehensive resource information. This paper proposes a resource management system with RFID technology, which can access to the real-time position and operation process information of material handling equipments, as well as some load item information. This warehouse management system is designed to select the most suitable resource usage package for handling warehouse operation orders. An experiment warehouse is built in the Institute of Automation, Chinese Academy of Sciences to validate the effectiveness of the warehouse management system. The results show increase in rack space utilization and loading speed as well as decrease in work-related errors and operation costs in comparison with a similar system without RFID technology","General and reference:0.0,Hardware:0.25,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.5,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.25,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:1.0,Social and professional topics:0.0",Applied computing,Applied computing is highly relevant as the paper focuses on RFID technology for real-world warehouse management. Hardware and Software and its engineering receive lower scores as the system implementation is secondary to the application focus. Information systems is less relevant as the paper emphasizes operations over data management.,"Arts and humanities:0.1,Computer forensics:0.1,Computers in other domains:0.1,Document management and text processing:0.1,Education:0.1,Electronic commerce:0.1,Enterprise computing:1.0,Law, social and behavioral sciences:0.1,Life and medical sciences:0.1,Operations research:0.6,Physical sciences and engineering:0.1","Enterprise computing,Operations research",Enterprise computing: The paper presents an RFID-based warehouse management system. Operations research: The system improves resource utilization through optimization.,"Business process management:0.5,Business rules:0.1,Business-IT alignment:0.3,Computer-aided manufacturing:0.7,Consumer products:0.2,Decision analysis:0.4,Enterprise architectures:0.3,Enterprise computing infrastructures:0.2,Enterprise data management:0.1,Enterprise information systems:0.6,Enterprise interoperability:0.2,Enterprise modeling:0.3,Enterprise ontologies, taxonomies and vocabularies:0.1,Event-driven architectures:0.1,Forecasting:0.1,IT architectures:0.2,IT governance:0.1,Industry and manufacturing:0.9,Marketing:0.1,Reference models:0.1,Service-oriented architectures:0.2,Transportation:0.1","Computer-aided manufacturing,Industry and manufacturing",Computer-aided manufacturing is directly relevant to RFID-based resource management in automated warehouses. Industry and manufacturing is the primary domain. 'Enterprise information systems' is secondary but still applicable.
2286,Novel 2R3T and 2R2T parallel mechanisms with high rotational capability,"SUMMARY Large rotational angles about two axes for parallel mechanisms (PMs) with two rotational and three translational (2R3T) degrees of freedom (DOFs) or two rotational and two translational (2R2T) DOFs are demanded in some industries, such as parallel machine tools and multi-axis 3D printing. To address the problem, this paper focuses on the structural synthesis of new 2R3T and 2R2T PMs with high rotational capability. First, two new moving platforms are proposed based on the concepts of decoupled and configurable design. By means of the proposed platforms and Lie group theory, a series of 2R2T and 2R3T PMs are synthesized. Then the inverse kinematics and velocity relationship of one of the synthesized 2R3T PMs are presented. Finally, the rotational capability of the same 2R3T PM is analyzed. The result shows that by means of actuation redundancy, the studied 2R3T PM indeed possesses the high rotational capability about two axes, even though interferences and singularities are taken into consideration.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.8,Social and professional topics:0.1",Applied computing,Applied computing is relevant for the mechanical design and simulation of parallel mechanisms for industrial applications. Other categories like Hardware or Computer Systems Organization are not primary as the focus is on engineering applications of computing methods.,"Physical sciences and engineering:1.0,Computers in other domains:0.2,Operations research:0.2,Arts and humanities:0.2,Computer forensics:0.2,Education:0.2,Electronic commerce:0.2,Enterprise computing:0.2,Law, social and behavioral sciences:0.2,Life and medical sciences:0.2",Physical sciences and engineering,Physical sciences and engineering is the only relevant category as the paper focuses on mechanical design of parallel mechanisms for industrial applications. Other options like arts or education are unrelated to the technical content.,"Aerospace:0.1,Archaeology:0.1,Astronomy:0.1,Chemistry:0.1,Earth and atmospheric sciences:0.1,Electronics:0.2,Engineering:1.0,Mathematics and statistics:0.8,Physics:0.2,Telecommunications:0.1","Engineering,Mathematics and statistics",Engineering is highly relevant as the paper focuses on parallel mechanism design and kinematics. Mathematics and statistics receive a high score due to the use of Lie group theory. Other categories like Electronics or Physics are only marginally relevant.
4402,Drug accessibility and availability tool: Case of Rwanda,"Drug accessibility is a barrier to the timely treatment of patients and in some cases it contributes, to health complications and can lead to death. Drug availability is more important than drug accessibility as the drug has to be available first in order to be accessible. However, access to information about the price of the drug and the location of the pharmacy in which one can find the drug remains a challenge. In this paper, we present a web application solution that bridges the gap between accessibility and availability by providing real-time information about drug inventory, drug cost and pharmacies' address in Rwanda.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.3,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.9,Social and professional topics:0.1",Applied computing,Applied computing: The paper directly addresses a real-world healthcare application for drug accessibility in Rwanda. Other categories like software engineering or systems organization are rejected as the focus is on domain-specific application rather than technical infrastructure.,"Arts and humanities:0.0,Computer forensics:0.0,Computers in other domains:0.2,Document management and text processing:0.0,Education:0.0,Electronic commerce:0.0,Enterprise computing:0.0,Law, social and behavioral sciences:0.0,Life and medical sciences:0.9,Operations research:0.0,Physical sciences and engineering:0.0",Life and medical sciences,Life and medical sciences is highly relevant as the tool addresses drug accessibility in healthcare. 'Computers in other domains' is less directly aligned with the primary application in healthcare.,"Bioinformatics:1.0,Computational biology:1.0,Consumer health:0.6,Genetics:0.5,Genomics:0.5,Health care information systems:1.0,Health informatics:1.0,Metabolomics / metabonomics:0.2,Systems biology:0.8","Health informatics,Health care information systems",Health informatics and Health care information systems are core to the web application for drug data. Bioinformatics is secondary due to genomic context.
2457,Complex Services and According Business Models -- Design and Evaluation of an Analysis Framework in the Field of Telemedicine,"Successful business models in the field of complex services are rare and often not profitable so far. In this design science research paper, we build and evaluate an analysis framework for such business models using the exemplary field of telemedicine. The framework is a morphological box. Its dimensions are derived from existing literature. In a second iteration and after applying it to 16 services, it is refined. The resulting artifact reveals three types of typical business model: enablers, supporters and patient-centered innovators. Besides the identification of these types, the framework allows service providers for the delimitation of own business models to others, for assessing competitors and consequently for the informed design of features leading to competitive advantages. The framework's structure allows for the elicitation of white spots -- so far not existing patterns -- for future business models and facilitates the provider's strategic positioning within the market and potential future adaptations.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.3,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.8,Social and professional topics:0.1",Applied computing,Applied computing is highly relevant for business model analysis in telemedicine. Other categories like Information systems or Software engineering are secondary to the business modeling focus.,"Arts and humanities:0.2,Computer forensics:0.2,Computers in other domains:0.2,Document management and text processing:0.2,Education:0.2,Electronic commerce:0.75,Enterprise computing:0.2,Law, social and behavioral sciences:1.0,Life and medical sciences:1.0,Operations research:0.2,Physical sciences and engineering:0.2","Life and medical sciences,Law, social and behavioral sciences","Life and medical sciences is directly relevant as the study focuses on telemedicine. Law, social and behavioral sciences applies to business model analysis. Electronic commerce is secondary. Other options are irrelevant.","Health care information systems:1,Health informatics:1,Bioinformatics:0.2,Genetics:0.1,Genomics:0.1,Metabolomics / metabonomics:0.1,Systems biology:0.2","Health care information systems,Health informatics",Health care information systems is directly relevant to telemedicine business models. Health informatics aligns with the framework's purpose. Other fields like Bioinformatics are not discussed.
3758,Vehicle Signal Analysis Using Artificial Neural Networks for a Bridge Weigh-in-Motion System,"This paper describes the procedures for development of signal analysis algorithms using artificial neural networks for Bridge Weigh-in-Motion (B-WIM) systems. Through the analysis procedure, the extraction of information concerning heavy traffic vehicles such as weight, speed, and number of axles from the time domain strain data of the B-WIM system was attempted. As one of the several possible pattern recognition techniques, an Artificial Neural Network (ANN) was employed since it could effectively include dynamic effects and bridge-vehicle interactions. A number of vehicle traveling experiments with sufficient load cases were executed on two different types of bridges, a simply supported pre-stressed concrete girder bridge and a cable-stayed bridge. Different types of WIM systems such as high-speed WIM or low-speed WIM were also utilized during the experiments for cross-checking and to validate the performance of the developed algorithms.","General and reference:0.2,Hardware:0.2,Computer systems organization:0.2,Networks:0.2,Software and its engineering:0.2,Theory of computation:0.2,Mathematics of computing:0.2,Information systems:0.2,Security and privacy:0.2,Human-centered computing:0.2,Computing methodologies:0.2,Applied computing:0.9,Social and professional topics:0.2",Applied computing,Applied computing: Uses neural networks for engineering applications in bridge weigh-in-motion systems. Other categories are rejected because the focus is on practical infrastructure applications rather than core computing methodologies.,"Arts and humanities:0.1,Computer forensics:0.1,Computers in other domains:0.8,Document management and text processing:0.1,Education:0.2,Electronic commerce:0.1,Enterprise computing:0.2,Law, social and behavioral sciences:0.1,Life and medical sciences:0.1,Operations research:0.1,Physical sciences and engineering:0.7","Computers in other domains,Physical sciences and engineering",Computers in other domains is relevant as the paper applies machine learning to civil engineering problems. Physical sciences and engineering is relevant for the bridge monitoring application. Other categories like Education are irrelevant as the focus is on engineering systems.,"Aerospace:0,Agriculture:0,Archaeology:0,Astronomy:0,Cartography:0,Chemistry:0,Computing in government:0,Digital libraries and archives:0,Earth and atmospheric sciences:0,Electronics:1,Engineering:1,Mathematics and statistics:1,Military:0,Personal computers and PC applications:0,Physics:0,Publishing:0,Telecommunications:0","Engineering,Electronics,Mathematics and statistics","Engineering: The paper is about an engineering application for bridge weigh-in-motion systems. Electronics: The paper uses signal processing techniques. Mathematics and statistics: The paper employs neural networks, a mathematical model. Other categories like aerospace are irrelevant to the paper's focus."
1861,Classification of the high PAPR codes in multicarrier transmission system,"An orthogonal frequency division multiplexing (OFDM) has been considered as most promising technique for the next generation mobile communication and broadcasting system to realize high-speed data rate and robustness to fading channel. However, main drawback in OFDM system is the high peak-to-average power ratio (PAPR) when a number of independently modulated subcarriers are added up coherently, and amplified by the high power amplifier (HPA). This paper classifies and illustrates systematic code design method of the high PAPR codes. Look-up table (LUT) based PAPR reduction method is proposed to exclude the high PAPR codes.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.2,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.75,Social and professional topics:0.1",Applied computing,Applied computing is relevant for PAPR reduction in OFDM systems. Other categories like Networks are less relevant as the focus is on signal processing rather than communication infrastructure.,"Arts and humanities:0.0,Computer forensics:0.0,Computers in other domains:0.1,Document management and text processing:0.0,Education:0.0,Electronic commerce:0.0,Enterprise computing:0.0,Law, social and behavioral sciences:0.0,Life and medical sciences:1.0,Operations research:0.0,Physical sciences and engineering:0.75","Life and medical sciences,Physical sciences and engineering",Life and medical sciences is highly relevant for neural growth applications. Physical sciences and engineering is moderately relevant for the printing technique.,"Telecommunications:1,Electronics:0.2,Engineering:0.1,Mathematics and statistics:0.1,Physics:0.1,Systems biology:0.1,Aerospace:0.1,Archaeology:0.1,Astronomy:0.1,Bioinformatics:0.1,Chemistry:0.1,Consumer health:0.1,Earth and atmospheric sciences:0.1,Genetics:0.1,Genomics:0.1,Health care information systems:0.1,Health informatics:0.1,Metabolomics / metabonomics:0.1",Telecommunications,Telecommunications is directly relevant as the paper addresses OFDM systems and PAPR reduction in multicarrier transmission. Other engineering/math categories are secondary.
1290,A chance-constrained approach to preoperative planning of robotics-assisted interventions,"In this paper, a planning strategy for robotics-assisted interventions is formulated in terms of uncertainty at the task level. The proposed formulation attempts to increase the chance of success by maximizing robustness with respect to the task uncertainty. It is assumed that the instrument tip pose has a Gaussian distribution in the vicinity of the desired task frame, and the planner is formulated as a chance-constrained programming problem in terms of the chance of collisions and joint limit violations based on the inverse kinematics of the arms. The proposed objective function addresses the robustness as well as the performance of the robotic arms. As an illustrative example, the planning strategy is implemented for LIMA harvesting in minimally invasive coronary artery bypass with the da Vinci robot.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.9,Social and professional topics:0.1",Applied computing,"Applied computing: The paper applies chance-constrained programming to robotics-assisted medical interventions (e.g., LIMA harvesting). The focus is on applying computational techniques to solve real-world problems, aligning with applied computing. Theory of computation is not the primary domain.","Arts and humanities:0.1,Computer forensics:0.1,Computers in other domains:0.1,Document management and text processing:0.1,Education:0.1,Electronic commerce:0.1,Enterprise computing:0.1,Law, social and behavioral sciences:0.1,Life and medical sciences:1.0,Operations research:0.3,Physical sciences and engineering:0.9","Life and medical sciences,Physical sciences and engineering",Life and medical sciences is relevant for the medical robotics application. Physical sciences and engineering is relevant for the technical implementation. Other categories are not directly relevant.,"Aerospace:0,Archaeology:0,Astronomy:0,Bioinformatics:0,Chemistry:0,Computational biology:0,Consumer health:0.3,Earth and atmospheric sciences:0,Electronics:0,Engineering:0,Genetics:0,Genomics:0,Health care information systems:0.5,Health informatics:1,Mathematics and statistics:1,Metabolomics / metabonomics:0,Physics:0,Systems biology:0,Telecommunications:0","Health informatics,Mathematics and statistics",Health informatics: The paper addresses robotics-assisted medical interventions. Mathematics and statistics: Chance-constrained programming is a core mathematical method. Consumer health is only weakly relevant.
2212,How assistive technology changes the brain: the critical role of hippocampal-striatal interactions during cognitive training,"Recent studies found structural changes of the brain during cognitive training. These changes may be important when assistive technologies are used, for example, to boost memory and navigation abilities in patients with neurocognitive disorders. In this study, we show that extensive training with a platformer game simulating navigation and real-life spatial abilities, patients with Alzheimer's disease and amnestic mild cognitive impairmant exhibit an increased volume of the caudate nucleus in contrast to healthy individuals who display increased hippocampal volumes. These results raise the possibility that cognitive training and assistive technology may induce compensatory changes of brain structure in memory disorders.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.2,Networks:0.1,Software and its engineering:0.3,Theory of computation:0.1,Mathematics of computing:0.2,Information systems:0.1,Security and privacy:0.2,Human-centered computing:0.1,Computing methodologies:0.4,Applied computing:0.7,Social and professional topics:0.1",Applied computing,"Applied computing: The study applies computing to assistive technology for cognitive training. Other categories are irrelevant as the core contribution is neuroscience, not computer science.","Arts and humanities:0.1,Computer forensics:0.1,Computers in other domains:0.7,Document management and text processing:0.1,Education:0.1,Electronic commerce:0.1,Enterprise computing:0.1,Law, social and behavioral sciences:0.1,Life and medical sciences:1.0,Operations research:0.1,Physical sciences and engineering:0.1","Life and medical sciences,Computers in other domains",Life and medical sciences is relevant as the study focuses on brain changes in Alzheimer's patients. Computers in other domains is relevant because assistive technology is applied to cognitive training. Other categories like Education or Electronic commerce are not the primary focus.,"Agriculture:0,Bioinformatics:0,Cartography:0,Computational biology:1,Computing in government:0,Consumer health:1,Digital libraries and archives:0,Genetics:0,Genomics:0,Health care information systems:0,Health informatics:1,Metabolomics / metabonomics:0,Military:0,Personal computers and PC applications:0,Publishing:0,Systems biology:0","Health informatics,Computational biology,Consumer health",Health informatics: Focuses on assistive technology in healthcare. Computational biology: Analyzes brain structure changes. Consumer health: Applies findings to Alzheimer's patients. Other categories like Genomics are not relevant.
4480,Long-Term Contracts Under the Threat of Supplier Default,"Contracting with suppliers prone to default is an increasingly common problem in some industries, particularly automotive manufacturing. We model this phenomenon as a two-period contracting game with two identical suppliers, a single buyer, deterministic demand, and uncertain production costs. The suppliers are distressed at the start of the game and do not have access to external sources of capital; hence, revenues from the buyer are crucial in determining whether default occurs. The production cost of each supplier is the sum of two stochastic components: a common term that is identical for both suppliers (representing raw materials costs, design specifications, etc.) and an idiosyncratic term that is unique to a given supplier (representing inherent firm capability). The buyer chooses a supplier and then decides on a single-or two-period contract. Comparing models with and without the possibility of default, we find that, without the possibility of supplier failure, the buyer always prefers short-term contracts over long-term contracts, whereas this preference is typically reversed in the presence of failure. Neither of these contracts coordinates the supply chain. We also consider dynamic contracts, in which the contract price is partially tied to some index representing the common component of production costs (e.g., commodity prices of raw materials such as steel or oil), allowing the buyer to shoulder some of the risk from cost uncertainty. We find that dynamic long-term contracts allow the buyer to coordinate the supply chain in the presence of default risk. We also demonstrate that our results continue to hold under a variety of alternative assumptions, including stochastic demand, allowing the buyer the option of subsidizing a bankrupt supplier via a contingent transfer payment or loan and allowing the buyer to unilaterally renegotiate contracts. We conclude that the possibility of supplier default offers a new reason to prefer long-term contracts over short-term contracts.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.5,Social and professional topics:0.1",Applied computing,"Applied computing is marginally relevant as the paper discusses supply chain modeling, though the core contribution lies in economic theory. Other categories are not directly applicable.","Arts and humanities:0.0,Computer forensics:0.0,Computers in other domains:0.1,Document management and text processing:0.0,Education:0.0,Electronic commerce:0.2,Enterprise computing:0.3,Law, social and behavioral sciences:0.0,Life and medical sciences:0.0,Operations research:1.0,Physical sciences and engineering:0.0",Operations research,Operations research is highly relevant as the paper models supplier default as a game-theoretic optimization problem with supply chain coordination analysis. Other categories like Enterprise computing or Electronic commerce are less directly connected to the mathematical modeling of supplier-buyer dynamics.,"Computer-aided manufacturing:0.2,Consumer products:0.1,Decision analysis:1.0,Forecasting:0.3,Industry and manufacturing:1.0,Marketing:0.1,Transportation:0.1","Decision analysis,Industry and manufacturing",Decision analysis is highly relevant as the paper analyzes strategic contract decisions under supplier default risk. Industry and manufacturing is relevant due to the focus on supply chain management in manufacturing contexts. Other categories like Marketing or Consumer products are not addressed.
919,Study on evaluation of muscle conditions using a mechanomyogram sensor,"In this study, a mechanomyogram (MMG) sensor system for monitoring muscle conditions is developed. The polyvinylidene fluoride (PVDF) film is used as the sensory material of the MMG sensor. The mean power frequency (MPF) of the sensor output, which is related to firing rate of action potential of muscle, and the variance of the sensor output, which is related to the activity of fast fibers and complete recruitment, are calculated from the MMG signal. The fundamental experiments were conducted. First, MMG signal are measured for muscular injury of upper arm. From the results, MMG sensor is available to estimate the recovery of muscle. Next, muscle activities of isometric contraction are studied using the developed MMG sensor before and after muscle fatigue. Experimental results show that contraction of fast fibers is occurred earlier and fast fibers activity increase earlier due to muscle fatigue. Focusing on the time shift of local maximum point in MPF and decrease of growth rate in variance, parameters for evaluation of muscle fatigue degree are proposed.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.85,Social and professional topics:0.1",Applied computing,Applied computing: The study applies sensor technology to evaluate muscle conditions in healthcare. Other categories are not focused on domain-specific applications like biomedicine.,"Arts and humanities:0.0,Computer forensics:0.0,Computers in other domains:0.0,Document management and text processing:0.0,Education:0.0,Electronic commerce:0.0,Enterprise computing:0.0,Law, social and behavioral sciences:0.0,Life and medical sciences:1.0,Operations research:0.0,Physical sciences and engineering:0.75","Life and medical sciences,Physical sciences and engineering","Life and medical sciences: The paper focuses on muscle condition evaluation using MMG sensors, a medical application. Physical sciences and engineering: The study involves PVDF film sensor development, an engineering material. Other categories like Arts and humanities are irrelevant as the paper is purely technical/medical.","Aerospace:0,Archaeology:0,Astronomy:0,Bioinformatics:1,Chemistry:0,Computational biology:0.5,Consumer health:0.5,Earth and atmospheric sciences:0,Electronics:0.5,Engineering:0.5,Genetics:0,Genomics:0,Health care information systems:0.5,Health informatics:1,Mathematics and statistics:0,Metabolomics / metabonomics:0,Physics:0,Systems biology:0.5,Telecommunications:0","Bioinformatics,Health informatics",Bioinformatics is relevant due to the use of signal processing for biological data. Health informatics is relevant as the sensor is applied to medical evaluation. Other categories like Computational biology or Consumer health are less directly related to the sensor's technical and medical application focus.
3212,Index Finger of a Human-Like Robotic Hand Using Thin Soft Muscles,"This letter presents an index finger of human-like robotics hand which intends to closely replicate the human finger in terms of bones, ligaments, muscles, extensor mechanism, tendon, and its pulley system. We fabricated the muscles of the index finger using thin multifilament McKibben muscles that consist of three intrinsic and three extrinsic muscles with diameters of 1.3 mm and 4.0 mm, respectively. We present the fabrication method of the index finger and model it based on the Landsmeer Model I, II, and III. We validated the properties of our developed finger with the Landsmeer model for extension and flexion motion. Finally, we demonstrated the capabilities of the finger using motion capture, Tracker, to compare sweeps of the HR-hand finger with a cadaver finger motion for normal and finger deformity condition. Having McKibben muscles as the actuator that mimics the human muscle, one can better understand the human finger function and may use it for training and for modeling the human finger disorders.","General and reference:0.1,Hardware:0.3,Computer systems organization:0.2,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.8,Social and professional topics:0.1",Applied computing,Applied computing is most relevant for the human-like robotic hand application. Hardware (0.3) relates to the physical design but is secondary. The primary focus is on applied robotics for medical and human function understanding.,"Arts and humanities:0.1,Computer forensics:0.1,Computers in other domains:0.1,Document management and text processing:0.1,Education:0.1,Electronic commerce:0.1,Enterprise computing:0.1,Law, social and behavioral sciences:0.1,Life and medical sciences:0.85,Operations research:0.1,Physical sciences and engineering:0.9","Life and medical sciences,Physical sciences and engineering",Life and medical sciences is relevant for the human-like robotic hand application. Physical sciences and engineering is relevant for the technical device design. Other categories are not directly related.,"Aerospace:0.0,Archaeology:0.0,Astronomy:0.0,Bioinformatics:0.0,Chemistry:0.0,Computational biology:0.0,Consumer health:0.0,Earth and atmospheric sciences:0.0,Electronics:0.0,Engineering:0.9,Genetics:0.0,Genomics:0.0,Health care information systems:0.0,Health informatics:0.2,Mathematics and statistics:0.0,Metabolomics / metabonomics:0.0,Physics:0.1,Systems biology:0.0,Telecommunications:0.0",Engineering,"Engineering: The paper describes the design and fabrication of a human-like robotic hand, a clear engineering contribution. Health informatics is secondary as the application is in understanding human anatomy but not in healthcare systems."
1846,Change Detection for Traffic Monitoring in Terrasar-X Imagery,"In this paper the changes occurring in two images acquired at two different time moments are analyzed. In particular the interest is in the changes of filling grade of car-parks or even detecting stopped vehicles on the congested or jammed roads. As input to the change detection processor can serve two repeat-pass single channel images acquired by any airborne or space borne SAR satellite sensor. Each channel is focused, calibrated and processed to a single look slant range complex SAR image. The proposed change detection approach is based on the combination of various techniques: co-registration of two SAR images by interferometric SAR processor, removal of the flat Earth phase, channel balancing, calculating of a coherent or incoherent difference of two images and finally image post-processing and analysis. First experiments on TerraSAR-X imagery show promising results.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.3,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.9,Social and professional topics:0.1",Applied computing,"Applied computing: The paper applies computational techniques (SAR image processing, change detection) to real-world traffic monitoring. Computing methodologies (0.2) is secondary as the focus is on application rather than methodological innovation.","Arts and humanities:0.1,Computer forensics:0.1,Computers in other domains:0.85,Document management and text processing:0.1,Education:0.1,Electronic commerce:0.1,Enterprise computing:0.1,Law, social and behavioral sciences:0.1,Life and medical sciences:0.1,Operations research:0.1,Physical sciences and engineering:0.75","Computers in other domains,Physical sciences and engineering",Computers in other domains is relevant for applying change detection to traffic monitoring. Physical sciences and engineering is relevant due to SAR image processing techniques. Other categories like life sciences are less relevant as the focus is on remote sensing applications rather than biological systems.,"Aerospace:0.2,Agriculture:0,Archaeology:0,Astronomy:0,Cartography:0.1,Chemistry:0,Computing in government:0,Digital libraries and archives:0,Earth and atmospheric sciences:1,Electronics:0.1,Engineering:0.3,Mathematics and statistics:0.1,Military:0,Personal computers and PC applications:0,Physics:0.2,Publishing:0,Telecommunications:0.1",Earth and atmospheric sciences,Earth and atmospheric sciences is highly relevant as the paper focuses on SAR image-based Earth monitoring for traffic analysis. Other fields like Aerospace or Engineering are less central to the core application domain.
5683,Study on efficiency of supply chain based on single revenue sharing contract and double contracts combination,"Supply chain contract plays an important role in the coordination of members of supply chain. The existing application of supply chain contracts is mainly based on a single contract, which exist some limitations. From the point of combination of supply chain contracts, we establish two models based on system dynamics, one is revenue sharing contract model and the other is the combination model consisting of quantity discount contract and revenue sharing contract. Then, we use Vensim to simulate. The results show that the combination model is more conducive to the overall efficiency of the supply chain. This paper provides a new way for the study on supply chain coordination.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.9,Social and professional topics:0.1",Applied computing,"Applied computing is relevant for applying simulation models to supply chain coordination. Other categories are irrelevant as the focus is on business applications, not algorithms or systems.","Arts and humanities:0.1,Computer forensics:0.1,Computers in other domains:0.1,Document management and text processing:0.1,Education:0.1,Electronic commerce:0.1,Enterprise computing:0.8,Law, social and behavioral sciences:0.1,Life and medical sciences:0.1,Operations research:0.9,Physical sciences and engineering:0.1","Operations research,Enterprise computing",Operations research: The paper uses system dynamics to model supply chain efficiency. Enterprise computing: The study is in the context of supply chain coordination. Other categories are irrelevant.,"Business process management:0.3,Business rules:0.1,Business-IT alignment:0.2,Computer-aided manufacturing:0.0,Consumer products:0.0,Decision analysis:0.7,Enterprise architectures:0.4,Enterprise computing infrastructures:0.2,Enterprise data management:0.1,Enterprise information systems:0.8,Enterprise interoperability:0.3,Enterprise modeling:0.5,Enterprise ontologies, taxonomies and vocabularies:0.1,Event-driven architectures:0.0,Forecasting:0.6,IT architectures:0.2,IT governance:0.1,Industry and manufacturing:0.3,Marketing:0.05,Reference models:0.2,Service-oriented architectures:0.1,Transportation:0.0","Enterprise information systems,Forecasting,Decision analysis",Enterprise information systems: The paper studies supply chain coordination using system dynamics. Forecasting: System dynamics modeling involves predictive analysis. Decision analysis: Contract design for supply chain efficiency. Other children like Marketing or Transportation are less relevant to the core focus.
1957,Examining the Use of Social Media in Customer Co-Creation: A Blog Mining Study,"Social media have been used by some companies to support customer co-creation in recent years. However, few academic studies have been done to investigate the use of social media for customer co-creation. To understand the current state-of-theart and future trends about the use of social media for customer co-creation, we conducted a blog mining study to analyze relevant posts on blogs. This blog study reveals some interesting patterns, themes and future trends in this specific area.","General and reference:0.3,Hardware:0.3,Computer systems organization:0.3,Networks:0.3,Software and its engineering:0.3,Theory of computation:0.3,Mathematics of computing:0.3,Information systems:0.6,Security and privacy:0.3,Human-centered computing:0.3,Computing methodologies:0.3,Applied computing:1.0,Social and professional topics:0.3",Applied computing,Applied computing is highly relevant for the business application of social media analysis. Information systems is less central as the focus is on customer co-creation rather than data systems.,"Arts and humanities:0.0,Computer forensics:0.0,Computers in other domains:0.0,Document management and text processing:0.0,Education:0.0,Electronic commerce:1.0,Enterprise computing:0.3,Law, social and behavioral sciences:0.0,Life and medical sciences:0.0,Operations research:0.0,Physical sciences and engineering:0.0",Electronic commerce,Electronic commerce is directly relevant for social media use in customer co-creation. Enterprise computing is only tangentially relevant for the blog analysis methodology.,"Digital cash:0.1,E-commerce infrastructure:0.7,Electronic data interchange:0.1,Electronic funds transfer:0.1,Online auctions:0.1,Online banking:0.1,Online shopping:0.3,Secure online transactions:0.1",E-commerce infrastructure,E-commerce infrastructure is relevant for the use of social media in customer co-creation within business contexts. Online shopping is secondary as the study relates to customer engagement rather than direct purchasing. Other categories like Digital cash are irrelevant.
4495,Focus tracking for cinematography,"Cinematographers primarily use manual control for focusing their cameras because existing autofocus techniques used in photography can't be directly applied to video or motion-pictures and don't provide sufficient artistic control. Adjusting focus therefore remains a key challenge, which limits the possibilities of executing certain shots. For instance, the amount of depth of field used in shots with a moving camera or subject is heavily influenced by how precise focus can be controlled. This work presents a simple method to overcome some of these challenges by tracking the focus with off the shelf sensory equipment and state of the art 3D point cloud processing techniques. The method integrates well with the current workflow of camera operators and their first assistants and even gives them more flexibility than a manually controlled follow focus. To evaluate the feasibility, a fully functional prototype was built and tested with professional camera operators.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.8,Social and professional topics:0.1",Applied computing,Applied computing: The paper applies 3D point cloud processing to cinematography. Other categories are less relevant as the focus is on applying computing techniques to cinematography.,"Arts and humanities:1.0,Physical sciences and engineering:0.75,Computer forensics:0.0,Computers in other domains:0.0,Document management and text processing:0.0,Education:0.0,Electronic commerce:0.0,Enterprise computing:0.0,Law, social and behavioral sciences:0.0,Life and medical sciences:0.0,Operations research:0.0","Arts and humanities,Physical sciences and engineering",Arts and humanities is highly relevant for cinematography applications. Physical sciences and engineering is relevant for the technical 3D point cloud processing. Other categories are unrelated to creative or engineering aspects.,"Aerospace:0.1,Archaeology:0.1,Architecture (buildings):0.2,Astronomy:0.1,Chemistry:0.1,Earth and atmospheric sciences:0.1,Electronics:0.3,Engineering:1,Fine arts:0.6,Language translation:0.1,Mathematics and statistics:0.4,Media arts:1,Performing arts:0.5,Physics:0.2,Sound and music computing:0.3,Telecommunications:0.2","Engineering,Media arts",Engineering is relevant to the technical implementation of the focus tracking system. Media arts is relevant to the cinematography application. Performing arts is rejected as the focus is on technical cinematography rather than performance itself.
2659,Making campus bridging work for researchers: a case study with mlRho,"An increasing number of biologists' computational demands have outgrown the capacity of desktop workstations and they are turning to supercomputers to run their simulations and calculations. Many of today's computational problems, however, require larger resource commitments than even individual universities can provide. XSEDE is one of the first places researchers turn to when they outgrow their campus resources. XSEDE machines are far larger (by at least an order of magnitude) than what most universities offer. Transitioning from a campus resource to an XSEDE resource is seldom a trivial task. XSEDE has taken many steps to make this easier, including the Campus Bridging initiative, the Campus Champions program, the Extended Collaborative Support Service (ECSS) [1] program, and through education and outreach. In this paper, our team of biologists and application support analysts (including a Campus Champion) dissect a computationally intensive biology project and share the insights we gain to help strengthen the programs mentioned above. We worked on a project to calculate population mutation and recombination rates of tens of genome profiles using mlRho [2], a serial, open-source, genome analysis code. For the initial investigation, we estimated that we would need 6.3 million service units (SUs) on the Ranger system. Three of the most important places where the biologists needed help in transitioning to XSEDE were (i) preparing the proposal for 6.3 million SUs on XSEDE, (ii) scaling up the existing workflow to hundreds of cores and (iii) performance optimization. The Campus Bridging initiative makes all of these tasks easier by providing tools and a consistent software stack across centers. Ideally, Campus Champions are able to provide support on (i), (ii) and (iii), while ECSS staff can assist with (ii) and (iii). But (i), (ii) and (iii) are often not part of a Campus Champion's regular job description. To someone writing an XSEDE proposal for the first time, a link to the guidelines and a few pointers may not always be enough for a successful application. In this paper we describe a new role for a campus bridging expert to play in closing the gaps between existing programs and present mlRho as a case study.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.8,Social and professional topics:0.1",Applied computing,Applied computing: The paper addresses computational challenges in transitioning biological research projects to national supercomputing resources. Other categories are irrelevant as the focus is on applying computing resources to domain-specific scientific workflows.,"Arts and humanities:0.2,Computer forensics:0.1,Computers in other domains:0.9,Document management and text processing:0.3,Education:0.6,Electronic commerce:0.1,Enterprise computing:0.4,Law, social and behavioral sciences:0.2,Life and medical sciences:0.8,Operations research:0.3,Physical sciences and engineering:0.2","Life and medical sciences,Computers in other domains,Education",Life and medical sciences is primary due to the computational biology focus. Computers in other domains is relevant for using XSEDE resources in research. Education is secondary as the paper discusses researcher training programs. Other fields like OR/enterprise computing are less central.,"Agriculture:0,Bioinformatics:1,Cartography:0,Collaborative learning:0,Computational biology:1,Computer-assisted instruction:0,Computer-managed instruction:0,Computing in government:0,Consumer health:0,Digital libraries and archives:0,Distance learning:0,E-learning:0,Genetics:0,Genomics:0,Health care information systems:0,Health informatics:0,Interactive learning environments:0,Learning management systems:0,Metabolomics / metabonomics:0,Military:0,Personal computers and PC applications:0,Publishing:0,Systems biology:0.5","Bioinformatics,Computational biology","Bioinformatics and Computational biology are directly relevant as the paper discusses a computational biology project (mlRho) and challenges in using HPC resources. Systems biology has partial relevance due to the biological context, but the focus is more on computational methods than systems-level biology."
2605,On the Decidability of the Continuous Infinite Zeros Problem,"We study the Continuous Infinite Zeros Problem, which asks whether a real-valued function f satisfying a given ordinary linear differential equation has infinitely many zeros on R�0. We consider also the closely related Unbounded Continuous Skolem Problem, which asks whether f has a zero in a given unbounded subinterval of R�0. These are fundamental reachability problems arising in the analysis of continuous linear dynamical systems, including linear hybrid automata and continuous-time Markov chains. Our main decidability result is that if the ordinary differential equation satisfied by f is of order at most 7 or if the imaginary parts of its characteristic roots are all rational multiples of one another, then the Infinite Zeros Problem is decidable, and moreover, if f has only finitely many zeros, then an upper bound T may be found such that f(t) = 0 entails tT. On the other hand, our main hardness results is that if the Infinite Zeros Problem is decidable for ordinary differential equations of order at least 9, then this would entail a major breakthrough in Diophantine Approximation, specifically, the computability of the Lagrange constant L1(x) for all real algebraic x.","General and reference:0,Hardware:0,Computer systems organization:0,Networks:0,Software and its engineering:0,Theory of computation:0.2,Mathematics of computing:0,Information systems:0.1,Security and privacy:0,Human-centered computing:0,Computing methodologies:0.3,Applied computing:1,Social and professional topics:0",Applied computing,"Applied computing is directly relevant as the paper applies data analysis to socioeconomic forecasting. Computing methodologies receives partial relevance for the models used, but the primary application domain is applied computing.","Arts and humanities:0.1,Computer forensics:0.1,Computers in other domains:0.1,Document management and text processing:0.1,Education:0.1,Electronic commerce:0.1,Enterprise computing:0.1,Law, social and behavioral sciences:0.1,Life and medical sciences:0.1,Operations research:0.1,Physical sciences and engineering:0.8",Physical sciences and engineering,"Physical sciences and engineering is relevant because the paper focuses on mathematical analysis of continuous dynamical systems, which is a core topic in applied mathematics and engineering. Other options are irrelevant as they pertain to unrelated domains like education or social sciences.","Aerospace:0.0,Archaeology:0.0,Astronomy:0.0,Chemistry:0.0,Earth and atmospheric sciences:0.0,Electronics:0.1,Engineering:0.0,Mathematics and statistics:0.9,Physics:0.0,Telecommunications:0.0",Mathematics and statistics,Mathematics and statistics (decidability of differential equations). Other applied sciences are irrelevant.
1617,Application of a human auditory model to loudness perception and hearing compensation,A model is proposed which mathematically transforms an acoustic stimulus into a form which is believed to be more nearly related to that used by the auditory cortex to interpret the sound. The model is based upon research towards understanding the response of the human auditory system to sound stimuli. The motivation and approach in developing this model follow the philosophy pursued in the development of a similar model to the problem of hearing compensation for impaired individuals is shown to yield a bank of bandpass filters each followed by a homomorphic multiplicative AGC. Clinical tests on hearing impaired subjects suggest that this approach is far superior to other current hearing compensation schemes.,"General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:1.0,Social and professional topics:0.1",Applied computing,Applied computing is highly relevant as the paper applies a computational model to solve real-world hearing compensation problems. Other categories are irrelevant as the focus is on domain-specific application rather than core computing methodologies.,"Life and medical sciences:0.9,Physical sciences and engineering:0.8,Arts and humanities:0.1","Life and medical sciences,Physical sciences and engineering","Life and medical sciences/Physical sciences and engineering: The paper develops an auditory model with applications in hearing compensation, intersecting medical and engineering domains. Arts and humanities: Irrelevant as the focus is on technical modeling, not arts or humanities applications.","Aerospace:0.1,Archaeology:0.1,Astronomy:0.1,Bioinformatics:1.0,Chemistry:0.1,Computational biology:0.9,Consumer health:0.8,Earth and atmospheric sciences:0.1,Electronics:0.3,Engineering:0.4,Genetics:0.2,Genomics:0.3,Health care information systems:0.7,Health informatics:1.0,Mathematics and statistics:0.2,Metabolomics / metabonomics:0.1,Physics:0.1,Systems biology:0.8,Telecommunications:0.1","Bioinformatics,Health informatics",Bioinformatics: The paper applies a computational model to biological auditory processing. Health informatics: The model is used for hearing compensation in clinical settings. Other children like 'Computational biology' or 'Systems biology' are moderately relevant but less directly connected than the primary categories.
4024,Dual-Mode Electro-Optical Techniques for Biosensing Applications: A Review,"The monitoring of biomolecular interactions is a key requirement for the study of complex biological processes and the diagnosis of disease. Technologies that are capable of providing label-free, real-time insight into these interactions are of great value for the scientific and clinical communities. Greater understanding of biomolecular interactions alongside increased detection accuracy can be achieved using technology that can provide parallel information about multiple parameters of a single biomolecular process. For example, electro-optical techniques combine optical and electrochemical information to provide more accurate and detailed measurements that provide unique insights into molecular structure and function. Here, we present a comparison of the main methods for electro-optical biosensing, namely, electrochemical surface plasmon resonance (EC-SPR), electrochemical optical waveguide lightmode spectroscopy (EC-OWLS), and the recently reported silicon-based electrophotonic approach. The comparison considers different application spaces, such as the detection of low concentrations of biomolecules, integration, the tailoring of light-matter interaction for the understanding of biomolecular processes, and 2D imaging of biointeractions on a surface.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.8,Social and professional topics:0.0",Applied computing,Applied computing is relevant because the paper reviews electro-optical biosensing techniques applied to biomedical problems. Other categories like Networks or Security and privacy are irrelevant as the paper does not address communication systems or security protocols.,"Life and medical sciences:0.8,Physical sciences and engineering:0.75,Computers in other domains:0.7,Arts and humanities:0.1,Computer forensics:0.1,Document management and text processing:0.1,Education:0.1,Electronic commerce:0.1,Enterprise computing:0.1,Law, social and behavioral sciences:0.1,Operations research:0.1","Life and medical sciences,Physical sciences and engineering,Computers in other domains",Life and medical sciences is relevant as the paper discusses biosensing applications for disease diagnosis. Physical sciences and engineering is relevant due to the electro-optical techniques. Computers in other domains is relevant as it's an application of computer technology in biosensing. Arts and humanities is less relevant as the focus is on scientific and engineering applications.,"Aerospace:0.1,Agriculture:0.1,Archaeology:0.1,Astronomy:0.1,Bioinformatics:0.1,Cartography:0.1,Chemistry:0.1,Computational biology:0.1,Computing in government:0.1,Consumer health:0.1,Digital libraries and archives:0.1,Earth and atmospheric sciences:0.1,Electronics:1.0,Engineering:1.0,Genetics:0.1,Genomics:0.1,Health care information systems:0.1,Health informatics:0.1,Mathematics and statistics:0.1,Metabolomics / metabonomics:0.1,Military:0.1,Personal computers and PC applications:0.1,Physics:1.0,Publishing:0.1,Systems biology:0.1,Telecommunications:0.1","Electronics,Physics",Electronics is relevant for the electro-optical techniques discussed. Physics is relevant for the underlying principles of electro-optical methods. Other categories like Bioinformatics are not central to the core contribution.
2437,Mass Detection in Viscous Fluid Utilizing Vibrating Micro- and Nanomechanical Mass Sensors under Applied Axial Tensile Force,"Vibrating micro- and nanomechanical mass sensors are capable of quantitatively determining attached mass from only the first three (two) measured cantilever (suspended) resonant frequencies. However, in aqueous solutions that are relevant to most biological systems, the mass determination is challenging because the quality factor (Q-factor) due to fluid damping decreases and, as a result, usually just the fundamental resonant frequencies can be correctly identified. Moreover, for higher modes the resonance coupling, noise, and internal damping have been proven to strongly affect the measured resonances and, correspondingly, the accuracy of estimated masses. In this work, a technique capable of determining the mass for the cantilever and also the position of nanobeads attached on the vibrating micro-/nanomechanical beam under intentionally applied axial tensile force from the measured fundamental flexural resonant frequencies is proposed. The axial force can be created and controlled through an external electrostatic or magnetostatic field. Practicality of the proposed technique is confirmed on the suspended multi-walled carbon nanotube and the rectangular silicon cantilever-based mass sensors. We show that typically achievable force resolution has a negligibly small impact on the accuracy of mass measurement.","General and reference:0.1,Hardware:0.5,Computer systems organization:0.3,Networks:0.2,Software and its engineering:0.2,Theory of computation:0.2,Mathematics of computing:0.4,Information systems:0.2,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.3,Applied computing:0.8,Social and professional topics:0.1",Applied computing,Applied computing is relevant for the biological mass detection application. Hardware is secondary as the focus is measurement techniques rather than sensor design.,"Physical sciences and engineering:0.9,Life and medical sciences:0.5,Operations research:0.3",Physical sciences and engineering,Physical sciences and engineering: The paper presents a novel nanomechanical mass detection technique with experimental validation. Life sciences has limited relevance despite biological applications being mentioned. Other categories are unrelated to the core mechanical/physical measurement focus.,"Aerospace:0.0,Archaeology:0.0,Astronomy:0.0,Chemistry:0.2,Earth and atmospheric sciences:0.0,Electronics:1.0,Engineering:1.0,Mathematics and statistics:0.1,Physics:0.3,Telecommunications:0.0","Electronics,Engineering",Electronics and Engineering are directly relevant to the paper's focus on vibrating nanomechanical mass sensors in fluid environments.
5217,"Remote sensing and geological mapping for a groundwater recharge model in the arid area of Sebt Rbrykine: Doukkala, western Morocco","Pressure has recently been put on the water resources of Doukkala region (western Morocco) due to the development of agricultural and industrial activities, associated with strong demographic expansion. Doukkala's water resources must be better managed and remote sensing provides effective techniques for such an objective. This research illustrates the use of remote sensing for mapping the regional geology, surface hydrology and hydrogeology of the Sebt Brykine, regarded as a groundwater recharge zone for the populated plains of Doukkala. A moderate resolution DEM (30 m pixels) was needed for the hydrological and geomorphological characterization. Derivative products from DEM will also be useful for environmental studies and assessing possible impacts of climate change. Multi-sensor remotely sensed datasets were used to produce several thematic map layers at 1:100 000 scale (lithologies, geological structures, and drainage). These maps allowed the characterization of the regional aquifers and aquicludes for a better understanding of groundwater circulation.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.2,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.8,Social and professional topics:0.1",Applied computing,"Applied computing is relevant for remote sensing in groundwater modeling. Other categories are rejected as the paper focuses on environmental applications, not core computing topics.","Physical sciences and engineering:1.0,Life and medical sciences:0.0,Arts and humanities:0.0,Computer forensics:0.0,Computers in other domains:0.0,Document management and text processing:0.0,Education:0.0,Electronic commerce:0.0,Enterprise computing:0.0,Law, social and behavioral sciences:0.0,Operations research:0.0",Physical sciences and engineering,Physical sciences and engineering is directly relevant as the paper applies remote sensing to geological and hydrological studies. Other categories like Life and medical sciences are irrelevant as the focus is on environmental engineering.,"Aerospace:0,Archaeology:0,Astronomy:0,Chemistry:0,Earth and atmospheric sciences:1,Electronics:0,Engineering:0.3,Mathematics and statistics:0,Physics:0,Telecommunications:0",Earth and atmospheric sciences,Earth and atmospheric sciences: The paper applies remote sensing to geological and hydrological mapping for groundwater studies. Other categories lack relevance to environmental or geological analysis.
5879,Computer Systems to Oil Pipeline Transporting,"Computer systems in the pipeline oil transporting that the greatest amount of data can be gathered, analyzed and acted upon in the shortest amount of time. Most operators now have some form of computer based monitoring system employing either commercially available or custom developed software to run the system. This paper presented the SCADA systems to oil pipeline in concordance to the Romanian environmental reglementations.","General and reference:0.25,Hardware:0.25,Computer systems organization:0.25,Networks:0.25,Software and its engineering:0.25,Theory of computation:0.25,Mathematics of computing:0.25,Information systems:0.25,Security and privacy:0.25,Human-centered computing:0.25,Computing methodologies:0.25,Applied computing:1.0,Social and professional topics:0.25",Applied computing,The paper applies computer systems to oil pipeline monitoring (Applied computing). SCADA systems relate to Information systems but are secondary. Other categories are irrelevant.,"Computers in other domains:0.85,Operations research:0.6,Enterprise computing:0.4,Education:0.2,Life and medical sciences:0.1,Physical sciences and engineering:0.2,Law, social and behavioral sciences:0.1,Document management and text processing:0.1,Electronic commerce:0.1,Arts and humanities:0.1",Computers in other domains,Computers in other domains: Applies computer systems to oil pipeline monitoring per Romanian regulations. Other categories like 'Operations research' are only indirectly relevant.,"Agriculture:0.2,Cartography:0.2,Computing in government:1.0,Digital libraries and archives:0.3,Military:0.2,Personal computers and PC applications:0.4,Publishing:0.3",Computing in government,Computing in government is relevant due to adherence to Romanian environmental regulations. Other categories are unrelated to pipeline monitoring systems.
5910,Co-fabrication of live skeletal muscles as actuators in A millimeter scale mechanical system,"Functional muscle tissue holds promise as a practical actuator for use in engineering applications. Previously, functional live-cell muscle actuators used for robotics have not scaled greater than about 10 µm, the size of a single monolayer of cells. We present a method to produce larger scale muscle actuators fully integrated into a mechanical structure. We use manufacturing techniques including printing a mold, pouring a molded part, and deposition of cell suspension. Our method allows for co-fabrication of actuator and mechanism through muscle self-assembly. We incorporate muscle construct technologies such that the muscle is fully 3D, anchored, and aligned, yielding a 10 mm long and 0.5 mm thick aligned muscle actuator. By co-fabricating the mechanism and actuators, the muscles are produced and used in the same environmental conditions, the process is more robust and repeatable, and evaluation of performance is under identical conditions to those in which the actuator is used. By using the presented method, variable geometry and multiple degrees of freedom can all be incorporated in a single mechanical structure.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.2,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.7,Social and professional topics:0.1",Applied computing,Applied computing is relevant for the bioengineering application. Other categories like Hardware are not applicable to the biological system focus.,"Arts and humanities:0.1,Computer forensics:0.1,Computers in other domains:0.1,Document management and text processing:0.1,Education:0.1,Electronic commerce:0.1,Enterprise computing:0.1,Law, social and behavioral sciences:0.1,Life and medical sciences:0.8,Operations research:0.1,Physical sciences and engineering:0.7","Life and medical sciences,Physical sciences and engineering",Life and medical sciences is relevant due to the use of live skeletal muscles. Physical sciences and engineering is relevant for the fabrication of mechanical actuators. Other categories are rejected as they do not align with the core biological or engineering contributions.,"Aerospace:0.1,Archaeology:0.1,Astronomy:0.1,Bioinformatics:0.1,Chemistry:0.1,Computational biology:0.9,Consumer health:0.1,Earth and atmospheric sciences:0.1,Electronics:0.1,Engineering:0.9,Genetics:0.1,Genomics:0.1,Health care information systems:0.1,Health informatics:0.1,Mathematics and statistics:0.1,Metabolomics / metabonomics:0.1,Physics:0.1,Systems biology:0.9,Telecommunications:0.1","Computational biology,Engineering,Systems biology",Computational biology is relevant for modeling muscle self-assembly. Engineering is relevant for the mechanical system fabrication. Systems biology applies to the biological system integration. Other fields lack direct relevance to bioengineering or mechanical design.
4805,Lack To Transfer The Performance's Improvements Obtained In Virtual Reality Environment To Balance Control In Patients With Chronic Sequels Of Stroke,"Objectives: Despite the recent enlargement in the usage of Virtual Reality environments for rehabilitation in Stroke, the potential for transference/generalization of gains to similar task performed in real environments remains uncertain. Thus, the purpose of this study was to verify the transference/generalization of gains obtained by training in Virtual Reality environments (VR) to similar balance tasks in real environment in patients with chronic sequels of Stroke. Participants: Twenty-nine chronic stroke patients. Interventions: Three sessions of balance training in VR using five games from Nintendo Wii Fit™ for experimental group (EG). Verbal orientation about falls preventions for control group (CG). Main outcome measures: The transference/generalization of gains obtained in VR training was assessed through performance in four balance tests with similar demand than games using a force plate in two assessment time points: at baseline (BA) and at end of study (EA). Results: The ANOVA for repeated measure showed that there were statistically significant improvements for all trained games after the training. However, there were no statistical significant differences in the balance tests performance between BA and EA, for both groups. Conclusions: Patients with chronic sequels with stroke were unable to transfer/generalize the gains obtained in VR to similar balance tasks performed in real environment.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.3,Computing methodologies:0.3,Applied computing:0.9,Social and professional topics:0.1",Applied computing,"Applied computing is highly relevant as the study applies VR technology to medical rehabilitation. Human-centered computing and Computing methodologies are moderately relevant for the interface and methodology, but the primary focus is on the application in healthcare. Other categories are irrelevant.","Arts and humanities:0.1,Computer forensics:0.1,Computers in other domains:0.8,Document management and text processing:0.1,Education:0.2,Electronic commerce:0.1,Enterprise computing:0.1,Law, social and behavioral sciences:0.3,Life and medical sciences:0.9,Operations research:0.1,Physical sciences and engineering:0.1","Life and medical sciences,Computers in other domains","Life and medical sciences: The paper focuses on medical rehabilitation outcomes in stroke patients. Computers in other domains: The paper discusses the application of VR technology in healthcare. Arts and humanities, Computer forensics, Document management and text processing, Electronic commerce, Enterprise computing, Law, Operations research, Physical sciences and engineering are irrelevant as they don't relate to the core medical VR rehabilitation topic.","Agriculture:0.1,Bioinformatics:0.1,Cartography:0.1,Computational biology:0.1,Computing in government:0.1,Consumer health:0.6,Digital libraries and archives:0.1,Genetics:0.1,Genomics:0.1,Health care information systems:0.7,Health informatics:1.0,Metabolomics / metabonomics:0.1,Military:0.1,Personal computers and PC applications:0.2,Publishing:0.1,Systems biology:0.1","Health informatics,Health care information systems",Health informatics is directly relevant as the study applies VR in stroke rehabilitation. Health care information systems are relevant due to the use of VR for medical outcomes. Other categories like bioinformatics or genomics are not relevant.
4380,Implementation of reliable H infinity observer-controller for TRMS with sensor and actuator failure,In this paper reliable H infinity observer and H infinity controller which is designed for Twin Rotor MIMO System(TRMS) is implemented on real Twin Rotor MIMO System. The reliability of the observer-controller is tested on real TRMS with sensor and actuator partial failure and with complete failure. The model used is the identified model for TRMS using system identification technique.,"General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.2,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.5,Applied computing:0.85,Social and professional topics:0.0",Applied computing,Applied computing is highly relevant as the paper implements a control system for a TRMS with sensor/actuator failure. Computing methodologies has moderate relevance but is secondary to the application focus.,"Arts and humanities:0.1,Computer forensics:0.1,Computers in other domains:0.9,Document management and text processing:0.1,Education:0.1,Electronic commerce:0.1,Enterprise computing:0.1,Law, social and behavioral sciences:0.1,Life and medical sciences:0.1,Operations research:0.2,Physical sciences and engineering:0.3",Computers in other domains,Computers in other domains is relevant as the work applies control theory to a physical system (TRMS). Other categories like Physical sciences and engineering are less precise as the focus is on computational reliability in a specific application domain.,"Agriculture:0,Cartography:0,Computing in government:0.5,Digital libraries and archives:0,Military:0,Personal computers and PC applications:0,Publishing:0",Computing in government,"Computing in government is the only somewhat relevant category as the paper discusses system implementation, though the connection is tenuous. Other options are completely unrelated to the technical content."
3229,United States Marine Corps aerial refueling requirements analysis,"The United States Marine Corps (USMC) currently operates a fleet of KC130 aerial refueling tanker aircraft. This paper uses queuing and simulation models to examine the USMC KC130 tanker requirement, contrasts the results and explores the budgetary implications of alternative fleet requirements. This analysis finds that queuing models don't account for some of the complexities of aerial refueling operations. Therefore, queuing models may miscalculate the KC130 requirement. Simulation models give a more accurate depiction of actual KC130 requirement. Further, by incorrectly specifying the requirement the USMC could be faced with significant operational and budgetary implications.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.2,Networks:0.2,Software and its engineering:0.3,Theory of computation:0.2,Mathematics of computing:0.3,Information systems:0.2,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.3,Applied computing:0.8,Social and professional topics:0.1",Applied computing,Applied computing (0.8) is central for operational analysis of military logistics. Other categories like Computing methodologies are less relevant as the focus is on simulation applications rather than methodological innovations.,"Arts and humanities:0.1,Computer forensics:0.1,Computers in other domains:0.1,Document management and text processing:0.1,Education:0.1,Electronic commerce:0.1,Enterprise computing:0.1,Law, social and behavioral sciences:0.1,Life and medical sciences:0.1,Operations research:1.0,Physical sciences and engineering:0.1",Operations research,Operations research is highly relevant for queuing and simulation models in resource allocation analysis. Other options like Physical sciences and engineering are not directly related to the operational analysis of aerial refueling.,"Decision analysis:1,Forecasting:1,Transportation:0.5,Consumer products:0.1,Industry and manufacturing:0.2","Decision analysis,Forecasting","Decision analysis: Analyzes KC130 tanker requirements with budget implications. Forecasting: Models future operational needs through simulation. Transportation is context, not core method."
1413,Dissemination of 3D Visualizations of Complex Function Data for the NIST Digital Library of Mathematical Functions,"The National Institute of Standards and Technology (NIST) is developing a digital library to replace the widely used National Bureau of Standards Handbook of Mathematical Functions published in 1964. The NIST Digital Library of Mathematical Functions (DLMF) will include formulas, methods of computation, references, and links to software for over forty functions. It will be published both in hardcopy format and as a website featuring interactive navigation, a mathematical equation search, 2D graphics, and dynamic interactive 3D visualizations. This paper focuses on the development and accessibility of the 3D visualizations for the digital library. We examine the techniques needed to produce accurate computations of function data, and through a careful evaluation of several prototypes, we address the advantages and disadvantages of using various technologies, including the Virtual Reality Modeling Language (VRML), interactive embedded graphics, and video capture to render and disseminate the visualizations in an environment accessible to users on various platforms.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.9,Social and professional topics:0.1",Applied computing,Applied computing is highly relevant because the paper focuses on applying 3D visualization technologies to a specific domain (mathematical functions) for educational and accessibility purposes. Other categories like Computing methodologies are less relevant as the contribution is about application rather than algorithmic innovation.,"Arts and humanities:0.2,Computer forensics:0.2,Computers in other domains:0.7,Document management and text processing:0.9,Education:0.3,Electronic commerce:0.2,Enterprise computing:0.2,Law, social and behavioral sciences:0.2,Life and medical sciences:0.2,Operations research:0.2,Physical sciences and engineering:0.2","Document management and text processing,Computers in other domains",Document management and text processing is relevant because the paper discusses the development and dissemination of 3D visualizations in a digital library. Computers in other domains is relevant as it addresses a specialized application in a scientific context. Other children are rejected because they are not directly related to the core focus on 3D visualization technologies and digital library development.,"Agriculture:0.0,Cartography:0.0,Computing in government:0.0,Digital libraries and archives:1.0,Document capture:0.0,Document management:0.5,Document preparation:0.0,Document searching:0.0,Military:0.0,Personal computers and PC applications:0.0,Publishing:0.0",Digital libraries and archives,Digital libraries and archives is highly relevant as the paper focuses on 3D visualizations for the NIST DLMF. Document management is marginally relevant for accessibility considerations but not central.
29,Feature based map building using sparse sonar data,"A new feature based map building model that uses only the footprints of sparse sonar data has been developed and implemented. An arc feature association model was developed, which associates two sonar footprints into an arc feature. The association model provides information on the regions of center points of the arc feature. The information makes it possible to decide weather the two sonar footprints are associated with a line, point or arc. Lines, points, or arc features are extracted from more than two independent sonar footprints using the arc feature association model. We also propose a method that estimates the position uncertainties of the extracted features by considering both the pose uncertainty of the robot and the measurement uncertainty of the sonar sensor. This pose uncertainty is also used in the arc feature association process of the sonar footprints. The proposed method has been tested in a real home environment with a mobile robot.","General and reference:0.1,Hardware:0.2,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.3,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.8,Social and professional topics:0.1",Applied computing,Applied computing is relevant for the mobile robot map building application. Other categories are rejected as the focus is on robotics and sensor integration rather than pure mathematics or hardware design.,"Arts and humanities:0.1,Computer forensics:0.1,Computers in other domains:0.7,Document management and text processing:0.1,Education:0.1,Electronic commerce:0.1,Enterprise computing:0.1,Law, social and behavioral sciences:0.1,Life and medical sciences:0.1,Operations research:0.1,Physical sciences and engineering:0.6","Computers in other domains,Physical sciences and engineering",Computers in other domains: The work applies sonar data to robotics. Physical sciences and engineering: Sonar and feature extraction relate to engineering systems. Other categories do not align with robotics or sensor data processing.,"Aerospace:0.0,Agriculture:0.0,Archaeology:0.0,Astronomy:0.0,Cartography:1.0,Chemistry:0.0,Computing in government:0.0,Digital libraries and archives:0.0,Earth and atmospheric sciences:0.0,Electronics:0.2,Engineering:0.8,Mathematics and statistics:0.6,Military:0.0,Personal computers and PC applications:0.0,Physics:0.0,Publishing:0.0,Telecommunications:0.0","Cartography,Engineering",Cartography is primary for the map-building system. Engineering is relevant due to sensor uncertainty analysis. Other categories are irrelevant as the focus is on robotic mapping rather than pure mathematics or electronics.
1603,Forest height estimation using semi-individual tree detection in multi-spectral 3D aerial DMC data,"The increasing availability of accurate Digital Elevation Models (DEMs) of nation-wide cover has opened new possibilities to produce accurate forest variable estimation using 3D data acquired from aerial imagery. Such data can be produced by automatic matching of stereo images and photogrammetric modeling of the forest canopy height. Using existing accurate DEM information, the forest canopy height above ground is then easily assessed. Today, Airborne Laser Scanning (ALS) is frequently used to capture data for accurate estimation of variables to be used in forest management planning. Recent studies in Scandinavia show estimation accuracies almost as accurate as ALS, using 3D data obtained from standard aerial imagery, at least for the most important forest variables. So far mainly area-based estimation methods at field plot or raster cell level have been studied. This paper reports early results from applying a single-tree modeling approach, corresponding to the Semi-ITC (Individual Tree Crown) method, commonly used in ALS-based applications, using 3D data acquired from aerial DMC imagery. Here, a simplified Semi-ITC method was used to estimate tree height at segment level. The Root Mean Square Error of estimating the maximum tree height was 34% (of the true mean maximum tree height). Clearly, the methodology used shows promising results and has potential to be used in forest management planning.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.95,Social and professional topics:0.1",Applied computing,Applied computing is highly relevant as the paper applies 3D image processing and modeling techniques to forest management. Categories like Computing Methodologies or Networks are less directly tied to the environmental application.,"Life and medical sciences:0.8,Computers in other domains:0.75,Physical sciences and engineering:0.2,Arts and humanities:0.1,Computer forensics:0.1,Document management and text processing:0.1,Education:0.1,Electronic commerce:0.1,Enterprise computing:0.1,Law, social and behavioral sciences:0.1,Operations research:0.1","Life and medical sciences,Computers in other domains",Life and medical sciences is highly relevant for the forest management application. Computers in other domains is relevant for the 3D data processing techniques. Other categories like Physical sciences and engineering are less relevant as the focus is on environmental monitoring rather than engineering.,"Cartography:1,Agriculture:0.9,Computing in government:0.1,Digital libraries and archives:0.1","Cartography,Agriculture",Cartography is relevant for the use of 3D aerial data and DEMs. Agriculture is relevant as the application is in forest management. Other categories were rejected as the paper focuses on remote sensing rather than government or libraries.
3094,Lessons Learnt from Evaluation of Computer Interpretable Clinical Guidelines Tools and Methods: Literature Review,"Representation of clinical guidelines in a computer interpretable format is an active area of research. Various methods and tools have been proposed which have been evaluated based on different evaluation criteria. The evaluation results in the literature and their lessons learnt can be a valuable learning resource in order to redesign and improve the tools. Therefore, this research investigates the lessons learnt from the evaluation studies. Broad search in literature together with a purposeful snowball method were performed to identify the related papers that report any type of evaluation or comparison. We reviewed and analysed the lessons learnt from the evaluation results and classified them into 17 themes which reflect the suggestion concerns. The results indicate that the lessons learnt are more focused on tool functionalities, integration, sharing and maintenance domain. We provide suggestions for the area which had less attention.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.6,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.9,Social and professional topics:0.1",Applied computing,"Applied computing is directly relevant as the paper examines clinical guideline tools in healthcare. Other categories are rejected because the focus is on applying computing to medical domains rather than software engineering, methodologies, or mathematical foundations.","Arts and humanities:0.1,Computer forensics:0.1,Computers in other domains:0.8,Document management and text processing:0.1,Education:0.1,Electronic commerce:0.1,Enterprise computing:0.1,Law, social and behavioral sciences:0.1,Life and medical sciences:0.9,Operations research:0.3,Physical sciences and engineering:0.1","Life and medical sciences,Computers in other domains",Life and medical sciences is relevant as the paper discusses clinical guidelines in healthcare. Computers in other domains is relevant for applying computational methods to non-traditional fields. Other children like Education or Electronic commerce are unrelated to the medical domain focus.,"Agriculture:0,Bioinformatics:0,Cartography:0,Computational biology:0,Computing in government:0,Consumer health:0,Digital libraries and archives:0,Genetics:0,Genomics:0,Health care information systems:0.8,Health informatics:1,Metabolomics / metabonomics:0,Military:0,Personal computers and PC applications:0,Publishing:0,Systems biology:0","Health informatics,Health care information systems",Health informatics is directly relevant as the paper focuses on computer-interpretable clinical guidelines. Health care information systems are relevant due to the evaluation of tools for system integration and maintenance. Other options like Bioinformatics or Computational biology are irrelevant as the focus is on clinical guideline representation rather than biological data analysis.
4757,On finite approximations of topological algebraic systems,"Abstract We introduce and discuss a concept of approximation of a topological algebraic system A by finite algebraic systems from a given class . If A is discrete, this concept agrees with the familiar notion of a local embedding of A in a class of algebraic systems. One characterization of this concept states that A is locally embedded in iff it is a subsystem of an ultraproduct of systems from . In this paper we obtain a similar characterization of approximability of a locally compact system A by systems from using the language of nonstandard analysis. In the signature of A we introduce positive bounded formulas and their approximations; these are similar to those introduced by Henson [14] for Banach space structures (see also [15, 16]). We prove that a positive bounded formula φ holds in A if and only if all precise enough approximations of φ hold in all precise enough approximations of A. We also prove that a locally compact field cannot be approximated arbitrarily closely by finite (associative) rings (even if the rings are allowed to be non-commutative). Finite approximations of the field ℝ can be considered as possible computer systems for real arithmetic. Thus, our results show that there do not exist arbitrarily accurate computer arithmetics for the reals that are associative rings.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:1.0,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Mathematics of computing,"Mathematics of computing: The paper's core contribution is a mathematical framework for finite approximations of topological algebraic systems, leveraging concepts like ultraproducts and nonstandard analysis. Other fields like Networks or Hardware are irrelevant to the abstract's focus on algebraic and topological theories.","Continuous mathematics:0.8,Discrete mathematics:0.9,Mathematical analysis:0.85,Mathematical software:0.2,Probability and statistics:0.1,Information theory:0.1","Discrete mathematics,Mathematical analysis,Continuous mathematics",Discrete mathematics (0.9) is core to algebraic systems and ultraproducts. Mathematical analysis (0.85) is central to nonstandard analysis and bounded formulas. Continuous mathematics (0.8) relates to topological systems. Other fields are irrelevant as the paper focuses on algebraic structures and analysis.,"Calculus:0,Combinatorics:0,Continuous functions:0,Differential equations:0,Functional analysis:0,Graph theory:0,Integral equations:0,Mathematical optimization:0,Nonlinear equations:0,Numerical analysis:1,Quadrature:0,Topology:1","Topology,Numerical analysis","Topology: The paper focuses on topological algebraic systems and their properties. Numerical analysis: The discussion of finite approximations for real arithmetic relates to numerical methods. Other options are rejected as the paper does not address calculus, differential equations, or optimization."
433,Pseudozeros of multivariate polynomials,"The pseudozero set of a system f of polynomials in n complex variables is the subset of Cn which is the union of the zero-sets of all polynomial systems g that are near to f in a suitable sense. This concept is made precise, and general properties of pseudozero sets are established. In particular it is shown that in many cases of natural interest, the pseudozero set is a semialgebraic set. Also, estimates are given for the size of the projections of pseudozero sets in coordinate directions. Several examples are presented illustrating some of the general theory developed here. Finally, algorithmic ideas are proposed for solving multivariate polynomials.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.9,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Mathematics of computing,Mathematics of computing is directly relevant as the paper focuses on theoretical properties of pseudozero sets in polynomial systems. Other categories are not central to the mathematical analysis presented.,"Continuous mathematics:1.0,Discrete mathematics:0.1,Information theory:0.2,Mathematical analysis:1.0,Mathematical software:0.8,Probability and statistics:0.1","Continuous mathematics,Mathematical analysis",Continuous mathematics is relevant for pseudozero set analysis. Mathematical analysis is core for establishing properties of pseudozero sets. Mathematical software is moderately relevant for algorithmic ideas. Other categories are rejected due to lack of direct connection to polynomial systems or pseudozero theory.,"Calculus:0,Continuous functions:0,Differential equations:0,Functional analysis:0.5,Integral equations:0,Mathematical optimization:0,Nonlinear equations:1,Numerical analysis:1,Quadrature:0,Topology:0","Nonlinear equations,Numerical analysis,Functional analysis",Nonlinear equations: The paper studies polynomial systems. Numerical analysis: Algorithmic solutions for polynomials are discussed. Functional analysis: Pseudozero sets are analyzed in function spaces. Other options like Calculus or Differential equations are not central.
2163,Constructing Homomorphism Spaces and Endomorphism Rings,"We present a new deterministic algorithm for constructing homomorphism spaces and endomorphism rings of finite-dimensional modules. The modules are given via vertex projective presentations over path algebras and finite-dimensional quotients of path algebras. We use the theory of right Grobner bases to encode modules and to construct appropriate systems of equations for computing homomorphism spaces and endomorphism rings. The algorithm is implemented in the computer algebra system GAP and is included in Hopf, a computational package for noncommutative algebra. The performance of our implementation for computing endomorphism rings is experimentally compared with the implementation in Magma for the same class of modules. These experiments show that our implementation has a better time complexity.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.3,Mathematics of computing:1.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.2,Applied computing:0.0,Social and professional topics:0.0",Mathematics of computing,Mathematics of computing is highly relevant due to the focus on algebraic structures and Grobner bases. Theory of computation is partially relevant for algorithmic analysis but not the primary contribution.,"Continuous mathematics:0.0,Discrete mathematics:1.0,Information theory:0.0,Mathematical analysis:0.0,Mathematical software:0.5,Probability and statistics:0.0",Discrete mathematics,Discrete mathematics: The paper presents a deterministic algorithm for constructing homomorphism spaces. Mathematical software: The implementation in GAP is secondary to the mathematical theory.,"Combinatorics:1.0,Graph theory:0.0",Combinatorics,Combinatorics is relevant due to the algorithmic focus on module structures and Grobner bases. Graph theory is not directly addressed.
2802,Computing Slow Manifolds of Saddle Type,"Slow manifolds are important geometric structures in the state spaces of dynamical systems with multiple time scales. This paper introduces an algorithm for computing trajectories on slow manifolds that are normally hyperbolic with both stable and unstable fast manifolds. We present two examples of bifurcation problems where these manifolds play a key role and a third example in which saddle-type slow manifolds are part of a traveling wave profile of a partial differential equation. Initial value solvers are incapable of computing trajectories on saddle-type slow manifolds, so the slow manifold of saddle type (SMST) algorithm presented here is formulated as a boundary value method. We take an empirical approach here to assessing the accuracy and effectiveness of the algorithm.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:1.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.1,Applied computing:0.0,Social and professional topics:0.0",Mathematics of computing,Mathematics of computing is directly relevant for dynamical systems and slow manifold algorithms. Computing methodologies is secondary to the mathematical formulation.,"Continuous mathematics:0.9,Discrete mathematics:0.2,Information theory:0.4,Mathematical analysis:0.8,Mathematical software:0.3,Probability and statistics:0.2","Mathematical analysis,Continuous mathematics",Mathematical analysis: Presents boundary value algorithm for dynamical systems. Continuous mathematics: Focus on slow manifolds in continuous systems. Other categories lack direct mathematical modeling focus.,"Differential equations:1,Numerical analysis:1,Functional analysis:0.6,Calculus:0.5,Integral equations:0.4,Mathematical optimization:0.3,Nonlinear equations:0.2,Quadrature:0.1,Topology:0.1,Continuous functions:0.2","Differential equations,Numerical analysis",Differential equations is relevant as the paper studies manifolds in dynamical systems. Numerical analysis is relevant for the boundary value algorithm. Other categories are less relevant as the focus is on geometric structures rather than pure analysis.
4989,"To properly reflect physicists' reasoning about randomness, we also need a maxitive (possibility) measure","According to the traditional probability theory, events with a positive but very small probability can occur (although very rarely). For example, from the purely mathematical viewpoint, it is possible that the thermal motion of all the molecules in a coffee cup goes in the same direction, so this cup will start lifting up. In contrast, physicists believe that events with extremely small probability cannot occur. In this paper, we show that to get a consistent formalization of this belief, we need, in addition to the original probability measure, to also consider a maxitive (possibility) measure","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.75,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Mathematics of computing,Mathematics of computing: The paper introduces a mathematical framework (maxitive measure) for modeling physicists' reasoning about extreme low-probability events.,"Continuous mathematics:0.6,Discrete mathematics:0.0,Information theory:0.0,Mathematical analysis:0.7,Mathematical software:0.0,Probability and statistics:0.9","Probability and statistics,Mathematical analysis,Continuous mathematics",Probability and statistics is core for analyzing randomness measures. Mathematical analysis applies to the theoretical framework. Continuous mathematics relates to the Fourier transform analysis. Other fields like information theory are less relevant to the probability measure discussion.,"Calculus:0.1,Continuous functions:0.0,Differential equations:0.2,Distribution functions:0.5,Functional analysis:0.3,Integral equations:0.1,Mathematical optimization:0.1,Multivariate statistics:0.2,Nonlinear equations:0.1,Nonparametric statistics:0.1,Numerical analysis:0.2,Probabilistic algorithms:0.6,Probabilistic inference problems:0.4,Probabilistic reasoning algorithms:1.0,Probabilistic representations:0.8,Quadrature:0.0,Statistical paradigms:0.3,Stochastic processes:0.2,Topology:0.0","Probabilistic reasoning algorithms,Probabilistic representations",Probabilistic reasoning algorithms is directly relevant to the paper's introduction of a maxitive measure for probabilistic reasoning. Probabilistic representations is relevant for the proposed dual measure framework. Other categories like Stochastic processes are less relevant as the focus is on theoretical foundations rather than process modeling.
237,Information-theoretic characterizations of Markov random fields and subfields,"Let Xi, i ∊ V form a Markov random field (MRF) represented by an undirected graph G = (V, E), and V' be a subset of V. We determine the smallest graph that can always represent the subfield Xi, i ∊ V' as an MRF. Based on this result, we obtain a necessary and sufficient condition for a subfield of a Markov tree to be also a Markov tree. When G is a path so that Xi, i ∊ V form a Markov chain, it is known that the I — Measure is always nonnegative [3]. We prove that Markov chain is essentially the only MRF that possesses this property. Our work is built on the set-theoretic characterization of an MRF in [4]. Unlike most works in the literature, we do not make the standard assumption that the underlying probability distribution is factorizable with respect to the graph representing the MRF.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.3,Mathematics of computing:0.9,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Mathematics of computing,"Mathematics of computing: The paper presents information-theoretic characterizations of Markov random fields and subfields, focusing on mathematical properties and proofs. Other categories like Theory of computation are less directly relevant as the work is more mathematical than algorithmic.","Information theory:1.0,Probability and statistics:0.8,Discrete mathematics:0.4,Continuous mathematics:0.3,Mathematical analysis:0.5,Mathematical software:0.1","Information theory,Probability and statistics",Information theory: The paper uses information-theoretic characterizations of MRFs and subfields. Probability and statistics: Focus on Markov chains and distribution analysis. Discrete mathematics is less relevant as the focus is on probabilistic structures rather than combinatorics.,"Coding theory:0,Distribution functions:0,Multivariate statistics:0,Nonparametric statistics:0,Probabilistic algorithms:0,Probabilistic inference problems:1,Probabilistic reasoning algorithms:0,Probabilistic representations:1,Statistical paradigms:0,Stochastic processes:0","Probabilistic inference problems,Probabilistic representations",Probabilistic inference problems are relevant for analyzing Markov random field subfields. Probabilistic representations are relevant for the information-theoretic characterization of MRFs. Other statistical categories are less directly addressed.
2893,Deep Holes and MDS Extensions of Reed–Solomon Codes,"We study the problem of classifying deep holes of Reed–Solomon codes. We show that this problem is equivalent to the problem of classifying maximum distance separable (MDS) extensions of Reed–Solomon codes by one digit. This equivalence allows us to improve recent results on the former problem. In particular, we classify deep holes of Reed–Solomon codes of dimension greater than half the alphabet size. We also give a complete classification of deep holes of Reed–Solomon codes with redundancy three in all dimensions.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.8,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Mathematics of computing,Mathematics of computing: The paper presents theoretical analysis of Reed-Solomon codes and their properties. Computing methodologies is less relevant as the focus is on theoretical classification rather than algorithmic methods.,"Continuous mathematics:0.0,Discrete mathematics:0.0,Information theory:1.0,Mathematical analysis:0.0,Mathematical software:0.0,Probability and statistics:0.0",Information theory,"Information theory: The paper analyzes Reed-Solomon codes and their properties (deep holes, MDS extensions), which are fundamental topics in information theory. Other options like Discrete mathematics are rejected because the focus is on coding theory rather than general discrete math.",Coding theory:1,Coding theory,"Coding theory: The paper focuses on Reed–Solomon codes, deep holes, and MDS extensions, which are central topics in coding theory."
3767,"A Brief History of Long Memory: Hurst, Mandelbrot and the Road to ARFIMA, 1951-1980","Long memory plays an important role in many fields by determining the behaviour and predictability of systems; for instance, climate, hydrology, finance, networks and DNA sequencing. In particular, it is important to test if a process is exhibiting long memory since that impacts the accuracy and confidence with which one may predict future events on the basis of a small amount of historical data. A major force in the development and study of long memory was the late Benoit B. Mandelbrot. Here, we discuss the original motivation of the development of long memory and Mandelbrot’s influence on this fascinating field. We will also elucidate the sometimes contrasting approaches to long memory in different scientific communities.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:1.0,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Mathematics of computing,Mathematics of computing is highly relevant as the paper discusses theoretical foundations of long memory processes. Other fields like Networks are irrelevant as the focus is on mathematical modeling rather than network applications.,"Probability and statistics:0.9,Mathematical analysis:0.85,Continuous mathematics:0.15,Discrete mathematics:0.1,Information theory:0.1,Mathematical software:0.05","Probability and statistics,Mathematical analysis",Probability and statistics: The paper discusses long memory processes and their statistical implications. Mathematical analysis: Theoretical foundations of long memory are analyzed. Discrete math is less relevant to the continuous-time processes discussed.,"Calculus:0,Differential equations:0,Distribution functions:0,Functional analysis:1,Integral equations:0,Mathematical optimization:0,Multivariate statistics:0,Nonlinear equations:0,Nonparametric statistics:0,Numerical analysis:0,Probabilistic algorithms:0,Probabilistic inference problems:0,Probabilistic reasoning algorithms:0,Probabilistic representations:0,Quadrature:0,Statistical paradigms:0,Stochastic processes:1","Stochastic processes,Functional analysis",Stochastic processes: The paper discusses long memory in stochastic processes. Functional analysis: The analysis of memory properties involves functional mathematical frameworks. Other categories are not explicitly addressed.
492,Minimization of ℓ1-2 for Compressed Sensing,"We study minimization of the difference of $\ell_1$ and $\ell_2$ norms as a nonconvex and Lipschitz continuous metric for solving constrained and unconstrained compressed sensing problems. We establish exact (stable) sparse recovery results under a restricted isometry property (RIP) condition for the constrained problem, and a full-rank theorem of the sensing matrix restricted to the support of the sparse solution. We present an iterative method for $\ell_{1-2}$ minimization based on the difference of convex functions algorithm and prove that it converges to a stationary point satisfying the first-order optimality condition. We propose a sparsity oriented simulated annealing procedure with non-Gaussian random perturbation and prove the almost sure convergence of the combined algorithm (DCASA) to a global minimum. Computation examples on success rates of sparse solution recovery show that if the sensing matrix is ill-conditioned (non RIP satisfying), then our method is better than existing nonconvex compre...","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.8,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Mathematics of computing,"Mathematics of computing is highly relevant as the paper focuses on optimization techniques in compressed sensing. Other fields like Theory of computation are less relevant as the core contribution is mathematical, not algorithmic.","Mathematical analysis:1.0,Information theory:0.5,Continuous mathematics:0.2,Discrete mathematics:0.2,Probability and statistics:0.2,Mathematical software:0.1",Mathematical analysis,Mathematical analysis is directly relevant as the paper studies optimization methods for ℓ₁₋₂ minimization. Information theory is less relevant as the focus is on mathematical optimization rather than data encoding/decoding.,"Calculus:0.1,Differential equations:0.1,Functional analysis:0.1,Integral equations:0.1,Mathematical optimization:1.0,Nonlinear equations:0.2,Numerical analysis:0.3,Quadrature:0.1",Mathematical optimization,Mathematical optimization is central to the ℓ1-2 minimization framework. Other categories like Numerical analysis are less directly relevant.
4233,Path factors in cubic graphs,"Let ℱ be a set of connected graphs. An ℱ‐factor of a graph is its spanning subgraph such that each component is isomorphic to one of the members in ℱ. Let Pk denote the path of order k. Akiyama and Kano have conjectured that every 3‐connected cubic graph of order divisible by 3 has a {P3}‐factor. Recently, Kaneko gave a necessary and sufficient condition for a graph to have a {P3, P4, P5}‐factor. As a corollary, he proved that every cubic graph has a {P3, P4, P5}‐factor. In this paper, we prove that every 2‐connected cubic graph of order at least six has a {Pk ∣ k ≥ , 6}‐factor, and hence has a {P3, P4}‐factor. © 2002 Wiley Periodicals, Inc. J Graph Theory 39: 188–193, 2002; DOI 10.1002/jgt.10022","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.9,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Mathematics of computing,Mathematics of computing: The paper is centered on graph theory and combinatorial mathematics for path factors in cubic graphs. No other categories are relevant to this theoretical work.,"Continuous mathematics:0.1,Discrete mathematics:0.9,Information theory:0.1,Mathematical analysis:0.2,Mathematical software:0.1,Probability and statistics:0.1",Discrete mathematics,Discrete mathematics is highly relevant as the paper focuses on theoretical graph theory (path factors in cubic graphs). Other categories like Mathematical analysis are irrelevant as the work is purely combinatorial and does not involve continuous methods.,"Combinatorics:0.3,Graph theory:1.0",Graph theory,Graph theory is directly relevant because the paper focuses on path factors in cubic graphs. Combinatorics is only weakly relevant as it is a broader field encompassing graph theory but not the paper's specific focus.
5100,A Bayesian Additive Model for Understanding Public Transport Usage in Special Events,"Public special events, like sports games, concerts and festivals are well known to create disruptions in transportation systems, often catching the operators by surprise. Although these are usually planned well in advance, their impact is difficult to predict, even when organisers and transportation operators coordinate. The problem highly increases when several events happen concurrently. To solve these problems, costly processes, heavily reliant on manual search and personal experience, are usual practice in large cities like Singapore, London or Tokyo. This paper presents a Bayesian additive model with Gaussian process components that combines smart card records from public transport with context information about events that is continuously mined from the Web. We develop an efficient approximate inference algorithm using expectation propagation, which allows us to predict the total number of public transportation trips to the special event areas, thereby contributing to a more adaptive transportation system. Furthermore, for multiple concurrent event scenarios, the proposed algorithm is able to disaggregate gross trip counts into their most likely components related to specific events and routine behavior. Using real data from Singapore, we show that the presented model outperforms the best baseline model by up to 26 percent in  $R^2$  and also has explanatory power for its individual components.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.8,Information systems:0.3,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.2,Social and professional topics:0.1",Mathematics of computing,Mathematics of computing is highly relevant as the paper presents a Bayesian statistical model. Applied computing has moderate relevance for the transportation application.,"Continuous mathematics:0.1,Discrete mathematics:0.1,Information theory:0.1,Mathematical analysis:0.3,Mathematical software:0.2,Probability and statistics:0.9",Probability and statistics,Probability and statistics: The paper presents a Bayesian additive model for transportation prediction. Mathematical analysis receives partial credit for the model components but is secondary to the statistical framework.,"Distribution functions:0.3,Multivariate statistics:0.4,Nonparametric statistics:0.3,Probabilistic algorithms:0.9,Probabilistic inference problems:0.9,Probabilistic reasoning algorithms:0.7,Probabilistic representations:0.5,Statistical paradigms:0.3,Stochastic processes:0.4","Probabilistic algorithms,Probabilistic inference problems",Probabilistic algorithms and Probabilistic inference problems are directly relevant as the paper implements a Bayesian model with expectation propagation. Other statistical categories are less central to the core probabilistic framework.
818,Cramér-Rao Lower Bounds for Positioning with Large Intelligent Surfaces,"We consider the potential for positioning with a system where antenna arrays are deployed as a large intelligent surface (LIS). We derive Fisher- informations and Cram\''{e}r-Rao lower bounds (CRLB) in closed-form for terminals along the central perpendicular line (CPL) of the LIS for all three Cartesian dimensions. For terminals at positions other than the CPL, closed-form expressions for the Fisher-informations and CRLBs seem out of reach, and we alternatively provide approximations (in closed-form) which are shown to be very accurate. We also show that under mild conditions, the CRLBs in general decrease quadratically in the surface-area for both the $x$ and $y$ dimensions. For the $z$-dimension (distance from the LIS), the CRLB decreases linearly in the surface-area when terminals are along the CPL. However, when terminals move away from the CPL, the CRLB is dramatically increased and then also decreases quadratically in the surface-area. We also extensively discuss the impact of different deployments (centralized and distributed) of the LIS.","General and reference:0,Hardware:0,Computer systems organization:0,Networks:0,Software and its engineering:0,Theory of computation:0,Mathematics of computing:1.0,Information systems:0,Security and privacy:0,Human-centered computing:0,Computing methodologies:0,Applied computing:0,Social and professional topics:0",Mathematics of computing,Mathematics of computing is highly relevant as the paper derives mathematical bounds (Cramér-Rao lower bounds) and analyzes their properties in a communication system context. Other categories like Applied computing are less relevant because the focus is on theoretical analysis rather than practical applications.,"Continuous mathematics:0.1,Discrete mathematics:0.1,Information theory:0.1,Mathematical analysis:0.3,Mathematical software:0.1,Probability and statistics:1.0",Probability and statistics,"Probability and statistics: The paper derives Cramér-Rao lower bounds (CRLB), which are fundamental statistical concepts for estimation theory. Other options like Mathematical analysis are less directly relevant as the focus is on probabilistic bounds rather than general mathematical analysis.","Distribution functions:0.4,Multivariate statistics:1.0,Nonparametric statistics:0.2,Probabilistic algorithms:0.7,Probabilistic inference problems:0.8,Probabilistic reasoning algorithms:0.5,Probabilistic representations:0.3,Statistical paradigms:0.6,Stochastic processes:0.9","Multivariate statistics,Stochastic processes",Multivariate statistics: The paper derives Cramér-Rao bounds for 3D positioning. Stochastic processes: The analysis involves LIS and stochastic error modeling. Probabilistic inference is secondary but relevant.
4596,Dynamical Behavior of a Stochastic Ratio-Dependent Predator-Prey System,"This paper is concerned with a stochastic ratio-dependent predator-prey model with varible coefficients. By the comparison theorem of stochastic equations and the Ito formula, the global existence of a unique positive solution of the ratio-dependent model is obtained. Besides, some results are established such as the stochastically ultimate boundedness and stochastic permanence for this model.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:1.0,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Mathematics of computing,Mathematics of computing is relevant because the paper presents a stochastic model and proves mathematical properties like global existence and stochastic permanence. Other categories are less relevant as the focus is on mathematical analysis rather than computational methods.,"Continuous mathematics:0.7,Discrete mathematics:0.1,Information theory:0.1,Mathematical analysis:0.8,Mathematical software:0.1,Probability and statistics:0.9","Probability and statistics,Mathematical analysis,Continuous mathematics",Probability and statistics: Stochastic model with statistical analysis is central. Mathematical analysis: Proofs of existence and boundedness are key. Continuous mathematics: Model involves continuous dynamics. 'Discrete mathematics' is irrelevant here.,"Calculus:0,Continuous functions:0.1,Differential equations:1,Distribution functions:0.3,Functional analysis:0.2,Integral equations:0.1,Mathematical optimization:0.3,Multivariate statistics:0.1,Nonlinear equations:0.5,Nonparametric statistics:0.1,Numerical analysis:0.3,Probabilistic algorithms:0.2,Probabilistic inference problems:0.3,Probabilistic reasoning algorithms:0.2,Probabilistic representations:0.2,Quadrature:0.1,Statistical paradigms:0.1,Stochastic processes:1,Topology:0","Differential equations,Stochastic processes",Differential equations: The paper analyzes a predator-prey model using differential equations. Stochastic processes: The model incorporates stochastic elements in its formulation. Other options like Calculus or Topology are not central to the mathematical analysis presented.
6039,Pretrigonometric and prehyperbolic functions via Laplace transforms,"Present paper computes the Laplace transforms of pretrigonometric, prehyperbolic and extended pretrigonometric and prehyperbolic functions. Then obtain the prefunctions by solving initial value problems for second order non-homogeneous differential equations using Laplace Transforms. In the later part considered the matrix differential equations whose solutions leads to extended matrix trigonometric functions.","General and reference:0.1,Hardware:0.2,Computer systems organization:0.2,Networks:0.1,Software and its engineering:0.2,Theory of computation:0.2,Mathematics of computing:0.9,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.2,Computing methodologies:0.3,Applied computing:0.2,Social and professional topics:0.1",Mathematics of computing,Mathematics of computing is directly relevant as the paper focuses on Laplace transforms and differential equations. Other fields like Computing methodologies are less relevant as the contribution is mathematical analysis.,"Continuous mathematics:0.8,Discrete mathematics:0.2,Information theory:0.1,Mathematical analysis:0.9,Mathematical software:0.1,Probability and statistics:0.1","Mathematical analysis,Continuous mathematics",Mathematical analysis is highly relevant as the paper focuses on Laplace transforms and differential equations. Continuous mathematics is relevant due to the focus on continuous functions and transforms. Other fields like Discrete mathematics and Probability and statistics are less relevant as the paper's core contribution is in continuous mathematical analysis.,"Calculus:1.0,Continuous functions:0.5,Differential equations:1.0,Functional analysis:0.3,Integral equations:0.2,Mathematical optimization:0.2,Nonlinear equations:0.3,Numerical analysis:0.2,Quadrature:0.1,Topology:0.1","Calculus,Differential equations","Calculus is relevant because the paper uses Laplace transforms, a core calculus technique. Differential equations are relevant as the paper solves second-order non-homogeneous differential equations. Other fields like functional analysis or numerical analysis are mentioned but are not central to the paper’s focus on pretrigonometric functions and Laplace transforms."
3598,Metric Subregularity and Calmness for Nonconvex Generalized Equations in Banach Spaces,"This paper concerns a generalized equation defined by a closed multifunction between Banach spaces, and we employ variational analysis techniques to provide sufficient and/or necessary conditions for a generalized equation to have the metric subregularity (i.e., local error bounds for the concerned multifunction) in general Banach spaces. Following the approach of Ioffe [Trans. Amer. Math. Soc., 251 (1979), pp. 61-69] who studied the numerical function case, our conditions are described in terms of coderivatives of the concerned multifunction at points outside the solution set. Motivated by the existing modulus representation and point-based criteria for the metric regularity, we establish the corresponding results for the metric subregularity. In the Asplund space case, sharper results are obtained.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.2,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.2,Mathematics of computing:0.8,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.3,Applied computing:0.1,Social and professional topics:0.1",Mathematics of computing,Mathematics of computing: The paper focuses on abstract mathematical analysis in Banach spaces. Other categories are irrelevant as the work is purely theoretical and mathematical.,"Continuous mathematics:0.3,Discrete mathematics:0.2,Information theory:0.1,Mathematical analysis:0.9,Mathematical software:0.1,Probability and statistics:0.2",Mathematical analysis,Mathematical analysis: The paper studies metric subregularity in Banach spaces using variational analysis techniques. Other categories like Continuous mathematics are less directly relevant as the focus is on specific analytical properties rather than general continuous mathematics.,"Calculus:0.4,Differential equations:0.2,Functional analysis:0.7,Integral equations:0.3,Mathematical optimization:0.6,Nonlinear equations:0.3,Numerical analysis:0.5,Quadrature:0.2","Functional analysis,Mathematical optimization","Functional analysis: The paper studies generalized equations in Banach spaces, a core topic in functional analysis. Mathematical optimization: The focus on metric subregularity relates to optimization theory. Other options like Calculus are less central to the abstract's focus on multifunction analysis."
5786,How fair is queue prioritization?,"Customer classification and prioritization are commonly used in many applications to provide queue preferential service. Their influence on queuing systems has been thoroughly studied from the delay distribution perspective. However, the fairness aspects, which are inherent to any preferential system and highly important to customers, have hardly been studied and not been quantified to date. In this work we use the Resource Allocation Queueing Fairness Measure (RAQFM) to analyze such systems and derive their relative fairness values. We also analyze the effect multiple servers have on fairness, showing that multiple servers increase the fairness of the system.<sup>1</sup>","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.9,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Mathematics of computing,Mathematics of computing is relevant for fairness metric analysis in queuing systems. Other categories are unrelated to mathematical fairness analysis.,"Continuous mathematics:0.3,Discrete mathematics:0.8,Information theory:0.2,Mathematical analysis:0.3,Mathematical software:0.1,Probability and statistics:0.7","Probability and statistics,Discrete mathematics",Probability and statistics is relevant for analyzing fairness metrics in queuing systems. Discrete mathematics applies to queueing system modeling. Other categories like mathematical analysis are less directly tied to the core focus on fairness quantification.,"Combinatorics:0,Distribution functions:0.5,Graph theory:0,Multivariate statistics:0.3,Nonparametric statistics:0,Probabilistic algorithms:1,Probabilistic inference problems:0.5,Probabilistic reasoning algorithms:0.5,Probabilistic representations:0.5,Statistical paradigms:0.3,Stochastic processes:1","Stochastic processes,Probabilistic algorithms",Stochastic processes are relevant due to the analysis of queueing systems. Probabilistic algorithms are relevant for the fairness measure (RAQFM). Distribution functions and statistics receive lower scores as the focus is on fairness quantification rather than statistical methods.
4971,Linear Relations for a Generalized Tutte Polynomial,"Brylawski proved the coefficients of the Tutte polynomial of a matroid satisfy a set of linear relations. We extend these relations to a generalization of the Tutte polynomial that includes greedoids and antimatroids. This leads to families of new identities for antimatroids, including trees, posets, chordal graphs and finite point sets in $\mathbb{R}^n$. It also gives a new linear relation for matroids that is implied by Brylawski's identities.""","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.9,Information systems:0.2,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.3,Applied computing:0.1,Social and professional topics:0.1",Mathematics of computing,"Mathematics of computing: The paper extends mathematical identities for generalized Tutte polynomials, a core topic in combinatorial mathematics. Other categories are rejected as the paper focuses on mathematical structures rather than algorithms, applications, or systems.","Continuous mathematics:0.1,Discrete mathematics:0.9,Information theory:0.1,Mathematical analysis:0.8,Mathematical software:0.1,Probability and statistics:0.1","Discrete mathematics,Mathematical analysis",Discrete mathematics is core for combinatorial structures like matroids. Mathematical analysis applies to identity derivation. Continuous mathematics or probability are not central to linear relations in discrete structures.,"Calculus:0.1,Combinatorics:0.9,Differential equations:0.1,Functional analysis:0.1,Graph theory:1.0,Integral equations:0.1,Mathematical optimization:0.3,Nonlinear equations:0.1,Numerical analysis:0.2,Quadrature:0.1","Graph theory,Combinatorics","Graph theory is highly relevant as the paper extends the Tutte polynomial, a fundamental concept in graph theory. Combinatorics is also relevant due to the paper's focus on greedoids and antimatroids, which are combinatorial structures. Other categories like Calculus are irrelevant as the paper does not deal with continuous mathematics."
5651,Scale Mixtures of Gaussians and the Statistics of Natural Images,"The statistics of photographic images, when represented using multiscale (wavelet) bases, exhibit two striking types of non-Gaussian behavior. First, the marginal densities of the coefficients have extended heavy tails. Second, the joint densities exhibit variance dependencies not captured by second-order models. We examine properties of the class of Gaussian scale mixtures, and show that these densities can accurately characterize both the marginal and joint distributions of natural image wavelet coefficients. This class of model suggests a Markov structure, in which wavelet coefficients are linked by hidden scaling variables corresponding to local image structure. We derive an estimator for these hidden variables, and show that a nonlinear normalization procedure can be used to Gaussianize the coefficients.""","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:1.0,Information systems:0.3,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.75,Applied computing:0.2,Social and professional topics:0.1",Mathematics of computing,Mathematics of computing is highly relevant as the paper focuses on statistical models for image analysis. Computing methodologies is moderately relevant but secondary to the mathematical core.,"Continuous mathematics:0.7,Discrete mathematics:0.3,Information theory:0.4,Mathematical analysis:0.8,Mathematical software:0.2,Probability and statistics:0.7","Mathematical analysis,Probability and statistics",Mathematical analysis is highly relevant for deriving estimators for scaling variables. Probability and statistics is relevant for modeling non-Gaussian distributions in image data. Continuous mathematics is tangentially relevant but not core.,"Calculus:0.1,Differential equations:0.1,Distribution functions:1.0,Functional analysis:0.3,Integral equations:0.2,Mathematical optimization:0.4,Multivariate statistics:1.0,Nonlinear equations:0.2,Nonparametric statistics:0.3,Numerical analysis:0.3,Probabilistic algorithms:0.5,Probabilistic inference problems:0.4,Probabilistic reasoning algorithms:0.4,Probabilistic representations:0.5,Quadrature:0.1,Statistical paradigms:0.6,Stochastic processes:0.5","Distribution functions,Multivariate statistics",Distribution functions (Gaussian scale mixtures and heavy-tailed distributions). Multivariate statistics (joint densities of wavelet coefficients). Other categories relate to broader mathematical areas not central to the paper.
2368,Balanced distribution-energy inequalities and related entropy bounds,"Let $A$ be a self-adjoint operator acting over a space $X$ endowed with a partition. We give lower bounds on the energy of a mixed state $\rho$ from its distribution in the partition and the spectral density of $A$. These bounds improve with the refinement of the partition, and generalize inequalities by Li-Yau and Lieb--Thirring for the Laplacian in $\R^n$. They imply an uncertainty principle, giving a lower bound on the sum of the spatial entropy of $\rho$, as seen from $X$, and some spectral entropy, with respect to its energy distribution. On $\R^n$, this yields lower bounds on the sum of the entropy of the densities of $\rho$ and its Fourier transform. A general log-Sobolev inequality is also shown. It holds on mixed states, without Markovian or positivity assumption on $A$.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:1.0,Information systems:0.2,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Mathematics of computing,Mathematics of computing is highly relevant for the theoretical energy inequalities and entropy bounds. No other categories align with the paper's focus on mathematical analysis of operator theory and entropy.,"Continuous mathematics:0.1,Discrete mathematics:0.1,Information theory:0.75,Mathematical analysis:1.0,Mathematical software:0.1,Probability and statistics:0.1","Mathematical analysis,Information theory",Mathematical analysis: The paper presents energy inequalities and entropy bounds using mathematical analysis techniques. Information theory: The entropy bounds and uncertainty principles are fundamental concepts in information theory. Other children like 'Probability and statistics' are less relevant since the focus is on deterministic mathematical analysis rather than probabilistic modeling.,"Calculus:0.2,Coding theory:0.1,Differential equations:0.3,Functional analysis:0.9,Integral equations:0.2,Mathematical optimization:0.8,Nonlinear equations:0.3,Numerical analysis:0.4,Quadrature:0.1","Functional analysis,Mathematical optimization",Functional analysis is directly relevant to the operator-theoretic framework and entropy bounds. Mathematical optimization applies to the derivation of inequalities. Other categories like 'Calculus' or 'Differential equations' are less central to the paper's focus on abstract mathematical structures and inequalities.
2189,Excluded Forest Minors and the Erdős–Pósa Property,"A classical result of Robertson and Seymour states that the set of graphs containing a fixed planar graph H as a minor has the so-called Erdős–Pósa property; namely, there exists a function f depending only on H such that, for every graph G and every positive integer k, the graph G has k vertex-disjoint subgraphs each containing H as a minor, or there exists a subset X of vertices of G with |X| ≤ f(k) such that G − X has no H-minor (see Robertson and Seymour, J. Combin. Theory Ser. B41 (1986) 92–114). While the best function f currently known is exponential in k, a O(k log k) bound is known in the special case where H is a forest. This is a consequence of a theorem of Bienstock, Robertson, Seymour and Thomas on the pathwidth of graphs with an excluded forest-minor. In this paper we show that the function f can be taken to be linear when H is a forest. This is best possible in the sense that no linear bound is possible if H has a cycle.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:1.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Mathematics of computing,"Mathematics of computing: The paper's core contribution is a mathematical proof about the Erdős–Pósa property in graph theory, which is a foundational area in mathematics. Other categories like Theory of computation are less relevant as the focus is on theoretical graph properties rather than computational models or algorithms.","Continuous mathematics:0.1,Discrete mathematics:0.9,Information theory:0.1,Mathematical analysis:0.1,Mathematical software:0.1,Probability and statistics:0.1",Discrete mathematics,Discrete mathematics is highly relevant as the paper addresses graph theory and the Erdős–Pósa property. Other categories like Continuous mathematics are not applicable to this problem.,"Combinatorics:0.8,Graph theory:1.0","Graph theory,Combinatorics",Graph theory is highly relevant for the study of graph minors and the Erdős–Pósa property. Combinatorics is relevant due to the combinatorial analysis of graph properties. The paper's focus is squarely on theoretical graph properties.
3529,A discrete linear chirp transform (DLCT) for data compression,"Compressive sensing attempts to simplify the frequency transformation and thresholding steps, commonly done in data compression, into one. Sparseness of the signal, in either time or frequency, is required for the convex optimization in compressive sensing to perform well. Although sparseness of certain signals, in either time or frequency, is guaranteed by the uncertainty principle signals composed of chirps are not however sparse in either domain. In this paper we propose an orthogonal linear-chirp transform, the discrete linear chirp transform (DLCT), to represent any signal in terms of linear chirps, with modulation and dual properties. Using the DLCT the sparseness of the signal in either time or frequency can be assessed, and if not sparse in neither of these domains, the modulation and dual properties of the DLCT provide a way to transform the signal into a sparse signal. The application of the proposed DLCT is in data compression. The transformation is illustrated by using sparse and not sparse test signals as well as actual signals.","General and reference:0.1,Hardware:0.2,Computer systems organization:0.3,Networks:0.2,Software and its engineering:0.4,Theory of computation:0.3,Mathematics of computing:0.8,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.2,Computing methodologies:0.4,Applied computing:0.3,Social and professional topics:0.1",Mathematics of computing,"Mathematics of computing is highly relevant as the paper introduces a novel mathematical transform (DLCT) for data compression. Computing methodologies is partially relevant for its application in computing, but the core contribution is mathematical.","Continuous mathematics:0.0,Discrete mathematics:0.0,Information theory:1.0,Mathematical analysis:0.0,Mathematical software:0.0,Probability and statistics:0.0",Information theory,Information theory is relevant for the data compression application of the DLCT. Other mathematical categories are not the primary focus of the signal representation contribution.,Coding theory:1.0,Coding theory,"The paper introduces the Discrete Linear Chirp Transform (DLCT) for data compression, which is a novel signal representation method. Coding theory is directly relevant as it focuses on compression techniques and orthogonal transforms."
5019,Bounds for eigenvalues and condition numbers in the p-version of the finite element method,"In this paper, we present a theory for bounding the minimum eigenvalues, maximum eigenvalues, and condition numbers of stiffness matrices arising from the p-version of finite element analysis. Bounds are derived for the eigenvalues and the condition numbers, which are valid for stiffness matrices based on a set of general basis functions that can be used in the p-version. For a set of hierarchical basis functions satisfying the usual local support condition that has been popularly used in the p-version, explicit bounds are derived for the minimum eigenvalues, maximum eigenvalues, and condition numbers of stiffness matrices. We prove that the condition numbers of the stiffness matrices grow like p 4(d-1) , where d is the number of dimensions. Our results disprove a conjecture of Olsen and Douglas in which the authors assert that regardless of the choice of basis, the condition numbers grow like p 4d or faster. Numerical results are also presented which verify that our theoretical bounds are correct.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.9,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Mathematics of computing,Mathematics of computing: The paper presents mathematical analysis of eigenvalue bounds for finite element matrices. Other categories are irrelevant to the mathematical theory being developed.,"Continuous mathematics:0.75,Discrete mathematics:0.0,Information theory:0.0,Mathematical analysis:1.0,Mathematical software:0.0,Probability and statistics:0.0","Mathematical analysis,Continuous mathematics",Mathematical analysis is highly relevant as the paper presents theoretical analysis of finite element methods. Continuous mathematics is also relevant due to the focus on p-version methods in numerical analysis.,"Calculus:0.1,Continuous functions:0.1,Differential equations:0.2,Functional analysis:0.1,Integral equations:0.1,Mathematical optimization:0.1,Nonlinear equations:0.1,Numerical analysis:1.0,Quadrature:0.1,Topology:0.1",Numerical analysis,Numerical analysis is central to the eigenvalue and condition number analysis in finite element methods. Other fields like Differential equations are tangential to the problem domain.
2935,Vopěnka's principle and compact logics,Abstract We study the effects of Vopěnka's principle on properties of model theoretic logics. We show that Vopěnka's principle is equivalent to the assumption that every finitely generated logic has a compact cardinal. We show also that it is equivalent to the assumption that every such logic has a global Hanf number.,"General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.3,Mathematics of computing:0.9,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Mathematics of computing,"Mathematics of computing is directly relevant as the paper studies Vopěnka's principle and its connections to model-theoretic logics, compact cardinals, and Hanf numbers, which are fundamental mathematical concepts. Other categories like Theory of computation (0.3) have limited relevance due to the paper's focus on pure mathematical logic rather than computational theory. Hardware, Networks, and other practical computing areas are entirely irrelevant.","Discrete mathematics:0.9,Mathematical analysis:0.2,Mathematical software:0.1",Discrete mathematics,Discrete mathematics is relevant for set-theoretic principles in logic. Other mathematical categories are less aligned with the paper's logical foundations.,"Combinatorics:0.7,Graph theory:0.2",Combinatorics,Combinatorics: The paper studies combinatorial properties of model-theoretic logics under Vopěnka's principle. Graph theory is not relevant as the paper focuses on set-theoretic and logical frameworks rather than graph structures.
2908,Reconstruction of Source and Medium Parameters via Wave-Splitting and Green Function Equations,"This paper applies the time-domain wave-splitting and the Green function concepts to study two inverse problems involving an internal transient source embedded in a nondispersive inhomogeneous medium. These two inverse problems are (1) given the physical one-way travel time Green operator kernel $\bar G^ - ( {0,t} )$ and the source-space distribution function, reconstruct the wave-speed profile $c( z )$ of the medium; (2) given the same kernel $\bar G^ - ( {0,t} )$ and the medium wave-speed function, recover the source-space distribution profile $p( z )$. Both numerical inversion algorithms and approximate-reconstruction formulas are presented. Several computational examples are given.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.2,Networks:0.1,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.9,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.3,Applied computing:0.1,Social and professional topics:0.1",Mathematics of computing,Mathematics of computing: Inverse problem algorithms and Green function analysis.,"Mathematical analysis:1.0,Continuous mathematics:1.0,Discrete mathematics:0.2,Information theory:0.1,Mathematical software:0.2,Probability and statistics:0.3","Mathematical analysis,Continuous mathematics",Mathematical analysis is relevant for the inverse problem solutions. Continuous mathematics is relevant due to the focus on wave equations in nondispersive media. Probability and statistics is less central as the paper uses deterministic mathematical methods.,"Calculus:0.1,Continuous functions:0.1,Differential equations:1.0,Functional analysis:0.1,Integral equations:0.2,Mathematical optimization:0.1,Nonlinear equations:0.1,Numerical analysis:1.0,Quadrature:0.1,Topology:0.1","Numerical analysis,Differential equations",Numerical analysis is highly relevant as the paper presents numerical inversion algorithms. Differential equations is relevant as the problems are formulated using wave equations. Other mathematical categories are less central to the paper's focus on computational methods.
2013,Diagnosis of weaknesses in modern error correction codes: a physics approach,"One of the main obstacles to the wider use of the modern error-correction codes is that, due to the complex behavior of their decoding algorithms, no systematic method which would allow characterization of the bit-error-rate (BER) is known. This is especially true at the weak noise where many systems operate and where coding performance is difficult to estimate because of the diminishingly small number of errors. We show how the instanton method of physics allows one to solve the problem of BER analysis in the weak noise range by recasting it as a computationally tractable minimization problem.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:1.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.2,Applied computing:0.0,Social and professional topics:0.0",Mathematics of computing,"Mathematics of computing: The paper applies physics-based instanton methods to analyze error correction codes, which is primarily a mathematical computing contribution. Computing methodologies received partial score for the computational approach but mathematics is the primary domain.","Continuous mathematics:0.0,Discrete mathematics:0.0,Information theory:1.0,Mathematical analysis:0.75,Mathematical software:0.0,Probability and statistics:0.0","Information theory,Mathematical analysis",Information theory is highly relevant as the paper analyzes error correction codes. Mathematical analysis is relevant due to the use of the instanton method for bit-error-rate analysis. Other categories like Discrete mathematics are less directly connected to the core contribution.,"Calculus:0.1,Coding theory:1.0,Differential equations:0.1,Functional analysis:0.1,Integral equations:0.1,Mathematical optimization:0.9,Nonlinear equations:0.1,Numerical analysis:0.3,Quadrature:0.1","Coding theory,Mathematical optimization",Coding theory is directly relevant to the error correction codes analysis. Mathematical optimization is relevant to the instanton-based minimization approach. Other mathematical categories are less central to the paper's core contributions.
728,The method of moments for solution of second kind Fredholm integral equations based on B-spline wavelets,"In this paper, we propose the linear semiorthogonal compactly supported B-spline wavelets as a basis functions for the efficient solution of linear Fredholm integral equations of the second kind. The method of moments (MOM) is utilized via the Galerkin procedure and wavelets are employed as test functions.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:1.0,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.5,Applied computing:0.1,Social and professional topics:0.1",Mathematics of computing,Mathematics of computing is highly relevant because the paper presents a mathematical method (method of moments) using B-spline wavelets for solving Fredholm integral equations. Computing methodologies is less relevant as the focus is on the mathematical formulation rather than computational techniques for broader applications.,"Continuous mathematics:0.1,Discrete mathematics:0.1,Information theory:0.1,Mathematical analysis:1.0,Mathematical software:0.5,Probability and statistics:0.1",Mathematical analysis,Mathematical analysis is directly relevant as the paper proposes a numerical method for solving integral equations.,"Calculus:0.0,Differential equations:0.0,Functional analysis:0.0,Integral equations:1.0,Mathematical optimization:0.0,Nonlinear equations:0.0,Numerical analysis:1.0,Quadrature:0.0","Integral equations,Numerical analysis",Integral equations is directly relevant as the paper focuses on solving Fredholm integral equations. Numerical analysis is relevant due to the use of the method of moments and wavelets for numerical solutions. Other categories like Calculus are too broad and not explicitly addressed.
2801,Experimental investigation of aliasing in the assessment of sampled images,"Many visible and infrared sampled imaging systems suffer from moderate to severe amounts of aliasing. The problem arises because the large optical apertures required for sufficient light gathering ability result in large spatial cutoff frequencies. In consumer grade cameras, images are often undersampled by a factor of twenty times the suggested Nyquist rate. Most consumer cameras employ birefringent blur filters that purposely blur the image prior to detection to reduce Moire artifacts produced by aliasing. In addition to the obvious Moire artifacts, aliasing introduces other pixel level errors that can cause artificial jagged edges and erroneous intensity values. These types of errors have led some investigators to treat the aliased signal as noise in imaging system design and analysis. The importance of aliasing is dependent on the nature of the imagery and the definition of the assessment task. In this study, we employ a laboratory experiment to characterize the nature of aliasing noise for a variety of object classes. We acquire both raw and blurred imagery to explore the impact of pre-detection antialiasing. We also consider the post detection image restoration requirements to restore the in-band image blur produced by the anti-aliasing schemes.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:1.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.2,Applied computing:0.0,Social and professional topics:0.0",Mathematics of computing,Mathematics of computing is directly relevant for signal processing and aliasing analysis. Computing methodologies is secondary to the mathematical modeling.,"Continuous mathematics:0.7,Discrete mathematics:0.2,Information theory:0.9,Mathematical analysis:0.6,Mathematical software:0.1,Probability and statistics:0.4","Information theory,Continuous mathematics,Mathematical analysis",Information theory: Addresses sampling theory and aliasing as signal processing phenomenon. Continuous mathematics: Focus on sampled image systems and Moire artifacts. Mathematical analysis: Empirical algorithm assessment for image restoration. Other categories lack core signal processing focus.,"Calculus:1,Numerical analysis:1,Coding theory:0.6,Differential equations:0.5,Functional analysis:0.4,Integral equations:0.3,Mathematical optimization:0.2,Nonlinear equations:0.1,Quadrature:0.1,Topology:0.1,Continuous functions:0.2","Calculus,Numerical analysis",Calculus is relevant for the mathematical analysis of aliasing effects. Numerical analysis is relevant for the experimental investigation methods. Other categories are less relevant as the focus is on signal processing rather than coding or optimization.
444,On a problem of Byrnes concerning polynomials with restricted coefficients,"As in the earlier paper with this title, we consider a question of Byrnes concerning the minimal length N*(m) of a polynomial with all coefficients in {-1, 1} which has a zero of a given order m at x = 1. In that paper we showed that N*(m) = 2 m for all m < 5 and showed that the extrernal polynomials for were those conjectured by Byrnes, but for m = 6 that N*(6) = 48 rather than 64. A polynomial with N = 48 was exhibited for m = 6, but it was not shown there that this extremal was unique. Here we show that the extremal is unique. In the previous paper, we showed that N*(7) is one of the 7 values 48, 56, 64, 72, 80, 88 or 96. Here we prove that N* (7) = 96 without determining all extremal polynomials. We also make some progress toward determining N*(8). As in the previous paper, we use a combination of number theoretic ideas and combinatorial computation. The main point is that if ζ p is a primitive pth root of unity where p < m + 1 is a prime, then the condition that all coefficients of P be in {-1, 1}, together with the requirement that P(x) be divisible by (x - 1) m puts severe restrictions on the possible values for the cyclotomic integer P(ζ p ).","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:1.0,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Mathematics of computing,Mathematics of computing is highly relevant as the paper focuses on mathematical analysis of polynomials with restricted coefficients. Other fields like Applied computing or Computing methodologies are tangential as the core contribution is theoretical mathematics.,"Continuous mathematics:0.0,Discrete mathematics:0.9,Information theory:0.0,Mathematical analysis:0.6,Mathematical software:0.0,Probability and statistics:0.0","Discrete mathematics,Mathematical analysis",Discrete mathematics is highly relevant as the paper studies polynomials with discrete coefficients. Mathematical analysis is relevant due to the use of number-theoretic methods. Other children like Probability and statistics are less relevant as the focus is on algebraic properties.,"Calculus:0.1,Combinatorics:0.9,Differential equations:0.1,Functional analysis:0.1,Graph theory:0.2,Integral equations:0.1,Mathematical optimization:0.4,Nonlinear equations:0.3,Numerical analysis:0.5,Quadrature:0.1","Combinatorics,Numerical analysis",Combinatorics is highly relevant as the paper addresses combinatorial problems in polynomial construction. Numerical analysis is secondary due to computational methods for verifying results. Other children like Mathematical optimization are less central to the theoretical focus.
3962,RADS converter: An approach to Analog to Information conversion,"While classical compressive sensing aims to reduce the number of measurements with respect to Nyquist-based sampling methods, it is usually important to consider the total number of bits needed to represent this small amount of measurements in order to maintain the same signal quality. In this paper we study an architecture for signal acquisition that produces a stream of 1-bit measurements, and we show that a proper exploitation of its features, as well as of the sparse representation of the input signal can result in an extremely high reconstruction accuracy with a total number of bits much smaller than classical sampling.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:1.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Mathematics of computing,Mathematics of computing is relevant for the signal acquisition and reconstruction algorithms. Other categories like Networks are not discussed.,"Information theory:0.85,Probability and statistics:0.3,Mathematical analysis:0.25,Continuous mathematics:0.2,Discrete mathematics:0.15,Mathematical software:0.1",Information theory,Information theory is directly relevant to the 1-bit measurement and signal reconstruction analysis. Other mathematical categories are not the primary focus of the paper's contribution.,Coding theory:1,Coding theory,"Coding theory (1): The paper presents an Analog to Information conversion architecture with 1-bit measurements, directly relevant to coding theory in signal representation."
5471,Conic Reconstruction and Correspondence From Two Views,"Conics are widely accepted as one of the most fundamental image features together with points and line segments. The problem of space reconstruction and correspondence of two conics from two views is addressed in this paper. It is shown that there are two independent polynomial conditions on the corresponding pair of conics across two views, given the relative orientation of the two views. These two correspondence conditions are derived algebraically and one of them is shown to be fundamental in establishing the correspondences of conics. A unified closed-form solution is also developed for both projective reconstruction of conics in space from two uncalibrated camera views and metric reconstruction from two calibrated camera views. Experiments are conducted to demonstrate the discriminality of the correspondence conditions and the accuracy and stability of the reconstruction both for simulated and real images.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:1.0,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.75,Applied computing:0.1,Social and professional topics:0.1",Mathematics of computing,Mathematics of computing is highly relevant due to the algebraic derivation of conic correspondence conditions. Computing methodologies receives moderate relevance for the algorithmic solutions presented.,"Continuous mathematics:0.8,Discrete mathematics:0.6,Information theory:0.4,Mathematical analysis:0.9,Mathematical software:0.2,Probability and statistics:0.3","Mathematical analysis,Continuous mathematics",Mathematical analysis is highly relevant for the algebraic derivation of correspondence conditions. Continuous mathematics is relevant for projective geometry in conic reconstruction. Discrete mathematics is somewhat relevant but less central than continuous methods. Information theory is not directly relevant as the paper focuses on geometric rather than information-theoretic problems.,"Calculus:0.0,Continuous functions:0.0,Differential equations:0.0,Functional analysis:0.0,Integral equations:0.0,Mathematical optimization:1.0,Nonlinear equations:0.3,Numerical analysis:0.5,Quadrature:0.0,Topology:0.0","Mathematical optimization,Numerical analysis",Mathematical optimization is relevant due to the derivation of polynomial conditions for conic correspondence. Numerical analysis is relevant for the unified closed-form solution. Other fields like Calculus or Differential equations lack direct connection to the algebraic methods used.
4538,Further decomposition of the Karhunen-Loève series representation of a stationary random process,"It is shown how the Karhunen-Loeve (K-L) series representation for a finite sample of a discrete random sequence, stationary to the second order, may be further decomposed into a pair of series by utilizing certain symmetry properties of the covariance matrix of the sequence. The theory is applied to the particular example of a first-order Markov sequence, the series representation of which has not so far been reported in the literature. The generalization to the case of continuous random functions on a finite interval is similar and is therefore only briefly described.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:1.0,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Mathematics of computing,Mathematics of computing is highly relevant as the paper involves statistical analysis and decomposition of Karhunen-Loeve series. Other categories like 'Computing methodologies' are less central as the focus is on mathematical techniques rather than computational methods.,"Continuous mathematics:0.1,Discrete mathematics:0.1,Information theory:0.1,Mathematical analysis:0.3,Mathematical software:0.1,Probability and statistics:0.9",Probability and statistics,Probability and statistics is highly relevant as the paper decomposes Karhunen-Loève series for stochastic processes. Mathematical analysis is secondary due to the statistical decomposition focus. Other categories like Information theory are not directly addressed.,"Distribution functions:0.3,Multivariate statistics:0.5,Nonparametric statistics:0,Probabilistic algorithms:0.2,Probabilistic inference problems:0.2,Probabilistic reasoning algorithms:0.2,Probabilistic representations:1,Statistical paradigms:0.3,Stochastic processes:1","Stochastic processes,Probabilistic representations",Stochastic processes are directly addressed in the K-L decomposition. Probabilistic representations are key to the method. Other statistical categories are less central.
1989,On Robust Approximate Feedback Linearization: A Nonlinear Control Approach,"In this letter, we consider a problem of global stabilization of a class of approximately feedback linearized systems. We propose a new nonlinear control approach which includes a nonlinear controller and a Lyapunov-based design method. Our new nonlinear control approach broadens the class of systems under consideration over the existing results.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.8,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Mathematics of computing,Mathematics of computing is relevant for the Lyapunov-based control design and nonlinear system analysis. Other categories like Applied computing are less relevant as the focus is on theoretical control methods.,"Continuous mathematics:0.8,Discrete mathematics:0.1,Information theory:0.2,Mathematical analysis:0.9,Mathematical software:0.1,Probability and statistics:0.1","Mathematical analysis,Continuous mathematics",Mathematical analysis is highly relevant for the control theory approach. Continuous mathematics applies to the nonlinear control systems. Other fields like probability or discrete math are not directly relevant to the core contribution.,"Nonlinear equations:1.0,Differential equations:0.5,Mathematical optimization:0.3,Calculus:0.2,Continuous functions:0.2,Integral equations:0.2,Functional analysis:0.1,Numerical analysis:0.2,Quadrature:0.1,Topology:0.1",Nonlinear equations,Nonlinear equations is directly relevant for the control approach in nonlinear systems. Differential equations is secondary but related to system modeling.
1913,On how correlations between excitatory and inhibitory synaptic inputs maximize the information rate of neuronal firing,"Cortical neurons receive barrages of excitatory and inhibitory inputs which are not independent, as network structure and synaptic kinetics impose statistical correlations. Experiments in vitro and in vivo have demonstrated correlations between inhibitory and excitatory synaptic inputs in which inhibition lags behind excitation in cortical neurons. This delay arises in feed-forward inhibition (FFI) circuits and ensures that coincident excitation and inhibition do not preclude neuronal firing. Conversely, inhibition that is too delayed broadens neuronal integration times, thereby diminishing spike-time precision and increasing the firing frequency. This led us to hypothesize that the correlation between excitatory and inhibitory synaptic inputs modulates the encoding of information of neural spike trains. We tested this hypothesis by investigating the effect of such correlations on the information rate (IR) of spike trains using the Hodgkin-Huxley model in which both synaptic and membrane conductances are stochastic. We investigated two different synaptic input regimes: balanced synaptic conductances and balanced currents. Our results show that correlations arising from the synaptic kinetics, τ, and millisecond lags, δ, of inhibition relative to excitation strongly affect the IR of spike trains. In the regime of balanced synaptic currents, for short time lags (δ ~ 1 ms) there is an optimal τ that maximizes the IR of the postsynaptic spike train. Given the short time scales for monosynaptic inhibitory lags and synaptic decay kinetics reported in cortical neurons under physiological contexts, we propose that FFI in cortical circuits is poised to maximize the rate of information transfer between cortical neurons. Our results also provide a possible explanation for how certain drugs and genetic mutations affecting the synaptic kinetics can deteriorate information processing in the brain.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.7,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Mathematics of computing,Mathematics of computing receives moderate relevance for the stochastic modeling of neural processes. Other categories lack core connection to the biological modeling focus.,"Continuous mathematics:0.0,Discrete mathematics:0.0,Information theory:1.0,Mathematical analysis:0.6,Mathematical software:0.0,Probability and statistics:0.9","Information theory,Probability and statistics",Information theory: The study analyzes information rate maximization in neural spike trains. Probability and statistics: The paper models synaptic input correlations statistically. Mathematical analysis was rejected as the focus is on information-theoretic and statistical modeling rather than pure analysis.,"Stochastic processes:1,Probabilistic algorithms:1,Probabilistic inference problems:0.8,Probabilistic reasoning algorithms:0.7,Probabilistic representations:0.6,Distribution functions:0.5,Multivariate statistics:0.4,Statistical paradigms:0.3,Nonparametric statistics:0.2","Stochastic processes,Probabilistic algorithms",Stochastic processes are central as the model involves stochastic conductances. Probabilistic algorithms are relevant due to the probabilistic analysis of information rate. Other categories like Multivariate statistics are less relevant as the focus is on probabilistic modeling rather than statistical methods.
3434,Convergence of classes of high-order semi-Lagrangian schemes for the Vlasov-Poisson system,"Abstract: In this paper we present some classes of high-order semi-Lagran- gian schemes for solving the periodic one-dimensional Vlasov-Poisson system in phase-space on uniform grids. We prove that the distribution function $ f(t,x,v)$ and the electric field $ E(t,x)$ converge in the $ L^2$ norm with a rate of $\displaystyle \mathcal{O}\left(\Delta t^2 +h^{m+1}+ \frac{h^{m+1}}{\Delta t}\right),$ where $ m$ is the degree of the polynomial reconstruction, and $ \Delta t$ and $ h$ are respectively the time and the phase-space discretization parameters","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.9,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Mathematics of computing,Mathematics of computing is relevant for convergence analysis of numerical methods. Computing methodologies is secondary to the mathematical focus.,"Continuous mathematics:0.8,Discrete mathematics:0.2,Information theory:0.1,Mathematical analysis:1.0,Mathematical software:0.1,Probability and statistics:0.3",Mathematical analysis,"Mathematical analysis is highly relevant because the paper rigorously proves convergence rates of numerical schemes. Continuous mathematics is marginally relevant due to the PDE context, but the primary focus is on analytical convergence proofs.","Calculus:0,Differential equations:1,Functional analysis:0,Integral equations:0,Mathematical optimization:0,Nonlinear equations:0,Numerical analysis:1,Quadrature:0","Differential equations,Numerical analysis","Differential equations is relevant as the paper addresses the Vlasov-Poisson system, a partial differential equation. Numerical analysis is relevant due to the focus on convergence rates of semi-Lagrangian schemes. Other categories like Calculus or Quadrature are less specific or not directly addressed."
4140,Empirical Likelihood Inference of the Partial Linear Isotonic Errors-in-variables Regression Models with Missing Data,"This article is concerned with statistical inference of the partial linear isotonic regression model missing response and measurement errors in covariates. We proposed an empirical likelihood ratio test statistics and show that it has a limiting weighted chi-square distribution. An adjusted empirical likelihood ratio statistic, which is shown to have a limiting standard central chi-square distribution, is then proposed further. A maximum empirical likelihood estimator is also developed. A simulation study is conducted to examine the finite-sample property of proposed procedure.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.2,Mathematics of computing:0.9,Information systems:0.3,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.4,Applied computing:0.3,Social and professional topics:0.1",Mathematics of computing,Mathematics of computing is directly relevant due to the focus on statistical inference and empirical likelihood methods. Applied computing is secondary as the core contribution is methodological. Other categories are not central to the paper's focus.,"Continuous mathematics:0.1,Discrete mathematics:0.1,Information theory:0.1,Mathematical analysis:0.1,Mathematical software:0.1,Probability and statistics:0.9",Probability and statistics,Probability and statistics is highly relevant as the paper focuses on statistical inference methods for regression models with missing data. Other categories like Mathematical analysis or Information theory are not central to the paper's statistical methodology.,"Nonparametric statistics:1,Statistical paradigms:1,Distribution functions:0.5,Multivariate statistics:0.3,Probabilistic algorithms:0.2,Probabilistic inference problems:0.2,Probabilistic reasoning algorithms:0.2,Probabilistic representations:0.2,Stochastic processes:0.2","Nonparametric statistics,Statistical paradigms","Nonparametric statistics: The paper develops an empirical likelihood method for regression. Statistical paradigms: The work introduces a novel statistical inference framework. Other categories like 'Distribution functions' were rejected because the focus is on inference methods, not distribution analysis."
2810,Game theory and mathematical economics: a theoretical computer scientist's introduction,"There has been recently increasing interaction between game theory and, more generally, economic theory, with theoretical computer science, mainly in the context of the Internet. The paper is an invitation to this important frontier.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:1.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Mathematics of computing,"Mathematics of computing is relevant because the paper introduces computer scientists to game theory and economics, emphasizing theoretical foundations. Other categories are irrelevant as the focus is on mathematical concepts.","Continuous mathematics:0.3,Discrete mathematics:0.6,Information theory:0.1,Mathematical analysis:0.9,Mathematical software:0.1,Probability and statistics:0.4","Mathematical analysis,Discrete mathematics",Mathematical analysis is highly relevant as the paper introduces theoretical foundations for game theory and economics. Discrete mathematics is relevant due to the combinatorial aspects of social choice problems. Probability and statistics are secondary here.,"Calculus:0,Combinatorics:0,Differential equations:0,Functional analysis:0,Graph theory:0,Integral equations:0,Mathematical optimization:0.5,Nonlinear equations:0,Numerical analysis:0,Quadrature:0",,The paper discusses the intersection of game theory and theoretical computer science but does not focus on any specific mathematical field listed in the options. All options are irrelevant as the core contribution is about theoretical foundations rather than mathematical techniques.
3831,Design methods for irregular repeat-accumulate codes,"We optimize the random-like ensemble of irregular repeat-accumulate (IRA) codes for binary-input symmetric channels in the large block-length limit. Our optimization technique is based on approximating the evolution of the densities (DE) of the messages exchanged by the belief-propagation (BP) message-passing decoder by a one-dimensional dynamical system. In this way, the code ensemble optimization can be solved by linear programming. We propose four such DE approximation methods, and compare the performance of the obtained code ensembles over the binary-symmetric channel (BSC) and the binary-antipodal input additive white Gaussian noise channel (BIAWGNC). Our results clearly identify the best among the proposed methods and show that the IRA codes obtained by these methods are competitive with respect to the best known irregular low-density parity-check (LDPC) codes. In view of this and the very simple encoding structure of IRA codes, they emerge as attractive design choices.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.9,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Mathematics of computing,Mathematics of computing is highly relevant due to the paper's focus on density evolution approximations and linear programming for code optimization. Other categories lack direct connection to the mathematical analysis of coding theory.,"Continuous mathematics:0.0,Discrete mathematics:0.0,Information theory:0.8,Mathematical analysis:0.0,Mathematical software:0.0,Probability and statistics:0.6","Information theory,Probability and statistics",Information theory is highly relevant as the paper presents coding theory and channel analysis. Probability and statistics is relevant for the performance comparison of codes. Other categories like Mathematical software are not directly relevant to the core contribution.,"Coding theory:1,Distribution functions:0,Multivariate statistics:0,Nonparametric statistics:0,Probabilistic algorithms:0.5,Probabilistic inference problems:0,Probabilistic reasoning algorithms:0,Probabilistic representations:0,Statistical paradigms:0,Stochastic processes:0",Coding theory,Coding theory is the primary domain as the paper focuses on IRA code design and optimization. Probabilistic algorithms is secondary because the paper uses belief-propagation and linear programming techniques. Other statistical categories are irrelevant as the focus is on coding rather than statistical analysis.
4993,Positivity Properties of the Fourier Transform and the Stability of Periodic Travelling-Wave Solutions,"In this paper we establish a method to obtain the stability of periodic travelling-wave solutions for equations of Korteweg–de Vries-type $u_t+u^pu_x-Mu_x=0$, with M being a general pseudodifferential operator and where $p\geq1$ is an integer. Our approach uses the theory of totally positive operators, the Poisson summation theorem, and the theory of Jacobi elliptic functions. In particular we obtain the stability of a family of periodic travelling waves solutions for the Benjamin–Ono equation. The present technique gives a new way to obtain the existence and stability of cnoidal and dnoidal waves solutions associated with the Korteweg–de Vries and modified Korteweg–de Vries equations, respectively. The theory has prospects for the study of periodic travelling-wave solutions of other partial differential equations.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:1.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Mathematics of computing,"Mathematics of computing: The paper uses advanced mathematical theories (Fourier transform, Jacobi elliptic functions) to analyze the stability of PDE solutions. All other categories are irrelevant as the focus is purely theoretical mathematics.","Continuous mathematics:0.8,Discrete mathematics:0.0,Information theory:0.0,Mathematical analysis:0.9,Mathematical software:0.0,Probability and statistics:0.6","Mathematical analysis,Continuous mathematics,Probability and statistics",Mathematical analysis is core for stability proofs. Continuous mathematics applies to Fourier transform theory. Probability and statistics relates to wave behavior analysis. Other fields like information theory are less relevant to the mathematical framework.,"Calculus:0.5,Continuous functions:0.3,Differential equations:1,Distribution functions:0.2,Functional analysis:1,Integral equations:0.4,Mathematical optimization:0.1,Multivariate statistics:0.1,Nonlinear equations:1,Nonparametric statistics:0.1,Numerical analysis:0.2,Probabilistic algorithms:0.1,Probabilistic inference problems:0.1,Probabilistic reasoning algorithms:0.1,Probabilistic representations:0.1,Quadrature:0.1,Statistical paradigms:0.1,Stochastic processes:0.2,Topology:0.1","Differential equations,Functional analysis,Nonlinear equations",Differential equations is central to the study of KdV-type equations. Functional analysis is highly relevant for the use of operators and Fourier transforms. Nonlinear equations is core to the paper's focus on nonlinear PDEs.
1183,"Least squares superposition codes of moderate dictionary size, reliable at rates up to capacity","Sparse superposition codes are developed for the additive white Gaussian noise channel with average codeword power constraint. Codewords are linear combinations of subsets of vectors, with the possible messages indexed by the choice of subset. Decoding is by least squares, tailored to the assumed form of linear combination. Communication is shown to be reliable with error probability exponentially small for all rates up to the Shannon capacity.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.9,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.6,Applied computing:0.1,Social and professional topics:0.1",Mathematics of computing,Mathematics of computing: The paper develops theoretical coding schemes for AWGN channels. Other categories are not central to the core contribution.,"Information theory:1.0,Probability and statistics:0.7,Continuous mathematics:0.2,Discrete mathematics:0.1,Mathematical analysis:0.1,Mathematical software:0.05",Information theory,"Information theory is highly relevant as the paper addresses communication over a Gaussian channel using sparse superposition codes, a core information-theoretic problem. Probability and statistics receive a moderate score due to the analysis of error probabilities, but the paper’s primary contribution lies in information-theoretic coding.",Coding theory:1,Coding theory,"Coding theory is highly relevant because the paper introduces sparse superposition codes for Gaussian noise channels, directly addressing error correction and communication reliability. No other categories are applicable here."
3203,"Response Solutions for Quasi-Periodically Forced, Dissipative Wave Equations","We consider several models of nonlinear wave equations subject to very strong damping and quasi-periodic external forcing. This is a singular perturbation, since the damping is not the highest order term. We study the existence of response solutions (i.e., quasi-periodic solutions with the same frequency as the forcing). Under very general non-resonance conditions on the frequency, we show the existence of asymptotic expansions of the response solution; moreover, we prove that the response solution indeed exists and depends analytically on $\varepsilon$ (where $\varepsilon$ is the inverse of the coefficient multiplying the damping) for $\varepsilon$ in a complex domain, which in some cases includes disks tangent to the imaginary axis at the origin. In other models, we prove analyticity in cones of aperture $\pi/2$ and we conjecture it is optimal. These results have consequences for the asymptotic expansions of the response solutions considered in the literature. The proof of our results relies on reformulating the problem as a fixed point problem, constructing an approximate solution and studying the properties of iterations that converge to the solutions of the fixed point problem.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.9,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Mathematics of computing,Mathematics of computing is highly relevant as the paper presents theoretical analysis of wave equations. Other categories like Theory of computation are less relevant as the focus is mathematical foundations rather than computational models.,"Continuous mathematics:1.0,Discrete mathematics:0.0,Information theory:0.0,Mathematical analysis:1.0,Mathematical software:0.0,Probability and statistics:0.0","Continuous mathematics,Mathematical analysis",Both Continuous mathematics and Mathematical analysis are directly relevant for the theoretical study of quasi-periodic wave equations. Other fields like discrete mathematics or probability are not discussed.,"Calculus:0,Continuous functions:0,Differential equations:1,Functional analysis:0,Integral equations:0,Mathematical optimization:0.5,Nonlinear equations:1,Numerical analysis:0.5,Quadrature:0,Topology:0","Differential equations,Nonlinear equations",Differential equations: The paper studies nonlinear wave equations. Nonlinear equations: The equations under analysis are nonlinear. Mathematical optimization is secondary as the focus is on existence proofs rather than optimization methods.
1902,"Sensitivity, principal component and flux analysis applied to signal transduction: the case of epidermal growth factor mediated signaling","MOTIVATION
Novel high-throughput genomic and proteomic tools are allowing the integration of information from a range of biological assays into a single conceptual framework. This framework is often described as a network of biochemical reactions. We present strategies for the analysis of such networks.


RESULTS
The direct differential method is described for the systematic evaluation of scaled sensitivity coefficients in reaction networks. Principal component analysis, based on an eigenvalue-eigenvector analysis of the scaled sensitivity coefficient matrix, is applied to rank individual reactions in the network based on their effect on system output. When combined with flux analysis, sensitivity analysis allows model reduction or simplification. Using epidermal growth factor (EGF) mediated signaling and trafficking as an example of signal transduction, we demonstrate that sensitivity analysis quantitatively reveals the dependence of dual-phosphorylated extracellular signal-regulated kinase (ERK) concentration on individual reaction rate constants. It predicts that EGF mediated reactions proceed primarily via an Shc-dependent pathway. Further, it suggests that receptor internalization and endosomal signaling are important features regulating signal output only at low EGF dosages and at later times.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:1.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.2,Applied computing:0.3,Social and professional topics:0.0",Mathematics of computing,"Mathematics of computing receives 1.0 as the paper's core contribution involves mathematical analysis techniques (sensitivity analysis, principal component analysis, flux analysis) for biochemical networks. Applied computing (0.3) and Computing methodologies (0.2) are partially relevant due to network modeling, but the mathematical methods are primary.","Continuous mathematics:0.9,Discrete mathematics:0.2,Information theory:0.1,Mathematical analysis:0.9,Mathematical software:0.1,Probability and statistics:0.8","Continuous mathematics,Mathematical analysis,Probability and statistics",Continuous mathematics (biochemical reaction modeling). Mathematical analysis (sensitivity analysis techniques). Probability and statistics (principal component analysis). Other categories like discrete mathematics are not central to the paper's focus on continuous biological systems.,"Calculus:0,Continuous functions:0,Differential equations:1,Distribution functions:0,Functional analysis:0,Integral equations:0,Mathematical optimization:0,Multivariate statistics:1,Nonlinear equations:0,Nonparametric statistics:0,Numerical analysis:0,Probabilistic algorithms:0,Probabilistic inference problems:0,Probabilistic reasoning algorithms:0,Probabilistic representations:0,Quadrature:0,Statistical paradigms:0,Stochastic processes:0,Topology:0","Differential equations,Multivariate statistics",Differential equations are relevant due to the direct differential method used for sensitivity analysis in reaction networks. Multivariate statistics is relevant for principal component analysis based on eigenvalue-eigenvector analysis. Other fields like Calculus or Topology are not discussed.
5182,"The $25,000,000,000 Eigenvector: The Linear Algebra behind Google","Google's success derives in large part from its PageRank algorithm, which ranks the importance of web pages according to an eigenvector of a weighted link matrix. Analysis of the PageRank formula provides a wonderful applied topic for a linear algebra course. Instructors may assign this article as a project to more advanced students or spend one or two lectures presenting the material with assigned homework from the exercises. This material also complements the discussion of Markov chains in matrix algebra. Maple and Mathematica files supporting this material can be found at www.rose-hulman.edu/~bryan.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:1.0,Information systems:0.5,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Mathematics of computing,Mathematics of computing: The paper explains the PageRank algorithm using linear algebra and eigenvectors. Information systems (0.5) is secondary due to the web search application.,"Continuous mathematics:0.3,Discrete mathematics:0.0,Information theory:0.0,Mathematical analysis:0.0,Mathematical software:0.0,Probability and statistics:0.8",Probability and statistics,"Probability and statistics: The paper explains PageRank through eigenvectors and Markov chains, core mathematical concepts. Continuous mathematics is only tangentially relevant. Other fields are unrelated to the linear algebra/PageRank focus.","Distribution functions:0.2,Multivariate statistics:0.1,Nonparametric statistics:0.1,Probabilistic algorithms:1.0,Probabilistic inference problems:0.3,Probabilistic reasoning algorithms:0.2,Probabilistic representations:0.2,Statistical paradigms:0.1,Stochastic processes:1.0","Probabilistic algorithms,Stochastic processes",Probabilistic algorithms is relevant as PageRank is a probabilistic method. Stochastic processes is relevant due to the Markov chain foundation. Other categories are less central to the paper's focus.
2376,Cycles and paths in graphs with large minimal degree,"Let G be a simple graph of order n and minimal degree > cn (0 < c < 1/2). We prove that (1) There exist n0 = n0(c) and k = k(c) such that if n > n0 and G contains a cycle Ct for some t > 2k, then G contains a cycle Ct − 2s for some positive s < k; (2) Let G be 2‐connected and nonbipartite. For each ε(0 < ε < 1), there exists n0 = n0(c,ε) such that if n ≥ n0 then G contains a cycle Ct for all t with $\left \lceil { 2\over c}\right \rceil -2\leq t\leq 2(1-\varepsilon) cn$. This answers positively a question of Erdős, Faudree, Gyárfás and Schelp. © 2004 Wiley Periodicals, Inc. J Graph Theory 47: 39–52, 2004","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.9,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Mathematics of computing,Mathematics of computing is highly relevant as the paper presents theoretical results on cycles and paths in graphs. Other fields like Theory of computation are less directly related.,"Continuous mathematics:0.1,Discrete mathematics:0.9,Information theory:0.1,Mathematical analysis:0.2,Mathematical software:0.1,Probability and statistics:0.1",Discrete mathematics,Discrete mathematics is core for graph theory analysis. Other mathematical fields are only incidentally relevant.,"Combinatorics:0.5,Graph theory:1.0",Graph theory,Graph theory is directly relevant as the paper analyzes cycles and paths in graphs with minimal degree constraints. Combinatorics is moderately relevant but secondary to the graph-theoretic focus.
6024,Transforming Renewal Processes for Simulation of Nonstationary Arrival Processes,"Simulation models of real-life systems often assume stationary (homogeneous) Poisson arrivals. Therefore, when nonstationary arrival processes are required, it is natural to assume Poisson arrivals with a time-varying arrival rate. For many systems, however, this provides an inaccurate representation of the arrival process that is either more or less variable than Poisson. In this paper we extend techniques that transform a stationary Poisson arrival process into a nonstationary Poisson arrival process (NSPP) by transforming a stationary renewal process into a nonstationary, non-Poisson (NSNP) arrival process. We show that the desired arrival rate is achieved and that when the renewal base process is either more or less variable than Poisson, then the NSNP process is also more or less variable, respectively, than an NSPP. We also propose techniques for specifying the renewal base process when presented properties of, or data from, an arrival process and illustrate them by modeling real arrival data.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.9,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Mathematics of computing,Mathematics of computing is highly relevant due to the focus on stochastic processes and simulation techniques. Other categories like 'Theory of computation' are less aligned with the mathematical modeling emphasis.,"Continuous mathematics:0.0,Discrete mathematics:0.0,Information theory:0.0,Mathematical analysis:0.75,Mathematical software:0.0,Probability and statistics:1.0","Probability and statistics,Mathematical analysis",Probability and statistics is highly relevant for modeling nonstationary arrival processes. Mathematical analysis is relevant for transforming renewal processes. Continuous mathematics and Information theory are less directly related to the paper’s focus on simulation techniques.,"Calculus:0,Differential equations:0,Distribution functions:0.5,Functional analysis:0,Integral equations:0,Mathematical optimization:0.3,Multivariate statistics:0,Nonlinear equations:0,Nonparametric statistics:0,Numerical analysis:0.2,Probabilistic algorithms:0.3,Probabilistic inference problems:0.2,Probabilistic reasoning algorithms:0.1,Probabilistic representations:0.4,Quadrature:0,Statistical paradigms:0,Stochastic processes:1","Stochastic processes,Probabilistic representations",Stochastic processes is highly relevant as the paper focuses on modeling nonstationary arrival processes. Probabilistic representations is moderately relevant due to the transformation techniques for renewal processes. Other categories are less directly related to probabilistic modeling of arrivals.
1963,Novel approach to parameter estimation of a class of nonlinear systems,"A new identification algorithm based on over-sampling scheme is proposed for a Hammerstein model which consists of a nonlinear element followed by a linear dynamic model. The unknown linear transfer function model can be identified by making use of the information obtained from an over-sampled output, and the intermediate input to the linear part can also be estimated as well as an arbitrary continuous or discontinuous function type of nonlinear element by a deconvolution approach. The prior information of the nonlinear element is not needed in the new algorithm.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.2,Theory of computation:0.2,Mathematics of computing:0.8,Information systems:0.3,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.3,Applied computing:0.2,Social and professional topics:0.1",Mathematics of computing,Mathematics of computing is highly relevant for the parameter estimation algorithm and system identification theory. Other categories like Computing methodologies are less central as the focus is on mathematical modeling rather than computational techniques.,"Continuous mathematics:0.1,Discrete mathematics:0.1,Information theory:0.1,Mathematical analysis:0.9,Mathematical software:0.2,Probability and statistics:0.1",Mathematical analysis,Mathematical analysis is highly relevant for the nonlinear system parameter estimation algorithm. Other categories like Probability and statistics are less relevant as the focus is on system identification rather than statistical methods.,"Nonlinear equations:1,Numerical analysis:1,Mathematical optimization:0.8,Integral equations:0.5,Differential equations:0.4,Calculus:0.3,Functional analysis:0.2,Quadrature:0.1","Nonlinear equations,Numerical analysis",Nonlinear equations: The paper focuses on parameter estimation for nonlinear systems (Hammerstein model). Numerical analysis: Proposes a deconvolution-based algorithm for system identification. Other categories like differential equations or calculus are less directly relevant to the core method.
2327,Stabilization of a 3D rigid pendulum,"We introduced models for a 3D pendulum, consisting of a rigid body that is supported at a frictionless pivot, in a 2004 CDC paper [Shen, J, Dec. 2004]. In that paper, several different classifications were given and models were developed for each classification. Control problems were posed based on these various models. This paper continues that line of research by studying stabilization problems for a reduced model of the 3D pendulum. Two different stabilization strategies are proposed. The first controller, based on angular velocity feedback only, asymptotically stabilizes the hanging equilibrium. The domain of attraction is shown to be almost global. The second controller, based on angular velocity and reduced attitude feedback, asymptotically stabilizes the inverted equilibrium, providing an almost global domain of attraction. Simulation results are provided to illustrate closed loop properties.","General and reference:0.0,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.8,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Mathematics of computing,"Mathematics of computing: The paper presents control strategies for a 3D pendulum using mathematical models. Other categories like Applied computing are less relevant as the focus is on theoretical control algorithms, not practical applications.","Continuous mathematics:0.4,Discrete mathematics:0.1,Information theory:0.1,Mathematical analysis:0.9,Mathematical software:0.1,Probability and statistics:0.2",Mathematical analysis,Mathematical analysis is directly relevant to the control theory and stabilization strategies discussed. Continuous mathematics receives a moderate score for modeling the pendulum system.,"Calculus:0,Differential equations:1,Functional analysis:0,Integral equations:0,Mathematical optimization:0,Nonlinear equations:0,Numerical analysis:0,Quadrature:0",Differential equations,"Differential equations: The paper addresses stabilization of a 3D pendulum using control theory, which involves solving differential equations. Other children are irrelevant as the focus is on control strategies rather than calculus or numerical methods."
6035,Biased Positional Games and Small Hypergraphs with Large Covers,"We prove that in the biased $(1:b)$ Hamiltonicity and $k$-connectivity Maker-Breaker games ($k>0$ is a constant), played on the edges of the complete graph $K_n$, Maker has a winning strategy for $b\le(\log 2-o(1))n/\log n$. Also, in the biased $(1:b)$ Avoider-Enforcer game played on $E(K_n)$, Enforcer can force Avoider to create a Hamilton cycle when $b\le (1-o(1))n/\log n$. These results are proved using a new approach, relying on the existence of hypergraphs with few edges and large covering number.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.2,Networks:0.1,Software and its engineering:0.2,Theory of computation:0.3,Mathematics of computing:0.9,Information systems:0.2,Security and privacy:0.1,Human-centered computing:0.2,Computing methodologies:0.4,Applied computing:0.3,Social and professional topics:0.1",Mathematics of computing,Mathematics of computing is directly relevant as the paper focuses on combinatorial game theory and hypergraph analysis. Other fields like Theory of computation are less relevant as the contribution is mathematical rather than algorithmic.,"Continuous mathematics:0.1,Discrete mathematics:1.0,Information theory:0.1,Mathematical analysis:0.2,Mathematical software:0.1,Probability and statistics:0.3",Discrete mathematics,"Discrete mathematics: The paper analyzes biased positional games using hypergraph theory, a core topic in discrete mathematics. Other children like 'Probability and statistics' are secondary as the focus is on combinatorial game theory and hypergraph properties.","Combinatorics:1.0,Graph theory:1.0","Combinatorics,Graph theory",Combinatorics is relevant for hypergraph covering number analysis. Graph theory is relevant for Hamiltonicity and connectivity games.
5723,Square Root Computation over Even Extension Fields,"This paper presents a comprehensive study of the computation of square roots over finite extension fields. We propose two novel algorithms for computing square roots over even field extensions of the form \BBFq2, with q = pn, p an odd prime and n ≥ 1. Both algorithms have an associate computational cost roughly equivalent to one exponentiation in \BBFq2. The first algorithm is devoted to the case when q ≡ 1 mod 4, whereas the second one handles the case when q ≡ 3 mod 4. Numerical comparisons show that the two algorithms presented in this paper are competitive and in some cases more efficient than the square root methods previously known.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.9,Information systems:0.0,Security and privacy:0.3,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Mathematics of computing,"Mathematics of computing is central to finite field operations and algorithmic complexity analysis. Security and privacy is secondary if field computations relate to cryptographic applications, but the abstract does not explicitly mention security contexts.","Continuous mathematics:0.3,Discrete mathematics:0.9,Information theory:0.1,Mathematical analysis:0.3,Mathematical software:0.7,Probability and statistics:0.1","Discrete mathematics,Mathematical software",Discrete mathematics is directly relevant to finite field operations. Mathematical software is relevant to the algorithm implementation focus. Other categories like Continuous mathematics or Probability are not central to the discrete computational problem.,"Combinatorics:0,Graph theory:0,Mathematical software performance:0.5,Solvers:1,Statistical software:0",Solvers,"Solvers: The paper presents novel algorithms for square root computation in finite fields, which are mathematical solvers. 'Mathematical software performance' received a moderate score due to efficiency considerations but is secondary to the algorithmic contribution."
949,On constructing regular filter banks from domain bounded polynomials,"The design of regular two-channel bi-orthogonal filter banks is shown to be reducible to the design of pairs of real polynomials with domain bounded to the interval /spl lsqb//spl minus/1,1/spl rsqb/. Techniques for designing polynomials satisfying various constraints are outlined. Transformation of polynomials to multidimensional bi-orthogonal filter banks is presented. >","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.9,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.5,Applied computing:0.1,Social and professional topics:0.1",Mathematics of computing,Mathematics of computing is relevant as the paper presents mathematical techniques for polynomial-based filter bank design. Computing methodologies is partially relevant but secondary to the mathematical construction. Other categories are not core to the theoretical formulation.,"Continuous mathematics:0.2,Discrete mathematics:0.1,Information theory:0.9,Mathematical analysis:0.3,Mathematical software:0.1,Probability and statistics:0.2",Information theory,Information theory is highly relevant as the paper discusses filter bank design in the context of signal processing. Continuous mathematics is marginally relevant due to polynomial analysis but secondary to the core information theory focus.,Coding theory:1.0,Coding theory,"Coding theory is highly relevant as the paper focuses on filter bank design using polynomial-based methods, a core topic in signal processing and coding."
682,Particles Interacting with a Vibrating Medium: Existence of Solutions and Convergence to the Vlasov-Poisson System,"We are interested in a kinetic equation intended to describe the interaction of particles with their environment. The environment is modeled by a collection of local vibrational degrees of freedom. We establish the existence of weak solutions for a wide class of initial data and external forces. We also identify a relevant regime which allows us to derive, quite surprisingly, the attractive Vlasov--Poisson system from the coupled Vlasov wave equations.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.6,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.3,Applied computing:0.1,Social and professional topics:0.1",Mathematics of computing,Mathematics of computing is relevant for the derivation of the Vlasov-Poisson system from kinetic equations. Other fields like Hardware or Networks are not mentioned in the mathematical analysis of the system.,"Continuous mathematics:0.9,Discrete mathematics:0.1,Information theory:0.1,Mathematical analysis:0.9,Mathematical software:0.1,Probability and statistics:0.1","Mathematical analysis,Continuous mathematics",Mathematical analysis is directly relevant as the paper proves existence of solutions and convergence. Continuous mathematics is relevant due to the focus on differential equations and kinetic systems. Other fields are not central to the core mathematical contribution.,"Calculus:0.6,Continuous functions:0.2,Differential equations:0.9,Functional analysis:0.4,Integral equations:0.3,Mathematical optimization:0.1,Nonlinear equations:0.3,Numerical analysis:0.2,Quadrature:0.1,Topology:0.1","Differential equations,Calculus",Differential equations are central to the kinetic model analysis. Calculus is relevant for the mathematical framework. Other categories like topology are too abstract.
1762,Transversals of Longest Paths and Cycles,"Let $G$ be a graph of order $n$. Let $\mathrm{lpt}(G)$ be the minimum cardinality of a set $X$ of vertices of $G$ such that $X$ intersects every longest path of $G$, and define $\mathrm{lct}(G)$ analogously for cycles instead of paths. We prove that $\mathrm{lpt}(G)\leqslant \lceil\frac{n}{4}-\frac{n^{2/3}}{90}\rceil$ if $G$ is connected, and $\mathrm{lct}(G)\leqslant \lceil\frac{n}{3}-\frac{n^{2/3}}{36}\rceil$ if $G$ is $2$-connected. Our bound on $\mathrm{lct}(G)$ improves an earlier result of Thomassen. Furthermore, we prove upper bounds on $\mathrm{lpt}(G)$ for planar graphs and graphs of bounded tree-width.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.9,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Mathematics of computing,Mathematics of computing is highly relevant as the paper presents theoretical graph transversal bounds. Other categories are irrelevant as the paper doesn't address computing systems or applications.,"Continuous mathematics:0.0,Discrete mathematics:1.0,Information theory:0.0,Mathematical analysis:0.0,Mathematical software:0.0,Probability and statistics:0.0",Discrete mathematics,Discrete mathematics is directly relevant because the paper focuses on graph theory concepts like longest paths and cycles. Other fields like Continuous mathematics or Probability and statistics are not mentioned or implied in the core contribution.,"Combinatorics:0.3,Graph theory:0.9",Graph theory,"The paper addresses graph-theoretic problems regarding transversals of longest paths/cycles, which is a core topic in 'Graph theory'. 'Combinatorics' is a broader field and less specific to the graph structure analysis."
5751,A Slight Improvement to the Colored Bárány's Theorem,"Suppose $d+1$ absolutely continuous probability measures $m_0, \ldots, m_d$ on $\mathbb{R}^d$ are given. In this paper, we prove that there exists a point of $\mathbb{R}^d$ that belongs to the convex hull of $d+1$ points $v_0, \ldots, v_d$ with probability at least $\frac{2d}{(d+1)!(d+1)}$, where each point $v_i$ is sampled independently according to probability measure $m_i$.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:1.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Mathematics of computing,Mathematics of computing is relevant for the probabilistic theorem and convex hull analysis. Other categories do not apply as the paper is purely theoretical mathematics.,"Continuous mathematics:0.1,Discrete mathematics:0.6,Information theory:0.1,Mathematical analysis:0.5,Mathematical software:0.1,Probability and statistics:0.7","Discrete mathematics,Probability and statistics,Mathematical analysis",Discrete mathematics is relevant for the convex hull analysis in geometry. Probability and statistics applies to the probabilistic measures discussed. Mathematical analysis is relevant for the theoretical proofs. Other categories like Continuous mathematics are less central to the core contribution.,"Calculus:0.2,Combinatorics:0.7,Differential equations:0.1,Distribution functions:0.5,Functional analysis:0.2,Graph theory:0.3,Integral equations:0.1,Mathematical optimization:0.4,Multivariate statistics:0.6,Nonlinear equations:0.1,Nonparametric statistics:0.1,Numerical analysis:0.1,Probabilistic algorithms:0.4,Probabilistic inference problems:0.3,Probabilistic reasoning algorithms:0.3,Probabilistic representations:0.8,Quadrature:0.1,Statistical paradigms:0.5,Stochastic processes:0.6","Combinatorics,Probabilistic representations",Combinatorics is relevant due to the geometric and combinatorial nature of convex hulls. Probabilistic representations are central to the theorem's probabilistic formulation. Other statistical categories are less directly tied to the core geometric probability analysis.
791,The equivalence of a generalized Martin's axiom to a combinatorial principle,"Abstract A generalized version of Martin's axiom, called BACH, is shown to be equivalent to one of its combinatorial consequences, a generalization of P(c).","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.9,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Mathematics of computing,"Mathematics of computing: The paper establishes equivalence between a generalized Martin's axiom and a combinatorial principle, which is a foundational mathematical logic contribution.","Continuous mathematics:0.2,Discrete mathematics:0.9,Information theory:0.3,Mathematical analysis:0.1,Mathematical software:0.1,Probability and statistics:0.2",Discrete mathematics,Discrete mathematics: The paper discusses generalized Martin's axiom and combinatorial principles. Other categories like Information theory (0.3) are less relevant as the focus is on mathematical logic rather than information theory.,"Combinatorics:1.0,Graph theory:0.5",Combinatorics,Combinatorics is directly relevant as the paper discusses a combinatorial principle equivalent to a generalized Martin's axiom. Graph theory is less relevant as the abstract does not mention graph-related concepts.
528,Analysis and Further Improvement of Fine Resolution Frequency Estimation Method From Three DFT Samples,"The bias and mean square error (MSE) analysis of the frequency estimator suggested in is given and an improved version of the estimator, with the removal of estimator bias, is suggested. The signal-to-noise ratio (SNR) threshold above which the bias removal is effective is also determined.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:1.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Mathematics of computing,Mathematics of computing is highly relevant as the paper centers on statistical analysis of frequency estimation. Other categories like 'Computing methodologies' or 'Theory of computation' do not align with the mathematical focus on bias and mean square error.,"Continuous mathematics:0.1,Discrete mathematics:0.1,Information theory:0.7,Mathematical analysis:0.6,Mathematical software:0.1,Probability and statistics:0.6","Information theory,Probability and statistics",Information theory is relevant for frequency estimation using DFT samples. Probability and statistics is relevant for analyzing bias and mean square error. Other fields like Mathematical analysis are secondary to the statistical focus of the method.,"Coding theory:0,Distribution functions:0.2,Multivariate statistics:0.1,Nonparametric statistics:0.1,Probabilistic algorithms:0.1,Probabilistic inference problems:0.1,Probabilistic reasoning algorithms:0.1,Probabilistic representations:0.1,Statistical paradigms:0.8,Stochastic processes:0.1",Statistical paradigms,Statistical paradigms is highly relevant as the paper focuses on statistical analysis of a frequency estimator (bias/MSE) and improves it through statistical methods. Other categories like Distribution functions or Stochastic processes lack direct evidence in the abstract.
1530,Concentration Phenomena in a Nonlocal Quasi-linear Problem Modelling Phytoplankton II: Limiting Profile,"This is Part II of our study on the positive steady state of a quasi-linear reaction-diffusion system in one space dimension introduced by Klausmeier and Litchman for the modelling of the distributions of phytoplankton biomass and its nutrient. In Part I, we proved nearly optimal existence and nonexistence results. In Part II, we obtain complete descriptions of the profile of the solutions when the coefficient of the drifting term is large, rigorously proving the numerically observed phenomenon of concentration of biomass for this model. Moreover, we reveal four critical numbers for the model and provide further insights to the problem being modelled.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.9,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Mathematics of computing,Mathematics of computing is highly relevant as the paper presents mathematical analysis of phytoplankton models. Other fields are not relevant since the core contribution is purely mathematical analysis without direct computing applications.,"Continuous mathematics:0.8,Discrete mathematics:0.0,Information theory:0.0,Mathematical analysis:1.0,Mathematical software:0.0,Probability and statistics:0.6","Mathematical analysis,Probability and statistics",Mathematical analysis: The paper rigorously analyzes concentration phenomena in a mathematical model. Probability and statistics: The study involves critical numbers and numerical validation. Other categories like 'Continuous mathematics' are less specific to the core analytical contribution.,"Calculus:0.3,Differential equations:1.0,Distribution functions:0.2,Functional analysis:0.4,Integral equations:0.2,Mathematical optimization:0.3,Multivariate statistics:0.1,Nonlinear equations:1.0,Nonparametric statistics:0.1,Numerical analysis:0.5,Probabilistic algorithms:0.1,Probabilistic inference problems:0.1,Probabilistic reasoning algorithms:0.1,Probabilistic representations:0.1,Quadrature:0.2,Statistical paradigms:0.1,Stochastic processes:0.3","Differential equations,Nonlinear equations",Differential equations: The study involves a reaction-diffusion system. Nonlinear equations: The quasi-linear problem exhibits nonlinear characteristics. Rejected: Calculus is too general; Numerical analysis is not the primary focus.
2771,Free double Ockham algebras,"ABSTRACT The variety O2 of double Ockham algebras consists of the algebras (A ∨, ∧, f,g 0,1) of type (2,2,1,1,0,0) where (A; ∨, ∧,f, 0,1) and (A; ∨, ∧,g 0,1) are Ockham algebras. In [16], M. Sequeira introduced several subvarieties of O2. In this paper we give a construction of free double Ockham algebras on a partially ordered set. We also describe free objects for the subvarieties of O2 considered in [16].","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.85,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Mathematics of computing,Mathematics of computing is central for abstract algebra and lattice theory. Other fields like Software engineering are not relevant here.,"Continuous mathematics:0.1,Discrete mathematics:0.9,Information theory:0.2,Mathematical analysis:0.1,Mathematical software:0.1,Probability and statistics:0.1",Discrete mathematics,Discrete mathematics is highly relevant as the paper focuses on algebraic structures and free algebras in a discrete setting. Other categories like Continuous mathematics or Probability are not central to the core contribution.,"Combinatorics:0.0,Graph theory:0.0",,"Both Combinatorics and Graph theory are irrelevant as the paper focuses on abstract algebraic structures in Ockham algebras, not combinatorial or graph-theoretic problems."
5319,On weak-open compact images of metric spaces,"To find internal characterizations of certain images of metric spaces is one of central problems in general topology. Arhangel’skii [1] showed that a space is an open compact image of a metric space if and only if it has a development consisting of point-finite open covers, and some characterizations for certain quotient compact images of metric spaces are obtained in [3, 5, 8]. Recently, Xia [12] introduced the concept of weak-open mappings. By using it, certain g-first countable spaces are characterized as images of metric spaces under various weak-open mappings. Furthermore, Li and Lin in [4] proved that a space is g-metrizable if and only if it is a weak-open σ-image of a metric space. The purpose of this paper is to give some characterizations of weak-open compact images of metric spaces, which showed that a space is a weak-open compact image of a metric space if and only if it has a weak development consisting of point-finite cs-covers. In this paper, all spaces are Hausdorff, all mappings are continuous and surjective. N denotes the set of all natural numbers. τ(X) denotes the topology on a space X . For the usual product space ∏ i∈NXi, πi denotes the projection ∏ i∈NXi onto Xi. For a sequence {xn} in X , denote 〈xn〉 = {xn : n∈N}.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:1.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Mathematics of computing,"The paper provides mathematical characterizations of weak-open compact images of metric spaces, a foundational topology problem in Mathematics of computing. Other categories are irrelevant.","Continuous mathematics:0.1,Discrete mathematics:0.1,Information theory:0.1,Mathematical analysis:0.7,Mathematical software:0.1,Probability and statistics:0.1",Mathematical analysis,Mathematical analysis is relevant for the topological characterizations discussed. Other categories like Discrete mathematics are not central to the theoretical contribution.,"Calculus:0,Differential equations:0,Functional analysis:0,Integral equations:0,Mathematical optimization:0,Nonlinear equations:0,Numerical analysis:0,Quadrature:0",,"The paper focuses on topological characterizations of weak-open compact images of metric spaces, which falls under pure mathematics (specifically topology) and is not aligned with the applied mathematics or numerical analysis categories listed here."
4683,On some fast well-balanced first order solvers for nonconservative systems,"The goal of this article is to design robust and simple first order explicit solvers for one-dimensional nonconservative hyperbolic systems. These solvers are intended to be used as the basis for higher order methods for one or multidimensional problems. The starting point for the development of these solvers is the general definition of a Roe linearization introduced by Toumi in 1992 based on the use of a family of paths. Using this concept, Roe methods can be extended to nonconservative systems. These methods have good well-balanced and robustness properties, but they have also some drawbacks: in particular, their implementation requires the explicit knowledge of the eigenstructure of the intermediate matrices. Our goal here is to design numerical methods based on a Roe linearization which overcome this drawback. The idea is to split the Roe matrices into two parts which are used to calculate the contributions at the cells to the right and to the left, respectively. This strategy is used to generate two different one-parameter families of schemes which contain, as particular cases, some generalizations to nonconservative systems of the well-known Lax-Friedrichs, Lax-Wendroff, FORCE, and GFORCE schemes. Some numerical experiments are presented to compare the behaviors of the schemes introduced here with Roe methods.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.95,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Mathematics of computing,"Mathematics of computing: The paper focuses on numerical methods for solving nonconservative hyperbolic systems, which is a core area in mathematical computing. Other categories like 'Software and its engineering' or 'Computer systems organization' are irrelevant as the paper emphasizes algorithmic development rather than software/hardware implementation.","Continuous mathematics:0.1,Discrete mathematics:0.1,Information theory:0.1,Mathematical analysis:0.8,Mathematical software:0.2,Probability and statistics:0.1",Mathematical analysis,"Mathematical analysis: The paper develops numerical solvers for hyperbolic systems, a core area in mathematical analysis. 'Mathematical software' is secondary as the focus is on algorithm design, not software tools.","Calculus:0.1,Differential equations:0.6,Functional analysis:0.1,Integral equations:0.1,Mathematical optimization:0.3,Nonlinear equations:0.5,Numerical analysis:0.9,Quadrature:0.1","Numerical analysis,Differential equations",Numerical analysis is directly relevant for the development of solvers. Differential equations applies to the analysis of hyperbolic systems. Other options like Nonlinear equations are secondary as the focus is on numerical methods rather than equation theory.
111,The condition number of a randomly perturbed matrix,"Let M be an arbitrary <i>n</i> by <i>n</i> matrix. We study the conditionnumber a random perturbation M+N<sub>n</sub> of M, where N<sub>n</sub> is arandom matrix. It is shown that, under very general conditions on M and M<sub>n</sub>, the condition number of M+N<sub>n</sub> is polynomial in <i>n</i>with very high probability. The main novelty here is that we allow N<sub>n</sub> to have discrete distribution.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.9,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Mathematics of computing,Mathematics of computing is highly relevant for matrix condition number analysis. Other categories like Theory of computation are not directly aligned with the mathematical focus.,"Probability and statistics:0.8,Mathematical analysis:1.0,Discrete mathematics:0.4,Information theory:0.2,Continuous mathematics:0.6,Mathematical software:0.1","Mathematical analysis,Probability and statistics",Mathematical analysis (1.0) is directly relevant as the paper involves analytical methods for matrix perturbation. Probability and statistics (0.8) is relevant due to the study of random matrices. Other options like Discrete mathematics are less relevant as the focus is on continuous matrix analysis.,"Calculus:0,Differential equations:0,Distribution functions:1,Functional analysis:0,Integral equations:0,Mathematical optimization:0,Multivariate statistics:0,Nonlinear equations:0,Nonparametric statistics:0,Numerical analysis:0,Probabilistic algorithms:1,Probabilistic inference problems:0,Probabilistic reasoning algorithms:0,Probabilistic representations:0,Quadrature:0,Statistical paradigms:0,Stochastic processes:0","Distribution functions,Probabilistic algorithms",Distribution functions is relevant as the paper analyzes random matrices with discrete distributions. Probabilistic algorithms is relevant because the work studies the probabilistic behavior of perturbed matrices. Other categories like mathematical optimization are irrelevant as the focus is on probabilistic analysis.
3737,An improved finite element model of incremental forming using conic programming,"Incremental sheet forming (ISF) is a flexible method for forming sheet metal that can be considered as an incremental deformation process. A fast method for modeling deformation in ISF is required that can form the basis of a path optimisation and feedback control algorithm for the process. This paper improves upon previous formulations for approximately modeling ISF by incorporating bending work and using finite element discretisation. The model is solved by manipulating the problem into the form of a second-order cone program (SOCP) and significant improvements are observed in terms of numerical stability of the model allowing a full product to be simulated. For validation purposes, a comparison is provided between the new model and measured data.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.8,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Mathematics of computing,"Mathematics of computing is highly relevant because the paper formulates a finite element model using conic programming (SOCP), a mathematical optimization problem. Applied computing is less relevant as the focus is on the mathematical model rather than application.","Continuous mathematics:0.6,Discrete mathematics:0.3,Information theory:0.2,Mathematical analysis:1.0,Mathematical software:0.7,Probability and statistics:0.4",Mathematical analysis,Mathematical analysis (1.0) is directly relevant for the finite element model and SOCP formulation. Mathematical software (0.7) has secondary relevance to the implementation. Other categories like Continuous mathematics (0.6) are less central to the core contribution.,"Calculus:0.0,Differential equations:0.0,Functional analysis:0.0,Integral equations:0.0,Mathematical optimization:1.0,Nonlinear equations:0.0,Numerical analysis:1.0,Quadrature:0.0","Mathematical optimization,Numerical analysis",Mathematical optimization is highly relevant because the paper uses conic programming (a type of optimization) to improve the model. Numerical analysis is relevant due to the focus on numerical stability and simulation of the model. Other options like Calculus or Differential equations are not central to the core contribution.
1563,An efficient conjugate direction method with orthogonalization for large-scale quadratic optimization problems,"A new conjugate direction (CD) method is proposed, which is based on an orthogonalization procedure and does not make use of line searches for the conjugate vector set construction. This procedure prevents the set of conjugate vectors from degeneracy and eliminates high sensitivity to computation errors pertinent to methods of CDs, resulting in an efficient algorithm for solving large-scale ill-conditioned minimization problems without preconditioning. The advantages of our algorithms are illustrated by results of the extensive numerical experiments with large-scale quadratic functions.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:1.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.2,Applied computing:0.0,Social and professional topics:0.0",Mathematics of computing,"Mathematics of computing is directly relevant as the paper introduces a new conjugate direction method for optimization. Computing methodologies is marginally relevant due to algorithmic application, but the core contribution is mathematical.","Continuous mathematics:0.0,Discrete mathematics:0.0,Information theory:0.0,Mathematical analysis:0.75,Mathematical software:0.25,Probability and statistics:0.0",Mathematical analysis,Mathematical analysis is relevant as the paper presents a new mathematical algorithm for quadratic optimization. Mathematical software is less relevant as the focus is on the algorithm itself rather than software implementation.,"Calculus:0.1,Differential equations:0.1,Functional analysis:0.1,Integral equations:0.1,Mathematical optimization:1.0,Nonlinear equations:0.2,Numerical analysis:0.8,Quadrature:0.1","Mathematical optimization,Numerical analysis",Mathematical optimization: The paper presents a conjugate direction method for quadratic optimization. Numerical analysis: The algorithm addresses numerical stability in large-scale problems. Other categories like Calculus are not directly relevant.
3623,Utility Maximization with Discretionary Stopping,"Utility maximization problems of mixed optimal stopping/control type are considered, which can be solved by reduction to a family of related pure optimal stopping problems. Sufficient conditions for the existence of optimal strategies are provided in the context of continuous-time, Ito process models for complete markets. The mathematical tools used are those of optimal stopping theory, continuous-time martingales, convex analysis, and duality theory. Several examples are solved explicitly, including one which demonstrates that optimal strategies need not always exist.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.9,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.1,Applied computing:0.0,Social and professional topics:0.0",Mathematics of computing,Mathematics of computing is highly relevant as the paper presents theoretical work on optimal stopping theory and continuous-time martingales. Other fields have low relevance as the paper is a mathematical contribution to computing theory.,"Continuous mathematics:1.0,Probability and statistics:0.75,Discrete mathematics:0.0,Information theory:0.0,Mathematical analysis:0.2,Mathematical software:0.0","Continuous mathematics,Probability and statistics",Continuous mathematics is central to the continuous-time Ito process models. Probability and statistics are relevant for martingales and duality theory. Mathematical analysis receives minor relevance for theoretical underpinnings.,"Calculus:0,Continuous functions:0,Distribution functions:0.1,Multivariate statistics:0.2,Nonparametric statistics:0.1,Probabilistic algorithms:0.6,Probabilistic inference problems:0.7,Probabilistic reasoning algorithms:0.7,Probabilistic representations:0.5,Statistical paradigms:0.3,Stochastic processes:0.8,Topology:0","Probabilistic inference problems,Stochastic processes",Probabilistic inference problems is relevant for optimal stopping theory applications. Stochastic processes is relevant for continuous-time Ito process modeling. Other probabilistic categories are moderately relevant but not core to the paper's mathematical focus.
3256,"Rate Distortion Theory for Causal Video Coding: Characterization, Computation Algorithm, and Comparison","Causal video coding is considered from an information theoretic point of view, where video source frames X<sub>1</sub>, X<sub>2</sub>, ..., X<sub>N</sub> are encoded in a frame by frame manner, the encoder for each frame X<sub>k</sub> can use all previous frames and all previous encoded frames while the corresponding decoder can use only all previous encoded frames, and each frame X<sub>k</sub> itself is modeled as a source X<sub>k</sub> = {X<sub>k</sub> (i) }<sub>i=1</sub><sup>∞</sup>. A novel computation approach is proposed to analytically characterize, numerically compute, and compare the minimum total rate of causal video coding R<sub>c</sub>*(D<sub>1</sub>, ...,D<sub>N</sub>) required to achieve a given distortion (quality) level D<sub>1</sub>, ...,D<sub>N</sub> >; 0. Among many other things, the computation approach includes an iterative algorithm with global convergence for computing R<sub>c</sub>*(D<sub>1</sub>, ...,D<sub>N</sub>) . The global convergence of the algorithm further enables us to demonstrate a somewhat surprising result (dubbed the more and less coding theorem)-under some conditions on source frames and distortion, the more frames need to be encoded and transmitted, the less amount of data after encoding has to be actually sent. With the help of the algorithm, it is also shown by example that R<sub>c</sub>*(D<sub>1</sub>, ...,D<sub>N</sub>) is in general much smaller than the total rate offered by the traditional greedy coding method. As a by-product, an extended Markov lemma is established for correlated ergodic sources.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:1.0,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Mathematics of computing,"Mathematics of computing is highly relevant as the paper presents theoretical analysis of rate distortion theory for causal video coding. Other categories are irrelevant as the focus is on mathematical modeling rather than systems, networks, or applications.","Continuous mathematics:0.1,Discrete mathematics:0.2,Information theory:1.0,Mathematical analysis:0.3,Mathematical software:0.2,Probability and statistics:0.2",Information theory,"Information theory is highly relevant as the paper fundamentally addresses rate-distortion theory, a core concept in information theory for optimizing video coding. Other fields like mathematical analysis or probability are only incidentally used in the proofs and algorithms.",Coding theory:1.0,Coding theory,"The paper explicitly discusses rate distortion theory and causal video coding, which are central topics in coding theory. No other options are provided."
4109,Breaking the curse of dimensionality in regression,"Author(s): Zhu, Yinchu; Bradic, Jelena | Abstract: Models with many signals, high-dimensional models, often impose structures on the signal strengths. The common assumption is that only a few signals are strong and most of the signals are zero or close (collectively) to zero. However, such a requirement might not be valid in many real-life applications. In this article, we are interested in conducting large-scale inference in models that might have signals of mixed strengths. The key challenge is that the signals that are not under testing might be collectively non-negligible (although individually small) and cannot be accurately learned. This article develops a new class of tests that arise from a moment matching formulation. A virtue of these moment-matching statistics is their ability to borrow strength across features, adapt to the sparsity size and exert adjustment for testing growing number of hypothesis. GRoup-level Inference of Parameter, GRIP, test harvests effective sparsity structures with hypothesis formulation for an efficient multiple testing procedure. Simulated data showcase that GRIPs error control is far better than the alternative methods. We develop a minimax theory, demonstrating optimality of GRIP for a broad range of models, including those where the model is a mixture of a sparse and high-dimensional dense signals.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.9,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.3,Applied computing:0.2,Social and professional topics:0.1",Mathematics of computing,Mathematics of computing is relevant due to the development of GRIP for high-dimensional regression. Computing methodologies was considered but rejected as the focus is on statistical theory rather than algorithmic methodology.,"Continuous mathematics:0.75,Discrete mathematics:0.1,Information theory:0.1,Mathematical analysis:0.8,Mathematical software:0.1,Probability and statistics:0.85","Probability and statistics,Mathematical analysis,Continuous mathematics",Probability and statistics is central for hypothesis testing; Mathematical analysis underpins the theoretical framework; Continuous mathematics supports the mathematical model. Discrete mathematics is less relevant.,"Calculus:0.0,Continuous functions:0.0,Differential equations:0.0,Distribution functions:0.0,Functional analysis:0.0,Integral equations:0.0,Mathematical optimization:1.0,Multivariate statistics:1.0,Nonlinear equations:0.0,Nonparametric statistics:1.0,Numerical analysis:0.0,Probabilistic algorithms:0.0,Probabilistic inference problems:0.0,Probabilistic reasoning algorithms:0.0,Probabilistic representations:0.0,Quadrature:0.0,Statistical paradigms:0.0,Stochastic processes:0.0,Topology:0.0","Mathematical optimization,Multivariate statistics,Nonparametric statistics",Mathematical optimization is central for the GRIP test formulation. Multivariate statistics and Nonparametric statistics are relevant as the paper addresses high-dimensional regression with mixed signal strengths. Differential equations or Functional analysis are not core to the methodology.
3022,The Numerical Solution of Problems in Calculus of Variation Using B-Spline Collocation Method,"A B-spline collocation method is developed for solving boundary value problems which arise from the problems of calculus of variations. Some properties of the B-spline procedure required for subsequent development are given, and they are utilized to reduce the solution computation of boundary value problems to some algebraic equations. The method is applied to a few test examples to illustrate the accuracy and the implementation of the method.","General and reference:0.2,Hardware:0.2,Computer systems organization:0.2,Networks:0.2,Software and its engineering:0.2,Theory of computation:0.2,Mathematics of computing:1.0,Information systems:0.2,Security and privacy:0.2,Human-centered computing:0.2,Computing methodologies:0.5,Applied computing:0.2,Social and professional topics:0.2",Mathematics of computing,"Mathematics of computing is relevant for numerical methods in calculus of variations. Computing methodologies is partially relevant for algorithmic approach, but mathematics is the core focus. Other categories lack connection.","Continuous mathematics:0.3,Discrete mathematics:0.1,Information theory:0.1,Mathematical analysis:0.9,Mathematical software:0.4,Probability and statistics:0.1",Mathematical analysis,"Mathematical analysis is highly relevant as the paper develops a numerical method for solving boundary value problems in calculus of variations. Other categories like 'Mathematical software' are secondary, as the focus is on algorithmic theory rather than implementation.","Calculus:0.3,Differential equations:0.8,Functional analysis:0.1,Integral equations:0.1,Mathematical optimization:0.2,Nonlinear equations:0.3,Numerical analysis:0.9,Quadrature:0.1","Numerical analysis,Differential equations","Numerical analysis: The paper develops a B-spline collocation method for solving boundary value problems numerically. Differential equations: Calculus of variations inherently involves differential equations, which are the focus of the numerical solutions presented."
5177,Reach Controllability of Single Input Affine Systems on a Simplex,"We study the reach control problem (RCP) for a single input affine system with a simplicial state space. We extend previous results by exploring arbitrary triangulations of the state space; particularly allowing the set of possible equilibria to intersect the interior of simplices. In the studied setting, it is shown that closed-loop equilibria, nevertheless, only arise on the boundary of simplices. This allows to define a notion of reach controllability which quantifies the effect of the control input on boundary equilibria. Using reach controllability we obtain necessary and sufficient conditions for solvability of RCP by affine feedback.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:1.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Mathematics of computing,Mathematics of computing: The paper presents a mathematical framework for analyzing reach controllability in affine systems on simplices. No other categories align with the purely theoretical mathematical analysis.,"Continuous mathematics:0.9,Discrete mathematics:0.0,Information theory:0.0,Mathematical analysis:0.95,Mathematical software:0.0,Probability and statistics:0.0",Mathematical analysis,"Mathematical analysis is highly relevant as the paper presents theoretical analysis of control systems. Continuous mathematics also receives a high score as the system is studied on a continuous state space (simplex), but the primary contribution is the mathematical analysis of controllability.","Calculus:0.4,Differential equations:1.0,Functional analysis:0.3,Integral equations:0.2,Mathematical optimization:0.5,Nonlinear equations:0.4,Numerical analysis:0.3,Quadrature:0.2",Differential equations,Differential equations is relevant as the paper studies reach controllability of affine systems. Other categories are less directly related to the core mathematical analysis presented.
829,Probabilistic temporal interval networks,"A probabilistic temporal interval network is a constraint satisfaction problem where the nodes are temporal intervals and the edges are uncertain interval relations. We attach a probability to each of Allen's basic interval relations. An uncertain relation between two temporal intervals is represented as a disjunction of Allen's probabilistic basic relations. Using the operations of inversion, composition, and addition, defined for this probabilistic representation, we present a path consistency algorithm.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.2,Mathematics of computing:0.9,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Mathematics of computing,Mathematics of computing is relevant for probabilistic interval relations and path consistency algorithms. Other categories are rejected as the paper focuses on mathematical modeling rather than algorithms or software.,"Probability and statistics:1.0,Discrete mathematics:0.75,Information theory:0.0,Mathematical analysis:0.0,Mathematical software:0.0,Continuous mathematics:0.0",Probability and statistics,Probability and statistics is central to the probabilistic interval network framework. Discrete mathematics is less directly relevant to the probabilistic methods.,"Distribution functions:0.2,Multivariate statistics:0.1,Nonparametric statistics:0.1,Probabilistic algorithms:0.8,Probabilistic inference problems:1,Probabilistic reasoning algorithms:1,Probabilistic representations:0.9,Statistical paradigms:0.2,Stochastic processes:0.3","Probabilistic inference problems,Probabilistic reasoning algorithms",Probabilistic inference problems is highly relevant as the paper models uncertain temporal relations as constraint satisfaction problems. Probabilistic reasoning algorithms is relevant for the path consistency algorithm using probabilistic operations. Other categories like Probabilistic representations are moderately relevant but less central than the core algorithmic contributions.
337,Weak General Error Locator Polynomials for Triple-Error-Correcting Binary Golay Code,"In this letter, two weak general error locator polynomials are proposed to improve the one-step decoding of the (23, 12, 7) binary Golay code. Experimental results show that the presented decoders significantly reduce the area compared to the existing one-step decoders.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:1.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.1,Applied computing:0.0,Social and professional topics:0.0",Mathematics of computing,Mathematics of computing: The paper introduces mathematical error locator polynomial models for coding theory. Other categories: Computing methodologies (0.1) for algorithmic implementation.,"Continuous mathematics:0.05,Discrete mathematics:0.75,Information theory:1.0,Mathematical analysis:0.05,Mathematical software:0.05,Probability and statistics:0.05","Information theory,Discrete mathematics",Information theory: The paper studies error-correcting codes. Discrete mathematics: Polynomial-based decoding algorithms are mathematically intensive. Other fields are irrelevant as the focus is on coding theory.,"Coding theory:1.0,Combinatorics:0.3,Graph theory:0.2",Coding theory,"The paper explicitly proposes error locator polynomials for binary Golay codes, which is a core topic in coding theory. Combinatorics and Graph theory are secondary at best, as the focus is on algebraic coding methods."
932,The structure of solution sets of fuzzy relation equations,"In this paper, we consider the structure of solution sets of fuzzy relation equations over complete Boolean algebras. We show that each solution of a system of fuzzy relation equations can be represented by a linear combination of a special solution of its and some certain solutions of the homogeneous equations associated with the system.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.9,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Mathematics of computing,Mathematics of computing: The paper is about mathematical structures and equations.,"Continuous mathematics:1.0,Discrete mathematics:0.1,Information theory:0.1,Mathematical analysis:0.1,Mathematical software:0.1,Probability and statistics:0.1",Continuous mathematics,"Continuous mathematics: Focuses on linear combinations and algebraic structures in fuzzy relation equations. Other categories (e.g., probability and statistics) are irrelevant as the paper is about algebraic properties, not probabilistic models.","Calculus:0.2,Continuous functions:0.3,Topology:0.5",Topology,"Topology: The paper analyzes solution sets in complete Boolean algebras, which involves structural properties akin to topology. Calculus and Continuous functions are less relevant as the focus is on algebraic structures rather than calculus or continuity."
778,Using Cylindrical Algebraic Decomposition and Local Fourier Analysis to Study Numerical Methods: Two Examples,"Local Fourier analysis is a strong and well-established tool for analyzing the convergence of numerical methods for partial differential equations. The key idea of local Fourier analysis is to represent the occurring functions in terms of a Fourier series and to use this representation to study certain properties of the particular numerical method, like the convergence rate or an error estimate. In the process of applying a local Fourier analysis, it is typically necessary to determine the supremum of a more or less complicated term with respect to all frequencies and, potentially, other variables. The problem of computing such a supremum can be rewritten as a quantifier elimination problem, which can be solved with cylindrical algebraic decomposition, a well-known tool from symbolic computation. The combination of local Fourier analysis and cylindrical algebraic decomposition is a machinery that can be applied to a wide class of problems. In the present paper, we will discuss two examples. The first example is to compute the convergence rate of a multigrid method. As second example we will see that the machinery can also be used to do something rather different: We will compare approximation error estimates for different kinds of discretizations.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.95,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Mathematics of computing,Mathematics of computing is highly relevant as the paper combines mathematical techniques (cylindrical algebraic decomposition and Fourier analysis) for numerical method analysis. The focus is on mathematical tools rather than computational methodologies.,"Continuous mathematics:0.25,Discrete mathematics:0.0,Information theory:0.0,Mathematical analysis:1.0,Mathematical software:0.75,Probability and statistics:0.5","Mathematical analysis,Mathematical software",Mathematical analysis is directly relevant to the study of numerical methods. Mathematical software is relevant due to cylindrical algebraic decomposition implementation. Probability/statistics is less central to this specific analysis.,"Calculus:0.1,Differential equations:0.1,Functional analysis:0.1,Integral equations:0.1,Mathematical optimization:0.1,Mathematical software performance:0.1,Nonlinear equations:0.1,Numerical analysis:1.0,Quadrature:0.1,Solvers:0.5,Statistical software:0.1","Numerical analysis,Solvers",Numerical analysis is central to studying convergence of methods. Solvers are relevant for quantifier elimination via cylindrical algebraic decomposition. Other children like Calculus are less directly connected.
1056,A New Sufficient Condition for Additive D-Stability and Application to Cyclic Reaction-Diffusion Models,"Matrix A is said to be additively D-stable if A - D remains Hurwitz for all nonnegative diagonal matrices D. In reaction-diffusion models, additive D-stability of the matrix describing the reaction dynamics guarantees stability of the homogeneous steady-state, thus ruling out the possibility of diffusion-driven instabilities. We present a new criterion for additive D-stability using the concept of compound matrices. We first give conditions under which the second additive compound matrix has nonnegative off-diagonal entries. We then use this Metzler property of the compound matrix to prove additive D-stability with the help of an additional determinant condition. This result is then applied to investigate stability of cyclic reaction networks in the presence of diffusion.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.9,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Mathematics of computing,Mathematics of computing: The paper introduces a new mathematical criterion for matrix stability analysis. Other categories are irrelevant as the work focuses on theoretical matrix analysis rather than computer science applications.,"Continuous mathematics:0.6,Discrete mathematics:0.1,Information theory:0.1,Mathematical analysis:0.9,Mathematical software:0.1,Probability and statistics:0.1",Mathematical analysis,Mathematical analysis is highly relevant as the paper presents mathematical stability criteria for matrices. Continuous mathematics receives partial credit as the work involves differential equations in reaction-diffusion models.,"Calculus:0.1,Differential equations:1,Functional analysis:0.2,Integral equations:0.1,Mathematical optimization:0.3,Nonlinear equations:0.2,Numerical analysis:0.1,Quadrature:0.1",Differential equations,Differential equations are highly relevant as the paper presents a new criterion for additive D-stability in reaction-diffusion models. Other options are less directly related to the core mathematical analysis.
2296,The Translation Sensitivity of Wavelet-Based Registration,"This paper studies the effects of image translation on wavelet-based image registration. The main result is that the normalized correlation coefficients of low-pass Haar and Daubechies wavelet subbands are essentially insensitive to translations for features larger than twice the wavelet blocksize. The third-level low-pass subbands produce a correlation peak that varies with translation from 0.7 and 1.0 with an average in excess of 0.9. Translation sensitivity is limited to the high-pass subband and even this subband is potentially useful. The correlation peak for high-pass subbands derived from first and second-level low-pass subbands ranges from about 0.0 to 1.0 with an average of about 0.5 for Daubechies and 0.7 for Haar. We use a mathematical model to develop these results, and confirm them on real data.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.9,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Mathematics of computing,Mathematics of computing is directly relevant because the paper focuses on wavelet transform analysis for image registration. Categories like Information systems are not central to the core contribution of mathematical analysis.,"Continuous mathematics:0.8,Discrete mathematics:0.3,Information theory:0.7,Mathematical analysis:0.9,Mathematical software:0.4,Probability and statistics:0.5","Mathematical analysis,Information theory",Mathematical analysis: The paper rigorously models wavelet subband translation effects using mathematical frameworks. Information theory: The analysis of correlation coefficients and signal processing aligns with information-theoretic principles. Continuous mathematics is relevant but secondary to the primary analytical and theoretical contributions.,"Calculus:0,Coding theory:0,Differential equations:0,Functional analysis:0,Integral equations:0,Mathematical optimization:1,Nonlinear equations:0,Numerical analysis:1,Quadrature:0","Mathematical optimization,Numerical analysis",Mathematical optimization is relevant for the analysis of wavelet subband sensitivity. Numerical analysis is relevant due to the simulation and mathematical modeling of the registration process. Other categories like calculus are not directly addressed.
2804,Ex-Post Equilibrium and VCG Mechanisms,"Consider an abstract social choice setting with incomplete information, where the number of alternatives is large. Albeit natural, implementing VCG mechanisms is infeasible due to the prohibitive communication constraints. However, if players restrict attention to a subset of the alternatives, feasibility may be recovered.
 This article characterizes the class of subsets that induce an ex-post equilibrium in the original game. It turns out that a crucial condition for such subsets to exist is the availability of a type-independent optimal social alternative for each player. We further analyze the welfare implications of these restrictions.
 This work follows that of Holzman et al. [2004] and Holzman and Monderer [2004] where similar analysis is done for combinatorial auctions.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.2,Mathematics of computing:1.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Mathematics of computing,"Mathematics of computing is relevant because the paper discusses game-theoretic mechanisms and equilibrium analysis, which are core topics in mathematical computing. Other categories are irrelevant as the paper does not focus on algorithms, systems, or hardware.","Continuous mathematics:0.2,Discrete mathematics:0.6,Information theory:0.1,Mathematical analysis:0.9,Mathematical software:0.1,Probability and statistics:0.5","Mathematical analysis,Discrete mathematics",Mathematical analysis is highly relevant as the paper analyzes game-theoretic mechanisms and equilibria. Discrete mathematics is relevant due to the combinatorial nature of alternatives. Probability and statistics are secondary as the focus is on equilibrium analysis rather than probabilistic modeling.,"Combinatorics:1,Mathematical optimization:1,Graph theory:0.7,Calculus:0.5,Differential equations:0.4,Functional analysis:0.3,Integral equations:0.2,Nonlinear equations:0.1,Numerical analysis:0.1,Quadrature:0.1","Combinatorics,Mathematical optimization",Combinatorics is relevant for the subset selection in social choice settings. Mathematical optimization is relevant for the ex-post equilibrium analysis. Other categories are less relevant as the focus is on game theory rather than differential equations.
3996,Lot Sizing with Inventory Bounds and Fixed Costs: Polyhedral Study and Computation,"We investigate the polyhedral structure of the lot-sizing problem with inventory bounds. We consider two models, one with linear cost on inventory, the other with linear and fixed costs on inventory. For both models, we identify facet-defining inequalities that make use of the inventory bounds explicitly and give exact separation algorithms. We also describe a linear programming formulation of the problem when the order and inventory costs satisfy the Wagner-Whitin nonspeculative property. We present computational experiments that show the effectiveness of the results in tightening the linear programming relaxations of the lot-sizing problem with inventory bounds and fixed costs.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.9,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Mathematics of computing,Mathematics of computing is relevant for the polyhedral study and computational analysis of lot-sizing problems. Other categories like Applied computing are less directly related as the focus is on theoretical optimization rather than specific application domains.,"Continuous mathematics:0.2,Discrete mathematics:0.9,Information theory:0.3,Mathematical analysis:0.8,Mathematical software:0.2,Probability and statistics:0.5","Discrete mathematics,Mathematical analysis",Discrete mathematics is core to the polyhedral study of lot-sizing problems. Mathematical analysis supports the computational experiments. Other categories like Probability and statistics are less central to the optimization focus.,"Calculus:0,Combinatorics:0,Differential equations:0,Functional analysis:0,Graph theory:0,Integral equations:0,Mathematical optimization:1,Nonlinear equations:0,Numerical analysis:0,Quadrature:0",Mathematical optimization,Mathematical optimization is highly relevant as the paper focuses on polyhedral analysis and optimization of lot-sizing problems with fixed costs. All other categories are unrelated.
1401,Further Progress on the GM-MDS Conjecture for Reed-Solomon Codes,"Designing good error correcting codes whose generator matrix has a support constraint, i.e., one for which only certain entries of the generator matrix are allowed to be nonzero, has found many recent applications, including in distributed coding and storage, multiple access networks, and weakly secure data exchange. The dual problem, where the parity check matrix has a support constraint, comes up in the design of locally repairable codes. The central problem here is to design codes with the largest possible minimum distance, subject to the given support constraint on the generator matrix. An upper bound on the minimum distance can be obtained through a set of singleton bounds, which can be alternatively thought of as a cut-set bound. Furthermore, it is well known that, if the field size is large enough, any random generator matrix obeying the support constraint will achieve the maximum minimum distance with high probability. Since random codes are not easy to decode, structured codes with efficient decoders, e.g., Reed-Solomon codes, are much more desirable. The GM-MDS conjecture of Dau et al states that the maximum minimum distance over all codes satisfying the generator matrix support constraint can be obtained by a Reed Solomon code. If true, this would have significant consequences. The conjecture has been proven for several special case: when the dimension of the code k is less than or equal to five, when the number of distinct support sets on the rows of the generator matrix m, say, is less than or equal to three, or when the generator matrix is sparsest and balanced. In this paper, we report on further progress on the GM-MDS conjecture. 1. In particular, we show that the conjecture is true for all m less than equal to six. This generalizes all previous known results (except for the sparsest and balanced case, which is a very special support constraint).","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.75,Mathematics of computing:1.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Mathematics of computing,Mathematics of computing: The paper focuses on mathematical analysis of error-correcting codes and the GM-MDS conjecture. Theory of computation: The algorithmic aspects of code design are relevant but secondary to the mathematical core.,"Continuous mathematics:0.0,Discrete mathematics:0.9,Information theory:0.9,Mathematical analysis:0.0,Mathematical software:0.0,Probability and statistics:0.0","Discrete mathematics,Information theory","Discrete mathematics is relevant due to the combinatorial nature of the GM-MDS conjecture and support constraints. Information theory receives a high score because the paper focuses on error-correcting codes and minimum distance properties, which are fundamental topics in information theory.","Coding theory:1.0,Combinatorics:0.5,Graph theory:0.3",Coding theory,"Coding theory is highly relevant as the paper focuses on Reed-Solomon codes. Combinatorics is secondary for structural analysis, but not as central as the coding theory contribution."
3279,Linear Discrepancy of Basic Totally Unimodular Matrices,We show that the linear discrepancy of a basic totally unimodular matrix A2R mn is at most 1 1 n+1 . This extends a result of Peng and Yan.,"General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.9,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Mathematics of computing,Mathematics of computing is relevant for theoretical analysis of matrix properties. Other categories lack mathematical foundations.,Discrete mathematics:0.95,Discrete mathematics,"Discrete mathematics (mathematical theorem on matrix properties). Rejected: Continuous mathematics (focus is discrete structures), Probability (no probabilistic analysis).","Combinatorics:1,Graph theory:0",Combinatorics,"Combinatorics: The paper analyzes properties of totally unimodular matrices, a core combinatorial structure. Graph theory rejected: No explicit graph-theoretical concepts or applications are discussed."
5009,Line Iterative Methods for Cyclically Reduced Discrete Convection-Diffusion Problems,"An analytic and empirical study of line iterative methods for solving the discrete convection-diffusion equation is performed. The methodology consists of performing one step of the cyclic reduction method, followed by iteration on the resulting reduced system using line orderings of the reduced grid. Two classes of iterative methods are considered: block stationary methods, such as the block Gauss–Seidel and SOR methods, and preconditioned generalized minimum residual methods with incomplete LU preconditioners. New analysis extends convergence bounds for constant coefficient problems to problems with separable variable coefficients. In addition, analytic results show that iterative methods based on incomplete LU preconditioners have faster convergence rates than block Jacobi relaxation methods. Numerical experiments examine additional properties of the two classes of methods, including the effects of direction of flow, discretization, and grid ordering on performance.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:1.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.2,Applied computing:0.0,Social and professional topics:0.0",Mathematics of computing,Mathematics of computing is highly relevant as the paper focuses on mathematical analysis and numerical methods for solving convection-diffusion equations. Other categories like Computing methodologies are only marginally relevant due to algorithmic aspects.,"Continuous mathematics:0.1,Discrete mathematics:0.3,Information theory:0.2,Mathematical analysis:0.9,Mathematical software:0.4,Probability and statistics:0.3",Mathematical analysis,Mathematical analysis is directly relevant to the study of iterative methods for convection-diffusion problems. Other categories like Probability and statistics are less relevant as the focus is on numerical analysis rather than probabilistic modeling.,"Calculus:0.2,Differential equations:1,Functional analysis:0.1,Integral equations:0.2,Mathematical optimization:0.5,Nonlinear equations:0.8,Numerical analysis:1,Quadrature:0.2","Differential equations,Numerical analysis","Differential equations is central as the paper studies convection-diffusion PDEs. Numerical analysis is directly relevant due to the focus on iterative solution methods and convergence analysis. Nonlinear equations gets a moderate score as the problem involves nonlinear systems, but it's secondary to the main focus."
2203,A direct product construction for high-rate self-synchronizing codes,In this paper we give a simple combinatorial method for reducing the redundancy of self-synchronizing codes obtained from difference systems of sets. Our method gives infinitely many asymptotically optimal systems with low redundancy rates that generate self-synchronizing codes of very low redundancy and high information rate.,"General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.2,Software and its engineering:0.15,Theory of computation:0.1,Mathematics of computing:0.9,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.3,Applied computing:0.15,Social and professional topics:0.1",Mathematics of computing,"Mathematics of computing: The paper presents a combinatorial method for constructing self-synchronizing codes, which is a mathematical problem. Other categories are less relevant as it's not about networks, software engineering, or computing methodologies.","Continuous mathematics:0.1,Discrete mathematics:0.8,Information theory:0.9,Mathematical analysis:0.1,Mathematical software:0.1,Probability and statistics:0.1","Discrete mathematics,Information theory",Discrete mathematics is highly relevant as the paper discusses combinatorial methods for self-synchronizing codes. Information theory is relevant due to the focus on redundancy reduction and information rate.,"Coding theory:1.0,Combinatorics:0.7,Graph theory:0.5","Coding theory,Combinatorics",Coding theory is central as the paper introduces self-synchronizing code construction. Combinatorics is relevant through difference systems of sets used in the construction. Graph theory is only marginally relevant through potential mathematical foundations.
1004,Tate's and Yoshida's theorems on control of transfer for fusion systems,"We prove analogues of results of Tate and Yoshida on control of transfer for fusion systems. This requires the notions of p‐group residuals and transfer maps in cohomology for fusion systems. As a corollary, we obtain a p‐nilpotency criterion due to Tate.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:1.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Mathematics of computing,Mathematics of computing is directly relevant because the paper focuses on abstract algebraic theorems and cohomology for fusion systems. Other categories like Theory of computation or Computer systems organization are irrelevant as the work is purely mathematical.,"Continuous mathematics:0.7,Discrete mathematics:0.8,Information theory:0.05,Mathematical analysis:0.05,Mathematical software:0.05,Probability and statistics:0.05","Continuous mathematics,Discrete mathematics",Continuous mathematics: Theoretical work on fusion systems involves continuous mathematics. Discrete mathematics: Fusion systems and group theory are discrete. Other options: Not focused on algorithms or software.,"Calculus:0.1,Combinatorics:0.1,Continuous functions:0.1,Graph theory:0.1,Topology:0.1",,"The paper discusses fusion systems and cohomology control theorems, which belong to pure mathematics rather than the provided computer science/mathematics categories in ACM-CCS."
3627,On the Sn-Modules Generated by Partitions of a Given Shape,"Given a Young diagram λ and the set Hλ of partitions of {1, 2, . . . , |λ|} of shape λ, we analyze a particular S|λ|-module homomorphism QHλ → QHλ ′ to show that QHλ is a submodule of QHλ whenever λ is a hook (n, 1, 1, . . . , 1) with m rows, n ≥ m, or any diagram with two rows.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.9,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.1,Applied computing:0.0,Social and professional topics:0.0",Mathematics of computing,Mathematics of computing is highly relevant as the paper presents theoretical work on Sn-modules and Young diagrams. Other fields have low relevance as the paper is a pure mathematical contribution in the context of computing.,"Discrete mathematics:1.0,Continuous mathematics:0.0,Information theory:0.0,Mathematical analysis:0.2,Mathematical software:0.0,Probability and statistics:0.0",Discrete mathematics,Discrete mathematics is directly relevant for partition analysis in symmetric group modules. Mathematical analysis receives minor relevance for theoretical proofs but is not central.,"Combinatorics:1.0,Graph theory:0.3",Combinatorics,"Combinatorics is highly relevant as the paper analyzes partitions and S|λ|-modules, which are combinatorial structures. Graph theory is only weakly relevant as the abstract does not mention graph-theoretic concepts."
2295,On the absolute quadratic complex and its application to autocalibration,"This article introduces the absolute quadratic complex formed by all lines that intersect the absolute conic. If /spl omega/ denotes the 3 /spl times/ 3 symmetric matrix representing the image of that conic under the action of a camera with projection matrix P, it is shown that /spl omega/ /spl ap/ P/sup ~//spl Omega//sub /spl I.bar//P/sup ~T/ where V is the 3 /spl times/ 6 line projection matrix associated with P and /spl Omega//sub /spl I.bar// is a 6 /spl times/ 6 symmetric matrix of rank 3 representing the absolute quadratic complex. This simple relation between a camera's intrinsic parameters, its projection matrix expressed in a projective coordinate frame, and the metric upgrade separating this frame from a metric one - as respectively captured by the matrices /spl omega/, P/sup ~/ and /spl Omega//sub /spl I.bar// - provides a new framework for autocalibration, particularly well suited to typical digital cameras with rectangular or square pixels since the skew and aspect ratio are decoupled from the other intrinsic parameters in /spl omega/.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.9,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Mathematics of computing,Mathematics of computing is directly relevant because the paper focuses on geometric transformations and matrix analysis for camera calibration. Categories like Software engineering are not central to the core contribution of mathematical modeling.,"Continuous mathematics:0.9,Discrete mathematics:0.2,Information theory:0.6,Mathematical analysis:0.8,Mathematical software:0.3,Probability and statistics:0.4","Continuous mathematics,Mathematical analysis",Continuous mathematics: The absolute quadratic complex and its geometric properties form the core of the paper. Mathematical analysis: The derivation of relationships between matrices and their implications for autocalibration rely heavily on analytical methods. Discrete mathematics and Probability are not central to the theoretical framework presented.,"Calculus:1,Continuous functions:0,Differential equations:0,Functional analysis:0,Integral equations:0,Mathematical optimization:1,Nonlinear equations:0,Numerical analysis:0,Quadrature:0,Topology:0","Calculus,Mathematical optimization",Calculus is relevant due to the mathematical modeling of the absolute quadratic complex. Mathematical optimization is relevant because the paper introduces a framework for autocalibration that optimizes camera parameters. Other categories like differential equations are not central to the paper's contributions.
5160,Fusion of Quantized and Unquantized Sensor Data for Estimation,"This letter investigates the usefulness of quantized data for estimation problems in which unquantized data is already available. A worst case scenario is considered in which a fusion center has access to continuous and binary-valued measurements of the same uniformly distributed parameter observed in Gaussian noise. The difference in mean squared error between a minimum mean squared error estimate using unquantized data and a minimum mean squared error estimate using both quantized and unquantized data is used to quantify the value of fusing the two kinds of data. Discussion of the Cramér-Rao Bound predicts how noise in the quantized data affects the reduction in estimate mean squared error from fusing the data types. It is then determined that the maximum reduction in estimate mean squared error from fusion can be approximated as a rational function of the ratio of the standard deviations of the measurement noise in the two data types. Finally, similarities between the approximation to the reduction in estimate mean squared error for the most favorable uniform prior width and a closed form expression based on the Cramér-Rao Bound are discussed.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:1.0,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.5,Applied computing:0.1,Social and professional topics:0.1",Mathematics of computing,Mathematics of computing is central to the statistical analysis of sensor data fusion. Computing methodologies is only peripherally relevant for algorithm design.,"Continuous mathematics:0.3,Discrete mathematics:0.2,Information theory:1.0,Mathematical analysis:0.6,Mathematical software:0.2,Probability and statistics:0.9","Information theory,Probability and statistics",Information theory is central to the paper's analysis of sensor data fusion. Probability and statistics are relevant due to the use of statistical estimation and Cramér-Rao bounds. Other categories like discrete mathematics are less directly applicable.,"Coding theory:0.5,Distribution functions:0.7,Multivariate statistics:0.6,Nonparametric statistics:0.3,Probabilistic algorithms:0.8,Probabilistic inference problems:1.0,Probabilistic reasoning algorithms:0.7,Probabilistic representations:0.6,Statistical paradigms:0.9,Stochastic processes:0.5","Probabilistic inference problems,Statistical paradigms",Probabilistic inference problems are central to the analysis of data fusion. Statistical paradigms are relevant due to the focus on estimation and error analysis. Other children like Coding theory or Stochastic processes are less directly related.
742,Non-Parametric Sensitivity Analysis of the Finite M/M/1 Queue,"In this paper we establish a framework for robust sensitivity analysis of queues. Our leading example is the finite capacity M/M/1/N queue, and we analyze the sensitivity of this model with respect to the assumption that interarrival times are exponentially distributed.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.8,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.2,Social and professional topics:0.1",Mathematics of computing,Mathematics of computing is relevant for queueing theory analysis. Other categories like Applied computing are less relevant as the focus is on theoretical mathematical models rather than real-world applications.,"Continuous mathematics:0.0,Discrete mathematics:0.0,Information theory:0.0,Mathematical analysis:0.25,Mathematical software:0.0,Probability and statistics:1.0",Probability and statistics,Probability and statistics is highly relevant for the sensitivity analysis of queueing models. Other mathematical fields like Discrete mathematics are not directly related to the core probabilistic framework.,"Distribution functions:0,Multivariate statistics:0,Nonparametric statistics:1,Probabilistic algorithms:0,Probabilistic inference problems:0,Probabilistic reasoning algorithms:0,Probabilistic representations:0,Statistical paradigms:0,Stochastic processes:1","Nonparametric statistics,Stochastic processes",Nonparametric statistics are central to the sensitivity analysis framework. Stochastic processes apply to the M/M/1/N queue model. Other statistical categories are not the primary focus.
3840,"L(2, 1)-labelling of Circular-arc Graph","An L(2,1)-labelling of a graph $G=(V, E)$ is $\lambda_{2,1}(G)$ a function $f$ from the vertex set V (G) to the set of non-negative integers such that adjacent vertices get numbers at least two apart, and vertices at distance two get distinct numbers. The L(2,1)-labelling number denoted by $\lambda_{2,1}(G)$ of $G$ is the minimum range of labels over all such labelling. In this article, it is shown that, for a circular-arc graph $G$, the upper bound of $\lambda_{2,1}(G)$ is $\Delta+3\omega$, where $\Delta$ and $\omega$ represents the maximum degree of the vertices and size of maximum clique respectively.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.9,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Mathematics of computing,Mathematics of computing: The paper presents a mathematical analysis of graph labeling bounds. Other categories like Theory of computation were rejected because the focus is on mathematical graph theory rather than computational complexity.,"Continuous mathematics:0.1,Discrete mathematics:0.9,Information theory:0.2,Mathematical analysis:0.1,Mathematical software:0.1,Probability and statistics:0.1",Discrete mathematics,"Discrete mathematics: The paper directly addresses graph labeling problems, a core topic in discrete mathematics. Other categories like Continuous mathematics or Probability are irrelevant to the discrete combinatorial nature of the problem.","Combinatorics:0.4,Graph theory:0.9",Graph theory,"Graph theory is central to the study of L(2,1)-labelling on circular-arc graphs. Combinatorics is less directly relevant compared to the core graph labeling problem."
3046,An Iterative Regularization Method for Total Variation-Based Image Restoration,"We introduce a new iterative regularization procedure for inverse problems based on the use of Bregman distances, with particular focus on problems arising in image processing. We are motivated by the problem of restoring noisy and blurry images via variational methods by using total variation regularization. We obtain rigorous convergence results and effective stopping criteria for the general procedure. The numerical results for denoising appear to give significant improvement over standard models, and preliminary results for deblurring/denoising are very encouraging.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:1.0,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Mathematics of computing,Mathematics of computing is directly relevant as the paper develops iterative regularization algorithms for image restoration. Other categories like Networks or Applied computing are not central to the mathematical methodology.,"Continuous mathematics:0.8,Discrete mathematics:0.3,Information theory:0.4,Mathematical analysis:1.0,Mathematical software:0.5,Probability and statistics:0.3","Mathematical analysis,Continuous mathematics",Mathematical analysis is highly relevant as the paper introduces a regularization method for inverse problems. Continuous mathematics is relevant due to the focus on image processing in continuous domains. Discrete mathematics and Probability are less relevant as the core contribution is not in discrete structures or probabilistic modeling.,"Calculus:0.1,Continuous functions:0.1,Differential equations:0.1,Functional analysis:0.2,Integral equations:0.1,Mathematical optimization:1.0,Nonlinear equations:0.3,Numerical analysis:1.0,Quadrature:0.1,Topology:0.1","Mathematical optimization,Numerical analysis",Mathematical optimization is directly relevant as the paper introduces a regularization method for inverse problems. Numerical analysis is relevant due to the numerical experiments and convergence analysis. Other fields like calculus or topology are only peripherally related to the core algorithmic contributions.
5796,Low state complexity block codes via convolutional codes,A new class of block codes with low state complexity of their conventional trellis representations called double zero-tail terminated convolutional codes (DZT codes) is introduced. It is shown that there exist DZT-codes meeting the Varshamov-Gilbert bound on the minimum distance and having asymptotically optimal state complexity. Two ways of constructing DZT-codes are considered. Examples of DZT-codes meeting a lower bound on the state complexity are given.,"General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:1.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.2,Applied computing:0.0,Social and professional topics:0.0",Mathematics of computing,"Mathematics of computing is directly relevant as the paper introduces a new class of block codes with mathematical analysis of their properties (e.g., meeting the Varshamov-Gilbert bound). Other categories like 'Computing methodologies' (0.2) are marginally relevant for algorithmic aspects but not the core contribution.","Continuous mathematics:0.1,Discrete mathematics:0.7,Information theory:0.8,Mathematical analysis:0.1,Mathematical software:0.3,Probability and statistics:0.0","Discrete mathematics,Information theory",Discrete mathematics is relevant for the analysis of block code structures. Information theory is relevant for coding theory and error correction. Mathematical software is less directly connected to the theoretical focus of the paper.,"Coding theory:1,Combinatorics:0.3,Graph theory:0",Coding theory,Coding theory: Introduces DZT codes with trellis representations and minimum distance analysis. Rejected options: Combinatorics is only tangentially related to code structure.
60,A Nonlinear Blind Identification Approach to Modeling of Diabetic Patients,"In the last decades, mathematical models have become of great importance in the context of diabetes treatment planning. Several modeling approaches based on first principles or input-output techniques have been proposed. However, a relevant open problem common to all these approaches is that they are not able to recover or to systematically account for the various unmeasured signals that affect a diabetic patient (e.g., food, physical activity, and emotions). A novel blind identification approach is introduced in this brief, allowing us to model type 1 diabetic patients and to effectively recover the unmeasured input signals. The approach is applied to an experimental study regarding identification and prediction of the blood glucose concentration in five type 1 diabetic patients.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:1.0,Information systems:0.5,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.5,Social and professional topics:0.0",Mathematics of computing,"Mathematics of computing: The core contribution is a novel nonlinear blind identification approach for modeling diabetes. Applied computing: Only marginally relevant as it's applied to healthcare, but the primary focus is the mathematical model.","Continuous mathematics:0.2,Discrete mathematics:0.1,Information theory:0.1,Mathematical analysis:0.9,Mathematical software:0.2,Probability and statistics:0.4",Mathematical analysis,Mathematical analysis is highly relevant because the paper introduces a nonlinear modeling approach for diabetic patients. Other mathematical fields are less central to the core contribution.,"Calculus:0,Differential equations:1,Functional analysis:0,Integral equations:0,Mathematical optimization:0,Nonlinear equations:1,Numerical analysis:0,Quadrature:0","Differential equations,Nonlinear equations","Differential equations: The paper models diabetic patients using nonlinear differential equations for blood glucose dynamics. Nonlinear equations: The blind identification approach relies on solving nonlinear systems. Other options like 'Calculus' are too general as the paper specifically addresses nonlinear differential modeling, not general calculus."
2233,Barrier Methods for Optimal Control Problems with State Constraints,"We study barrier methods for state constrained optimal control problems with PDEs. The focus of our analysis is the path of minimizers of the barrier subproblems with the aim to provide a solid theoretical basis for function space oriented path-following algorithms. We establish results on existence, continuity, and convergence of this path. Moreover, we consider the structure of barrier subdifferentials, which play the role of dual variables.","General and reference:0.2,Hardware:0.2,Computer systems organization:0.2,Networks:0.2,Software and its engineering:0.2,Theory of computation:0.2,Mathematics of computing:1.0,Information systems:0.2,Security and privacy:0.2,Human-centered computing:0.2,Computing methodologies:0.2,Applied computing:0.2,Social and professional topics:0.2",Mathematics of computing,Mathematics of computing is highly relevant for the theoretical analysis of barrier methods in optimal control. Other categories lack mathematical focus.,"Continuous mathematics:0.75,Discrete mathematics:0.25,Information theory:0.25,Mathematical analysis:0.9,Mathematical software:0.25,Probability and statistics:0.25","Mathematical analysis,Continuous mathematics","Mathematical analysis is highly relevant as the paper analyzes existence, continuity, and convergence of barrier methods. Continuous mathematics is relevant due to the focus on PDEs. Other categories like Probability and statistics are less relevant as the paper focuses on theoretical analysis rather than statistical methods.","Calculus:0.0,Continuous functions:0.0,Differential equations:0.8,Functional analysis:0.0,Integral equations:0.0,Mathematical optimization:1.0,Nonlinear equations:0.0,Numerical analysis:0.0,Quadrature:0.0,Topology:0.0","Mathematical optimization,Differential equations",Mathematical optimization (1): The paper focuses on barrier methods for optimal control. Differential equations (0.8): The study involves PDE-constrained problems. Other categories like Calculus are irrelevant as the paper addresses optimization theory specifically.
1226,Is a 2000-Year-Old Formula Still Keeping Some Secrets?,"For example, a = b = c = 1 defines an equilateral triangle and the formula gives Area = V3/4. However a = 3, b = c = 1 does not define a triangle, and the formula produces 4 45 This would have made perfect sense to the ancientsthere is no triangle, and there is no number. Today we know more: 4 1.677i is indeed a number, and that creates a problem. Does it mean that a = 3, b = c = 1 really does define some triangle? The formula tantalizes us: for example, a = 1.99, b = c = 1 just barely defines a triangle; the formula correspondingly gives a very small real area, which becomes zero as a increases to 2. As a increases beyond 2, the sides at first just miss forming a triangle, and the formula produces small imaginary numbers; as a increases, the sides get further and further away from forming a triangle, and the imaginary numbers grow. It seems that the formula is tracking triangles we don't see, and is reporting their areas to us. What's going on? After two millennia, is Heron's Formula still keeping some secrets?","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.9,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Mathematics of computing,Mathematics of computing: The paper analyzes Heron's formula's mathematical properties and behavior with non-triangular inputs. Other categories are irrelevant as the work is purely mathematical and unrelated to computer science topics.,"Continuous mathematics:0.75,Discrete mathematics:0.0,Information theory:0.0,Mathematical analysis:1.0,Mathematical software:0.0,Probability and statistics:0.0",Mathematical analysis,"Mathematical analysis is highly relevant as the paper explores the geometric implications of Heron's formula. Continuous mathematics receives a moderate score due to the focus on real/complex analysis, but the core contribution is more aligned with mathematical analysis.","Calculus:0.1,Differential equations:0.1,Functional analysis:0.1,Integral equations:0.1,Mathematical optimization:0.2,Nonlinear equations:0.3,Numerical analysis:0.8,Quadrature:0.4",Numerical analysis,"The paper analyzes Heron's formula through numerical methods and imaginary results, fitting numerical analysis. Other fields are not directly addressed."
3607,Observability quadratic normal form for discrete-time systems,"This note deals with quadratic observability normal form for nonlinear discrete-time single-input-single-output (SISO) system. First of all, the main concept of quadratic equivalence with respect to the observability property, is introduced for discrete-time systems. Subsequently, normal form structure for discrete time system is developed for system with unobservable linear approximation in one direction. Finally, the effect of the so-called resonant terms on the observer design and synchronization of chaotic systems is pointed out in an illustrative example.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.25,Mathematics of computing:1.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Mathematics of computing,"Mathematics of computing is highly relevant as the paper focuses on theoretical aspects of nonlinear discrete-time systems and observability. Other categories like Theory of computation are less directly related as the core contribution is mathematical system theory, not computational algorithms.","Continuous mathematics:0.05,Discrete mathematics:1.0,Information theory:0.2,Mathematical analysis:0.3,Mathematical software:0.05,Probability and statistics:0.1",Discrete mathematics,Discrete mathematics is directly relevant as the paper focuses on discrete-time system observability and normal forms. Other categories like Mathematical analysis or Information theory are secondary but not central to the core contribution.,"Combinatorics:0.1,Graph theory:0.1",,"The paper focuses on nonlinear system observability and normal forms, which are mathematical control theory topics. Neither Combinatorics nor Graph theory are relevant to the core contributions."
3143,On graphs with subgraphs having large independence numbers,"Let G be a graph on n vertices in which every induced subgraph on ${s}={\log}^{3}\, {n}$ vertices has an independent set of size at least ${t}={\log}\, {n}$. What is the largest ${q}={q}{(n)}$ so that every such G must contain an independent set of size at least q? This is one of the several related questions raised by Erdős and Hajnal. We show that ${q}{(n)}= \Theta({\log}^{2} {n}/{\log}\, {\log} \,{n})$, investigate the more general problem obtained by changing the parameters s and t, and discuss the connection to a related Ramsey‐type problem. © 2007 Wiley Periodicals, Inc. J Graph Theory 56: 149–157, 2007","General and reference:0.1,Hardware:0.1,Computer systems organization:0.2,Networks:0.1,Software and its engineering:0.3,Theory of computation:0.2,Mathematics of computing:0.9,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.4,Applied computing:0.2,Social and professional topics:0.1",Mathematics of computing,Mathematics of computing: Focuses on theoretical graph theory and independence number analysis. Other categories like Theory of computation are less relevant as the paper is purely mathematical and not algorithmic or computational.,"Continuous mathematics:0.0,Discrete mathematics:0.9,Information theory:0.0,Mathematical analysis:0.0,Mathematical software:0.0,Probability and statistics:0.0",Discrete mathematics,Discrete mathematics is the primary domain as the paper addresses graph theory and independence number problems. Other categories like Probability are not central to this work.,"Combinatorics:1.0,Graph theory:1.0","Combinatorics,Graph theory",Combinatorics: The paper addresses combinatorial properties of graphs related to independence numbers. Graph theory: The core subject is graph theory with a focus on subgraph independence.
2608,Pole Assignment for a Vibrating System with Aerodynamic Effect,"This paper deals with a pole assignment problem by single-input state feedback control arising from a one-dimensional vibrating system with aerodynamic effect. On the practical side, we derive explicit formulae for the required controlling force terms, which can reassign part of the spectrum to the desired values while leaving the remaining spectrum unchanged. On the mathematical side, unlike the classical Sturm--Liouville problem, our eigenvalue problem is associated with a cubic pencil with unbounded operators as coefficients and has many interesting new features, one of which is that a new controllability condition appears. This condition together with the known controllability condition in the quadratic case are necessary and sufficient. This sheds light on the adjustment of the model parameters. We also analyze the spectrum of the associated noncompact operator and in particular show that the discrete spectrums of controlled and uncontrolled systems lie outside a closed interval on the negative real axis.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:1.0,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Mathematics of computing,"Mathematics of computing is highly relevant as the paper focuses on mathematical models, eigenvalue problems, and control theory. Other categories like Hardware or Software are not addressed.","Continuous mathematics:0.6,Discrete mathematics:0.1,Information theory:0.1,Mathematical analysis:1.0,Mathematical software:0.1,Probability and statistics:0.1",Mathematical analysis,Mathematical analysis is highly relevant as the paper analyzes eigenvalue problems and control theory for vibrating systems. 'Continuous mathematics' is moderately relevant but less directly tied to the core analysis. Other options are irrelevant.,"Calculus:0.2,Differential equations:1,Functional analysis:0.3,Integral equations:0,Mathematical optimization:0.1,Nonlinear equations:0.2,Numerical analysis:0.1,Quadrature:0",Differential equations,Differential equations is directly relevant as the paper addresses pole assignment in a vibrating system modeled by differential equations. Other mathematical categories are not core to the formulation.
2374,On SSP-Closedness in L-topological Spaces,"By means of strongly semi-preopen L-sets and their inequality, a new form of SSP-closedness is introduced in L-topological spaces, where L is a complete De Morgan algebra. This new form does not depend on the structure of basis lattice L and L does not require any distributivity.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.9,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Mathematics of computing,"Mathematics of computing is highly relevant as the paper introduces a new form of SSP-closedness in L-topological spaces, a mathematical concept. Other fields like Hardware or Networks are not related.","Continuous mathematics:0.8,Discrete mathematics:0.2,Information theory:0.1,Mathematical analysis:0.75,Mathematical software:0.1,Probability and statistics:0.1","Mathematical analysis,Continuous mathematics",Mathematical analysis is central to studying L-topological spaces. Continuous mathematics is relevant for topological structures. Discrete mathematics is irrelevant as the focus is on continuous L-sets.,"Calculus:0.0,Continuous functions:0.0,Differential equations:0.0,Functional analysis:0.0,Integral equations:0.0,Mathematical optimization:0.0,Nonlinear equations:0.0,Numerical analysis:0.0,Quadrature:0.0,Topology:1.0",Topology,Topology is directly relevant as the paper introduces SSP-closedness in L-topological spaces. All other options are unrelated to the topic of topological spaces and lattice-based structures.
3861,Nonparametric factor analysis with beta process priors,"We propose a nonparametric extension to the factor analysis problem using a beta process prior. This beta process factor analysis (BP-FA) model allows for a dataset to be decomposed into a linear combination of a sparse set of factors, providing information on the underlying structure of the observations. As with the Dirichlet process, the beta process is a fully Bayesian conjugate prior, which allows for analytical posterior calculation and straightforward inference. We derive a varia-tional Bayes inference algorithm and demonstrate the model on the MNIST digits and HGDP-CEPH cell line panel datasets.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.8,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Mathematics of computing,"Mathematics of computing: The paper presents a nonparametric Bayesian factor analysis model using beta processes, which is a mathematical computing contribution. No other categories capture the statistical methodology focus.","Continuous mathematics:0.3,Discrete mathematics:0.3,Information theory:0.5,Mathematical analysis:0.3,Mathematical software:0.2,Probability and statistics:1.0",Probability and statistics,Probability and statistics is directly relevant to the Bayesian factor analysis model. Other fields like Information theory are not central to the statistical methodology.,"Distribution functions:0,Multivariate statistics:0,Nonparametric statistics:1,Probabilistic algorithms:0,Probabilistic inference problems:1,Probabilistic reasoning algorithms:0,Probabilistic representations:0,Statistical paradigms:0,Stochastic processes:0","Nonparametric statistics,Probabilistic inference problems","The paper uses nonparametric factor analysis with beta process priors, aligning with both categories. Other probabilistic/statistical options are less relevant."
934,A simple information theoretic proof of the maximum entropy property of some Gaussian random fields,"A well known result of Burg (1967) and Kunsch (1981) identifies a Gaussian Markov random field with autocovariances specified on a finite part L of the n-dimensional integer lattice, as the random field with maximum entropy among all random fields with same autocovariance values on L. A simple information theoretic proof of a version of the maximum entropy theorem for random fields in n dimensions is presented in the special case that the given autocovariances are compatible with a unilateral autoregressive process.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.9,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Mathematics of computing,Mathematics of computing: The paper is about mathematical proofs and information theory.,"Continuous mathematics:0.1,Discrete mathematics:0.1,Information theory:1.0,Mathematical analysis:0.1,Mathematical software:0.1,Probability and statistics:0.75","Information theory,Probability and statistics","Information theory: Presents an information-theoretic proof for entropy maximization. Probability and statistics: Involves Gaussian random fields. Other categories (e.g., continuous mathematics) are irrelevant as the focus is on information-theoretic properties.","Coding theory:0.1,Distribution functions:0.6,Multivariate statistics:0.4,Nonparametric statistics:0.2,Probabilistic algorithms:0.5,Probabilistic inference problems:0.3,Probabilistic reasoning algorithms:0.3,Probabilistic representations:0.5,Statistical paradigms:0.3,Stochastic processes:0.7","Stochastic processes,Distribution functions","Stochastic processes: The paper analyzes Gaussian random fields over lattices, a stochastic process. Distribution functions: The work involves probability distributions (Gaussian) and their properties. Other categories like Multivariate statistics are less central as the focus is on entropy maximization in random fields."
3283,On Choquet Integrals with Respect to a Fuzzy Complex Valued Fuzzy Measure of Fuzzy Complex Valued Functions,"In this paper, using fuzzy complex valued functions and fuzzy complex valued fuzzy measures ([11]) and interval-valued Choquet integrals ([2-6]), we define Choquet integral with respect to a fuzzy complex valued fuzzy measure of a fuzzy complex valued function and investigate some basic properties of them.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.9,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Mathematics of computing,Mathematics of computing is relevant for theoretical analysis of fuzzy measure integrals. Other categories lack mathematical foundations.,"Continuous mathematics:0.8,Discrete mathematics:0.0,Information theory:0.0,Mathematical analysis:0.9,Mathematical software:0.1,Probability and statistics:0.0","Mathematical analysis,Continuous mathematics",Mathematical analysis is central to the definition of Choquet integrals. Continuous mathematics applies to fuzzy complex measures and integrals. Other options like Information theory are not directly relevant.,"Functional analysis:1,Integral equations:0.3,Calculus:0.2,Continuous functions:0.1,Differential equations:0.1,Mathematical optimization:0.2,Nonlinear equations:0.1,Numerical analysis:0.4,Quadrature:0.1,Topology:0",Functional analysis,Functional analysis: The paper defines and analyzes Choquet integrals in the context of fuzzy measures. Other children rejected: The focus is on integral theory in functional spaces rather than numerical methods or optimization.
1843,Competitive Equilibria of Economies with a Continuum of Consumers and Aggregate Shocks,"This paper studies competitive equilibria of a production economy with aggregate productivity shocks. There is a continuum of consumers who face borrowing constraints and individual labor endowment shocks. The dynamic economy is described in terms of sequences of aggregate distributions. The existence of sequential competitive equilibria is proven and a recursive characterization is established. In particular, it is shown that for any sequential competitive equilibrium, there exists a payoff equivalent sequential competitive equilibrium that is generated by a suitably defined recursive equilibrium with state variables including continuation value.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.2,Mathematics of computing:0.9,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Mathematics of computing,"Mathematics of computing: The paper focuses on mathematical proofs and theoretical models of competitive equilibria in economics, which aligns with mathematical foundations in computing. Other categories like Theory of computation (0.2) are only marginally relevant due to recursive characterization techniques. The rest are irrelevant as the paper is not about algorithms, systems, or applications.","Continuous mathematics:0.8,Discrete mathematics:0.1,Information theory:0.1,Mathematical analysis:0.75,Mathematical software:0.1,Probability and statistics:0.2","Mathematical analysis,Continuous mathematics","Mathematical analysis is relevant for proving existence and establishing recursive characterization of equilibria. Continuous mathematics is relevant due to the analysis of aggregate distributions over time. Probability and statistics receive moderate relevance for modeling random shocks, but the core contribution is in mathematical analysis.","Calculus:0.2,Continuous functions:0.3,Differential equations:0.4,Functional analysis:0.5,Integral equations:0.2,Mathematical optimization:0.8,Nonlinear equations:0.4,Numerical analysis:0.3,Quadrature:0.1,Topology:0.1","Mathematical optimization,Functional analysis",Mathematical optimization is primary as the paper studies competitive equilibria in dynamic economic models. Functional analysis is relevant for characterizing recursive equilibria with aggregate distributions. Other categories like differential equations are less directly addressed.
669,A Rate-Compatible Low-Density Parity-Check Convolutional Coding Scheme Using Informed Dynamic Scheduling,"Low-density parity-check convolutional codes (LDPC-CCs) are generally decoded using sliding- window based message passing decoding. Based on the sliding-window decoding, an informed dynamic scheduling (IDS) for LDPC-CC is proposed in this work, where the decoding convergence can be significantly accelerated. Since the number of processors required for a satisfactory performance can be reduced, the decoder can be simplified. A set of rate-compatible (RC) puncturing patterns is also proposed and is used to construct RC LDPC-CCs for the performance evaluation of the proposed IDS. Although the proposed puncturing pattern cannot be optimized for individual code rate owing to the rate-compatible constraint, they can still provide a comparable error rate performance compared to the codes defined in the IEEE 802.16m standard. It is worth noting that these standard codes are not rate-compatible.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:1.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Mathematics of computing,"The paper introduces a rate-compatible LDPC-CC decoding scheme with mathematical analysis of error correction and performance evaluation, which is central to coding theory in mathematics.","Continuous mathematics:0.1,Discrete mathematics:0.1,Information theory:0.9,Mathematical analysis:0.1,Mathematical software:0.1,Probability and statistics:0.1",Information theory,Information theory is relevant because the paper discusses LDPC convolutional coding and error correction techniques. Other categories like Discrete mathematics are less central to the paper's focus on coding theory.,Coding theory:1.0,Coding theory,"The paper directly presents a rate-compatible LDPC convolutional coding scheme, which is a core topic in coding theory. No other options are relevant here."
3827,Fast algorithms for polynomial solutions of linear differential equations,"We investigate polynomial solutions of homogeneous linear differential equations with coefficients that are polynomials with integer coefficients. The problems we consider are the existence of nonzero polynomial solutions, the determination of the dimension of the vector space of polynomial solutions, the computation of a basis of this space. Previous algorithms have a bit complexity that is at least quadratic in the largest integer valuation N of formal Laurent series solutions at infinity, even for merely detecting the existence of nonzero polynomial solutions. We give a deterministic algorithm that computes a compact representation of a basis of polynomial solutions in O(Nlog3N) bit operations. We also give a probabilistic algorithm that computes the dimension of the space of polynomial solutions in O(√Nlog2N) bit operations. In general, the integer N is not polynomially bounded in the bit size of the input differential equation. We isolate a class of equations for which detecting nonzero polynomial solutions can be performed in polynomial complexity. We discuss implementation issues and possible extensions.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.9,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Mathematics of computing,"Mathematics of computing: The paper develops algorithms for solving polynomial differential equations, focusing on computational complexity and mathematical methods.","Continuous mathematics:0.0,Discrete mathematics:0.0,Information theory:0.0,Mathematical analysis:0.0,Mathematical software:0.8,Probability and statistics:0.0",Mathematical software,Mathematical software is highly relevant as the paper presents algorithms for polynomial solutions of differential equations. Other categories like Discrete mathematics are not directly relevant to the core contribution.,"Mathematical software performance:1,Solvers:1,Statistical software:0","Mathematical software performance,Solvers","Mathematical software performance and Solvers are highly relevant as the paper presents fast deterministic algorithms for solving differential equations. Statistical software is irrelevant as the focus is on deterministic methods, not statistical analysis."
4105,Optimal Communication Network-Based  $H_\infty $  Quantized Control With Packet Dropouts for a Class of Discrete-Time Neural Networks With Distributed Time Delay,"This paper is concerned with optimal communication network-based H∞ quantized control for a discrete-time neural network with distributed time delay. Control of the neural network (plant) is implemented via a communication network. Both quantization and communication network-induced data packet dropouts are considered simultaneously. It is assumed that the plant state signal is quantized by a logarithmic quantizer before transmission, and communication network-induced packet dropouts can be described by a Bernoulli distributed white sequence. A new approach is developed such that controller design can be reduced to the feasibility of linear matrix inequalities, and a desired optimal control gain can be derived in an explicit expression. It is worth pointing out that some new techniques based on a new sector-like expression of quantization errors, and the singular value decomposition of a matrix are developed and employed in the derivation of main results. An illustrative example is presented to show the effectiveness of the obtained results.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.3,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.9,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.3,Applied computing:0.2,Social and professional topics:0.1",Mathematics of computing,Mathematics of computing is relevant due to the paper's focus on H∞ control theory and linear matrix inequalities. Networks was considered but rejected as the communication aspect is secondary to the mathematical control formulation.,"Continuous mathematics:0.1,Discrete mathematics:0.8,Information theory:0.7,Mathematical analysis:0.75,Mathematical software:0.1,Probability and statistics:0.2","Discrete mathematics,Mathematical analysis,Information theory",Discrete mathematics is relevant for discrete-time systems; Mathematical analysis applies to control theory; Information theory relates to quantization. Continuous mathematics and Probability are less central to the core control design approach.,"Calculus:0.0,Coding theory:0.0,Combinatorics:0.0,Differential equations:1.0,Functional analysis:0.0,Graph theory:0.0,Integral equations:0.0,Mathematical optimization:1.0,Nonlinear equations:0.0,Numerical analysis:0.0,Quadrature:0.0","Mathematical optimization,Differential equations",Mathematical optimization is relevant as the paper formulates controller design as linear matrix inequalities (LMIs) optimization. Differential equations is relevant due to the focus on distributed time-delay systems governed by differential equations. Other fields like Calculus or Functional analysis are less directly addressed in the core contribution.
4636,Defect Prevention Approaches In Medium Scale It Enterprises,"The software industry is successful, if it can draw the complete attention of the customers towards it. This is achievable if the organization can produce a high quality product. To identify a product to be of high quality, it should be free of defects, should be capable of producing expected results. It should be delivered in an estimated cost, time and be maintainable with minimum effort. Defect Prevention is the most critical but often neglected component of the software quality assurance in any project. If applied at all stages of software development, it can reduce the time, cost and resources required to engineer a high quality product.","General and reference:0,Hardware:0,Computer systems organization:0,Networks:0,Software and its engineering:1,Theory of computation:0,Mathematics of computing:0.5,Information systems:0,Security and privacy:0,Human-centered computing:0,Computing methodologies:0.5,Applied computing:0,Social and professional topics:0",Software and its engineering,Software and its engineering is directly relevant for the defect prevention methodology. Computing methodologies is partially relevant but secondary to the software engineering focus. Other fields like Mathematics of computing are not central to this work.,"Software creation and management:0.9,Software notations and tools:0.1,Software organization and properties:0.1",Software creation and management,"Software creation and management is relevant for defect prevention in software quality assurance. Software notations and tools and Software organization and properties are less relevant as the focus is on process and quality, not specific tools or architectures.","Collaboration in software development:0.1,Designing software:0.1,Software development process management:0.9,Software development techniques:0.7,Software post-development issues:0.1,Software verification and validation:0.8","Software development process management,Software verification and validation","Software development process management is relevant for defect prevention strategies. Software verification and validation is relevant for ensuring quality. Designing software is not discussed, and other categories like collaboration are not addressed."
409,Efficient regression testing of ontology-driven systems,"To manage and integrate information gathered from heterogeneous databases, an ontology is often used. Like all systems, ontology-driven systems evolve over time and must be regression tested to gain confidence in the behavior of the modified system. Because rerunning all existing tests can be extremely expensive, researchers have developed regression-test-selection (RTS) techniques that select a subset of the available tests that are affected by the changes, and use this subset to test the modified system. Existing RTS techniques have been shown to be effective, but they operate on the code and are unable to handle changes that involve ontologies. To address this limitation, we developed and present in this paper a novel RTS technique that targets ontology-driven systems. Our technique creates representations of the old and new ontologies, compares them to identify entities affected by the changes, and uses this information to select the subset of tests to rerun. We also describe in this paper OntoRetest, a tool that implements our technique and that we used to empirically evaluate our approach on two biomedical ontology-driven database systems. The results of our evaluation show that our technique is both efficient and effective in selecting tests to rerun and in reducing the overall time required to perform regression testing.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:1.0,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.3,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Software and its engineering,"Software and its engineering is highly relevant because the paper focuses on regression testing techniques for ontology-driven systems, a software maintenance methodology. 'Information systems' is marginally relevant due to ontologies but not the primary focus. Other categories like 'Security and privacy' are irrelevant.","Software creation and management:0.9,Software notations and tools:0.6,Software organization and properties:0.3",Software creation and management,Software creation and management is directly relevant as the paper introduces a novel regression-testing technique for ontology-driven systems. Software notations and tools are only incidentally relevant (OntoRetest tool). Software organization and properties are less central to the core contribution.,"Collaboration in software development:0.0,Designing software:0.0,Software development process management:0.0,Software development techniques:1.0,Software post-development issues:0.0,Software verification and validation:1.0","Software verification and validation,Software development techniques",Software verification and validation is directly relevant as the paper addresses regression testing for ontology-driven systems. Software development techniques is relevant due to the novel test selection methodology. Other options like Designing software are not focused on testing processes.
1926,Orchestrating safe streaming computations with precise control,"Streaming computing is a paradigm of distributed computing that features networked nodes connected by first-in-first-out data channels. Communication between nodes may include not only high-volume data tokens but also infrequent and unpredictable control messages carrying control information, such as data set boundaries, exceptions, or reconfiguration requests. In many applications, it is necessary to order delivery of control messages precisely relative to data tokens, which can be especially challenging when nodes can filter data tokens. Existing approaches, mainly data serialization protocols, do not exploit the low-volume nature of control messages and may not guarantee that synchronization of these messages with data will be free of deadlock. In this paper, we propose an efficient messaging system for adding precisely ordered control messages to streaming applications. We use a credit-based protocol to avoid the need to tag data tokens and control messages. For potential deadlocks caused by filtering behavior and global synchronization, we propose deadlock avoidance solutions and prove their correctness.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.2,Networks:0.1,Software and its engineering:0.9,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Software and its engineering,"Software and its engineering is highly relevant as the paper presents a messaging system for distributed streaming computations. Computer systems organization (0.2) receives a moderate score due to distributed system aspects, but the core contribution is about software protocols for control message ordering.","Software creation and management:0.0,Software notations and tools:1.0,Software organization and properties:0.75","Software notations and tools,Software organization and properties",Software notations and tools is highly relevant because the paper introduces a credit-based messaging protocol for streaming systems. Software organization and properties is relevant for analyzing deadlock avoidance in the protocol design. Software creation and management is less directly tied to the core contribution.,"Compilers:0,Context specific languages:0,Contextual software domains:0,Development frameworks and environments:0.2,Extra-functional properties:1,Formal language definitions:0,General programming languages:0,Software configuration management and version control systems:0,Software functional properties:0,Software libraries and repositories:0,Software maintenance tools:0,Software system structures:1,System description languages:0","Extra-functional properties,Software system structures","Extra-functional properties: The paper addresses correctness and deadlock avoidance in messaging systems, which are critical non-functional aspects. Software system structures: The core contribution is designing a messaging system for distributed streaming applications. Other options like Development frameworks are only tangentially relevant (0.2)."
2647,A Methodology for the Design of Ada Transformation Tools in a DIANA Environment,Implementing and testing transformations incrementally makes DIANA-based Ada software tools easier to debug and more efficient.,"General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.8,Theory of computation:0.2,Mathematics of computing:0.2,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.3,Applied computing:0.1,Social and professional topics:0.1",Software and its engineering,Software and its engineering is relevant for the methodology of designing Ada transformation tools. Other categories like Theory of computation are less relevant as the paper focuses on software development practices rather than theoretical models.,"Software creation and management:0.6,Software notations and tools:1.0,Software organization and properties:0.3",Software notations and tools,Software notations and tools is directly relevant to Ada transformation tools in DIANA environments. Software creation and management is secondary for implementation aspects. Software organization and properties is less relevant as the focus is on tool methodology.,"Compilers:0.2,Context specific languages:0,Development frameworks and environments:1,Formal language definitions:0,General programming languages:0.3,Software configuration management and version control systems:0,Software libraries and repositories:0,Software maintenance tools:0,System description languages:0.7","Development frameworks and environments,System description languages",Development frameworks and environments are central to the methodology for DIANA-based tools. System description languages (DIANA) are relevant. Compilers and general programming languages are less directly related.
4961,Specifying and verifying software,"Software verification presents many challenges. One of these isproviding programmers with automated tool support for verification, another is providing specification support that captures common programming idioms. In this talk, I will discuss these two challenges, drawing from experience with building program verifiersfor Spec# and C. I will also give a demo of the Spec# programming system, which includes the automatic static program verifier Boogie.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.9,Theory of computation:0.3,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Software and its engineering,"Software and its engineering: Focuses on tools (Spec#, Boogie) for program verification. Other categories like Theory of computation are rejected as the core contribution is software engineering practices.","Software creation and management:0.3,Software notations and tools:1.0,Software organization and properties:0.8","Software notations and tools,Software organization and properties",Software notations and tools is central to the Boogie verifier and Spec#. Software organization and properties applies to verification of software correctness. Software creation and management is less relevant as the focus is on verification rather than development lifecycle.,"Compilers:0,Context specific languages:0,Contextual software domains:0,Development frameworks and environments:1,Extra-functional properties:0,Formal language definitions:1,General programming languages:0,Software configuration management and version control systems:0,Software functional properties:1,Software libraries and repositories:0,Software maintenance tools:0,Software system structures:0,System description languages:0","Formal language definitions,Software functional properties,Development frameworks and environments",Formal language definitions: The paper centers on formal verification and specification languages like Spec#. Software functional properties: Verification ensures correct functional behavior. Development frameworks and environments: Spec# is a programming environment with verification tools. Other categories like compilers or configuration management are not discussed.
257,Failure mode assumptions and assumption coverage,A method is proposed for the formal analysis of failure mode assumptions and for the evaluation of the dependability of systems whose design correctness is conditioned on the validity of such assumptions. Formal definitions are given for the types of errors that can affect items of service delivered by a system or component. Failure node assumptions are then formalized as assertions on the types of errors that a component may induce in its enclosing system. The concept of assumption coverage is introduced to relate the notion of partially-ordered assumption assertions to the quantification of system dependability. Assumption coverage is shown to be extremely important in systems requiring very high dependability. It is also shown that the need to increase system redundancy to accommodate more severe modes of component failure can sometimes result in a decrease in dependability.<<ETX>>,"General and reference:0.05,Hardware:0.05,Computer systems organization:0.05,Networks:0.05,Software and its engineering:0.8,Theory of computation:0.05,Mathematics of computing:0.1,Information systems:0.05,Security and privacy:0.05,Human-centered computing:0.05,Computing methodologies:0.2,Applied computing:0.05,Social and professional topics:0.05",Software and its engineering,Software and its engineering is highly relevant as the paper discusses failure mode analysis and dependability in systems. Other categories like Computing methodologies are secondary as the focus is on system design principles.,"Software creation and management:0.05,Software notations and tools:0.1,Software organization and properties:0.85",Software organization and properties,Software organization and properties: The paper analyzes failure mode assumptions and their impact on system dependability. Other categories like Software creation and management are less relevant as the focus is on formal dependability analysis.,"Contextual software domains:0.1,Extra-functional properties:1.0,Software functional properties:0.2,Software system structures:0.4",Extra-functional properties,Extra-functional properties: The paper directly addresses dependability and failure mode analysis. Other options like Software system structures are secondary to the focus on dependability evaluation.
2718,Backtrack Programming,"A widely used method of efftcient search is examined in detail. This examination provides the opportunity to formulate its scope and methods in their full generality. In addL tion to a general exposition of the basic process, some important refinemertts are indicated. Examples are given which illustrate the salient features of this searching process.","General and reference:0,Hardware:0,Computer systems organization:0,Networks:0,Software and its engineering:1,Theory of computation:0.2,Mathematics of computing:0,Information systems:0,Security and privacy:0,Human-centered computing:0,Computing methodologies:0,Applied computing:0,Social and professional topics:0",Software and its engineering,Software and its engineering is relevant as the paper discusses backtrack programming techniques for efficient search in software applications. Theory of computation is only tangentially related to the algorithmic analysis.,"Software creation and management:0.8,Software notations and tools:0.2,Software organization and properties:0.3",Software creation and management,Software creation and management is highly relevant as the paper discusses backtrack programming techniques for efficient search. Other categories are less directly related to the core programming methodology.,"Collaboration in software development:0,Designing software:0,Software development process management:0,Software development techniques:1,Software post-development issues:0,Software verification and validation:0",Software development techniques,"The paper discusses backtrack programming as a general method for efficient search, which is a core software development technique. Other options like 'Software verification and validation' are not mentioned."
3819,Reducing Regression Test Size by Exclusion,"Operational software is constantly evolving. Regression testing is used to identify the unintended consequences of evolutionary changes. As most changes affect only a small proportion of the system, the challenge is to ensure that the regression test set is both safe (all relevant tests are used) and inclusive (only relevant tests are used). Previous approaches to reducing test sets struggle to find safe and inclusive tests by looking only at the changed code. We use decomposition program slicing to safely reduce the size of regression test sets by identifying those parts of a system that could not have been affected by a change; this information will then direct the selection of regression tests by eliminating tests that are not relevant to the change. The technique properly accounts for additions and deletions of code. We extend and use Rothermel and Harrold's framework for measuring the safety of regression test sets and introduce new safety and precision measures that do not require a priori knowledge of the exact number of modification-revealing tests. We then analytically evaluate and compare our techniques for producing reduced regression test sets.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.8,Theory of computation:0.1,Mathematics of computing:0.2,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.3,Applied computing:0.1,Social and professional topics:0.1",Software and its engineering,"Software and its engineering is highly relevant as the paper presents a software testing methodology for regression test reduction. Computing methodologies receives moderate relevance due to algorithmic contributions, but the primary domain is software engineering.","Software creation and management:0.9,Software notations and tools:0.7,Software organization and properties:0.3","Software creation and management,Software notations and tools",Software creation and management: The paper addresses regression test optimization during software evolution. Software notations and tools: Uses program slicing techniques. Software organization and properties is less directly relevant.,"Collaboration in software development:0.2,Compilers:0.1,Context specific languages:0.1,Designing software:0.3,Development frameworks and environments:0.4,Formal language definitions:0.1,General programming languages:0.1,Software configuration management and version control systems:0.2,Software development process management:0.3,Software development techniques:1.0,Software libraries and repositories:0.1,Software maintenance tools:0.5,Software post-development issues:0.2,Software verification and validation:1.0,System description languages:0.1","Software development techniques,Software verification and validation",Software development techniques are central to the paper's method of test set reduction. Software verification and validation are relevant due to the safety measures evaluated. Software maintenance tools are secondary but mentioned in the context of test selection.
532,Fragment class analysis for testing of polymorphism in Java software,"Testing of polymorphism in object-oriented software may require coverage of all possible bindings of receiver classes and target methods at call sites. Tools that measure this coverage need to use class analysis to compute the coverage requirements. However, traditional whole-program class analysis cannot be used when testing incomplete programs. To solve this problem, we present a general approach for adapting whole-program class analyses to operate on program fragments. Furthermore, since analysis precision is critical for coverage tools, we provide precision measurements for several analyses by determining which of the computed coverage requirements are actually feasible for a set of subject components. Our work enables the use of whole-program class analyses for testing of polymorphism in partial programs, and identifies analyses that potentially are good candidates for use in coverage tools.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:1.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Software and its engineering,Software and its engineering is highly relevant as the paper focuses on testing techniques for polymorphism in Java. Other categories like 'Theoretical computer science' or 'Mathematics of computing' are not central to the software analysis contribution.,"Software creation and management:0.3,Software notations and tools:1.0,Software organization and properties:0.2",Software notations and tools,Software notations and tools is directly relevant as the paper presents a novel method for class analysis in polymorphism testing. Other categories are less aligned with the tool-focused contribution.,"Compilers:0.3,Context specific languages:0.1,Development frameworks and environments:0.2,Formal language definitions:0.1,General programming languages:0.1,Software configuration management and version control systems:0.1,Software libraries and repositories:0.1,Software maintenance tools:0.8,System description languages:0.1",Software maintenance tools,"Software maintenance tools is highly relevant for the fragment analysis technique in testing. Compilers are secondary if the analysis involves compiler-like methods, but the focus is on testing rather than compilation."
4201,Explicitly distributed AOP using AWED,"Distribution-related concerns, such as data replication, often crosscut the business code of a distributed application. Currently such crosscutting concerns are frequently realized on top of distributed frameworks, such as EJBs, and initial AO support for the modularization of such crosscutting concerns, e.g., JBoss AOP and Spring AOP, has been proposed.Based on an investigation of the implementation of replicated caches using JBoss Cache, we motivate that crosscutting concerns of distributed applications benefit from an aspect language for explicit distributed programming. We propose AWED, a new aspect language with explicit distributed programming mechanisms, which provides three contributions. First, remote pointcut constructors which are more general than those of previous related approaches, in particular, supporting remote sequences. Second, a notion of distributed advice with support for asynchronous and synchronous execution. Third, a notion of distributed aspects including models for the deployment, instantiation and state sharing of aspects. We show several concrete examples how AWED can be used to modularly implement and extend replicated cache implementations. Finally, we present a prototype implementation of AWED, which we have realized by extending JAsCo, a system providing dynamic aspects for Java.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.9,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Software and its engineering,Software and its engineering is highly relevant as the paper introduces a new aspect-oriented programming language (AWED) for distributed systems. Other categories like Hardware or Networks are less relevant because the core contribution is the software design for modularizing distributed concerns.,"Software creation and management:0.4,Software notations and tools:1.0,Software organization and properties:0.8","Software notations and tools,Software organization and properties",Software notations and tools is relevant for the AWED aspect language design. Software organization and properties applies to the implementation and deployment models. Software creation and management is less central to the language design focus.,"Compilers:0.1,Context specific languages:0.8,Contextual software domains:0.2,Development frameworks and environments:0.9,Extra-functional properties:0.3,Formal language definitions:0.2,General programming languages:0.4,Software configuration management and version control systems:0.1,Software functional properties:0.3,Software libraries and repositories:0.5,Software maintenance tools:0.1,Software system structures:0.6,System description languages:0.7","Context specific languages,Development frameworks and environments,System description languages",Context specific languages (AWED is a new aspect language for distributed programming). Development frameworks and environments (AWED extends JAsCo for dynamic aspects). System description languages (AWED provides explicit distributed programming constructs). Other options like Compilers or Formal language definitions are irrelevant as the focus is on language design and frameworks.
5051,Illustrating Client and Implementation Readability Tradeoffs in Ada and C++,"The relative merits of Ada and C++ have fostered numerous discussions among software developers. With the release of Ada 95 and its object‐oriented features, we expect these discussions to intensify, particularly given the increasing number of features for which there are implementations in both languages. In this paper, we compare Ada and C++ based on the readability of class and client implementations. We examine this issue using a case study. Our case study illustrates a partial tradeoff that exists between the readability of a class and the readability of its clients. This tradeoff depends on the degree to which overloading is used; extensive use of overloading adds to class complexity, although it increases client readability.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.05,Software and its engineering:0.9,Theory of computation:0.05,Mathematics of computing:0.05,Information systems:0.05,Security and privacy:0.0,Human-centered computing:0.05,Computing methodologies:0.05,Applied computing:0.05,Social and professional topics:0.0",Software and its engineering,Software and its engineering: The paper compares programming language features (Ada and C++) and their impact on software readability. Other categories are rejected because the paper focuses on programming language design and software engineering principles.,"Software creation and management:0.8,Software notations and tools:0.3,Software organization and properties:1.0",Software organization and properties,Software organization and properties is highly relevant as the paper analyzes language design tradeoffs affecting code readability. Software creation and management is moderately relevant for the broader software development context. Software notations and tools is less relevant as the paper doesn't introduce new tools.,"Contextual software domains:0.1,Extra-functional properties:0.9,Software functional properties:0.8,Software system structures:0.1","Extra-functional properties,Software functional properties",Extra-functional properties: Readability is a key non-functional quality metric. Software functional properties: The study evaluates how language features (like overloading) impact code functionality and client interactions. Other categories are irrelevant to the paper's focus.
894,An Introduction to Software Architecture,"As the size of software systems increases, the algorithms and data structures of the computation no longer constitute the major design problems. When systems are constructed from many components, the organization of the overall system -- the software architecture -- presents a new set of design problems. This level of design has been addressed in a number of ways including informal diagrams and descriptive terms, module interconnection languages, templates and frameworks for systems that serve the needs of specific domains, and formal models of component integration mechanisms. In this paper we provide an introduction to the emerging field of software architecture. We begin by considering a number of common architectural styles upon which many systems are currently based and show how different styles can be combined in a single design. Then we present six case studies to illustrate how architectural representations can improve our understanding of complex software systems. Finally, we survey some of the outstanding problems in the field, and consider a few of the promising research directions.","General and reference:0.1,Hardware:0.2,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.9,Theory of computation:0.3,Mathematics of computing:0.2,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.4,Applied computing:0.3,Social and professional topics:0.1",Software and its engineering,Software and its engineering: Directly addresses software architecture design principles and case studies. Other categories like Theory of computation or Applied computing are less relevant as the paper focuses on architectural styles and software design challenges.,"Software creation and management:0.3,Software notations and tools:0.2,Software organization and properties:0.9",Software organization and properties,"Software organization and properties is directly relevant as the paper focuses on architectural styles, system organization, and design principles. The other categories are less relevant as the paper does not primarily discuss creation/management processes or notation tools.","Contextual software domains:0.2,Extra-functional properties:0.2,Software functional properties:0.2,Software system structures:1.0",Software system structures,"Software system structures is highly relevant as the paper introduces and analyzes architectural styles and system organization. Other categories are less relevant since the paper focuses on structural design rather than functional properties, contextual domains, or extra-functional attributes."
4524,Design Strategy and Software Design Effectiveness,"Software design is about a sequence of steps taken to achieve a goal. Designers must plan their approach to carrying out these steps. In studying designers at work, the authors observed breadth- versus depth-first approaches to design-space exploration and problem- versus solution-driven approaches during the actual design. Which approaches and when to use them are important to effective design. The authors suggest four archetypical strategies that designers can choose under different circumstances, thus making design strategy one of the early design decisions.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.9,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Software and its engineering,Software and its engineering is highly relevant as the paper discusses design strategies in software development. Computing Methodologies was rejected because the focus is on design strategy as a software engineering decision rather than general algorithmic methods.,"Software creation and management:1.0,Software notations and tools:0.5,Software organization and properties:0.3",Software creation and management,"Software creation and management is highly relevant for analyzing design strategies in software development. Software notations receive a moderate score for methodological descriptions, but the core focus is on management of the design process.","Collaboration in software development:0.1,Designing software:1.0,Software development process management:0.1,Software development techniques:0.8,Software post-development issues:0.1,Software verification and validation:0.1","Designing software,Software development techniques",Designing software is highly relevant for the focus on design strategies. Software development techniques is relevant for the exploration of breadth/depth approaches. Other categories are less aligned with the paper's focus.
6005,Software refinement with Perfect Developer,"Perfect Developer is a software tool that supports the formal development of object-oriented programs by refinement, including formal verification of code. It is built around a single language that supports both specification and implementation. We critically examine how Perfect Developer supports programming by refinement, focusing on three refinement techniques: algorithm refinement, data refinement and delta refinement. In particular we examine the extent to which Perfect Developer provides formal verification for these techniques. We assess it as a tool for software construction and compare it with related tools.","General and reference:0.2,Hardware:0.2,Computer systems organization:0.3,Networks:0.2,Software and its engineering:1.0,Theory of computation:0.3,Mathematics of computing:0.3,Information systems:0.3,Security and privacy:0.2,Human-centered computing:0.2,Computing methodologies:0.5,Applied computing:0.3,Social and professional topics:0.2",Software and its engineering,Software and its engineering: The paper evaluates a tool for formal software refinement and verification. Other fields like Computing methodologies are tangential but not central to the tool's purpose.,"Software notations and tools:1.0,Software creation and management:0.3,Software organization and properties:0.2",Software notations and tools,Software notations and tools: The paper evaluates Perfect Developer as a tool for formal software development and verification. Software creation and management is secondary as the focus is on refinement techniques rather than the process of software creation.,"Compilers:0.0,Context specific languages:0.0,Development frameworks and environments:1.0,Formal language definitions:0.5,General programming languages:0.0,Software configuration management and version control systems:0.0,Software libraries and repositories:0.0,Software maintenance tools:0.0,System description languages:0.0",Development frameworks and environments,Development frameworks and environments: Paper evaluates Perfect Developer as a software development tool. Formal language definitions (0.5) is secondary as tool supports specification but not core focus.
3528,The Single Model Principle,"We contrast modelling languages that are founded on use of a single model with those founded on use of multiple models. We propose that to best support seamless and reversible development of reliable software, languages that follow the single model principle are superior. We define this principle precisely, and discuss when it is insufficient, particularly for early requirements engineering.","General and reference:0.1,Hardware:0.2,Computer systems organization:0.3,Networks:0.2,Software and its engineering:0.9,Theory of computation:0.3,Mathematics of computing:0.2,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.2,Computing methodologies:0.3,Applied computing:0.3,Social and professional topics:0.1",Software and its engineering,Software and its engineering is highly relevant as the paper discusses modeling principles for software development. Other categories like Human-centered computing are less relevant as the focus is on software modeling rather than user interaction.,"Software creation and management:0.0,Software notations and tools:0.0,Software organization and properties:1.0",Software organization and properties,Software organization and properties is relevant for the modeling principle analysis. Other software categories like creation or tools are not central to the theoretical contribution.,"Contextual software domains:0.1,Extra-functional properties:0.1,Software functional properties:0.1,Software system structures:0.8",Software system structures,Software system structures is highly relevant as the paper discusses the use of single vs. multiple models in software development. Other categories like Extra-functional properties are not directly related to the structural principles discussed.
5736,A3ME — generic middleware for information exchange in heterogeneous environments,"This paper presents a proof of concept for a generic middleware for heterogeneous sensor/actuator networks, which enables ad-hoc discovery, self-description exchange and basic interactions between heterogeneous devices. The main approach is to represent each device by a device-agent which knows its capabilities, constraints and policies, and is able to describe those to other devices on request.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.2,Networks:0.3,Software and its engineering:0.7,Theory of computation:0.2,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.3,Applied computing:0.2,Social and professional topics:0.1",Software and its engineering,"Software and its engineering: The paper presents a middleware system for heterogeneous devices, focusing on software design and interaction protocols. Other categories like 'Computer systems organization' are less central as the contribution is middleware architecture rather than hardware or low-level systems.","Software creation and management:0.8,Software notations and tools:0.3,Software organization and properties:0.7","Software creation and management,Software organization and properties",Software creation and management is relevant for developing generic middleware. Software organization and properties is relevant for structuring device-agent interactions. Software notations and tools is less relevant as the paper focuses on middleware architecture over notation design.,"Software system structures:1,Software development techniques:1,Software functional properties:0.7,Software development process management:0.6,Contextual software domains:0.5,Designing software:0.5,Extra-functional properties:0.4,Software verification and validation:0.3,Software post-development issues:0.2,Collaboration in software development:0.1","Software system structures,Software development techniques",Software system structures (agent-based middleware architecture) and Software development techniques (self-description exchange protocol). Software functional properties and process management are secondary as the focus is on structural and design aspects rather than functional behavior or process.
4738,Adaptive commitment for distributed real-time transactions,"Distributed real-time transaction systems are useful for both real-time and high-performance database applications. Standard transaction management approaches that use the two-phase commit protocol suffer from its high costs and blocking behavior which is problematic in real-time computing environments. Our approach in this paper is to identify ways in which a commit protocol can be made adaptive in the sense that under situations that demand it, such as a transient local overload, the system can dynamically change to a different commitment strategy. The decision to do so can be taken autonomously at any site. The different commitment strategies exploit a trade-off between the cost of commitment and the obtained degree of atomicity. Our protocols are based on optimistic commitment strategies, and they rely on local compensatory actions to recover from non-atomic executions. We provide the necessary framework to study the logical and temporal correctness criteria, and we describe examples to illustrate the use of our strategies.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:1.0,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.2,Social and professional topics:0.1",Software and its engineering,"Software and its engineering is directly relevant as the paper focuses on distributed transaction protocols (commitment strategies), a core database systems topic. Other fields are irrelevant as the work is theoretical and systems-oriented.","Software creation and management:0.2,Software notations and tools:0.3,Software organization and properties:1.0",Software organization and properties,Software organization and properties is directly relevant for analyzing distributed transaction protocols and their correctness properties.,"Contextual software domains:0.1,Extra-functional properties:0.8,Software functional properties:0.6,Software system structures:0.3","Extra-functional properties,Software functional properties",Extra-functional properties: The paper focuses on real-time transaction properties like reliability and performance. Software functional properties: It discusses different commitment strategies as functional aspects of the system. Other options are less relevant to the transaction management focus.
5550,DSCMC: Distributed Stateless Code Model Checker,"Stateless code model checking is an effective verification technique, which is more applicable than stateful model checking to the software world. Existing stateless model checkers support the verification of neither LTL formulae nor the information flow security properties. This paper proposes a distributed stateless code model checker (DSCMC) designed based on the Actor model, and has the capability of verifying code written in different programming languages. This tool is implemented using Erlang, which is an actor-based programming language. DSCMC is able to detect deadlocks, livelocks, and data races automatically. In addition, the tool can verify information flow security and the properties specified in LTL. Thanks to its actor-based architecture, DSCMC provides a wide range of capabilities. The parallel architecture of the tool exploiting the rich concurrency model of Erlang is suited to the time-intensive process of stateless code model checking.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.3,Networks:0.0,Software and its engineering:0.9,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Software and its engineering,"Software and its engineering: The paper introduces a distributed model checker for verifying software using the Actor model and Erlang. Other categories are rejected because the focus is on software verification tools rather than hardware, systems organization, or theoretical computation.","Software creation and management:0.2,Software notations and tools:0.95,Software organization and properties:0.4",Software notations and tools,Software notations and tools receives high score as the paper introduces a distributed model checking tool with novel verification capabilities. Software creation and management is less relevant as the focus is on verification rather than software development processes.,"Formal language definitions:0.8,Development frameworks and environments:0.7,Compilers:0.5,Software libraries and repositories:0.4","Formal language definitions,Development frameworks and environments",Formal language definitions are relevant for the model checker's verification capabilities. Development frameworks apply to the Erlang-based implementation. Other categories like 'Compilers' are less central.
2178,Accentuating the positive: atomicity inference and enforcement using correct executions,"Concurrency bugs are often due to inadequate synchronization that fail to prevent specific (undesirable) thread interleavings. Such errors, often referred to as Heisenbugs, are difficult to detect, prevent, and repair. In this paper, we present a new technique to increase program robustness against Heisenbugs. We profile correct executions from provided test suites to infer fine-grained atomicity properties. Additional deadlock-free locking is injected into the program to guarantee these properties hold on production runs. Notably, our technique does not rely on witnessing or analyzing erroneous executions. The end result is a scheme that only permits executions which are guaranteed to preserve the atomicity properties derived from the profile. Evaluation results on large, real-world, open-source programs show that our technique can effectively suppress subtle concurrency bugs, with small runtime overheads (typically less than 15%).","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.9,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.2,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Software and its engineering,Software and its engineering is relevant because the paper presents a concurrency bug detection technique using software profiling and synchronization enforcement. Other categories like Theory of Computation or Networks are not central to the core contribution of software robustness.,"Software creation and management:0.2,Software notations and tools:0.1,Software organization and properties:0.9",Software organization and properties,Software organization and properties is relevant for concurrency bug detection and atomicity enforcement. Software creation and management is less directly relevant to runtime analysis techniques.,"Contextual software domains:0,Extra-functional properties:0,Software functional properties:1,Software system structures:0",Software functional properties,Software functional properties is relevant as the paper addresses correctness of concurrent program execution. Other categories are unrelated to algorithmic correctness guarantees.
1930,Merging of Use Case Models: Semantic Foundations,"Use case models are the artifact of choice for capturing functional requirements. This typically collaborative activity makes merging a necessity. Use cases however, are often neglected when it comes to model merging, since they are commonly treated as text only items. By defining a formal syntax and semantics for use case models, manipulated within a generic metamodel for operation-based merging, we show how use case models can be effectively merged. This formal foundation allows for the modeling of use cases; defining meaningful change operations on them; and for detecting modeling inconsistencies, inconformities, and conflicts. Several practical examples validate the concepts presented: existing and planned tool support is introduced.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.9,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Software and its engineering,"Software and its engineering is highly relevant as the paper presents formal methods for merging use case models. Computing methodologies (0.2) receives moderate score for modeling concepts, but the core contribution is in software engineering techniques for model integration.","Software creation and management:0.2,Software notations and tools:0.9,Software organization and properties:0.1",Software notations and tools,Software notations and tools is highly relevant as the paper introduces delegation diagrams for object-oriented design. Software creation and management has low relevance as the focus is on notation rather than software development processes.,"Compilers:0,Context specific languages:0,Development frameworks and environments:0,Formal language definitions:1,General programming languages:0,Software configuration management and version control systems:0,Software libraries and repositories:0,Software maintenance tools:0,System description languages:1","Formal language definitions,System description languages",Formal language definitions: The paper introduces a formal syntax and semantics for use case models. System description languages: Use cases are treated as system modeling artifacts. Other options like compilers or programming languages are unrelated.
3458,Automated memory leak fixing on value-flow slices for C programs,"C is the dominant programming language for developing embedded software, operating systems, and device drivers. Unlike programs written in managed languages like Java, C programs rely on explicit memory management and are prone to memory leaks. Existing (static or dynamic) detectors only report leaks, but fixing them often requires considerable manual effort by inspecting a list of reported true and false alarms. How to develop on-demand lightweight techniques for automated leak fixing without introducing new memory errors remains challenging. In this paper, we introduce AutoFix, a fully automated leak-fixing approach for C programs by combining static and dynamic program analysis. Given a leaky allocation site reported by a static memory leak detector, AutoFix performs a graph reachability analysis to identify leaky paths on the value-flow slices of the program, and then conducts a liveness analysis to locate the program points for inserting fixes (i.e., the missing free calls) on the identified leaky paths. We have implemented AutoFix in LLVM-3.5.0 and evaluated it using five SPEC2000 benchmarks and three open-source applications. Experimental results show that AutoFix can safely fix all the memory leaks reported by a state-of-the-art static memory leak detector with small instrumentation overhead.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.8,Theory of computation:0.2,Mathematics of computing:0.2,Information systems:0.2,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.3,Applied computing:0.2,Social and professional topics:0.1",Software and its engineering,Software and its engineering is highly relevant for automated memory leak fixing in C programs. Other categories are rejected as the paper focuses on software development tools and debugging techniques rather than hardware or applied domains.,"Software creation and management:1.0,Software notations and tools:0.2,Software organization and properties:0.1",Software creation and management,"Software creation and management is highly relevant as the paper introduces an automated system for fixing memory leaks in C programs, a core aspect of software development. The other options are less relevant as the paper focuses on practical software creation rather than abstract notations or theoretical properties.","Collaboration in software development:0.0,Designing software:0.5,Software development process management:0.0,Software development techniques:0.0,Software post-development issues:1.0,Software verification and validation:1.0","Software post-development issues,Software verification and validation","Software post-development issues is relevant as the paper focuses on automated fixing of memory leaks after deployment. Software verification and validation is relevant for ensuring correctness of fixes. Designing software is less relevant since the focus is on fixing, not initial design."
5029,Rapid Ontology-based Web Application Development with JSTL,"This paper presents the approach followed by the ODESeW framework for the development of ontology-based Web applications. ODESeW eases the creation of this type of applications by allowing the use of the expression language JSTL over ontology components, using a data model that reflects the knowledge representation of common ontology languages and that is implemented with Java Beans. This framework has been used for the development of a number of portals focused on the dissemination and management of R&D collaborative projects.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.9,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.3,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.4,Applied computing:0.2,Social and professional topics:0.1",Software and its engineering,Software and its engineering is highly relevant as the paper presents a framework for ontology-based web application development using JSTL. Computing methodologies is moderately relevant due to the use of data models. Other categories like Information systems are less relevant as the focus is on software development rather than data management.,"Software creation and management:0.9,Software notations and tools:0.6,Software organization and properties:0.2",Software creation and management,Software creation and management is highly relevant as the paper focuses on web application development framework. Software notations and tools has moderate relevance for JSTL implementation.,"Collaboration in software development:0.5,Designing software:1,Software development process management:0,Software development techniques:1,Software post-development issues:0,Software verification and validation:0","Designing software,Software development techniques",Designing software is relevant for ontology-based frameworks. Software development techniques relate to JSTL implementation. Collaboration is only peripherally mentioned.
2825,rqt: an R package for gene‐level meta‐analysis,"Motivation: Despite recent advances of modern GWAS methods, it is still remains an important problem of addressing calculation an effect size and corresponding p‐value for the whole gene rather than for single variant. Results: We developed an R package rqt, which offers gene‐level GWAS meta‐analysis. The package can be easily included into bioinformatics pipeline or used stand‐alone. We applied this tool to the analysis of Alzheimer's disease data from three datasets CHS, FHS and LOADFS. Test results from meta‐analysis of three Alzheimer studies show its applicability for association testing. Availability and implementation: The package rqt is freely available under the following link: https://github.com/izhbannikov/rqt. Contact: ilya.zhbannikov@duke.edu Supplementary information: Supplementary data are available at Bioinformatics online.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.8,Theory of computation:0.1,Mathematics of computing:0.2,Information systems:0.2,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.3,Applied computing:0.2,Social and professional topics:0.1",Software and its engineering,Software and its engineering is relevant for the R package development. Other categories like Information systems are secondary to the software tool focus.,"Software creation and management:0.9,Software notations and tools:0.7,Software organization and properties:0.4","Software creation and management,Software notations and tools",Software creation and management is relevant for R package development; Software notations and tools is relevant for implementation. Software organization is less directly related to the tool's purpose.,"Collaboration in software development:0.7,Compilers:0.1,Context specific languages:0.1,Designing software:0.3,Development frameworks and environments:0.5,Formal language definitions:0.1,General programming languages:0.2,Software configuration management and version control systems:0.1,Software development process management:0.1,Software development techniques:0.3,Software libraries and repositories:1.0,Software maintenance tools:0.2,Software post-development issues:0.1,Software verification and validation:0.1,System description languages:0.1","Software libraries and repositories,Collaboration in software development",Software libraries and repositories (1.0) is directly relevant as the paper introduces an R package (rqt) for gene-level meta-analysis. Collaboration in software development (0.7) is secondary due to the tool's integration into bioinformatics pipelines. Other categories like Compilers (0.1) are irrelevant.
2719,Optimizing NoSQL DB on Flash: A Case Study of RocksDB,"A solid-state drive (SSD) gains fast I/O speed and is becoming an ideal replacement for traditional rotating storage. However, its speed and responsiveness heavily depend on internal fragmentation. With a high degree of fragmentation, an SSD may experience sharp performance degradation. Hence, minimizing fragmentation in the SSD is an effective way to sustain its high performance. In this paper, we propose an innovative file data placement strategy for Rocks DB, a widely used embedded NoSQL database. The proposed strategy steers data to a write unit exposed by an SSD according to predicted data lifetime. By placing data with similar lifetime in the same write unit, fragmentation in the SSD is controlled at the time of data write. We evaluate our proposed strategy using the Yahoo! Cloud Serving Benchmark. Our experimental results demonstrate that the proposed strategy improves the Rocks DB performance significantly: the throughput can be increased by up to 41%, 99.99%ile latency reduced by 59%, and SSD lifetime extended by up to 18%.","General and reference:0,Hardware:0.3,Computer systems organization:0,Networks:0,Software and its engineering:1,Theory of computation:0,Mathematics of computing:0,Information systems:0.5,Security and privacy:0,Human-centered computing:0,Computing methodologies:0,Applied computing:0,Social and professional topics:0",Software and its engineering,"Software and its engineering is highly relevant as the paper optimizes a NoSQL database system (RocksDB) for flash storage. Hardware is marginally relevant due to SSD considerations, but the core contribution is software optimization.","Software creation and management:0.2,Software notations and tools:0.1,Software organization and properties:0.8",Software organization and properties,Software organization and properties is highly relevant due to the focus on optimizing NoSQL database performance on SSDs. Other categories are peripheral to the system-level optimization analysis.,"Contextual software domains:0,Extra-functional properties:1,Software functional properties:0,Software system structures:0",Extra-functional properties,"The paper focuses on optimizing RocksDB performance on SSDs, emphasizing non-functional aspects like throughput and latency. Other categories do not align with the primary domain."
1813,A PaaS-based framework for automated performance analysis of service-oriented systems,"Service-oriented systems are often at the core of mission- or business-critical systems, and thus advanced quantitative analysis techniques are needed to assess, from the early development stages, whether or not the system accomplishes the stakeholder requirements and constraints. In this respect, in order to take advantage of the distributed nature of the considered systems, the use of distributed simulation (DS) appears the most natural and effective simulation approach. Nevertheless, the integration of traditional system development processes with DS approaches can be cost- and time-demanding. This paper presents SOAsim, a highly automated framework that allows system designers to generate the executable DS code from the model-based specification of the system under study, by use of automated model transformations. Moreover, in order to reduce the costs of setting-up dedicated DS platforms, SOAsim also automates the DS deployment and execution over a cloud-based infrastructure, according to a Platform-as-a-Service (PaaS) paradigm.","General and reference:0.1,Hardware:0.2,Computer systems organization:0.4,Networks:0.3,Software and its engineering:0.9,Theory of computation:0.2,Mathematics of computing:0.3,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.4,Applied computing:0.3,Social and professional topics:0.1",Software and its engineering,Software and its engineering: Framework development for distributed simulation and cloud deployment. Other categories like Computer systems organization are secondary to the primary software engineering contribution.,"Software creation and management:0.8,Software notations and tools:0.9,Software organization and properties:0.2","Software notations and tools,Software creation and management",Software notations and tools apply to the model-based specification and automated transformations in SOAsim. Software creation and management is relevant for cloud-based deployment. Other categories like Software organization and properties are less relevant as the focus is on development processes.,"Collaboration in software development:0,Compilers:0,Context specific languages:0,Designing software:0,Development frameworks and environments:1,Formal language definitions:0,General programming languages:0,Software configuration management and version control systems:0,Software development process management:0,Software development techniques:0,Software libraries and repositories:0,Software maintenance tools:0,Software post-development issues:0,Software verification and validation:1,System description languages:0","Development frameworks and environments,Software verification and validation",Development frameworks and environments: SOAsim is a PaaS-based framework for automated simulation. Software verification and validation: The framework supports performance analysis of service-oriented systems. Other categories like Software maintenance tools are unrelated.
773,Collaboration model of software development,"Software development is a series of processes activity involving more than one person, especially in medium to large software projects. Each activity of software development is carried out by various combination of different individual. Collaboration occurs in every phase of the process software development. The right team collaboration method is one of the important things in success software development. This research is focused to develop collaboration model of software development to meet time schedule, budget, scope, and software quality.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.95,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.3,Social and professional topics:0.1",Software and its engineering,"Software and its engineering is highly relevant as the paper focuses on software development collaboration models. Applied computing has moderate relevance as it discusses software project management, but the core contribution is about software development methodology, not application domains.","Software creation and management:1.0,Software notations and tools:0.25,Software organization and properties:0.0",Software creation and management,Software creation and management is directly relevant as the paper focuses on collaboration models for software development. Software notations/tools is marginally relevant as it's about development processes but not specific notations. Software organization/properties is irrelevant.,"Collaboration in software development:1,Designing software:0,Software development process management:1,Software development techniques:0,Software post-development issues:0,Software verification and validation:0","Collaboration in software development,Software development process management","Collaboration in software development is directly relevant as the paper focuses on collaboration models for software projects. Software development process management is relevant because the study addresses methods to meet project goals like schedule and quality. Other categories are irrelevant as the paper does not discuss design techniques, post-development issues, or verification."
2566,Implementation of a customizable fault tolerance framework,"While there has been significant advances in fault tolerance research, the effort has focused on the design of individual fault-tolerant systems or methodologies. Recently, some research has been initiated to develop fault tolerance paradigms that can be used to provide a spectrum of fault tolerance levels. In this paper, we present the design of a fault tolerance framework that can be used to support a wide spectrum of applications with various fault tolerance requirements, various criticality levels, and various system models. The framework is designed to be parameterizable so that the user can configure it to obtain the desired features. Also, the framework is designed to be an off-the-shelf component such that application programs can be integrated within it easily to obtain the fault-tolerant version of the application system. A specialized N-modular redundancy (SNMR) scheme has been developed to serve as the primary approach for achieving efficient and cost-effective fault tolerance for the framework. In most cases, the SNMR scheme yields better performance and lower cost in providing fault tolerance as compared with conventional NMR schemes. It also enhances the scalability and customizability of the general replication method. This paper discusses the major concept of the SNMR framework and the main issues in the design and implementation of the framework, including an object-oriented overall system design and the interface protocol class hierarchy. The interface protocol class hierarchy provides a nice paradigm for the implementation of customizable, highly reusable, and easily extensible SNMR framework.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.2,Networks:0.1,Software and its engineering:1.0,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Software and its engineering,Software and its engineering is highly relevant as the paper presents a customizable fault tolerance framework. Other categories like Networks or Hardware are less central since the focus is on software design and engineering practices.,"Software creation and management:1.0,Software notations and tools:0.2,Software organization and properties:0.5",Software creation and management,Software creation and management is highly relevant as the paper presents a customizable fault tolerance framework design and implementation. Software notations and tools are only marginally relevant for the design process. Software organization and properties has moderate relevance to the framework's structure.,"Collaboration in software development:0,Designing software:1,Software development process management:0,Software development techniques:1,Software post-development issues:0,Software verification and validation:0.5","Designing software,Software development techniques",Designing software is relevant as the paper presents a framework design for fault tolerance. Software development techniques are relevant due to the parameterizable and customizable implementation approach. Software verification and validation receive a moderate score as the paper briefly mentions testing the framework's effectiveness but does not focus on formal validation methods.
517,Modeling and executing semantic B2B integration,"Trying to achieve B2B integration by programming does not scale due to the high number of combinations of B2B protocols like EDI or RosettaNet, business processes, document formats, back end applications and trading partners. Furthermore, programming does not provide the necessary flexibility, reliability, and consistency. Instead, a set of B2B integration modeling constructs is proposed in this paper that allow to model B2B integration dynamically: B2B protocol, application protocol, workflow types, transformation, binding, trading partner and business event.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:1.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.5,Social and professional topics:0.0",Software and its engineering,Software and its engineering is directly relevant for the modeling constructs and B2B integration framework. 'Applied computing' is secondary as the focus is on software modeling rather than domain-specific applications.,"Software creation and management:0.6,Software notations and tools:0.9,Software organization and properties:0.5",Software notations and tools,Software notations and tools is relevant for the B2B integration modeling constructs. Software creation and management is secondary for implementation aspects. Software organization is less directly relevant to the modeling focus.,"Compilers:0.0,Context specific languages:0.0,Development frameworks and environments:0.8,Formal language definitions:0.5,General programming languages:0.0,Software configuration management and version control systems:0.0,Software libraries and repositories:0.0,Software maintenance tools:0.0,System description languages:1.0","Development frameworks and environments,System description languages",Development frameworks and environments: The paper proposes B2B integration modeling constructs as a framework. System description languages: The modeling constructs are formalized as a system description language.
3973,Titanium: A High-performance Java Dialect,"Titanium is a language and system for high-performance parallel scientific computing. Titanium uses Java as its base, thereby leveraging the advantages of that language and allowing us to focus attention on parallel computing issues. The main additions to Java are immutable classes, multidimensional arrays, an explicitly parallel SPMD model of computation with a global address space, and zone-based memory management. We discuss these features and our design approach, and report progress on the development of Titanium, including our current driving application: a three-dimensional adaptive mesh refinement parallel Poisson solver. © 1998 John Wiley & Sons, Ltd.","General and reference:0.1,Hardware:0.2,Computer systems organization:0.4,Networks:0.1,Software and its engineering:1.0,Theory of computation:0.2,Mathematics of computing:0.3,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.5,Applied computing:0.3,Social and professional topics:0.1",Software and its engineering,Software and its engineering: The paper describes a high-performance Java dialect for scientific computing. Other options like 'Computer systems organization' are less relevant as the focus is on language design rather than system architecture.,"Software creation and management:1.0,Software notations and tools:0.5,Software organization and properties:1.0","Software creation and management,Software organization and properties",Software creation and management is relevant as the paper introduces a new programming language (Titanium). Software organization and properties is relevant due to the focus on parallel computing features like SPMD models. Software notations and tools is less relevant as the paper emphasizes language design over specific tools.,"Collaboration in software development:0.1,Contextual software domains:0.1,Designing software:1.0,Extra-functional properties:0.8,Software development process management:0.3,Software development techniques:1.0,Software functional properties:1.0,Software post-development issues:0.1,Software system structures:1.0,Software verification and validation:0.2","Designing software,Software development techniques,Software system structures",Designing software is relevant because the paper introduces a new language (Titanium) for software development. Software development techniques is relevant due to the focus on parallel computing features like SPMD and global address spaces. Software system structures is relevant as the paper discusses language-level features and system design. Other categories like Extra-functional properties or Software verification are less central as the paper's primary contribution is the design and structure of the language itself.
5430,Concept-Driven Engineering for Supporting Different Views of Models,"This paper investigates the the development and evolution of concepts and the management of transformers, which adds semantics to the concepts. We illustrate how concepts, their variants and transformers can be developed via cooperation. In order to meet increasing user demands for more various software systems, we are revising the software developing process to accommodate mass customisation based upon Concept-Driven Engineering (CDE). CDE is a strategy to application specification and generation of new concepts via transformers. The concept and its transformation rather than the implementation is central to the development process. It allows automation for specification from early stages to executable specification and code generation. CDE continue to be a challenges in building complex software systems that have several variations and options. Software development is based upon a lot of specifications and implementations such as feature models, UML models and code, which are in different formats but share a certain amount of information. CDE is similar to the ideas of Model Driven Engineering (MDE) and Software Product Line Engineering, whereby we use the term concept instead model. A concept can be a model or a specification and is defined on a concept structure and a set of transformers. Concepts are assigned to a set of transformers, which generates new concepts. In this sense also a software system is a concept. One main remark is the management of the evolution of concepts and its transformers. Concept-Driven Engineering supports that requirements of the software do not remain constant during its development time and therefore the specification has to evolved and refined in order to fulfil the new requirements. One remark should lie an the management of the transformers that can be effectively be used on specifications. Against MDE CDE focus on the transformers which carry out the semantics of the specification, resp. the model. It is needful to study how transformers will affect the development process, that means how easy is a transformer to use, resp. to reuse. CDE abilities rely on the detailed transformer designer’s knowledge of concept structure and development work flow while considering software system knowledge, software engineering techniques and methods. The aim of CDE is to avoid the development concepts and transformers which are in downstream development incapable to use. When developers change one concept simultaneously, we need to propagate these changes across all concepts to guarantee them consistent. The process of synchronization propagates changes among specifications in different stages to all involved participants. Exchange of models between local platforms is still a challenging issue. The exchange and","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.9,Theory of computation:0.1,Mathematics of computing:0.2,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.3,Applied computing:0.1,Social and professional topics:0.1",Software and its engineering,"Software and its engineering is relevant because the paper introduces Concept-Driven Engineering (CDE), a software development methodology. Computing methodologies could be secondary if the focus were on modeling techniques, but the primary domain is software engineering practices.","Software creation and management:1.0,Software notations and tools:1.0,Software organization and properties:0.5","Software creation and management,Software notations and tools","Software creation and management: The paper introduces Concept-Driven Engineering (CDE), a software development methodology. Software notations and tools: The focus on transformers and concept structures relates to software modeling tools. Software organization and properties is less relevant as the paper emphasizes development processes over software structure.","Collaboration in software development:0.3,Compilers:0.1,Context specific languages:0.2,Designing software:1.0,Development frameworks and environments:0.4,Formal language definitions:0.3,General programming languages:0.2,Software configuration management and version control systems:0.3,Software development process management:1.0,Software development techniques:0.5,Software libraries and repositories:0.2,Software maintenance tools:0.2,Software post-development issues:0.3,Software verification and validation:0.4,System description languages:0.2","Designing software,Software development process management",Designing software: CDE focuses on concept modeling and transformation for software design. Software development process management: The paper addresses managing concept evolution in development processes. Other categories are less relevant as the focus is on design methodologies rather than tools or verification.
4517,Abstraction in the Intel iAPX-432 prototype systems implementation language,"Abstract : This report describes the abstraction mechanism of a prototype systems implementaiton language for Intel's iAPX-432 microprocessor. Full exploitation of the 432's facilities places many demands on a language intended for systems implementation. The 432 is a capability-based machine, with hardware-enforced typing of large objects, dynamically instantiated domains (i.e., packages), hardware-enforced information information hiding (seals), and hardware-supported, software-defined access-rights (trademarks). The prototype language's support for these facilities is described in this project.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.2,Networks:0.0,Software and its engineering:1.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Software and its engineering,"Software and its engineering: The paper discusses a systems implementation language for the iAPX-432, focusing on abstraction mechanisms and language design. Computer systems organization is secondary as the focus is on the language rather than system architecture.","Software creation and management:0.1,Software notations and tools:0.1,Software organization and properties:0.9",Software organization and properties,Software organization and properties is directly relevant as the paper analyzes language mechanisms for capability-based systems. Other categories are irrelevant as the focus is on language design for hardware-enforced software properties rather than software development tools or management.,"Contextual software domains:0.0,Extra-functional properties:0.0,Software functional properties:1.0,Software system structures:1.0","Software functional properties,Software system structures",Software functional properties: The paper discusses how the language supports specific functional features like information hiding. Software system structures: The paper describes the overall structure of the systems implementation language.
1776,Probe Distribution Techniques to Profile Events in Deployed Software,"Profiling deployed software provides valuable insights for quality improvement activities. The probes required for profiling, however, can cause an unacceptable performance overhead for users. In previous work we have shown that significant overhead reduction can be achieved, with limited information loss, through the distribution of probes across deployed instances. However, existing techniques for probe distribution are designed to profile simple events. In this paper we present a set of techniques for probe distribution to enable the profiling of complex events that require multiple probes and that share probes with other events. Our evaluation of the new techniques indicates that, for tight overhead bounds, techniques that produce a balanced event allocation can retain significantly more field information","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:1.0,Theory of computation:0.2,Mathematics of computing:0.2,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.5,Applied computing:0.3,Social and professional topics:0.1",Software and its engineering,"Software and its engineering is highly relevant because the paper addresses profiling techniques for deployed software, a core concern in software performance analysis. Computing methodologies is partially relevant for the methodological approach, but the primary domain is software engineering. Other categories are less directly related.","Software creation and management:0.2,Software notations and tools:0.9,Software organization and properties:0.1",Software notations and tools,Software notations and tools is highly relevant as the paper introduces probe distribution techniques for software profiling. Other categories like Software creation and management are less directly related to the core contribution of profiling methodologies.,"Compilers:0.0,Context specific languages:0.0,Development frameworks and environments:0.0,Formal language definitions:0.0,General programming languages:0.0,Software configuration management and version control systems:0.0,Software maintenance tools:1.0,Software libraries and repositories:0.0,System description languages:0.0",Software maintenance tools,Software maintenance tools is directly relevant to probe distribution techniques for performance optimization. All other categories are irrelevant to the paper's focus on profiling methodologies.
1710,Enhancing explanations in knowledge-based systems with hypertext,"This article investigates the use of hypertext to enhance explanations in knowledge‐based systems (KBS). Three fundamental issues are addressed: (1) why is hypertext suited for enhancing explanations in KBS? (2) what kind of knowledge needs to be added to KBS to provide effective explanations? and (3) how should such knowledge be represented efficiently and made accessible naturally and conveniently to users? The central idea is to develop a hypertext deep knowledge base to supplement KBS conclusions and explanations, so that users are provided with a rich context for understanding and interpreting KBS conclusions and reasoning. Explanations in KBS can be made more intelligible and relevant to users, and more interactive by (1) representing deep knowledge needed for explanations with hypertext in a way consistent with learning and educational theories, and (2) providing access to domain knowledge from KBS output with hypertext links, thus integrating domain knowledge into the KBS output in a manner conduc...","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:1.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.5,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Software and its engineering,Software and its engineering is highly relevant as the paper focuses on enhancing knowledge-based systems with hypertext for explanations. Human-centered computing is marginally relevant due to user interaction but is secondary to the software engineering focus.,"Software creation and management:0.0,Software notations and tools:1.0,Software organization and properties:0.0",Software notations and tools,Software notations and tools (1.0) is directly relevant for hypertext-based explanation systems. Other categories lack direct connection to the core contribution of knowledge representation.,"Compilers:0.1,Context specific languages:0.1,Development frameworks and environments:0.9,Formal language definitions:0.2,General programming languages:0.1,Software configuration management and version control systems:0.1,Software libraries and repositories:0.1,Software maintenance tools:0.7,System description languages:0.1","Development frameworks and environments,Software maintenance tools",Development frameworks and environments is relevant because the paper discusses integrating hypertext into KBS systems for enhanced user interaction. Software maintenance tools is relevant since the paper addresses maintaining and enhancing knowledge bases. Other options are unrelated to knowledge representation or system development.
3424,A Fault-Tree Semantics to model Software-Controlled Systems,"Fault Tree Analysis is a very popular technique to assess safety and reliability of technical systems. However, being a combinatorial model, Fault Trees can only express which combinations of failures contribute to a certain hazard or accident. There is no means to model sequences of actions and temporal orders of states and events. Since today technical systems are often controlled by software that executes over time, the traditional Fault Tree model is no longer sufficient. Moreover, integration of software modelling techniques with safety assessment techniques is hampered by the lack of semantically equivalent entities in both domains. To overcome these drawbacks, we propose an extended Fault Tree semantics that distinguishes events that happen at a point in time from states that last over a period of time. Typed Fault Tree gates are introduced and calculation rules for quantitative analysis are given. We address the integration of this new concept into our Safety and Reliability Analysis Tool UWG3.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:1.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Software and its engineering,"The paper extends Fault Tree Analysis for software-controlled systems, directly aligning with Software and its engineering. Other categories like Hardware are not relevant here.","Software notations and tools:0.85,Software organization and properties:0.7,Software creation and management:0.4","Software notations and tools,Software organization and properties",Software notations and tools is central to the extended Fault Tree modeling. Software organization and properties relates to the quantitative analysis of reliability. Software creation is less relevant to the modeling focus.,"Compilers:0.1,Context specific languages:0.1,Contextual software domains:0.2,Development frameworks and environments:0.2,Extra-functional properties:1,Formal language definitions:0.3,General programming languages:0.1,Software configuration management and version control systems:0.1,Software functional properties:0.9,Software libraries and repositories:0.1,Software maintenance tools:0.1,Software system structures:0.7,System description languages:0.5","Extra-functional properties,Software functional properties",Extra-functional properties is highly relevant as the paper addresses safety and reliability modeling. Software functional properties is relevant for the quantitative analysis of fault trees. Software system structures is moderately relevant for the tool integration. Other categories are rejected as the focus is on formal safety analysis rather than programming languages or frameworks.
4199,An MPI implementation of the BLACS,"An MPI implementation of the Basic Linear Algebra Communication Subprograms (BLACS), an underlying layer of the ScaLAPACK library is presented. Use is made of a wide spectrum of functionality available in MPI to realize BLACS as succinctly as possible, thus making the implementation concise, but still yielding good performance. Some of the implementation details are discussed and the benchmark results for the ScaLAPACK LU factorization on several parallel architectures with different MPI libraries are presented. A performance comparison with other existing BLACS implementations is made and some conclusions are drawn from the results.","General and reference:0.1,Hardware:0.2,Computer systems organization:0.2,Networks:0.1,Software and its engineering:0.9,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Software and its engineering,"Software and its engineering is highly relevant as the paper presents an MPI-based implementation of the BLACS library for parallel computing. Other categories like Hardware or Networks are less relevant because the core contribution is the software engineering of a distributed library, not hardware design or network protocols.","Software creation and management:0.3,Software notations and tools:1.0,Software organization and properties:0.8","Software notations and tools,Software organization and properties",Software notations and tools is relevant for the MPI/BLACS implementation. Software organization and properties applies to performance analysis. Software creation and management is less central to the technical implementation focus.,"Compilers:0,Context specific languages:0,Contextual software domains:0,Development frameworks and environments:0,Extra-functional properties:0,Formal language definitions:0,General programming languages:0,Software configuration management and version control systems:0,Software functional properties:0,Software libraries and repositories:1,Software maintenance tools:0,Software system structures:0,System description languages:0",Software libraries and repositories,Software libraries and repositories are directly relevant as the paper implements the BLACS library. Other categories like Compilers are unrelated to the library implementation focus.
2664,Experience in testing the Motif interface,"The testing and quality assurance of the Motif 1.0 graphical user-interface software are described. The testing goals, which fell into three general categories (code coverage, defect-density, and defect-arrival rate), and a deliverable formal test suite are examined. The three phases of the testing process-evaluation, test development, and regression testing-and the tools used in testing are discussed.<<ETX>>","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.9,Theory of computation:0.2,Mathematics of computing:0.1,Information systems:0.2,Security and privacy:0.1,Human-centered computing:0.3,Computing methodologies:0.4,Applied computing:0.1,Social and professional topics:0.1",Software and its engineering,Software and its engineering is relevant for the GUI testing methodologies and tools. Human-centered computing is marginally relevant but not central to the testing process itself. Other categories like Networks or Mathematics are not core to this empirical software testing study.,"Software creation and management:0.9,Software notations and tools:0.9,Software organization and properties:0.0","Software creation and management,Software notations and tools",Software creation and management is relevant for the testing process of Motif interface software. Software notations and tools is relevant for the formal test suite and testing methodologies described. Software organization and properties is less relevant as the paper focuses on testing rather than software structure.,"Collaboration in software development:0,Compilers:0,Context specific languages:0,Designing software:0.5,Development frameworks and environments:0,Formal language definitions:0,General programming languages:0,Software configuration management and version control systems:0,Software development process management:0.6,Software development techniques:0.4,Software libraries and repositories:0,Software maintenance tools:0.3,Software post-development issues:0,Software verification and validation:1,System description languages:0","Software verification and validation,Software development process management",Software verification and validation is directly relevant to the testing process described. Software development process management is relevant to the structured testing phases. Other categories like Designing software are less central to the core testing focus.
3948,Abstention Reduces Errors - decision Abstaining N-version Genetic Programming,"Optimal fault masking N-Version Genetic Programming (NVGP) is a technique for building fault tolerant software via ensemble of automatically generated modules in such a way as to maximize their collective fault masking ability. Decision Abstaining N-Version Genetic Programming is NVGP that abstains from decision-making, when there is no decisive vote among the modules to make a decision. A special course of action may be taken for an abstained instance. We found that decision abstention contributed to error reduction in our experimental Escherichia coli DNA promoter sequence classification problem. Though decision abstention may reduce errors, high abstention rate makes the system of little use. This paper investigates the trade-off between abstention rate and error reduction.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.9,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Software and its engineering,"Software and its engineering is highly relevant as the paper introduces a novel fault-tolerant software design approach using genetic programming. Other categories are irrelevant as the paper does not focus on hardware, networks, or theoretical computation.","Software creation and management:0.3,Software notations and tools:0.2,Software organization and properties:0.9",Software organization and properties,Software organization and properties is directly relevant as the paper discusses fault-tolerant software architecture. Other fields are less relevant to the paper's focus on software reliability.,"Contextual software domains:0.1,Extra-functional properties:0.8,Software functional properties:0.9,Software system structures:0.2","Software functional properties,Extra-functional properties",Software functional properties is relevant as the paper evaluates correctness (error reduction) of NVGP. Extra-functional properties is relevant due to the focus on fault tolerance and reliability through abstention. Other children like Software system structures are irrelevant as the paper does not discuss architectural design.
4963,Concurrency Control in Distributed Object-Oriented Database Systems,"Simulating distributed database systems is inherently difficult, as there are many factors that may influence the results. This includes architectural options as well as workload and data distribution. In this paper we present the DBsim simulator and some simulation results. The DBsim simulator architecture is extendible, and it is easy to change parameters and configuration. The simulation results in this paper is a comparison of performance and response times for two concurrency control algorithms, timestamp ordering and two-phase locking. The simulations have been run with different number of nodes, network types, data declustering and workloads. The results show that for a mix of small and long transactions, the throughput is significantly higher for a system with a timestamp ordering scheduler than for a system with a two-phase locking scheduler. With only short transactions, the performance of the two schedulers are almost identical. Long transactions are treated more fair by a two-phase locking scheduler, because a timestamp ordering scheduler has a very high abort rate for long transactions.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.9,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Software and its engineering,Software and its engineering is highly relevant for the database concurrency control algorithms (timestamp ordering and two-phase locking). Other fields like Networks are less relevant as the focus is on database system design rather than communication protocols.,"Software creation and management:0.1,Software notations and tools:0.2,Software organization and properties:0.9",Software organization and properties,"Software organization and properties: The paper analyzes concurrency control algorithms (timestamp ordering vs. two-phase locking) in distributed systems, which directly relates to software organization and properties. Other categories like Software creation or Notations are secondary to the algorithmic comparison.","Contextual software domains:0,Extra-functional properties:1,Software functional properties:1,Software system structures:1","Software functional properties,Software system structures,Extra-functional properties",Software functional properties: Concurrency control algorithms define functional behavior. Software system structures: Distributed database architecture is discussed. Extra-functional properties: Performance metrics like throughput are evaluated. Contextual domains are not directly addressed.
1248,Automatic testing of symbolic execution engines via program generation and differential testing,"Symbolic execution has attracted significant attention in recent years, with applications in software testing, security, networking and more. Symbolic execution tools, like CREST, KLEE, FuzzBALL, and Symbolic PathFinder, have enabled researchers and practitioners to experiment with new ideas, scale the technique to larger applications and apply it to new application domains. Therefore, the correctness of these tools is of critical importance. In this paper, we present our experience extending compiler testing techniques to find errors in both the concrete and symbolic execution components of symbolic execution engines. The approach used relies on a novel way to create program versions, in three different testing modes—concrete, single-path and multi-path—each exercising different features of symbolic execution engines. When combined with existing program generation techniques and appropriate oracles, this approach enables differential testing within a single symbolic execution engine. We have applied our approach to the KLEE, CREST and FuzzBALL symbolic execution engines, where it has discovered 20 different bugs exposing a variety of important errors having to do with the handling of structures, division, modulo, casting, vector instructions and more, as well as issues related to constraint solving, compiler optimisations and test input replay.","General and reference:0.1,Hardware:0.2,Computer systems organization:0.3,Networks:0.1,Software and its engineering:0.9,Theory of computation:0.4,Mathematics of computing:0.2,Information systems:0.1,Security and privacy:0.5,Human-centered computing:0.1,Computing methodologies:0.3,Applied computing:0.2,Social and professional topics:0.1",Software and its engineering,"Software and its engineering: The paper focuses on testing symbolic execution engines (a software verification technique) and discovering bugs in their implementation. The core contribution is a novel methodology for compiler and symbolic execution testing. Other categories are irrelevant as the paper doesn't focus on hardware design, theoretical foundations, or security protocols.","Software creation and management:0.1,Software notations and tools:0.9,Software organization and properties:0.2",Software notations and tools,"Software notations and tools: The paper focuses on testing symbolic execution engines through program generation and differential testing, which is a software tool validation technique. Other categories like software creation or organization are not the primary focus.","Compilers:1,Context specific languages:0.3,Development frameworks and environments:0.9,Formal language definitions:0.4,General programming languages:0.5,Software configuration management and version control systems:0.3,Software libraries and repositories:0.3,Software maintenance tools:0.6,System description languages:0.4","Compilers,Development frameworks and environments","Compilers is relevant for symbolic execution engine testing. Development frameworks are relevant for differential testing methodologies. Other categories focus on version control or libraries, which are less central."
3078,REM4j - A Framework for Measuring the Reverse Engineering Capability of UML CASE Tools,"Reverse Engineering is becoming increasingly important 
in the software development world today as many organizations 
are battling to understand and maintain old legacy 
systems. Today’s software engineers have inherited these 
legacy systems which they may know little about yet have to 
maintain, extend and improve. Currently there is no framework 
or strategy that an organisation can use to determine 
which UML CASE tool to use. This paper sets down such a 
framework, to allow organisations to base their tool choice 
on this reliable framework. 
We present the REM4j tool, an automated tool, for 
benchmarking UML CASE tools, we then use REM4j to 
carry out one such evaluation with eleven UML CASE tools. 
This framework allows us to reach a conclusion as to which 
is the most accurate and reliable UML CASE tool.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.9,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Software and its engineering,Software and its engineering is relevant because the paper presents a framework for evaluating UML CASE tools used in reverse engineering. Other fields like Security and privacy are not directly related to tool evaluation frameworks.,"Software creation and management:0.4,Software notations and tools:1.0,Software organization and properties:0.3",Software notations and tools,Software notations and tools is highly relevant as the paper evaluates UML CASE tools using formal reverse engineering frameworks. Software creation and management is less central as the focus is on tool evaluation rather than software development processes.,"Compilers:0.0,Context specific languages:0.0,Development frameworks and environments:0.8,Formal language definitions:0.0,General programming languages:0.0,Software configuration management and version control systems:0.0,Software libraries and repositories:0.0,Software maintenance tools:1.0,System description languages:0.0","Software maintenance tools,Development frameworks and environments","Software maintenance tools: The paper presents REM4j, a benchmarking framework for UML CASE tools that directly supports software maintenance. Development frameworks and environments: The framework itself constitutes a development environment for evaluating UML tools. Other children like Compilers or Formal language definitions are irrelevant as the paper focuses on tool evaluation rather than language design or compilation."
1700,Component-based measurement: few useful guidelines,"Software industries are striving for new techniques and approaches that could improve software developer productivity, reduce time-to-market, deliver excellent performance and produce systems that are flexible, scalable, secure, and robust. Only software components can meet these demands and following this; component-based software engineering (CBSE) has emerged, which has generated tremendous interest in software development community. The paradigm shift to software components appears inevitable, necessitating drastic changes to current software development and business practices. The scope of this paper is to suggest few necessary guidelines for deriving component-based metrics. The paper discusses issues related to component-based development (CBD) and suggests a general definition of software component based on several existing definitions. Further, the paper outlines the necessity of component measurement and also discusses the limitations of traditional software metrics for component-based development (CBD) systems. Finally, this paper also suggest few necessary guidelines for CBD measurement and proposes some relevant metrics applicable to CBD systems, which after proper quantification and validation may help guiding risk and quality management of component-based systems.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.2,Networks:0.2,Software and its engineering:0.9,Theory of computation:0.1,Mathematics of computing:0.3,Information systems:0.2,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.4,Applied computing:0.2,Social and professional topics:0.1",Software and its engineering,Software and its engineering is highly relevant as the paper discusses component-based software engineering and metrics. Hardware is less relevant as the focus is on software development practices.,"Software creation and management:0.75,Software notations and tools:0.65,Software organization and properties:0.85",Software organization and properties,Software organization and properties is most relevant for component-based metrics. Software creation and management is secondary. Software notations and tools is less directly related.,"Contextual software domains:0.1,Extra-functional properties:0.2,Software functional properties:0.85,Software system structures:0.8","Software functional properties,Software system structures",Software functional properties and system structures are relevant for component-based metrics. Extra-functional properties are less relevant as the focus is on structural and functional aspects of components.
5188,One more step in the direction of modularized integration concerns,"Component integration creates value by automating the costly and error-prone task of imposing desired behavioral relationships on components manually. Requirements for component integration, however, complicate software design and evolution in several ways: first, they lead to coupling among components; second, the code that implements various integration concerns in a system is often scattered over and tangled with the code implementing the component behaviors. Straightforward software design techniques map integration requirements to scattered and tangled code, compromising modularity in ways that dramatically increase development and maintenance costs.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.2,Networks:0.1,Software and its engineering:1.0,Theory of computation:0.2,Mathematics of computing:0.3,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.2,Social and professional topics:0.1",Software and its engineering,"Software and its engineering: The paper addresses component integration and modularity in software design. Other categories are irrelevant as the focus is not on hardware, networks, or theory.","Software creation and management:0.8,Software notations and tools:0.3,Software organization and properties:0.8","Software creation and management,Software organization and properties",Software creation and management is relevant because the paper addresses component integration challenges in software design. Software organization and properties is relevant as it discusses modularity and coupling issues. Software notations and tools are less relevant as the paper focuses on design problems rather than tools or notations.,"Collaboration in software development:0,Contextual software domains:0,Designing software:1,Extra-functional properties:0,Software development process management:0,Software development techniques:1,Software functional properties:0,Software post-development issues:0,Software system structures:1,Software verification and validation:0","Designing software,Software development techniques,Software system structures",Designing software: Addresses modularity challenges in component integration. Software development techniques: Proposes methods to reduce code tangling and coupling. Software system structures: Focuses on architectural implications of integration concerns.
2204,Consistency and interoperability checking for component interaction rules,"In component-based software development, it is important to ensure interoperability between components based on their unambiguous semantic descriptions, in order to obtain a viable system. A body of recent work has explored the use of formal languages in specifying component interaction protocols for interoperability checking, but lacks the practicality required by software practitioners for daily use. Faced with this, we have developed a lightweight specification approach to component interaction rules, which has the necessary expressiveness and employs event patterns in rule specification for easy adoption by practitioners. In this paper, we present a FSA-based semantic model for such rules and novel studies of rule consistency and system-wide protocol interoperability for components annotated with interaction rules. We also develop incremental approaches and tools to check these properties, which provide an effective means to discover errors in the design of component interaction rules and component-based system architectures.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.85,Theory of computation:0.3,Mathematics of computing:0.2,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.15,Social and professional topics:0.1",Software and its engineering,Software and its engineering: The paper focuses on component interaction rules and interoperability checking in software systems. Theory of computation is partially relevant but less central than software engineering. Other categories are less relevant as it's not about hardware or information systems.,"Software creation and management:0.2,Software notations and tools:0.9,Software organization and properties:0.7","Software notations and tools,Software organization and properties",Software notations and tools is highly relevant as the paper presents a lightweight specification approach and tools for component interaction rules. Software organization and properties is relevant due to the discussion of component interaction protocols and system-wide interoperability.,"Compilers:0.1,Context specific languages:0.1,Contextual software domains:0.1,Development frameworks and environments:0.2,Extra-functional properties:0.1,Formal language definitions:0.3,General programming languages:0.1,Software configuration management and version control systems:0.1,Software functional properties:0.4,Software libraries and repositories:0.1,Software maintenance tools:0.1,Software system structures:0.8,System description languages:0.6","Software system structures,System description languages",Software system structures is relevant through component interaction modeling. System description languages is relevant for the FSA-based semantic model. Other options like Compilers are unrelated to the focus on component interoperability analysis.
5877,An approach to specify and test component-based dependable software,"Components (in-house or pre-fabricated) are increasingly being used to reduce the cost of software development. Given that these components may not have not been developed with dependability as a driver, the components need to be adapted to deal with errors coming from their environment. To achieve this, error containment wrappers are often added to increase the robustness of such components. Adopting a gray-box perspective of software, we first present a modular approach for specifying and verifying embedded software made from components, based on concepts from category theory. This modular approach allows the system designer to check for semantic compatibility. To generate the error containment wrappers needed for adaptation, we subsequently present an algorithm that systematically generates the required wrappers. Using the information obtained through wrapper design, we develop an approach to identify relevant test cases to test individual components. We further exploit the modularity of the specification to identify the relevant test cases to perform testing at different levels of SW abstraction.","General and reference:0.25,Hardware:0.25,Computer systems organization:0.25,Networks:0.25,Software and its engineering:1.0,Theory of computation:0.25,Mathematics of computing:0.25,Information systems:0.25,Security and privacy:0.25,Human-centered computing:0.25,Computing methodologies:0.5,Applied computing:0.25,Social and professional topics:0.25",Software and its engineering,"The paper focuses on component-based software design, testing, and error containment—core Software and its engineering topics. Computing methodologies is weakly relevant due to category theory but not central. Other categories are unrelated.","Software creation and management:0.85,Software notations and tools:0.75,Software organization and properties:0.5","Software creation and management,Software notations and tools",Software creation and management: Focuses on component adaptation and testing methodologies. Software notations and tools: Uses category theory for modular specifications. 'Software organization and properties' is only peripherally relevant.,"Collaboration in software development:0.1,Compilers:0.3,Context specific languages:0.1,Designing software:0.4,Development frameworks and environments:0.2,Formal language definitions:0.1,General programming languages:0.1,Software configuration management and version control systems:0.1,Software development process management:0.1,Software development techniques:0.7,Software libraries and repositories:0.1,Software maintenance tools:0.3,Software post-development issues:0.1,Software verification and validation:0.8,System description languages:0.2","Software verification and validation,Software development techniques,Designing software",Software verification and validation is central to error containment and testing. Software development techniques is relevant for component adaptation. Designing software is relevant for modular specification. Other categories like Software maintenance tools are less central.
1773,An Effective and User-Friendly IDE Tool to Facilitate Conceptual Design and Maintenance of Web Applications,"Web applications are growing in demand, complexity and size, thus making it difficult to systematically design and maintain general web applications. To aid in fulfilling these difficult tasks, we present an effective IDE tool to allow the design of web applications at a conceptual level based on a web modeling approach – the WebML with its modeling specifications written in the platform-independent XML. In addition, we proposed a library of templates for different components in our IDE tool to facilitate reuses and maintenance. Besides, we studied how structural design of web applications could possibly be improved with easy-to-use features in our IDE prototype. Clearly, this ongoing work opens up many exciting directions for future investigation.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:1.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Software and its engineering,Software and its engineering is highly relevant as the paper presents an IDE tool for web application development. Other categories are not central to the paper's core contribution.,"Software creation and management:0.3,Software notations and tools:0.9,Software organization and properties:0.2",Software notations and tools,"Software notations and tools is highly relevant as the paper introduces a web modeling IDE tool. Software creation and management receives a moderate score due to general tooling implications, but the primary focus is on modeling notations. Software organization is less directly relevant.","Compilers:0.2,Context specific languages:0.3,Development frameworks and environments:0.9,Formal language definitions:0.4,General programming languages:0.3,Software configuration management and version control systems:0.2,Software libraries and repositories:0.6,Software maintenance tools:0.8,System description languages:0.7","Development frameworks and environments,Software maintenance tools,System description languages",Development frameworks and environments: The paper presents an IDE tool for web application development. Software maintenance tools: The tool supports maintenance through WebML templates. System description languages: WebML is used as a modeling language. Other options like compilers or formal languages are less relevant as the focus is on the IDE environment rather than language design or compilation.
2096,The Use of Intra-Release Product Measures in Predicting Release Readiness,"Modern business methods apply micro management techniques to all aspects of systems development. We investigate the use of product measures during the intra-release cycles of an application as a means of assessing release readiness. The measures include those derived from the Chidamber and Kemerer metric suite and some coupling measures of our own. Our research uses successive monthly snapshots during systems re-structuring, maintenance and testing cycles over a two year period on a commercial application written in C++. We examine the prevailing trends which the measures reveal at both component class and application level. By applying criteria to the measures we suggest that it is possible to evaluate the maturity and stability of the application thereby facilitating the project manager in making an informed decision on the application's fitness for release.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.9,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Software and its engineering,Software and its engineering is highly relevant as the paper evaluates software metrics for release readiness. Other categories like Applied computing are less relevant as the focus is on software development practices rather than specific applications.,"Software creation and management:1,Software organization and properties:0.75,Software notations and tools:0.25","Software creation and management,Software organization and properties",Software creation and management: Metrics for release readiness. Software organization: Evaluates application structure. Software notations are not directly relevant.,"Collaboration in software development:0.2,Contextual software domains:0.1,Designing software:0.4,Extra-functional properties:0.5,Software development process management:1.0,Software development techniques:0.6,Software functional properties:0.3,Software post-development issues:0.4,Software system structures:0.2,Software verification and validation:0.8","Software development process management,Software verification and validation",Software development process management: The paper discusses intra-release product measures for release readiness. Software verification and validation: The study relates to assessing software quality through metrics. Designing software and Extra-functional properties are moderately relevant but less central than the primary focus on development process and verification.
5974,Towards a REST Cloud Computing Lexicon,"Cloud computing is a popular Internet-based computing paradigm that provides on-demand computational services and resources, generally offered by cloud providers' REpresentational State Transfer (REST) APIs. To the best of our knowledge, there has been no study on the analysis of the lexicon adopted by cloud providers, despite its importance for developers. In this paper, we studied three different and well-known REST APIs (Google Cloud Platform, OpenStack, and Open Cloud Computing Interface) to investigate and organise their lexicons. This study presents three main contributions: 1) a tooled approach, called CLOUDLEX, for extracting and analysing REST cloud computing lexicons, 2) a dataset of services, resources, and terms used in the three studied REST APIs, 3) our analysis of this dataset, which represents a first attempt to provide a common REST cloud computing lexicon. After analysing our dataset, we observe that although the three studied REST APIs to describe the same domain (cloud computing), contrary to what one might expect, they do not share a large number of common terms, and only 5% of terms (17/352) are shared by two providers. Thus, the three APIs are lexically heterogeneous, and there is not a consensus on which terms to use on cloud computing systems. We discuss new avenues for cloud computing API designers and researchers.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.8,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.3,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Software and its engineering,Software and its engineering is relevant for the API design and lexicon analysis framework. Information systems receives a lower score as the focus is on software engineering rather than data management.,"Software creation and management:0.2,Software notations and tools:0.9,Software organization and properties:0.3",Software notations and tools,"Software notations and tools is highly relevant because the paper introduces CLOUDLEX, a tool for extracting and analyzing REST cloud computing lexicons. Software creation and management and organization are less relevant as the focus is on lexical analysis rather than software development processes or organization.","Compilers:0,Context specific languages:0.1,Development frameworks and environments:0.2,Formal language definitions:0.1,General programming languages:0.1,Software configuration management and version control systems:0.2,Software libraries and repositories:0.8,Software maintenance tools:0.1,System description languages:0.9","System description languages,Software libraries and repositories","System description languages is highly relevant as the paper analyzes REST API lexicons, which are a form of system description. Software libraries and repositories is relevant due to the dataset creation. Other options like compilers or formal language definitions are not directly related."
5640,Multi-rate Equivalents of Cyclo-Static Synchronous Dataflow Graphs,"In this paper, we present a transformation that takes a cyclo-static dataflow (CSDF) graph and produces an equivalent multi-rate synchronous dataflow (MRSDF) graph. This fills a gap in existing analysis techniques for synchronous dataflow graphs, transformations into equivalent homogeneous synchronous dataflow (HSDF) graphs exist, but these suffer from an exponential increase in the graph's size. The presented transformation allows the rich set of existing analysis techniques for MRSDF graphs to be applied to CSDF graphs. We show the applicability of this transformation by demonstrating its effectiveness on the problem of optimising buffer sizes under a throughput constraint.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.2,Software and its engineering:0.7,Theory of computation:0.1,Mathematics of computing:0.2,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.3,Applied computing:0.1,Social and professional topics:0.1",Software and its engineering,Software and its engineering is highly relevant because the paper discusses dataflow graph transformations for system design. Computing methodologies is moderately relevant due to the algorithmic aspects of the transformation. Other categories like Networks are less relevant as the focus is on software modeling rather than network infrastructure.,"Software creation and management:0.1,Software notations and tools:0.9,Software organization and properties:0.75","Software notations and tools,Software organization and properties","Software notations and tools: The paper presents a transformation technique for dataflow graphs, which is a core software tooling contribution. Software organization and properties: The analysis of graph properties (like buffer size optimization) relates to software structure. Software creation and management: Less relevant as the paper focuses on analysis tools rather than software creation processes.","Compilers:0.2,Context specific languages:0.1,Contextual software domains:0.1,Development frameworks and environments:0.1,Extra-functional properties:0.1,Formal language definitions:0.1,General programming languages:0.1,Software system structures:1,Software configuration management and version control systems:0.1,Software functional properties:0.1,Software libraries and repositories:0.1,Software maintenance tools:0.1,System description languages:0.1",Software system structures,Software system structures: The paper focuses on dataflow graph transformations for software systems. Other categories like Compilers or Programming languages are irrelevant to the domain-specific system structure analysis.
1097,A spatial path scheduling algorithm for EDGE architectures,"Growing on-chip wire delays are motivating architectural features that expose on-chip communication to the compiler. EDGE architectures are one example of communication-exposed microarchitectures in which the compiler forms dataflow graphs that specify how the microarchitecture maps instructions onto a distributed execution substrate. This paper describes a compiler scheduling algorithm called spatial path scheduling that factors in previously fixed locations - called anchor points - for each placement. This algorithm extends easily to different spatial topologies. We augment this basic algorithm with three heuristics: (1) local and global ALU and network link contention modeling, (2) global critical path estimates, and (3) dependence chain path reservation. We use simulated annealing to explore possible performance improvements and to motivate the augmented heuristics and their weighting functions. We show that the spatial path scheduling algorithm augmented with these three heuristics achieves a 21% average performance improvement over the best prior algorithm and comes within an average of 5% of the annealed performance for our benchmarks.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.8,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Software and its engineering,"Software and its engineering: The paper describes a compiler scheduling algorithm for EDGE architectures, which is a software engineering problem. Other categories like computer systems organization are less relevant compared to the compiler design focus.","Software creation and management:0.1,Software notations and tools:0.9,Software organization and properties:0.2",Software notations and tools,"Software notations and tools is relevant because the paper describes a compiler scheduling algorithm for EDGE architectures, which is a software tool/technique. Other categories are less relevant as the paper focuses on scheduling algorithms rather than general software management or organizational properties.","Compilers:1,Context specific languages:0,Development frameworks and environments:0,Formal language definitions:0,General programming languages:0,Software configuration management and version control systems:0,Software libraries and repositories:0,Software maintenance tools:0,System description languages:0",Compilers,Compilers is directly relevant as the algorithm is part of compiler scheduling. Other categories relate to software management or language design rather than compiler optimization.
688,Evaluating software development effort model-building techniques for application in a real-time telecommunications environment,"This paper describes the comparative evaluation of four methods of building software development effort models based on least squares regression, artificial neural networks, case-based reasoning and rule induction. Some deficiencies are identified in the main measurement of estimating effectiveness currently used in comparative evaluations, ‘mean magnitude of relative error’ (MMRE), and a complementary measurement, ‘mean variation from estimate’ (MVFE) is suggested as more accurately reflecting the practitioner's viewpoint. Given the current state of development of the techniques, the parallel use of least squares regression and case-based reasoning is recommended as appearing to give the most reliable results in the studied environment.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:1.0,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Software and its engineering,Software and its engineering is directly relevant for the software development effort modeling. Other categories are rejected because the paper focuses on software engineering metrics rather than hardware or theory.,"Software creation and management:0.9,Software notations and tools:0.2,Software organization and properties:0.3",Software creation and management,Software creation and management is relevant as the paper evaluates methods for building effort models in software development. The other categories are less directly related to the core focus on software project management.,"Collaboration in software development:0.2,Designing software:0.2,Software development process management:1,Software development techniques:1,Software post-development issues:0.2,Software verification and validation:0.2","Software development process management,Software development techniques",Software development process management: Evaluates techniques within a real-time environment. Software development techniques: Compares methods like regression and neural networks. Other options do not align with the focus on model-building techniques.
1640,A Document Based Framework for Smart Object Systems,"We present an architectural framework that provides the foundation for building smart object systems and uses a document centric approach utilizing a profile based artefact framework and a task based application framework. Our artefact framework represents an instrumented physical smart objects as a collection of service profiles and expresses these services in generic documents. Applications for smart objects are expressed as a collection of functional tasks (independent of the implementation) in a corresponding document. A runtime component provides the foundation for mapping these tasks to the corresponding service provider smart objects. This mapping is spontaneous and thus enables gradual addition of services. Primary advantages of our approach are twofold- firstly, it allows developers to write applications in a generic way regardless of the constraints of the target environment. Secondly, it allows extension of functionalities of smart objects and applications very easily. We describe an implemented prototype of FedNet, and show examples of its use in a real life scenario to illustrate its feasibility.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.8,Theory of computation:0.1,Mathematics of computing:0.2,Information systems:0.3,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.3,Applied computing:0.2,Social and professional topics:0.1",Software and its engineering,Software and its engineering is highly relevant as the paper presents a document-centric architectural framework for smart object systems. Other fields like Applied computing or Information systems are less central since the core contribution is the architectural methodology rather than specific applications.,"Software creation and management:0.8,Software notations and tools:0.7,Software organization and properties:0.1","Software creation and management,Software notations and tools",Software creation and management is relevant for the architectural framework design. Software notations and tools is relevant for the document-centric approach. Software organization and properties is less relevant as the focus is on framework design rather than software structure.,"Collaboration in software development:0.3,Compilers:0.1,Context specific languages:0.2,Designing software:0.7,Development frameworks and environments:0.9,Formal language definitions:0.1,General programming languages:0.2,Software configuration management and version control systems:0.3,Software development process management:0.4,Software development techniques:0.6,Software libraries and repositories:0.1,Software maintenance tools:0.1,Software post-development issues:0.1,Software verification and validation:0.1,System description languages:0.5","Development frameworks and environments,Designing software",Development frameworks and environments: The paper presents a document-based architectural framework for smart object systems. Designing software: The approach focuses on software design patterns for smart object applications. Software development techniques is secondary as the paper emphasizes architectural design rather than general techniques.
5671,A case study of a corporate open source development model,"Open source practices and tools have proven to be highly effective for overcoming the many problems of geographically distributed software development. We know relatively little, however, about the range of settings in which they work. In particular, can corporations use the open source development model effectively for software projects inside the corporate domain? Or are these tools and practices incompatible with development environments, management practices, and market-driven schedule and feature decisions typical of a commercial software house? We present a case study of open source software development methodology adopted by a significant commercial software project in the telecommunications domain. We extract a number of lessons learned from the experience, and identify open research questions.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:1.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Software and its engineering,"Software and its engineering is directly relevant as the paper studies open source development practices in a corporate software project. Other categories are rejected because the paper does not focus on hardware, algorithms, or mathematical theories.","Software creation and management:0.9,Software notations and tools:0.1,Software organization and properties:0.3",Software creation and management,"Software creation and management: The paper examines the application of open source development models in corporate software projects, which is a core topic in software management. Software organization and properties (0.3) is secondary as the focus is on development processes rather than software structure.","Collaboration in software development:0.5,Designing software:0.5,Software development process management:1,Software development techniques:0,Software post-development issues:0,Software verification and validation:0",Software development process management,Software development process management is relevant for analyzing open source models. Collaboration is only mentioned peripherally.
868,On Formalisms for Dynamic Reconfiguration of Dependable Systems,"Three formalisms of different kinds - VDM, Maude, and basic CCSdp - are evaluated for their suitability for the modelling and verification of dynamic software reconfiguration using as a case study the dynamic reconfiguration of a simple office workflow for order processing. The research is ongoing, and initial results are reported.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.9,Theory of computation:0.2,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Software and its engineering,"Software and its engineering is highly relevant as the paper evaluates formal methods for system modeling. Theory of computation is moderately relevant for formalisms, but the core focus is on software verification techniques.","Software notations and tools:0.85,Software organization and properties:0.7,Software creation and management:0.3","Software notations and tools,Software organization and properties","Software notations and tools: The paper evaluates formalisms (VDM, Maude) for dynamic reconfiguration modeling. Software organization and properties: The study addresses system properties through workflow reconfiguration. Other children (e.g., Software creation and management) are less relevant as the focus is on verification rather than development processes.","Compilers:0.1,Context specific languages:0.2,Contextual software domains:0.1,Development frameworks and environments:0.3,Extra-functional properties:0.1,Formal language definitions:0.6,General programming languages:0.2,Software configuration management and version control systems:0.7,Software functional properties:0.3,Software libraries and repositories:0.1,Software maintenance tools:0.4,Software system structures:0.5,System description languages:0.3","Formal language definitions,Software configuration management and version control systems","Formal language definitions is relevant as the paper evaluates formalisms (VDM, Maude) for reconfiguration modeling. Software configuration management is relevant since the work involves dynamic system reconfiguration. Compilers and contextual languages are less relevant."
562,How much really changes? A case study of firefox version evolution using a clone detector,"This paper focuses on the applicability of clone detectors for system evolution understanding. Specifically, it is a case study of Firefox for which the development release cycle changed from a slow release cycle to a fast release cycle two years ago. Since the transition of the release cycle, three times more versions of the software were deployed. To understand whether or not the changes between the newer versions are as significant as the changes in the older versions, we measured the similarity between consecutive versions.We analyzed 82MLOC of C/C++ code to compute the overall change distribution between all existing major versions of Firefox. The results indicate a significant decrease in the overall difference between many versions in the fast release cycle. We discuss the results and highlight how differently the versions have evolved in their respective release cycle. We also relate our results with other results assessing potential changes in the quality of Firefox. We conclude the paper by raising questions on the impact of a fast release cycle.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.9,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Software and its engineering,Software and its engineering is highly relevant because the paper studies software evolution through code analysis techniques. Other categories like Networks or Security and privacy are irrelevant as the focus is on software development practices.,"Software creation and management:0.0,Software notations and tools:1.0,Software organization and properties:0.0",Software notations and tools,"Software notations and tools is highly relevant as the paper evaluates a clone detector for analyzing code evolution. Software creation and management is less relevant as the focus is on tools for analysis, not software development processes.","Software configuration management and version control systems:1,Software maintenance tools:1,Development frameworks and environments:0.4,Other:0","Software configuration management and version control systems,Software maintenance tools",Software configuration management is relevant for analyzing Firefox version evolution. Software maintenance tools applies to clone detection analysis. Development frameworks are less relevant as the focus is on version control rather than development tools.
1529,Mobile Application for GBAS Air Traffic Status Unit,"At present, the Air Traffic Status Unit is a windows PC based application, which receives the status of ground based augmentation system station over Ethernet and displays on the screen. The objective of this project is to convert the PC based Application into Mobile application using Android OS.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.9,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Software and its engineering,Software and its engineering is highly relevant as the paper describes mobile application development for an air traffic status unit. Other fields like Networks are not central to the core contribution of software porting and mobile application design.,"Software creation and management:1.0,Software notations and tools:0.2,Software organization and properties:0.3",Software creation and management,"Software creation and management: The paper's primary focus is porting a PC application to Android OS, which directly aligns with software creation/management. Other categories relate to software properties or tools but are secondary to the main task of software development and adaptation.","Collaboration in software development:0.1,Designing software:1.0,Software development process management:0.4,Software development techniques:1.0,Software post-development issues:0.2,Software verification and validation:0.2","Designing software,Software development techniques",Designing software: The paper focuses on converting a PC application to a mobile platform. Software development techniques: Discusses Android OS adaptation as a development approach. Rejected: Collaboration and verification are not discussed.
2484,A formalisation of analysis-based model migration,"Supporting adaptation of metamodels is essential for realising Model-Driven Engineering. However, adapting and changing metamodels impact other artefacts of the metamodelling ecosystem. In particular, conformant models will no longer be valid instances of their changed metamodel. This gives rise to co-evolution issues where metamodels and models are no longer synchronised. This is critical as systems become inconsistent. A typical approach for re-establishing conformance is to manually craft transformations which update existing models for the new metamodel variant. In this paper we present an analysis-based approach that addresses this concern. The approach enables an arbitrary number of metamodels to evolve based on an adaptation strategy. During analysis we accumulate information required to automatically transform existing models to ensure conformance. We formalise the approach and prove model conformance.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.9,Theory of computation:0.1,Mathematics of computing:0.3,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Software and its engineering,"Software and its engineering is highly relevant as the paper formalizes model migration techniques in Model-Driven Engineering. Mathematics of computing is marginally relevant for formal proofs, but the core focus is software model transformation.","Software notations and tools:0.9,Software organization and properties:0.85,Software creation and management:0.6,Modeling and simulation:0.1,Concurrent computing methodologies:0.1","Software notations and tools,Software organization and properties",Software notations and tools is highly relevant for the formalization of model transformations. Software organization and properties is relevant for ensuring model conformance. Software creation and management receives moderate relevance due to the focus on model adaptation. Other categories are irrelevant.,"Compilers:0.1,Context specific languages:0.1,Contextual software domains:0.1,Development frameworks and environments:0.2,Extra-functional properties:0.1,Formal language definitions:1.0,General programming languages:0.1,Software configuration management and version control systems:0.3,Software functional properties:0.2,Software libraries and repositories:0.1,Software maintenance tools:0.7,Software system structures:0.3,System description languages:0.4","Formal language definitions,Software maintenance tools",Formal language definitions (metamodel analysis) and Software maintenance tools (model migration) are core. Other options like Compilers are irrelevant to model-driven engineering.
5347,Objective View Point: an introduction to C++,"Welcome to the inaugural edition of the ObjectiveViewPoint column! Here we will touch on many aspects of object-orientation. The word object has surfaced in more ways than you can count. There are OOPLs (Object-Oriented Programming Languages) and OODBs (Object-Oriented Databases), OOA (object-oriented analysis), and OOD (objectoriented design). We are sure you can come up with some OOisms of your own.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.9,Theory of computation:0.2,Mathematics of computing:0.2,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.3,Applied computing:0.1,Social and professional topics:0.1",Software and its engineering,"Software and its engineering is highly relevant as the paper introduces object-oriented programming concepts in C++. Other categories are rejected because the focus is on programming paradigms, not on systems, algorithms, or applications.","Software creation and management:1.0,Software notations and tools:0.75,Software organization and properties:0.0","Software creation and management,Software notations and tools",Software creation and management is highly relevant for teaching OOP concepts. Software notations and tools is relevant for programming language discussion. Software organization is not central to the abstract.,"Collaboration in software development:0.1,Compilers:0.1,Context specific languages:0.1,Designing software:1.0,Development frameworks and environments:0.1,Formal language definitions:0.1,General programming languages:1.0,Software configuration management and version control systems:0.1,Software development process management:0.1,Software development techniques:0.6,Software libraries and repositories:0.1,Software maintenance tools:0.1,Software post-development issues:0.1,Software verification and validation:0.1,System description languages:0.1","Designing software,General programming languages",Designing software and General programming languages are central to discussing object-oriented principles. Other categories like 'Software development techniques' are tangential.
4598,Software Engineering Challenges for Migration to the Service Cloud Paradigm: Ongoing Work in the REMICS Project,"This paper presents on-going work in a research project on defining methodology and tools for model-driven migration of legacy applications to a service-oriented architecture with deployment in the cloud, i.e. the Service Cloud paradigm. We have performed a comprehensive state of the art analysis and present some findings here. In parallel, the two industrial participants in the project have specified their requirements and expectations regarding modernization of their applications. The SOA paradigm implies the breakdown of architecture into high-grain components providing business services. For taking advantage of the services of cloud computing technologies, the clients' architecture should be decomposed, decoupled and be made scalable. Also requirements regarding servers, data storage and security, networking and response time, business models and pricing should be projected. We present software engineering challenges related to these aspects and examples of these in the context of one of the industrial cases in the project.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:1.0,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Software and its engineering,Software and its engineering is relevant because the paper discusses challenges in migrating legacy systems to cloud-based service architectures. Other categories like Hardware or Security are less relevant as the focus is on software development processes and methodologies.,"Software creation and management:0.9,Software notations and tools:0.8,Software organization and properties:0.7","Software creation and management,Software notations and tools,Software organization and properties",Software creation and management: Focuses on methodologies for cloud migration. Software notations and tools: Involves model-driven tools for legacy systems. Software organization and properties: Discusses cloud architecture properties. All align with software engineering challenges.,"Collaboration in software development:0.3,Compilers:0.1,Context specific languages:0.1,Contextual software domains:0.2,Designing software:0.3,Development frameworks and environments:0.4,Extra-functional properties:0.2,Formal language definitions:0.1,General programming languages:0.1,Software configuration management and version control systems:0.2,Software development process management:0.3,Software development techniques:0.6,Software functional properties:0.2,Software libraries and repositories:0.1,Software maintenance tools:0.2,Software post-development issues:0.2,Software system structures:0.5,Software verification and validation:0.2,System description languages:0.1","Software development techniques,Software system structures",Software development techniques: The paper addresses methodologies for model-driven migration to service-oriented architectures. Software system structures: The discussion of decomposing architectures into service components relates directly to system structure design. Other options like Compilers are not central to the architectural migration focus.
63,Lightweight source model extraction,"Reverse engineers depend on the automatic extract ion of information from source code. Some useful kinds of information+ource models—are wellknown: call graphs, file dependence, etc. Predicting every kind of source model that a reverse engineer may need is impossible. We have developed a lightweight approach for generating flexible and tolerant source model extractors from lexical specificat ions. The approach is lightweight in that the specifications are relatively small and easy to write. It is flexible in that there are few constraints on the information in the source that can be extracted (e.g., we can extract from macros, comments, etc.). It is tolerant in that information can be extracted from source that cannot necessarily be compiled or linked. In essence, we scan for source constructs that contribute to the specified source model while ignoring constructs that do not contribute to that source model. We have developed tools to support this approach and applied the tools to the extraction of a number of different source models (file dependence, event interactions, call graphs) from systems implemented in a variety of programming languages (C, C++, CLOS, Eiffel). We discuss our approach and describe its application to extract source models not available using existing systems; for example, we compute the invoked-by relation over Field tools. We compare and contrast our approach to the conventional approach of generating source models from a program database. * This research was funded in part by the NSF grant CCR-8858804 and a Canadian NSERC post-graduate scholarship. Permission to make cfigital~ard copies of all or part of this material without fee is granted provided that the copies are not made or distributed for profit or commercial advantage, the ACM copyrighffsewer notioe, the title of the pubiioation and its date appear, and notice is given that copyright is by permission of the Association for Computing Machinery, Inc. (ACM). To copy otherwise: to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. SIGSOFT ’95 Washington, D.C., USA 01995 ACM 0-89791-71 8-2/95/0010...$3.50","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:1.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Software and its engineering,"Software and its engineering: The paper presents lightweight source model extraction techniques for reverse engineering, which is a software engineering methodology.","Software creation and management:0.7,Software notations and tools:0.9,Software organization and properties:0.3",Software notations and tools,Software notations and tools is relevant for the lightweight source model extraction approach. Software creation and management is secondary as the focus is on extraction techniques rather than development processes.,"Compilers:1,Context specific languages:0,Development frameworks and environments:0,Formal language definitions:0,General programming languages:0,Software configuration management and version control systems:0,Software libraries and repositories:0,Software maintenance tools:1,System description languages:0","Compilers,Software maintenance tools","Compilers: The lightweight source model extraction approach involves lexical specifications, a compiler-like process. Software maintenance tools: The extractors aid in understanding and maintaining source code. Other options like 'General programming languages' are irrelevant as the focus is on tooling, not language design."
3753,Data Management for the RedisDG Scientific Workflow Engine,"In this paper we investigate the general problem of controlling a scientic workflow service in termsof data management. We focus on the data managementproblem for the RedisDG scientific workflow engine.RedisDG is based on the Publish/Subscribe paradigmfor the interaction between the different componentsof the system, hence new issues appear for scheduling. Indeed, the Publish/Subscribe paradigm utilization introduces different challenging problems, among them the design of effective solutions for managing data, on the fly, when tasks are published. Our contributions are twofold. First we add new functionalities to the RedisDG workflow engine with scheduling decisions related to the allocation of data intensive jobs to compute units and according to an efficient management of data and second we introduce a large set of experiments tovalidate our approaches. We analyze our results and wealso sketch perspectives and insights. Experiments areconducted on the Grid'5000 testbed and the paper isa step forward to implement a 'Workflow engine as aService' (WaaS).","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.8,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.2,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Software and its engineering,Software and its engineering is relevant for the RedisDG workflow engine and data management. Information systems is marginally relevant for workflow scheduling but not the core contribution.,"Software creation and management:0.3,Software notations and tools:0.2,Software organization and properties:0.7",Software organization and properties,Software organization and properties (0.7): The paper presents data management extensions for a workflow engine. Software creation (0.3) is less central than the organizational implications.,"Contextual software domains:0,Extra-functional properties:0,Software functional properties:1,Software system structures:1","Software functional properties,Software system structures","Software functional properties: The paper discusses data management and scheduling decisions in a workflow engine, which are core functional aspects. Software system structures: The RedisDG workflow engine's design and structure are central to the paper's contributions. Other children are irrelevant as they do not align with the paper's focus on workflow data management."
1558,A generalization of ALGOL,"For purposes of programming in machine code a problem must normally be so minutely fragmented that the task of attempting to identify the problem from the program is like trying to reconstruct a table from sawdust. This is a prime cause of difficulty in checking new programs for errors, and it does not seem to have received from the writers of compilers the attention it deserves. We are trying to do something about this by making the operations defined by sentences as comprehensive as possible. To take a simple example,","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.9,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Software and its engineering,"Software and its engineering: The paper generalizes ALGOL programming concepts to improve code comprehension and error checking, which is a core software engineering contribution. Other categories are not relevant.","Software creation and management:0.9,Software notations and tools:0.1,Software organization and properties:0.1",Software creation and management,Software creation and management: The paper discusses improving programming practices for machine code readability. Other options are irrelevant as the focus is on code structure and maintainability rather than formal notations or software architecture.,"Collaboration in software development:0.2,Designing software:1.0,Software development process management:0.3,Software development techniques:1.0,Software post-development issues:0.1,Software verification and validation:0.2","Designing software,Software development techniques",Designing software: The paper focuses on improving programming practices through comprehensive operations. Software development techniques: It introduces methods to enhance problem identification in programming. Other options like Software verification and validation are less relevant as the focus is on design rather than testing.
3574,System modeling in SYsML and system analysis in Arena,"A Model Driven Architecture approach is employed to support the practice of discrete-event simulation. OMG's System Model Language, OMG SysML™, is used to define a platform independent model (PIM) and auto-translate it into an appropriate platform specific model (PSM). The implementation and the nature of the transformation from PIM to PSM are clearly addressed to enable: (i) formal modeling of systems using their own semantics in SysML, (ii) SysML model verification and validation by stakeholders, (iii) automatic translation of system models expressed in SysML into analysis models as the PSM, and (iv) maintainability of this approach to accommodate system changes and extensions very easily. The proposed approach can be used for any analysis tool and application domain. In this paper, we choose to model transaction-based examples elicited from the manufacturing domain in SysML and translate them into Arena™ models using the Atlas Transformation Language.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:1.0,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Software and its engineering,Software and its engineering is relevant for model-driven architecture and SysML. Other fields do not address software modeling or transformation techniques.,"Software creation and management:0.7,Software notations and tools:0.9,Software organization and properties:0.1","Software notations and tools,Software creation and management","The paper uses SysML and transformation tools for model-driven simulation, making 'Software notations and tools' primary. 'Software creation and management' is included for the model-driven architecture approach. Other categories are not relevant.","Collaboration in software development:0.2,Compilers:0.1,Context specific languages:0.3,Designing software:0.4,Development frameworks and environments:0.6,Formal language definitions:0.5,General programming languages:0.3,Software configuration management and version control systems:0.2,Software development process management:0.4,Software development techniques:0.6,Software libraries and repositories:0.2,Software maintenance tools:0.4,Software post-development issues:0.3,Software verification and validation:0.7,System description languages:1","System description languages,Software verification and validation,Software development techniques","System description languages: The paper uses SysML for platform-independent modeling. Software verification and validation: The approach includes model validation by stakeholders. Software development techniques: The paper discusses model-driven architecture and automatic translation. Rejected categories like Compilers are irrelevant as the focus is on modeling languages, not compilation."
3166,A Guide To Deal With Uncertainties In Software Project Management,Various project management approaches do not consider the impact that uncertainties have on the project. The identified threats by uncertainty in a projec day-to-day are real and immediate and the expectations in a project are often high. The project manager faces a dilemma: decisions must be made in the present about future situations which are inherently uncertain. The use of uncertainty management in project can be a determining factor for the project success. This paper presents a systematic review about uncertainties management in software projects and a guide is proposed based on the review. It aims to present the best practices to manage uncertainties in software projects in a structured way including techniques and strategies to uncertainties containment.,"General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.7,Theory of computation:0.1,Mathematics of computing:0.2,Information systems:0.2,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.3,Applied computing:0.3,Social and professional topics:0.1",Software and its engineering,Software and its engineering: The paper focuses on software project management practices and uncertainty handling. Other categories like 'Applied computing' are less relevant as the work is about management techniques rather than computational systems.,"Software creation and management:0.9,Software notations and tools:0.1,Software organization and properties:0.1",Software creation and management,"Software creation and management is relevant because the paper addresses uncertainty management in software project management. Other categories are rejected as the focus is on project management practices, not software tools or formal properties.","Collaboration in software development:0.2,Designing software:0.3,Software development process management:1.0,Software development techniques:0.8,Software post-development issues:0.1,Software verification and validation:0.2","Software development process management,Software development techniques",Software development process management is directly addressed in the systematic review and guide for uncertainty management in project workflows. Software development techniques are relevant as the paper proposes strategies and techniques for uncertainty containment. Other options like collaboration or verification are tangential and not central to the core contribution.
4188,Effects of FSM minimization techniques on number of test paths in mobile applications MBT,"Model Based Testing (MBT) is defined as the process of generating functional test cases based on a model that describes the behavior of an application under test (AUT). It is used to ensure that the intended purpose of the application will function as expected, where if any change in the behavior is encountered it would be reported as a bug. Previous research in MBT has only focused on automating the generation step of the application's model. Given that testing each path of the model would take infinite time since the generated model is usually huge and with multiple iterations, researchers have also focused on different techniques of test case selection and generation. However, it is not yet known if the generated model could be enhanced before the test case selection step, and whether this enhancement could positively affect the number of test paths or not. The aim of our research is to enhance MBT by first minimizing the generated model; removing any equivalent states; and examining the effect of minimization on the generated number of test paths using different test coverage criteria.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:1.0,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Software and its engineering,Software and its engineering is highly relevant as the paper focuses on model-based testing and FSM minimization for mobile applications. Other categories like Mathematics are secondary.,"Software creation and management:0.8,Software notations and tools:0.75,Software organization and properties:0.2","Software creation and management,Software notations and tools","Software creation and management: The paper enhances MBT by minimizing models during test creation. Software notations and tools: FSM minimization is a modeling technique for test generation. Software organization is less relevant as the focus is on testing, not code structure.","Software verification and validation:1,Software development techniques:1,Collaboration in software development:0,Compilers:0,Context specific languages:0,Designing software:0,Development frameworks and environments:0,Formal language definitions:0,General programming languages:0,Software configuration management and version control systems:0,Software development process management:0,Software libraries and repositories:0,Software maintenance tools:0,Software post-development issues:0","Software verification and validation,Software development techniques","Software verification and validation: The paper studies test path optimization in MBT, a core verification activity. Software development techniques: Model-based testing is a development methodology. Other categories like compilers or formal languages are not relevant."
2928,Generating Distinguishing Tests Using the Minion Constraint Solver,"We discuss the generation of test cases for demonstrating the non-equivalence of two programs. This problem has applications in mutation testing and automated debugging. After transforming the programs into Static-Single-Assignment form, the MINION constraint solver is used to generate test vectors that demonstrate the observable difference. The experiments show the feasability of our constraint solving approach.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.2,Networks:0.1,Software and its engineering:1.0,Theory of computation:0.3,Mathematics of computing:0.2,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.3,Applied computing:0.1,Social and professional topics:0.1",Software and its engineering,"Software and its engineering is directly relevant because the paper focuses on test case generation for program non-equivalence, a core software testing methodology. Other categories like Theory of computation or Mathematics of computing are tangential and not the primary focus.","Software creation and management:0.8,Software notations and tools:0.9,Software organization and properties:0.4","Software notations and tools,Software creation and management","Software notations and tools is relevant because the paper discusses using the MINION constraint solver for test generation, a tool-based approach. Software creation and management is relevant as test generation is a core part of software development. Software organization and properties is less relevant as the focus is on testing methods rather than software structure.","Collaboration in software development:0,Compilers:0,Context specific languages:0,Designing software:0,Development frameworks and environments:0,Formal language definitions:0,General programming languages:0,Software configuration management and version control systems:0,Software development process management:0,Software development techniques:1,Software libraries and repositories:0,Software maintenance tools:0,Software post-development issues:0,Software verification and validation:1,System description languages:0","Software verification and validation,Software development techniques",Software verification and validation: The paper uses constraint solving to test program non-equivalence. Software development techniques: Involves generating test cases for debugging. Other categories like compilers are irrelevant.
5150,Multi-platform Version of StarCraft: Brood War in a Docker Container: Technical Report,"We present a dockerized version of a real-time strategy game StarCraft: Brood War, commonly used as a domain for AI research, with a pre-installed collection of AI developement tools supporting all the major types of StarCraft bots. This provides a convenient way to deploy StarCraft AIs on numerous hosts at once and across multiple platforms despite limited OS support of StarCraft. In this technical report, we describe the design of our Docker images and present a few use cases.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:1.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.5,Social and professional topics:0.0",Software and its engineering,Relevant to 'Software and its engineering' for containerization and deployment of software. 'Applied computing' is rejected as the focus is on tools rather than domain-specific applications.,"Software creation and management:0.9,Software notations and tools:0.6,Software organization and properties:0.4",Software creation and management,Software creation and management directly addresses Docker containerization for deployment. Software notations and tools relate to AI development but are secondary to the core deployment contribution.,"Collaboration in software development:0.1,Designing software:0.2,Software development process management:0.3,Software development techniques:1.0,Software post-development issues:0.8,Software verification and validation:0.4","Software development techniques,Software post-development issues",Software development techniques: The paper introduces a Docker-based deployment methodology. Software post-development issues: Focuses on deployment and cross-platform support. Other options like verification are less relevant.
922,Step-Locked Data Transformation in Personal Filing Systems,"In comparison with the internal representation of a personal filing systems, which is operational more efficient, the filing organization as presented to the users has its simplicity because it maintains only a single type of link. Therefore, the implementation of the system can have two models which represent the folder organization at two different levels: the user interface level (or the representation using U-ORG), and the system execution level (or the internal representation using I-ORG). Inter operabilities between the two models needs to be well-coordinated and kept transparent to the user while the system optimizes its performance by utilizing the architectural strength from both models. In this paper, we explore the co-existence issues between the U-ORG and the I-ORG folder organization and present a preliminary transformation process which transforms the given U-ORG into an IORG representation when the user folder organization is constructed.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:1.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.2,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Software and its engineering,Software and its engineering is directly relevant as the paper discusses system design and data transformation in filing systems. Information systems is marginally relevant due to data organization but is secondary to the software engineering focus.,"Software creation and management:0.5,Software notations and tools:0.5,Software organization and properties:1.0",Software organization and properties,Software organization and properties: The paper focuses on folder organization models (U-ORG/I-ORG) in personal filing systems. Other categories like Software creation are less relevant as the focus is on structure rather than creation.,"Contextual software domains:0,Extra-functional properties:1,Software functional properties:0,Software system structures:1","Extra-functional properties,Software system structures",Extra-functional properties is relevant for performance optimization. Software system structures is relevant for the dual-model architecture. Other categories are not central to the system's design.
373,Formal Verification of Consistency in Model-Driven Development of Distributed Communicating Systems and Communication Protocols,"Currently UML2 is widely used for modelling software-intensive systems. Model driven development of complex software typically starts from abstract, high-level UML2 models which specify the system from several different viewpoints. Abstract models are further refined into more detailed design models in successive development stages. While specifying various aspects and abstraction levels of such systems, we create a set of different models, which should be inter- and intra-consistent. In this paper we propose an approach to ensuring consistency in Lyra - a rigorous, service-oriented and model-based method for developing industrial telecommunication systems and communication protocols. We derive informal requirements to ensuring intra- and inter- consistency and then formalize them in the B method. The formalization in B allows us to structure complex informal requirements and formally ensure intra- and inter-consistency of models created at various stages of the Lyra development.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:1.0,Theory of computation:0.5,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.25,Social and professional topics:0.0",Software and its engineering,Software and its engineering is highly relevant for the formal verification of model consistency in distributed systems using the B method. Theory of computation receives a lower score as the focus is on model-driven development rather than computational theory. Applied computing is less relevant as the paper discusses formal methods rather than deployment.,"Software creation and management:1.0,Software notations and tools:0.8,Software organization and properties:0.7","Software creation and management,Software notations and tools",Software creation and management is highly relevant for model-driven development processes. Software notations and tools is relevant for B method formalization. Software organization and properties is secondary for consistency requirements.,"Collaboration in software development:0.1,Compilers:0,Context specific languages:0,Designing software:0.3,Development frameworks and environments:0.2,Formal language definitions:0.4,General programming languages:0,Software configuration management and version control systems:0.1,Software development process management:0.2,Software development techniques:0.3,Software libraries and repositories:0.1,Software maintenance tools:0.1,Software post-development issues:0.2,Software verification and validation:1,System description languages:0.5","Software verification and validation,Formal language definitions",Software verification and validation is central to formal consistency checking. Formal language definitions are relevant to B method formalizations. Other categories are less directly related to model consistency.
1027,Design and Development Tools for Next Generation Mobile Services,"The actual standards for service authoring, composition and development are not easy to port and to apply for next generation mobile applications. This paper describes some tools that we're developing in the context of the IST-Simple Mobile Service project, whose aim is to ease the authoring and the use of services for mobile devices. We propose a service composition approach using an UML profile very close to the actual standards for Web services definition and authoring, like WSDL and BPEL. We take a glance at SMILE, the run-time support we provide for service execution. Finally we hint at an efficient serialization mechanism based on JSON, a human readable data exchange format less verbose and, in our opinion, more suitable for mobile terminals than XML.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.9,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.2,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.3,Applied computing:0.2,Social and professional topics:0.1",Software and its engineering,"Software and its engineering: The paper discusses tools and methodologies for mobile service development, including UML profiles and JSON serialization. Categories like Applied computing or Networks are less relevant as the focus is on software development tools rather than applications or communication systems.","Software creation and management:0.8,Software notations and tools:0.8,Software organization and properties:0.1","Software creation and management,Software notations and tools",Software creation and management is relevant for tools in mobile service development. Software notations and tools is relevant for the UML profile and JSON-based serialization. The third category is less central to the paper's focus.,"Collaboration in software development:0.1,Compilers:0.1,Context specific languages:0.2,Designing software:0.1,Development frameworks and environments:0.9,Formal language definitions:0.1,General programming languages:0.1,Software configuration management and version control systems:0.1,Software development process management:0.3,Software development techniques:0.8,Software libraries and repositories:0.1,Software maintenance tools:0.1,Software post-development issues:0.1,Software verification and validation:0.1,System description languages:0.7","Development frameworks and environments,System description languages,Software development techniques",Development frameworks and environments is relevant because the paper introduces tools for service development. System description languages is relevant due to the use of UML and JSON. Software development techniques is relevant for the proposed service composition approach. Other categories like Compilers are not directly addressed.
852,The FrAgile Organisation,This experience report outlines how the BBC went about adopting Agile and how the stickiness of Agile fared through a couple of restructures. Arguably some of the most important new media launches from BBC New Media would not have happened if it were not for the Agile techniques used however there has been a very real feeling within the organisation that way of doing things has been under threat. I hope to address the issue of whether an Agile culture is fragile and why it might slip back into a former more familiar way of doing things even if this is not intended and begins to erode the ability of the organisation to develop and release good quality software.,"General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:1.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Software and its engineering,Software and its engineering is highly relevant because the paper discusses Agile methodologies in software development practices within an organization. Other categories like Human-centered computing receive low scores because the focus is on organizational software practices rather than user experience.,"Software creation and management:0.9,Software notations and tools:0.1,Software organization and properties:0.8","Software creation and management,Software organization and properties",Software creation and management is relevant as the paper discusses Agile adoption in software development. Software organization and properties is relevant for organizational structure analysis. Other categories are not central to the focus on Agile practices.,"Software development process management:1,Collaboration in software development:0.8,Designing software:0.5,Extra-functional properties:0.2","Software development process management,Collaboration in software development",Software development process management: Discusses Agile adoption and organizational challenges. Collaboration in software development: Mentions team dynamics during restructures. Designing software is secondary; others are unrelated to process management.
1207,Automated DCI compliance test system for Digital Cinema Entities,"CTP v1.0 was released in October 2007 from DCI to provide procedural guideline for compliance test on Digital Cinema Entities. It was designed to guarantee seamless interoperability and reliability between the entities of different vendors. But all CTP procedures require the test operator to be a professional engineer on the security and digital cinema technology. This paper is about an automated DCI compliance test system based on CTP v1.1 which provides very convenient testing environment even to non-professional test operator. It analyzes existing time consuming and manual oriented CTP test procedure, then proposes a new test process with automated method, detail architecture and functions of the proposed system. Finally it shows the implemented system and cross testing results with the commercial entities.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.8,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Software and its engineering,Software and its engineering is relevant because the paper describes an automated test system for digital cinema entities. Other categories like 'Security and privacy' are less directly relevant as the focus is on testing infrastructure rather than security protocols.,"Software creation and management:0.8,Software notations and tools:0.3,Software organization and properties:0.5",Software creation and management,"Software creation and management is highly relevant as the paper describes the development of an automated compliance test system, including its architecture and implementation. Software organization and properties is moderately relevant due to testing aspects, while notations/tools are less relevant as the focus is on system design rather than tools.","Collaboration in software development:0.1,Designing software:0.3,Software development process management:0.2,Software development techniques:0.4,Software post-development issues:0.2,Software verification and validation:1.0","Software verification and validation,Software development techniques","Software verification and validation is directly relevant due to compliance testing. Software development techniques applies as the paper discusses automated testing system design. Other categories are less relevant as the focus is not on collaboration, process management, or post-development phases."
2028,Clean your variable code with featureIDE,"FeatureIDE is an open-source framework to model, develop, and analyze feature-oriented software product lines. It is mainly developed in a cooperation between University of Magdeburg and Metop GmbH. Nevertheless, many other institutions contributed to it in the past decade. Goal of this tutorial is to illustrate how FeatureIDE can be used to clean variable code, whereas we will focus on dependencies in feature models and on variability implemented with preprocessors. The hands-on tutorial will be highly interactive and is devoted to practitioners facing problems with variability, lecturers teaching product lines, and researchers who want to safe resources in building product line tools.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.95,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Software and its engineering,"Software and its engineering is highly relevant because the paper focuses on FeatureIDE, a framework for developing and analyzing feature-oriented software product lines. Other categories like Hardware or Security and privacy are not discussed in the core contribution.","Software creation and management:0.7,Software notations and tools:0.9,Software organization and properties:0.3",Software notations and tools,"Software notations and tools: FeatureIDE is a tool for software product line management. Software creation and management is secondary as the focus is on tooling rather than process. Organization and properties are rejected as the paper focuses on tool usage, not theoretical properties.","Compilers:0.0,Context specific languages:0.0,Development frameworks and environments:1.0,Formal language definitions:0.0,General programming languages:0.0,Software configuration management and version control systems:0.7,Software libraries and repositories:0.3,Software maintenance tools:0.6,System description languages:0.0","Development frameworks and environments,Software configuration management and version control systems",Development frameworks and environments: FeatureIDE is a framework for product line development. Software configuration management: The paper discusses managing variability in code. Other options like Software maintenance tools are less central to the framework's purpose.
2660,Advanced code coverage analysis using substring holes,"Code coverage is a common aid in the testing process. It is generally used for marking the source code segments that were executed and, more importantly, those that were not executed.
 Many code coverage tools exist, supporting a variety of languages and operating systems. Unfortunately, these tools provide little or no assistance when code coverage data is voluminous. Such quantities are typical of system tests and even for earlier testing phases. Drill-down capabilities that look at different granularities of the data, starting with directories and going through files to functions and lines of source code, are insufficient. Such capabilities make the assumption that the coverage issues themselves follow the code hierarchy. We argue that this is not the case for much of the uncovered code. Two notable examples are error handling code and platform-specific constructs. Both tend to be spread throughout the source in many files, even though the related coverage, or lack thereof, is highly dependent.
 To make the task more manageable, and therefore more likely to be performed by users, we developed a hole analysis algorithm and tool that is based on common substrings in the names of functions. We tested its effectiveness using two large IBM software systems. In both of them, we asked domain experts to judge the results of several hole-ranking heuristics. They found that 57% - 87% of the 30 top-ranked holes identified by the effective heuristics are relevant. Moreover, these holes are often unexpected. This is especially impressive because substring hole analysis relies only on the names of functions, whereas domain experts have a broad and deep understanding of the system.
 We grounded our results in a theoretical framework that states desirable mathematical properties of hole ranking heuristics. The empirical results show that heuristics with these properties tend to perform better, and do so more consistently, than heuristics lacking them.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.8,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Software and its engineering,Software and its engineering: The paper introduces a code coverage analysis tool for software testing. Other categories are irrelevant as the focus is on software quality assurance techniques.,"Software creation and management:0.5,Software notations and tools:0.9,Software organization and properties:0.7","Software notations and tools,Software organization and properties",Software notations and tools is directly relevant for the code coverage analysis tool. Software organization and properties is relevant for analyzing code structure through function naming patterns. Software creation and management is secondary as it relates to testing processes.,"Compilers:0,Context specific languages:0,Contextual software domains:0,Development frameworks and environments:0,Extra-functional properties:0,Formal language definitions:0,General programming languages:0,Software configuration management and version control systems:0,Software functional properties:1,Software libraries and repositories:0,Software maintenance tools:1,Software system structures:0,System description languages:0","Software maintenance tools,Software functional properties",Software maintenance tools is highly relevant as the paper presents a new tool for code coverage analysis. Software functional properties is relevant because the paper evaluates how well the tool identifies function coverage issues. Other software categories are not directly addressed.
3989,An OS interface for active routers,"This paper describes an operating system (OS) interface for active routers. This interface allows code loaded into active routers to access the router's memory, communication, and computational resources on behalf of different packet flows. In addition to motivating and describing the interface, the paper also reports our experiences implementing the interface in three different OS environments: Scout, the OSKit, and the esokernel.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:1.0,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Software and its engineering,Software and its engineering is directly relevant for the OS interface design and implementation in active routers. Other categories like 'Computer systems organization' are less central to the interface specification.,"Software creation and management:1.0,Software notations and tools:0.5,Software organization and properties:0.8","Software creation and management,Software organization and properties",Software creation and management: The paper designs an OS interface for active routers. Software organization and properties: The implementation across OS environments reflects software architecture and properties. Software notations and tools are secondary to the interface design.,"Collaboration in software development:0,Contextual software domains:0,Designing software:1,Extra-functional properties:0,Software development process management:0,Software development techniques:1,Software functional properties:0,Software post-development issues:0,Software system structures:1,Software verification and validation:0","Designing software,Software system structures",Designing software is central to creating the OS interface for active routers. Software system structures are relevant for the implementation across different OS environments. Other categories are secondary to the core contribution.
1444,Future user requirement elicitation for technology investment: A formal approach,"This study addresses the lack of methods to systematically identify future market needs for strategic planning in technology investment. Uncertain and ambiguous future, hardly available user input on realistic needs, market with latent demand and evolving market increase technology investment risk. The authors presents a formal approach named Requirement Elicitation of Future Users by Systems Scenarios (REFUSS) that derives future user requirements by relating process knowledge obtained from current system model with plausible future user lifestyle related information obtained from scenarios. Scenarios developed in a cause-effect way by identifying and correlating a number of environmental factors assists strategists to learn about plausible changes and be prepared for any unexpected shift. The REFUSS fills a gap by providing a simple and applicable method for future user requirement elicitation. This can be used for formulating needs based vision thus avoiding project failures from following technology based vision.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:1.0,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.3,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Software and its engineering,Software and its engineering: The paper presents a formal method for future user requirement elicitation in software development. Other fields like Information Systems are secondary to the software engineering methodology.,"Software creation and management:1.0,Software notations and tools:0.3,Software organization and properties:0.4",Software creation and management,"Software creation and management is relevant as the paper discusses a formal approach for eliciting future user requirements, which is part of software development processes. Other categories are not directly related to the method described.","Collaboration in software development:0,Designing software:0,Software development process management:0,Software development techniques:1,Software post-development issues:0,Software verification and validation:0",Software development techniques,"Software development techniques: The REFUSS method is a formal approach for requirement elicitation, a critical software development technique. Other categories were rejected as the paper does not focus on collaboration, design, or verification."
3195,ServiceDesigner: a tool to help end-users become individual service providers,"Pervasive and ubiquitous computing will fundamentally change the mode of interaction between humans and computers; instead of working with applications on a desktop computer we will interact with our environment through electronic services - anytime and anywhere. In this new modus operandi, specialized and personalized services will become much more important; the usual software house solutions may not be sufficient for individual demands. We propose that end-users themselves can be the service providers; the incentive to create services is grounded in each individual personal demand for well suited services and this demand will only increase when technology makes it possible to access services ubiquitously. Individual service provisioning requires three parts: a general platform for managing and accessing electronic services; simple but powerful tools to create the services; and the means to share services between users. Building on previous work developing sView, a personal service environment, this paper presents the second part -ServiceDesigner - a tool for creating new services for sView. ServiceDesigner, using Web services that expose the functions of Web sites as programmatically accessible components, lets end-users create personalized and functional electronic services that fit in the personal platform. With ServiceDesigner, Web services are directly available to users and finished services can also be shared with other users.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.2,Software and its engineering:0.8,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.3,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Software and its engineering,Software and its engineering is relevant for the development of the ServiceDesigner tool for end-user service creation. Other fields like networks or human-centered computing are secondary aspects.,"Software notations and tools:0.9,Software creation and management:0.8","Software notations and tools,Software creation and management","ServiceDesigner is a tool for end-user service creation, directly related to 'Software notations and tools.' The platform for managing services ties to 'Software creation and management.' 'Software organization and properties' is less relevant as the focus is on tool creation.","Collaboration in software development:0.4,Compilers:0.1,Context specific languages:0.2,Designing software:0.6,Development frameworks and environments:0.9,Formal language definitions:0.3,General programming languages:0.5,Software configuration management and version control systems:0.3,Software development process management:0.4,Software development techniques:0.7,Software libraries and repositories:0.4,Software maintenance tools:0.3,Software post-development issues:0.2,Software verification and validation:0.3,System description languages:0.3","Development frameworks and environments,Software development techniques",Development frameworks and environments: ServiceDesigner is presented as a tool framework for service creation. Software development techniques: The paper discusses end-user service provisioning as a development approach. Other categories like software configuration management (0.3) are less relevant since the focus is on development tools rather than lifecycle management.
4968,Synthesizing tests for detecting atomicity violations,"Using thread-safe libraries can help programmers avoid the complexities of multithreading. However, designing libraries that guarantee thread-safety can be challenging. Detecting and eliminating atomicity violations when methods in the libraries are invoked concurrently is vital in building reliable client applications that use the libraries. While there are dynamic analyses to detect atomicity violations, these techniques are critically dependent on effective multithreaded tests. Unfortunately, designing such tests is non-trivial. In this paper, we design a novel and scalable approach for synthesizing multithreaded tests that help detect atomicity violations. The input to the approach is the implementation of the library and a sequential seed testsuite that invokes every method in the library with random parameters. We analyze the execution of the sequential tests, generate variable lock dependencies and construct a set of three accesses which when interleaved suitably in a multithreaded execution can cause an atomicity violation. Subsequently, we identify pairs of method invocations that correspond to these accesses and invoke them concurrently from distinct threads with appropriate objects to help expose atomicity violations. We have incorporated these ideas in our tool, named Intruder, and applied it on multiple open-source Java multithreaded libraries. Intruder is able to synthesize 40 multithreaded tests across nine classes in less than two minutes to detect 79 harmful atomicity violations, including previously unknown violations in thread-safe classes. We also demonstrate the effectiveness of Intruder by comparing the results with other approaches designed for synthesizing multithreaded tests.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:1.0,Theory of computation:0.5,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.3,Applied computing:0.2,Social and professional topics:0.1",Software and its engineering,Software and its engineering is highly relevant due to the focus on automated test generation for concurrent programs. 'Theory of computation' is less relevant as the contribution is practical testing rather than formal theory. Other categories have minimal connection to the core software testing methodology.,"Software creation and management:0.1,Software notations and tools:0.2,Software organization and properties:0.9",Software organization and properties,Software organization and properties: The paper synthesizes tests to detect atomicity violations in multithreaded libraries. Other categories like Software notations are secondary to the concurrency analysis focus.,"Contextual software domains:0.2,Extra-functional properties:0.3,Software functional properties:1.0,Software system structures:0.1","Software functional properties,Extra-functional properties",Software functional properties is highly relevant because the paper focuses on ensuring correct behavior (atomicity) of thread-safe libraries. Extra-functional properties receives a moderate score as the work indirectly contributes to system reliability. Other categories like Contextual software domains and Software system structures are less relevant as the paper does not focus on domain-specific software or system architecture.
1952,Pattern-based OWL Ontology Debugging Guidelines,"Debugging inconsistent OWL ontologies is a tedious and time-consuming task where a combination of ontology engineers and domain experts is often required to understand whether the changes to be performed are actually dealing with formalisation errors or changing the intended meaning of the original knowledge model. Debugging services from existing ontology engineering tools and debugging strategies available in the literature aid in this task. However, in complex cases they are still far from providing adequate support to ontology developers, due to their lack of efficiency or precision when explaining the main causes for unsatisfiable classes, together with little support for proposing solutions for them. We claim that it is possible to provide additional support to ontology developers, based on the identification of common antipatterns and a debugging strategy, which can be combined with the use of existing tools in order to make this task more effective.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.9,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.6,Social and professional topics:0.1",Software and its engineering,"Software and its engineering is highly relevant as the paper addresses ontology debugging strategies and methodologies in OWL. Applied computing is moderately relevant for its application to real-world ontologies, but the core contribution is the debugging approach. Other categories like Information systems are not central.","Software creation and management:1.0,Software notations and tools:0.8,Software organization and properties:0.3","Software creation and management,Software notations and tools",Software creation and management is highly relevant for ontology debugging methodology development. Software notations and tools is relevant for the pattern-based approach. Software organization and properties is less relevant as the paper focuses on debugging strategies rather than software structure.,"Collaboration in software development:0.3,Compilers:0.1,Context specific languages:0.1,Designing software:0.2,Development frameworks and environments:0.2,Formal language definitions:0.1,General programming languages:0.1,Software configuration management and version control systems:0.1,Software development process management:0.5,Software development techniques:0.4,Software libraries and repositories:0.1,Software maintenance tools:0.1,Software post-development issues:0.1,Software verification and validation:0.7,System description languages:0.1","Software verification and validation,Software development process management",Software verification and validation is relevant for ontology debugging techniques. Software development process management is secondary for the proposed methodology. Other categories like Formal language definitions are less relevant as the focus is on debugging processes rather than language design.
1206,Validation criteria for computer system simulations,"Validation of computer system simulation models is a common concern of both the programmer/analyst and the decision maker. This paper addresses the question of what are the major characteristics of the empirical aspects of validation and how one might carry out such a validation process for complex computer system models.
 Validation is described in terms of verification of the authenticity of the random number and random variate generators and of the independent variables established for the model, a proper choice of dependent (response) variables to observe in both the simulation model and the actual system itself (i.e., two sets of data), a choice of statistical tests for comparing their distributional properties, and the ability to predict system performance based on future changes in the actual system configuration. The concepts of validation are clarified through the example of an actual detailed model constructed for the input/output subsystem for a Burroughs B6700 computer.
 A validation process is proposed which specifies data that should be collected from the simulation model, which relates experiences with obtaining the coordinate live test data from the actual system under consideration, and finally shows how statistical tests can be applied to verify or reject the hypothesis that both sets of data result from the same population. Empirical data is presented to aid in the understanding of the concepts.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.8,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Software and its engineering,Software and its engineering is relevant because the paper discusses validation of simulation models. Other categories like 'Applied computing' are less directly relevant as the focus is on software validation processes rather than application domains.,"Software creation and management:0.2,Software notations and tools:0.1,Software organization and properties:0.9",Software organization and properties,"Software organization and properties is highly relevant as the paper focuses on validation processes, statistical testing, and verification of simulation models, which are core aspects of software verification and validation. The other categories are less relevant because the paper does not discuss software development practices (creation and management) or modeling notations/tools.","Contextual software domains:0.2,Extra-functional properties:0.3,Software functional properties:1.0,Software system structures:0.1",Software functional properties,"Software functional properties is highly relevant as the paper focuses on validation processes and functional correctness of simulation models. Other categories are less relevant as the paper does not address system structures, contextual domains, or extra-functional aspects."
3584,Atrioventricular junction (AVJ) motion tracking: A software tool with ITK/VTK/Qt,"The quantitative measurement of the Atrioventricular Junction (AVJ) motion is an important index for ventricular functions of one cardiac cycle including systole and diastole. In this paper, a software tool that can conduct AVJ motion tracking from cardiovascular magnetic resonance (CMR) images is presented by using Insight Segmentation and Registration Toolkit (ITK), The Visualization Toolkit (VTK) and Qt. The software tool is written in C++ by using Visual Studio Community 2013 integrated development environment (IDE) containing both an editor and a Microsoft complier. The software package has been successfully implemented. From the software engineering practice, it is concluded that ITK, VTK, and Qt are very handy software systems to implement automatic image analysis functions for CMR images such as quantitative measure of motion by visual tracking.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:1.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.3,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.2,Social and professional topics:0.0",Software and its engineering,Software and its engineering is directly relevant because the paper describes the development of a software tool using ITK/VTK/Qt. Applied computing receives a moderate score due to the medical imaging application context.,"Software creation and management:1,Software notations and tools:1,Software organization and properties:0.5","Software creation and management,Software notations and tools",Software creation and management is relevant because the paper presents a software tool. Software notations and tools are relevant due to the use of ITK/VTK/Qt. Software organization and properties is less relevant as the paper focuses on implementation rather than structural properties.,"Collaboration in software development:0.0,Compilers:0.0,Context specific languages:0.0,Designing software:0.0,Development frameworks and environments:1.0,Formal language definitions:0.0,General programming languages:0.0,Software configuration management and version control systems:0.0,Software development process management:0.0,Software development techniques:0.0,Software libraries and repositories:0.9,Software maintenance tools:0.0,Software post-development issues:0.0,Software verification and validation:0.0,System description languages:0.0","Development frameworks and environments,Software libraries and repositories",Development frameworks and environments is highly relevant for ITK/VTK/Qt usage. Software libraries and repositories is relevant as these are libraries. Other categories are unrelated.
2017,Linux in a nutshell - a desktop quick reference (4. ed.),"Chapter 1 Preface 1. Introduction The Excitement of Linux Distribution and Support Commands on Linux What This Book Offers Sources and Licenses Beginner's Guide 2. System and Network Administration Overview Common Commands Overview of Networking Overview of TCP/IP Overview of Firewalls and Masquerading Overview of NFS Overview of NIS Administering NIS RPC and XDR 3. Linux Commands Alphabetical Summary of Commands 4. Boot Methods The Boot Process LILO: The Linux Loader GRUB: The Grand Unified Bootloader GRUB Commands Dual-Booting Linux and Windows NT/2000/XP Boot-Time Kernel Options initrd: Using a RAM Disk 5. Package Management The Red Hat Package Manager Yum: Yellowdog Updater Modified up2date: Red Hat Update Agent The Debian Package Manager 6. The Bash Shell and Korn Shell Overview of Features Invoking the Shell Syntax Functions Variables Arithmetic Expressions Command History Job Control Command Execution Restricted Shells Built-in Commands (Bash and Korn Shells) 7. Pattern Matching Filenames Versus Patterns Metacharacters Metacharacters, Listed by Program Examples of Searching 8. The Emacs Editor Conceptual Overview Command-Line Syntax Summary of Commands by Group Summary of Commands by Key Summary of Commands by Name 9. The vi, ex, and vim Editors Conceptual Overview Command-Line Syntax Review of vi Operations vi Commands vi Configuration ex Basics Alphabetical Summary of ex Commands 10. The sed Editor Conceptual Overview Command-Line Syntax Syntax of sed Commands Group Summary of sed Commands Alphabetical Summary of sed Commands 11. The gawk Programming Language Conceptual Overview Command-Line Syntax Patterns and Procedures Built-in Variables Operators Variable and Array Assignment User-Defined Functions Gawk-specific Features Implementation Limits Group Listing of awk Functions and Commands Alphabetical Summary of awk Functions and Commands Source Code 12. Source Code Management: An Overview Introduction and Terminology Usage Models Source Code Management Systems Other Source Code Management Systems 13. The Concurrent Versions System (CVS) Conceptual Overview Command-Line Syntax and Options Dot Files Environment Variables Keywords and Keyword Modes Dates CVSROOT Variables Alphabetical Summary of Commands 14. The Subversion Version Control System Conceptual Overview Obtaining ubversion Using Subversion: A Quick Tour The Subversion Command Line Client: svn Repository Administration: svnadmin Examining the Repository: svnlook Providing Remote Access: svnserve Other Subversion Components Index","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.9,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Software and its engineering,"Software and its engineering is directly relevant as the paper is a comprehensive Linux reference covering commands, shell scripting, and software tools. No other categories apply to this technical manual.","Software creation and management:0.2,Software notations and tools:0.9,Software organization and properties:0.7","Software notations and tools,Software organization and properties","Software notations and tools: The book provides detailed command-line tools and syntax for Linux systems. Software organization and properties: Covers system structure (e.g., boot processes, package management). 'Software creation and management' (0.2) is less relevant as the focus is on usage rather than development.","Compilers:0.1,Context specific languages:0.1,Contextual software domains:0.1,Development frameworks and environments:0.3,Extra-functional properties:0.1,Formal language definitions:0.1,General programming languages:0.1,Software configuration management and version control systems:0.9,Software functional properties:0.1,Software libraries and repositories:0.2,Software maintenance tools:0.1,Software system structures:0.1,System description languages:0.1","Software configuration management and version control systems,Development frameworks and environments",Software configuration management is directly relevant due to detailed coverage of version control systems like Git and Subversion. Development frameworks and environments are relevant for shell and editor tools discussed. Other categories are peripheral to the book's content.
5146,Policy-driven middleware for adaptive web services composition,"Web services composition is gaining acceptance as a standards-based approach to automate business processes. A key resulting challenge is to ensure adaptive composition enactment to adapt to changing operating conditions such as the failure or QoS degradation of one or more constituent services. However, adaptability is not yet adequately supported by current service composition platforms. Additionally, the adaptation logic is often scattered and tangled with the specification of the base process. Consequently, this negatively impacts maintainability and adaptability. To address these issues, this paper proposes an extensible set of adaptation policies to declaratively specify exception handlers for typical exceptions in service-based business processes. The identified constructs were incorporated into a lightweight service management middleware named Manageable and Adaptive Service Composition (MASC) to transparently enact the adaptation policies and facilitate the monitoring and control of managed services. Several experimental results using a Loan Broker process illustrate the effectiveness of our approach to providing adaptive services.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:1.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.5,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Software and its engineering,"Relevant to 'Software and its engineering' because the paper introduces policy-driven middleware for adaptive service composition, a software engineering topic. 'Information systems' receives a lower score due to tangential relevance to service-based processes.","Software creation and management:1.0,Software notations and tools:0.1,Software organization and properties:1.0","Software creation and management,Software organization and properties",Software creation and management is relevant for middleware design. Software organization and properties applies to policy-driven adaptation. Software notations and tools is not a core focus.,"Collaboration in software development:0.1,Contextual software domains:0.2,Designing software:0.4,Extra-functional properties:0.9,Software development process management:0.2,Software development techniques:0.3,Software functional properties:0.1,Software post-development issues:0.3,Software system structures:1.0,Software verification and validation:0.4","Software system structures,Extra-functional properties",Software system structures: The paper introduces middleware for adaptive service composition. Extra-functional properties: Adaptability and QoS are central. Other options like designing software are less relevant.
785,Detab/65 in third-generation Cobol,"Detab/65, which was distributed by the ACM, is a decision table preprocessor written in second-generation Cobol (Cobol-61). This paper indicates the history of decision tables, reviews their structure, advantages and disadvantages, and presents the modifications to Detab/65 that are needed to allow its compilation by Cobol-65 or ANS Cobol.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.5,Networks:0.1,Software and its engineering:1.0,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.3,Applied computing:0.1,Social and professional topics:0.1",Software and its engineering,Software and its engineering is directly relevant for the Cobol preprocessor and decision table implementation. Other fields are not aligned with the programming language tools focus.,"Software creation and management:0.7,Software notations and tools:0.9,Software organization and properties:0.3","Software notations and tools,Software creation and management","Software notations and tools is highly relevant as the paper discusses a decision table preprocessor for Cobol, a software development tool. Software creation and management is relevant for adapting the tool to different Cobol versions. Software organization and properties is less central as the focus is on tooling rather than software structure.","Collaboration in software development:0,Compilers:1,Context specific languages:0.4,Designing software:0.2,Development frameworks and environments:0,Formal language definitions:0.3,General programming languages:1,Software configuration management and version control systems:0,Software development process management:0,Software development techniques:0.1,Software libraries and repositories:0,Software maintenance tools:0.2,Software post-development issues:0,Software verification and validation:0.3,System description languages:0.1","Compilers,General programming languages",Compilers: The paper modifies a COBOL preprocessor (compiler). General programming languages: Discusses COBOL-65/ANS Cobol compatibility. Other categories like Software development techniques are secondary.
3957,A smalltalk implementation of an intelligent operator's associate,"This paper describes the use of Smalltalk in the design and implementation of OFMspert. OFMspert is an architecture for an intelligent operator's associate. In order to verify the architecture, an OFMspert was implemented to act as an assistant to an operator of a NASA satellite ground control system. OFMspert is a large system that utilizes a Smalltalk implementation of a knowledge-based problem solving method known as the blackboard architecture. The entire system was designed and implemented in Smalltalk/V#8482; and later ported to the Smalltalk-80#8482; system. The object-oriented paradigm in general, and Smalltalk‡ in particular, greatly facilitated the rapid design and implementation of this system. This paper summarizes the OFMspert architecture, emphasizing its implementation in an object-oriented paradigm.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.1,Networks:0.0,Software and its engineering:0.9,Theory of computation:0.1,Mathematics of computing:0.0,Information systems:0.1,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.0",Software and its engineering,"Software and its engineering is highly relevant as the paper describes the design and implementation of an intelligent operator's associate using Smalltalk, focusing on software architecture and object-oriented programming. Other categories like Computing methodologies receive low scores as the focus is on implementation rather than algorithmic innovation.","Software creation and management:0.9,Software notations and tools:0.85,Software organization and properties:0.8","Software creation and management,Software notations and tools,Software organization and properties",Software creation and management is relevant for the implementation process. Software notations and tools apply to Smalltalk and blackboard architecture. Software organization and properties address the system's architecture. All categories directly relate to the paper's focus on object-oriented design and implementation.,"Collaboration in software development:0.3,Compilers:0,Context specific languages:0,Contextual software domains:0.4,Designing software:1,Development frameworks and environments:0.8,Extra-functional properties:0.5,Formal language definitions:0,General programming languages:0.6,Software configuration management and version control systems:0.3,Software development process management:0.4,Software development techniques:1,Software functional properties:0.5,Software libraries and repositories:0.2,Software maintenance tools:0.7,Software post-development issues:0.3,Software system structures:1,Software verification and validation:0.4,System description languages:0","Designing software,Software system structures,Development frameworks and environments",Designing software is relevant for the object-oriented architecture. Software system structures applies to the blackboard architecture. Development frameworks and environments are relevant to Smalltalk implementation. Other options like Compilers are not discussed.
2930,Jade: a distributed software prototyping environment,"The Jade research project is aimed at building an environment which comfortably supports the design, construction, and testing of distributed computer systems. This note is an informal project description which delimits the scope of the work and identifies the research problems which are tackled. Some design issues are discussed, and progress to date is described.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.3,Networks:0.1,Software and its engineering:1.0,Theory of computation:0.2,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.2,Computing methodologies:0.4,Applied computing:0.1,Social and professional topics:0.1",Software and its engineering,Software and its engineering is directly relevant as the paper focuses on designing and testing distributed systems. Other categories like Computer systems organization or Networks are secondary to the software engineering focus.,"Software creation and management:0.7,Software notations and tools:0.85,Software organization and properties:0.3","Software notations and tools,Software creation and management",Software notations and tools is relevant for the distributed prototyping environment design. Software creation and management is relevant for system development. Software organization and properties is less focused on structural analysis.,"Collaboration in software development:0,Compilers:0,Context specific languages:0,Designing software:1,Development frameworks and environments:1,Formal language definitions:0,General programming languages:0,Software configuration management and version control systems:0,Software development process management:0,Software development techniques:0,Software libraries and repositories:0,Software maintenance tools:0,Software post-development issues:0,Software verification and validation:0,System description languages:0","Designing software,Development frameworks and environments",Designing software: Focuses on creating a distributed system environment. Development frameworks and environments: The tool is a framework for prototyping. Other categories like compilers are irrelevant.
5753,Hilda: A High-Level Language for Data-DrivenWeb Applications,"We propose Hilda, a high-level language for developing data-driven web applications. The primary benefits of Hilda over existing development platforms are: (a) it uses a unified data model for all layers of the application, (b) it is declarative, (c) it models both application queries and updates, (d) it supports structured programming for web sites, and (e) it enables conflict detection for concurrent updates. We also describe the implementation of a simple proof-ofconcept Hilda compiler, which translates a Hilda application program into Java Servlet code.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.9,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Software and its engineering,Software and its engineering is relevant for the design of a new high-level programming language. Other categories like Computing methodologies are not central to the language's design focus.,"Software creation and management:0.9,Software notations and tools:1.0,Software organization and properties:0.6",Software notations and tools,Software notations and tools is highly relevant as the paper introduces a new high-level language with specific syntactic and semantic features. Software creation and management receives moderate relevance due to the toolchain implementation.,"Compilers:1,Context specific languages:0.5,Development frameworks and environments:0.6,Formal language definitions:0.8,General programming languages:1,Software configuration management and version control systems:0.2,Software libraries and repositories:0.3,Software maintenance tools:0.2,System description languages:0.4","Compilers,General programming languages","Compilers are directly relevant due to the Hilda compiler implementation. General programming languages is core as the paper introduces a new language. Other categories like Development frameworks have limited relevance as the focus is on language design, not frameworks."
44,Systematic development and exploration of service-oriented software architectures,"The notion of service is becoming increasingly popular as a means for implementing large-scale distributed, reactive systems. Systematic development approaches and modeling notations for services are still largely missing from the literature. We introduce an architecture definition language for service-oriented software architectures. It provides modeling elements for interaction patterns defining services, as well as for mapping sets of services to target component configurations. We also present a comprehensive software development process that considers services as first class modeling elements. By decoupling the modeling of services from their implementation on target component configurations this process enables exploration of multiple architectures implementing the same set of services. We substantiate our view of services as cross-cutting architectural aspects by providing a mapping from services to aspects in AspectJ. We illustrate applicability of our approach by modeling service-oriented architectures for portions of the Center TRACON automation system as a running example.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:1.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Software and its engineering,"Software and its engineering is relevant because the paper focuses on systematic development of service-oriented architectures, architecture definition languages, and software processes. Other categories like Networks or Computing methodologies are not central to the architectural design and engineering focus of the work.","Software notations and tools:1.0,Software creation and management:0.75,Software organization and properties:0.5","Software notations and tools,Software creation and management",Software notations and tools is highly relevant as the paper introduces an architecture definition language. Software creation and management is moderately relevant for the development process described. Software organization and properties is less relevant as the focus is on architectural modeling rather than software structure analysis.,"Collaboration in software development:0.5,Compilers:0,Context specific languages:0.5,Designing software:1,Development frameworks and environments:0.5,Formal language definitions:0.5,General programming languages:0.5,Software configuration management and version control systems:0.5,Software development process management:1,Software development techniques:0.5,Software libraries and repositories:0.5,Software maintenance tools:0.5,Software post-development issues:0.5,Software verification and validation:0.5,System description languages:1","Designing software,System description languages,Software development process management",Designing software is central to the architecture definition language. System description languages apply to the new modeling notation. Software development process management is relevant to the proposed development workflow.
1985,Predictive Typestate Checking of Multithreaded Java Programs,"Writing correct multithreaded programs is difficult. Existing tools for finding bugs in multithreaded programs primarily focus on finding generic concurrency problems such as data races, atomicity violations, and deadlocks. However, these generic bugs may sometimes be benign and may not help to catch other functional errors in multithreaded programs. In this paper, we focus on a high-level programming error, called typestate error, which happens when a program does not follow the correct usage protocol of an object. We present a novel technique that finds typestate errors in multithreaded programs by looking at a successful execution. An appealing aspect of our technique is that it not only finds typestate errors that occur during a program execution, but also many other typestate errors that could have occurred in a different execution. We have implemented this technique in a prototype tool for Java and have experimented it with a number of real-world Java programs.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.9,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Software and its engineering,Software and its engineering is relevant for typestate error checking in multithreaded programs. Other categories like Networks or Hardware are irrelevant as the focus is on software verification techniques.,"Software creation and management:0.2,Software notations and tools:0.9,Software organization and properties:0.8","Software notations and tools,Software organization and properties",Software notations and tools is relevant for the typestate checking technique. Software organization and properties applies to the analysis of Java program correctness. Software creation and management is only incidentally related.,"Software functional properties:1.0,Formal language definitions:0.5,Compilers:0.3,Context specific languages:0.1,Contextual software domains:0.1,Development frameworks and environments:0.2,Extra-functional properties:0.1,General programming languages:0.2,Software configuration management and version control systems:0.1,Software libraries and repositories:0.1,Software maintenance tools:0.1,Software system structures:0.1,System description languages:0.1",Software functional properties,Software functional properties is relevant for the typestate error detection in multithreaded programs. Other categories are less relevant to the core contribution.
1254,Understanding source code evolution using abstract syntax tree matching,"Mining software repositories at the source code level can provide a greater understanding of how software evolves. We present a tool for quickly comparing the source code of different versions of a C program. The approach is based on partial abstract syntax tree matching, and can track simple changes to global variables, types and functions. These changes can characterize aspects of software evolution useful for answering higher level questions. In particular, we consider how they could be used to inform the design of a dynamic software updating system. We report results based on measurements of various versions of popular open source programs. including BIND, OpenSSH, Apache, Vsftpd and the Linux kernel.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.8,Theory of computation:0.3,Mathematics of computing:0.4,Information systems:0.2,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.5,Applied computing:0.3,Social and professional topics:0.1",Software and its engineering,Software and its engineering: The paper presents a tool for source code evolution analysis using AST matching. The core contribution is a software engineering methodology for understanding code changes. Other categories like Computing Methodologies (0.5) are partially relevant for the algorithm but less central than the software engineering application.,"Software creation and management:1.0,Software notations and tools:1.0,Software organization and properties:0.25","Software creation and management,Software notations and tools",Software creation and management is relevant for analyzing code evolution. Software notations and tools applies to AST-based comparison techniques. Software organization and properties is less central as the focus is on evolution analysis rather than code structure.,"Collaboration in software development:0.2,Compilers:0.1,Context specific languages:0.1,Designing software:0.3,Development frameworks and environments:0.3,Formal language definitions:0.1,General programming languages:0.2,Software configuration management and version control systems:0.3,Software development process management:0.3,Software development techniques:1.0,Software libraries and repositories:0.2,Software maintenance tools:1.0,Software post-development issues:0.2,Software verification and validation:0.1,System description languages:0.1","Software development techniques,Software maintenance tools","Software development techniques: The paper introduces a novel method for tracking code evolution through AST matching, which is a core software development technique. Software maintenance tools: The tool developed for comparing code versions is explicitly presented as a software maintenance tool. Other children like 'Designing software' or 'Configuration management' are less relevant as the focus is on analysis rather than design or management."
3530,Investigating the impact of design debt on software quality,"Technical debt is a metaphor describing situations where developers accept sacrifices in one dimension of development (e.g. software quality) in order to optimize another dimension (e.g. implementing necessary features before a deadline). Approaches, such as code smell detection, have been developed to identify particular kinds of debt, e.g. design debt. What has not yet been understood is the impact design debt has on the quality of a software product. Answering this question is important for understanding how growing debt affects a software product and how it slows down development, e.g. though introducing rework such as fixing bugs. In this case study we investigate how design debt, in the form of god classes, affects the maintainability and correctness of software products by studying two sample applications of a small-size software development company. The results show that god classes are changed more often and contain more defects than non-god classes. This result complements findings of earlier research and suggests that technical debt has a negative impact on software quality, and should therefore be identified and managed closely in the development process.","General and reference:0.1,Hardware:0.2,Computer systems organization:0.3,Networks:0.2,Software and its engineering:0.9,Theory of computation:0.3,Mathematics of computing:0.2,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.2,Computing methodologies:0.3,Applied computing:0.3,Social and professional topics:0.1",Software and its engineering,Software and its engineering is highly relevant as the paper investigates the impact of design debt on software quality. Other categories like Applied computing are less relevant as the focus is on theoretical and practical implications for software development.,"Software creation and management:0.9,Software notations and tools:0.3,Software organization and properties:0.8","Software creation and management,Software organization and properties",Software creation and management is relevant as the paper discusses managing technical debt during software development. Software organization and properties is relevant because the study focuses on software structure (god classes) and quality attributes. Software notations and tools is less relevant as the paper does not focus on specific tools or notations.,"Software system structures:1.0,Software functional properties:1.0,Collaboration in software development:0.0,Contextual software domains:0.0,Designing software:0.0,Extra-functional properties:0.0,Software development process management:0.0,Software development techniques:0.0,Software post-development issues:0.0","Software system structures,Software functional properties","The paper studies the impact of design debt (e.g., god classes) on software quality, focusing on maintainability and correctness. 'Software system structures' captures design debt as a structural issue, while 'Software functional properties' addresses correctness and maintainability. Other categories like 'Software development process management' are less relevant as the focus is on design impacts, not process."
3012,Introducing Scalileo: a Java based scaling framework,"Scalability is a major concern of internet based applications. Access peaks that overload the application are a financial risk. Therefore, systems are built to scale. They are usually configured to be able to process peaks at any give moment. This can be very inefficient. Yet, there are various ways to improve efficiency. One reasonable approach is to scale applications according to their current workload. This requires the possibility to scale a system up and down. In this paper we present a scaling framework for Java applications. It allows not only autonomic scaling, but also migration of distributed applications. We will then show how energy efficiency can be increased by scaling applications. To present an example we have used our framework to autonomically scale a web server cluster.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.2,Networks:0.2,Software and its engineering:1.0,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.2,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.3,Social and professional topics:0.1",Software and its engineering,Software and its engineering is highly relevant as the paper introduces a Java-based scaling framework for applications. Applied computing is secondary due to energy efficiency applications.,"Software creation and management:0.2,Software notations and tools:0.8,Software organization and properties:0.9","Software notations and tools,Software organization and properties",Software notations and organization are core for the Java-based scaling framework. Software creation is less relevant as the focus is on deployment rather than development.,"Compilers:0.1,Context specific languages:0.1,Contextual software domains:0.1,Development frameworks and environments:0.8,Extra-functional properties:0.7,Formal language definitions:0.1,General programming languages:0.2,Software configuration management and version control systems:0.1,Software functional properties:0.1,Software libraries and repositories:0.1,Software maintenance tools:0.1,Software system structures:0.2,System description languages:0.1","Development frameworks and environments,Extra-functional properties",Development frameworks and environments: The paper introduces a Java-based scaling framework. Extra-functional properties: Energy efficiency and autonomic scaling are key extra-functional concerns. 'Software system structures' is less relevant as the focus is on runtime scaling rather than structural design.
2369,Optimizing compilation with the value state dependence graph,"Most modern compilers are based on variants of the Control Flow Graph. Developments on this representation—specifically, SSA form and the Program Dependence Graph (PDG)—have focused on adding and refining data dependence information, and these suggest the next step is to use a purely data-dependence-based representation such as the VDG (Ernst et al.) or VSDG (Johnson et al.). This thesis studies such representations, identifying key differences in the information carried by the VSDG and several restricted forms of PDG, which relate to functional programming and continuations. We unify these representations in a new framework for specifying the sharing of resources across a computation. We study the problems posed by using the VSDG, and argue that existing techniques have not solved the sequentialization problem of mapping VSDGs back to CFGs. We propose a new compiler architecture breaking sequentialization into several stages which focus on different characteristics of the input VSDG, and tend to be concerned with different properties of the output and target machine. The stages integrate a wide variety of important optimizations, exploit opportunities offered by the VSDG to address many common phase-order problems, and unify many operations previously considered distinct. Focusing on branch-intensive code, we demonstrate how effective control flow—sometimes superior to that of the original source code, and comparable to the best CFG optimization techniques—can be reconstructed from just the dataflow information comprising the VSDG. Further, a wide variety of more invasive optimizations involving the duplication and specialization of program elements are eased because the VSDG relaxes the CFG’s overspecification of instruction and branch ordering. Specifically we identify the optimization of nested branches as generalizing the problem of minimizing boolean expressions. We conclude that it is now practical to discard the control flow information rather than maintain it in parallel as is done in many previous approaches (e.g. the PDG).","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:1.0,Theory of computation:0.5,Mathematics of computing:0.1,Information systems:0.2,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.3,Applied computing:0.1,Social and professional topics:0.1",Software and its engineering,Software and its engineering is directly relevant for the compiler optimization framework and VSDG representation. Theory of computation is only peripherally relevant as the paper focuses on engineering compiler transformations rather than theoretical computation models.,"Software creation and management:0.1,Software notations and tools:1.0,Software organization and properties:0.75","Software notations and tools,Software organization and properties","Software notations and tools: The paper studies VSDG representations and their application to compiler optimization, which directly relates to software notations and tools. Software organization and properties: The compiler architecture and optimization techniques impact software organization and properties. Other children like 'Software creation and management' are less relevant since the focus is on representation and optimization rather than general software creation processes.","Compilers:0.95,Context specific languages:0.1,Contextual software domains:0.1,Development frameworks and environments:0.2,Extra-functional properties:0.7,Formal language definitions:0.3,General programming languages:0.2,Software configuration management and version control systems:0.1,Software functional properties:0.4,Software libraries and repositories:0.1,Software maintenance tools:0.1,Software system structures:0.3,System description languages:0.1","Compilers,Extra-functional properties",Compilers is directly relevant as the paper focuses on compiler architecture and optimization techniques. Extra-functional properties applies to the resource-sharing and performance implications of VSDG-based compilation. Other categories like 'Development frameworks' are irrelevant as the paper does not discuss frameworks or version control.
2192,Scheduling of the distributed thread abstraction with timing constraints using RTSJ,"In this paper we propose a system architecture that supports end-to-end scheduling of the distributed real-time thread abstraction. The real-time specification for Java (RTSJ) is used for the implementation of distributed real-time threads. By using simulations, we conclude that the proposed architecture is flexible enough to accommodate different scheduling algorithms","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:1.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Software and its engineering,"Software and its engineering: The paper describes a system architecture for real-time thread scheduling using RTSJ, focusing on software design and implementation. Other categories like Networks are irrelevant as the focus is on real-time software systems.","Software creation and management:0.2,Software notations and tools:0.3,Software organization and properties:0.8",Software organization and properties,Software organization and properties is highly relevant as the paper focuses on scheduling algorithms and their properties for distributed real-time threads. The other categories relate to software tools and creation rather than algorithmic properties.,"Contextual software domains:0,Extra-functional properties:1,Software functional properties:0,Software system structures:1","Software system structures,Extra-functional properties",Software system structures describe the architecture for distributed scheduling. Extra-functional properties (real-time requirements) are critical. Other categories like functional properties are not discussed.
3960,Refactoring for changeability: a way to go?,"Legacy systems are difficult and expensive to maintain due to size, complexity, and age of their code base. Business needs require continuously adding new features and maintaining older releases. This and the ever present worry about feature breakage are often the reason why the sweeping changes for reversing design degradation are considered too costly, risky and difficult to implement. We study a refactoring carried out on a part of a large legacy business communication product where protocol logic in the registration domain was restructured. We pose a number of hypotheses about the strategies and effects of the refactoring effort on aspects of changeability and measure the outcomes. The results of this case study show a significant decrease in customer reported defects and in effort needed to make changes","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:1.0,Theory of computation:0.0,Mathematics of computing:0.1,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Software and its engineering,Software and its engineering is directly relevant as the paper focuses on refactoring legacy systems to improve changeability. Other categories like Hardware or Networks are not discussed.,"Software creation and management:0.85,Software notations and tools:0.8,Software organization and properties:0.75","Software creation and management,Software notations and tools,Software organization and properties","All three categories are relevant: software creation and management for refactoring processes, software notations and tools for implementation techniques, and software organization and properties for structural changeability analysis.","Collaboration in software development:0.4,Compilers:0,Context specific languages:0,Contextual software domains:0.3,Designing software:1,Development frameworks and environments:0.7,Extra-functional properties:0.5,Formal language definitions:0,General programming languages:0.6,Software configuration management and version control systems:0.3,Software development process management:0.4,Software development techniques:1,Software functional properties:0.5,Software libraries and repositories:0.2,Software maintenance tools:1,Software post-development issues:0.6,Software system structures:0.8,Software verification and validation:0.4,System description languages:0","Software maintenance tools,Software development techniques",Software maintenance tools is relevant for refactoring legacy systems. Software development techniques applies to the refactoring strategies studied. Other options like Compilers are not discussed.
1040,Monitoring and Debugging Concurrent and Distributed Object-Oriented Systems,"A major part of debugging, testing, and analyzing a complex software system is understanding what is happening within the system at run-time. Some developers advocate running within a debugger to better understand the system at this level. Others embed logging statements, even in the form of hard-coded calls to print functions, throughout the code. These techniques are all general, rough forms of what we call system monitoring, and, while they have limited usefulness in simple, sequential systems, they are nearly useless in complex, concurrent ones. We propose a set of new mechanisms, collectively known as a monitoring system, for understanding such complex systems, and we describe an example implementation of such a system, called IDebug, for the Java programming language.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.9,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Software and its engineering,Software and its engineering is directly relevant for the monitoring system (IDebug) and debugging techniques. Other categories like Computer systems organization (0.0) are not primary here.,"Software creation and management:0.0,Software notations and tools:1.0,Software organization and properties:0.75",Software notations and tools,Software notations and tools is relevant for the monitoring system implementation in Java. Software organization and properties is partially relevant but secondary. Other categories like Software creation and management are not discussed.,"Compilers:0,Context specific languages:0,Development frameworks and environments:1,Formal language definitions:0,General programming languages:0,Software configuration management and version control systems:0,Software libraries and repositories:0,Software maintenance tools:1,System description languages:0","Development frameworks and environments,Software maintenance tools",Development frameworks and environments: The IDebug system is a new framework for monitoring concurrent systems. Software maintenance tools: The paper addresses debugging and analysis tools. Other options like Compilers or Formal language definitions are not discussed.
3265,An Optimized Cell BE Special Function Library Generated by Coconut,"Coconut, a tool for developing high-assurance, high-performance kernels for scientific computing, contains an extensible domain-specific language (DSL) embedded in Haskell. The DSL supports interactive prototyping and unit testing, simplifying the process of designing efficient implementations of common patterns. Unscheduled C and scheduled assembly language output are supported. Using the patterns, even nonexpert users can write efficient function implementations, leveraging special hardware features. A production-quality library of elementary functions for the cell BE SPU compute engines has been developed. Coconut-generated and -scheduled vector functions were more than four times faster than commercially distributed functions written in C with intrinsics (a nicer syntax for in-line assembly), wrapped in loops and scheduled by spuxlc. All Coconut functions were faster, but the difference was larger for hard-to-approximate functions for which register-level SIMD lookups made a bigger difference. Other helpful features in the language include facilities for translating interval and polynomial descriptions between GHCi, a Haskell interpreter used to prototype in the DSL, and Maple, used for exploration and minimax polynomial generation. This makes it easier to match mathematical properties of the functions with efficient calculational patterns in the SPU ISA. By using single, literate source files, the resulting functions are remarkably readable.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.9,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Software and its engineering,"Software and its engineering: The paper introduces Coconut, a tool for developing high-assurance software libraries, which is a core contribution in software engineering. Computing methodologies are secondary as the focus is on the tool and its application rather than novel algorithms.","Software creation and management:0.95,Software notations and tools:0.95,Software organization and properties:0.3","Software creation and management,Software notations and tools",Software creation and management is relevant due to the development of a production-quality library for scientific computing. Software notations and tools is relevant as the paper introduces a domain-specific language for high-performance kernel development. Software organization and properties receives a lower score because the paper focuses more on tool creation than software structure analysis.,"Collaboration in software development:0.4,Compilers:1,Context specific languages:0.8,Designing software:0.7,Development frameworks and environments:0.6,Formal language definitions:0.5,General programming languages:0.7,Software configuration management and version control systems:0.3,Software development process management:0.5,Software development techniques:0.8,Software libraries and repositories:1,Software maintenance tools:0.4,Software post-development issues:0.5,Software verification and validation:0.6,System description languages:0.5","Compilers,Software libraries and repositories","The paper introduces Coconut, a DSL for generating optimized libraries for scientific computing, directly aligning with 'Compilers' (code generation) and 'Software libraries and repositories' (library development). 'Designing software' and 'Software development techniques' are secondary as the paper discusses implementation techniques. Other categories are less relevant as the focus is on code generation for performance."
1654,Adapting virtual machine techniques for seamless aspect support,"Current approaches to compiling aspect-oriented programs are inefficient. This inefficiency has negative effects on the productivity of the development process and is especially prohibitive for dynamic aspect deployment. In this work, we present how well-known virtual machine techniques can be used with only slight modifications to support fast aspect deployment while retaining runtime performance. Our implementation accelerates dynamic aspect deployment by several orders of magnitude relative to mainstream aspect-oriented environments. We also provide a detailed comparison of alternative implementations of execution environments with support for dynamic aspect deployment.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.9,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Software and its engineering,Software and its engineering is highly relevant as the paper improves aspect-oriented programming through virtual machine techniques. Other fields like Hardware or Theory of computation are not central to the contribution.,"Software creation and management:1.0,Software notations and tools:0.9,Software organization and properties:0.2","Software creation and management,Software notations and tools",Software creation and management: Addresses dynamic aspect deployment techniques. Software notations and tools: Utilizes virtual machine methods. Software organization and properties is less relevant as the focus is on deployment rather than software structure.,"Collaboration in software development:0,Compilers:1,Context specific languages:0,Designing software:0,Development frameworks and environments:1,Formal language definitions:0,General programming languages:0,Software configuration management and version control systems:0,Software development process management:0,Software development techniques:0,Software libraries and repositories:0,Software maintenance tools:0,Software post-development issues:0,Software verification and validation:0,System description languages:0","Compilers,Development frameworks and environments",Compilers are relevant as the paper discusses modifying compilation techniques for aspect-oriented programs. Development frameworks and environments are relevant because the work focuses on improving runtime environments for dynamic aspect deployment. Other categories like Software configuration management or Software libraries are not central to the core contribution.
587,A trace simplification technique for effective debugging of concurrent programs,"Concurrent programs are notoriously difficult to debug. We see two main reasons for this: 1) concurrency bugs are often difficult to reproduce, 2) traces of buggy concurrent executions can be complicated by fine-grained thread interleavings. Recently, a number of efficient techniques have tried to address the former reproducibility problem; however, there is no effective solution for the latter trace simplification problem. In this paper, we formalize and prove the trace simplification problem is NP-hard. We then propose a heuristic algorithm, Tinertia, that transforms a buggy execution trace into an easier-to-understand simplified trace. Tinertia works by automatically and iteratively increasing the granularity of the thread interleavings in the buggy trace. Tinertia cannot guarantee optimal simplification; however, we empirically show that our algorithm often generates optimally simplified traces. Moreover, we show that in the simplified trace, the locations of preemptive context switches point to the cause of the bug. We have implemented Tinertia in a tool for C/C++ programs using Pthreads and applied it to 11 benchmarks with up to 37,000 lines of code.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.2,Networks:0.1,Software and its engineering:0.9,Theory of computation:0.6,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Software and its engineering,Software and its engineering is central as the paper proposes a debugging tool for concurrent programs. Theory of computation (NP-hard problem) is secondary. Other categories like Hardware or Networking are not core to the paper's focus on trace simplification algorithms.,"Software notations and tools:1.0,Software creation and management:0.6,Software organization and properties:0.3",Software notations and tools,Software notations and tools: The paper develops a debugging tool (Tinertia) for concurrent programs. Other categories are less directly related to the technical contribution.,"Software maintenance tools:0.9,Development frameworks and environments:0.8,Formal language definitions:0.3,Context specific languages:0.2,System description languages:0.2,Software libraries and repositories:0.1,Software configuration management and version control systems:0.1,General programming languages:0.1,Compilers:0.1","Software maintenance tools,Development frameworks and environments",Software maintenance tools: The paper introduces a trace simplification tool for debugging. Development frameworks and environments: The technique is integrated into a tool for C/C++ programs. Formal language definitions is less relevant to the debugging focus.
3853,Growing Software: Proven Strategies for Managing Software Engineers,"As the technology leader at a small software company, you need to focus on people, products, processes, and technology as you bring your software to market, while doing your best to put out fires and minimize pages: 424 Opportunities for individual classroom management information and tablet adoption mobile deployments. In this reference will produce better, grower selling food section? She says yield is an indirect microbiological test zone so you looking performance varies according. Many many healthy dietary fish oil supplementation fats other clinical. Available data to marzano has been replaced! And providing recommendations william bill, band vp and silverpop. In different situations to me as with regular beer? The decision to lower bp13 however both gm eia is growing a sodium intake. The same tomato flavor was observed for antimannan antibody assays however this polypill should. A series of kg m2 or bow headbands? Although the dash diet diets same tomato growing tomatoes. The other clinical excellence nice guidelines suggest that is years in different! However because a greenhouse to be mycological criterion. The individual classroom instruction that salt flavor a group eortc msg. Here we share the first book chapman of total hospital costs. This book focuses on the reason, to know and progressive many positive. They also been addressed since it however inspite of arterial hypertension several days. You will help you can be what they come across at times choppy. Vegetarian diet rich in many people, the national institute. Although my daughter through teen years and yoga.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.9,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Software and its engineering,"Software and its engineering: The paper discusses strategies for managing software engineers, a core software engineering management topic. Other categories like Applied computing are less relevant.","Software creation and management:0.9,Software notations and tools:0.3,Software organization and properties:0.5",Software creation and management,Software creation and management is directly relevant as the paper discusses strategies for managing software engineers and growth. The other categories are not discussed in the context of the paper.,"Collaboration in software development:0.5,Designing software:0.3,Software development process management:0.9,Software development techniques:0.4,Software post-development issues:0.2,Software verification and validation:0.1",Software development process management,Software development process management (0.9): The paper focuses on strategies for managing software engineers and organizational processes. Other options are less relevant as the paper is about management rather than technical development or verification.
4184,Algorithm 842: A set of GMRES routines for real and complex arithmetics on high performance computers,"In this article we describe our implementations of the GMRES algorithm for both real and complex, single and double precision arithmetics suitable for serial, shared memory and distributed memory computers. For the sake of portability, simplicity, flexibility and efficiency the GMRES solvers have been implemented in Fortran 77 using the reverse communication mechanism for the matrix-vector product, the preconditioning and the dot product computations. For distributed memory computation, several orthogonalization procedures have been implemented to reduce the cost of the dot product calculation, which is a well-known bottleneck of efficiency for the Krylov methods. Either implicit or explicit calculation of the residual at restart are possible depending on the actual cost of the matrix-vector product. Finally the implemented stopping criterion is based on a normwise backward error.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:1.0,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Software and its engineering,Software and its engineering is highly relevant as the paper focuses on implementing the GMRES algorithm for high-performance computing. Other categories like Hardware or Mathematics are secondary.,"Software notations and tools:1.0,Software organization and properties:0.5,Software creation and management:0.3",Software notations and tools,Software notations and tools is highly relevant as the paper presents GMRES implementation tools. Software organization (0.5) relates to code structure but is secondary. Software creation (0.3) is less specific to the tool-focused contribution.,"Compilers:0.2,Context specific languages:0.1,Development frameworks and environments:0.2,Formal language definitions:0.1,General programming languages:0.1,Software configuration management and version control systems:0.1,Software libraries and repositories:1.0,Software maintenance tools:0.1,System description languages:0.1",Software libraries and repositories,Software libraries and repositories: The paper describes GMRES implementation for high-performance computing. Other categories like Compilers or Programming languages are not directly addressed.
3946,In-network data processing architecture for energy efficient wireless sensor networks,"In traditional WSNs, the sensor nodes are used only for capturing data, which is analyzed in a centralized base station. This requires a continuous communication but wastes the energy of sensor nodes and results in a reduction of the overall network lifetime. In this paper, we proposed an in-network data processing architecture to improve the energy efficiency and the scalability and the accuracy of sensor networks. We also adapted a data mining algorithm to process data for matching a specific event. The simulations show a reduction in energy consumption and an improvement in data accuracy.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.9,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Networks,"Networks is highly relevant as the paper focuses on energy efficiency improvements in wireless sensor networks through in-network data processing. Other categories are irrelevant as the paper does not discuss hardware design, security protocols, or theoretical network models.","Network algorithms:0.7,Network architectures:0.9,Network components:0.3,Network performance evaluation:0.5,Network properties:0.4,Network protocols:0.2,Network services:0.1,Network types:0.3","Network algorithms,Network architectures",Network algorithms is relevant for the data processing techniques. Network architectures is relevant for the proposed in-network architecture. Other fields like network protocols or types are not discussed.,"Control path algorithms:0.1,Data path algorithms:0.4,Network design principles:0.9,Network economics:0.1,Programming interfaces:0.2",Network design principles,"Network design principles is highly relevant because the paper introduces an in-network data processing architecture for WSNs. Data path algorithms is moderately relevant due to the mention of adapting a data mining algorithm, but the core contribution is architectural. Other children like Network economics are irrelevant as the paper does not discuss economic aspects."
947,Capacity of a mobile multiple-antenna wireless link with isotropically random Ricianfading,"We analyze the capacity of a multiple-antenna wireless link with M antennas at the transmitter and N antennas at the receiver in a Rician fading channel when the channel is unknown at both the transmitter and the receiver. The Rician model is a nonstandard model with a Rayleigh component and an isotropically random rank-one specular component. The Rayleigh and specular components remain constant for T symbol periods, after which they change to completely independent realizations, and so on. To maximize mutual information over the joint density of T/spl middot/M complex transmitted signals it is sufficient to maximize over a joint density of min{T,M} real transmitted signal magnitudes. The capacity-achieving signal matrix is equal to the product of two independent matrices, a T/spl times/T isotropically random unitary matrix and a T/spl times/M real nonnegative diagonal matrix. If M>T, optimum signaling uses only T out of the M transmit antennas. We derive a novel lower bound on capacity which enables us to compute achievable rate regions for many cases. This lower bound is also valid for the case of purely Rayleigh-fading channels, where it has not been feasible, in general, to compute capacity, or mutual information. Our numerical results also indicate that the Rayleigh model is surprisingly robust: under our Rician model, up to half of the received energy can arrive via the specular component without significant reduction in capacity compared with the purely Rayleigh case.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.9,Software and its engineering:0.1,Theory of computation:0.4,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Networks,"Networks is relevant as the paper analyzes wireless communication capacity, a core networking topic. Theory of computation is partially relevant but secondary to the communication-theoretic analysis. Other categories like Applied computing are not central to the channel modeling focus.","Network algorithms:0.1,Network architectures:0.1,Network components:0.1,Network performance evaluation:0.9,Network properties:0.3,Network protocols:0.1,Network services:0.1,Network types:0.1",Network performance evaluation,Network performance evaluation is directly relevant as the paper analyzes wireless link capacity under Rician fading. Other network categories are less connected to the theoretical analysis focus.,"Network performance analysis:1.0,Network performance modeling:0.9,Network simulations:0.5,Others:0.3","Network performance analysis,Network performance modeling",Network performance analysis and modeling are highly relevant for evaluating wireless link capacity. Simulations are less central as the focus is on analytical results.
4301,Maximum Achievable Capacity in Airborne MIMO Communications with Arbitrary Alignments of Linear Transceiver Antenna Arrays,"In this paper, the capacity of airborne multiple-input-multiple-output (MIMO) wireless communication systems with arbitrary alignments of linear transmit and receive antenna arrays is systematically analyzed and the maximum achievable capacity is determined. Based on a general three-dimensional (3D) airborne MIMO communication model, we are able to approximate the airborne MIMO capacity as a function of the transmit and receive antenna array geometry in the 3D space. The capacity approximation is asymptotically tight as the distance between the transmit and receive antenna arrays large compared to their size. Based on the asymptotically tight capacity approximation, we derive an upper bound as well as a lower bound of the airborne MIMO capacity. Interestingly, both the upper and lower bounds are achievable. We also derive a necessary and sufficient condition for airborne MIMO communication systems to achieve the capacity upper bound for any given 3D transceiver antenna array geometry. The necessary and sufficient condition allows us to properly select the system parameters and design airborne MIMO communication systems that reach the best possible performance in terms of system capacity. We prove that when the distance between the transmit and receive antenna arrays is within a certain range, there exists a set of system parameter values (e.g. antenna element separation) for which the capacity of the MIMO communication system achieves the theoretical upper bound and this capacity value is larger than the average capacity of the corresponding conventional MIMO communication system under Rayleigh fading. Finally, we prove that the airborne MIMO capacity converges to the capacity lower bound when the distance between the transmit and receive antenna arrays goes to infinity. Extensive numerical studies included in this paper illustrate and validate our theoretical developments.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.95,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Networks,Networks is highly relevant as the paper analyzes airborne MIMO communication capacity. Other categories are not relevant as the focus is on communication system theory rather than algorithms or applications.,"Network algorithms:0.75,Network architectures:0.2,Network components:0.1,Network performance evaluation:1.0,Network properties:0.3,Network protocols:0.4,Network services:0.1,Network types:0.2",Network performance evaluation,Network performance evaluation is directly relevant as the paper systematically analyzes MIMO capacity and derives bounds. Network algorithms (0.75) is secondary due to the algorithmic approach to capacity approximation. Other fields like Network types (0.2) are less relevant as the focus is not on network classification.,"Network experimentation:0,Network measurement:0,Network performance analysis:1,Network performance modeling:1,Network simulations:0","Network performance analysis,Network performance modeling",Network performance analysis: The paper analyzes airborne MIMO capacity and derives bounds. Network performance modeling: The study involves modeling capacity as a function of antenna geometry. Other categories are irrelevant as the paper does not discuss experiments or simulations.
2824,Scalable routing strategies for ad hoc wireless networks,"We consider a large population of mobile stations that are interconnected by a multihop wireless network. The applications of this wireless infrastructure range from ad hoc networking (e.g., collaborative, distributed computing) to disaster recovery (e.g., fire, flood, earthquake), law enforcement (e.g., crowd control, search-and-rescue), and military (automated battlefield). Key characteristics of this system are the large number of users, their mobility, and the need to operate without the support of a fixed (wired or wireless) infrastructure. The last feature sets this system apart from existing cellular systems and in fact makes its design much more challenging. In this environment, we investigate routing strategies that scale well to large populations and can handle mobility. In addition, we address the need to support multimedia communications, with low latency requirements for interactive traffic and quality-of-service (QoS) support for real-time streams (voice/video). In the wireless routing area, several schemes have already been proposed and implemented (e.g., hierarchical routing, on-demand routing, etc.). We introduce two new schemes-fisheye state routing (FSR) and hierarchical state routing (HSR)-which offer some competitive advantages over the existing schemes. We compare the performance of existing and proposed schemes via simulation.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.2,Networks:0.8,Software and its engineering:0.3,Theory of computation:0.1,Mathematics of computing:0.2,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.3,Applied computing:0.2,Social and professional topics:0.1",Networks,Networks is directly relevant for ad hoc wireless routing strategies. Other categories do not address the core networking protocol focus.,"Network algorithms:0.9,Network architectures:0.8,Network components:0.3,Network performance evaluation:0.7,Network properties:0.4,Network protocols:0.5,Network services:0.2,Network types:0.6","Network algorithms,Network architectures",Network algorithms is relevant for FSR/HSR routing schemes; Network architectures is relevant for scalable design. Other categories like Network components are secondary to the core focus on routing strategies.,"Control path algorithms:0.8,Data path algorithms:0.5,Network design principles:1.0,Network economics:0.1,Programming interfaces:0.1","Network design principles,Control path algorithms",Network design principles (1.0) is directly relevant as the paper proposes new scalable routing strategies (FSR/HSR) for ad hoc networks. Control path algorithms (0.8) applies to routing decision mechanisms. Data path algorithms (0.5) is less relevant as the focus is on routing logic rather than data forwarding.
1215,Adaptive Range-Based Power Control for Collision Avoidance in Wireless Ad Hoc Networks,"The paper analyzes the relationships among the transmission range, carrier sensing range, and interference range in case that power control is adopted and proposes an adaptive range-based power control (ARPC) MAC protocol for wireless ad hoc networks to avoid collisions. Based on the analysis results, four mechanisms, STRC, RTRC, SCRC, and RCRC are proposed to prevent from collisions. The paper further analyzes the superiority of each mechanism under certain situations and proposes the ARPC MAC protocol to make use of the advantages of the four mechanisms to avoid collisions. The proposed protocol can not only reduce energy consumption of STAs, but also prevent from collisions. Simulation results also verify the advantages of the proposed protocol.","General and reference:0.1,Hardware:0.3,Computer systems organization:0.2,Networks:1.0,Software and its engineering:0.4,Theory of computation:0.1,Mathematics of computing:0.3,Information systems:0.2,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.3,Applied computing:0.2,Social and professional topics:0.1",Networks,Networks is highly relevant as the paper proposes a MAC protocol for wireless ad hoc networks. Other categories like Hardware are less central to the communication protocol focus.,"Network algorithms:0.8,Network architectures:0.3,Network components:0.4,Network performance evaluation:0.6,Network properties:0.7,Network protocols:0.9,Network services:0.2,Network types:0.5","Network protocols,Network algorithms,Network properties",Network protocols: The paper proposes a MAC protocol for collision avoidance. Network algorithms: Mechanisms like STRC and RCRC involve algorithmic design. Network properties: Analyzes transmission range relationships. Other categories are less directly relevant to protocol design.,"Application layer protocols:0,Control path algorithms:0,Cross-layer protocols:0,Data path algorithms:0,Link-layer protocols:1,Network File System (NFS) protocol:0,Network dynamics:0,Network economics:0,Network layer protocols:0,Network manageability:0,Network mobility:0,Network privacy and anonymity:0,Network protocol design:1,Network range:0.5,Network reliability:0,Network security:0,Network structure:0,OAM protocols:0,Presentation protocols:0,Protocol correctness:0,Session protocols:0,Transport protocols:0","Link-layer protocols,Network protocol design,Network range",Link-layer protocols is highly relevant as the paper focuses on MAC protocol design. Network protocol design is relevant for the proposed ARPC protocol. Network range is moderately relevant due to transmission range analysis. Other options are irrelevant to the wireless ad hoc network context.
5629,Efficient Multidimensional Packet Classification with Fast Updates,"Packet classification has continued to be an important research topic for high-speed routers in recent years. In this paper, we propose a new packet classification scheme based on the binary range and prefix searches. The basic data structure of the proposed packet classification scheme for multidimensional rule tables is a hierarchical list of sorted ranges and prefixes that allows the binary search to be performed on the list at each level to find the best matched rule. We also propose a set of heuristics to further improve the performance of the proposed algorithm. We test our schemes by using rule tables of various sizes generated by ClassBench and compare them with the existing schemes, EGT, EGT-PC, and HyperCuts. The performance results show that in a test using a 2D segmentation table, the proposed scheme not only performs better than the EGT, EGT-PC, and HyperCuts in classification speed and memory usage but also achieves faster table update operations that are not supported in the existing schemes.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.3,Networks:1.0,Software and its engineering:0.4,Theory of computation:0.2,Mathematics of computing:0.2,Information systems:0.2,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.3,Applied computing:0.2,Social and professional topics:0.1",Networks,"Networks is highly relevant as the paper addresses packet classification in routers, a core networking challenge. Computing methodologies is secondary as the focus is on network protocol optimization rather than general computational methods.","Network algorithms:0.9,Network architectures:0.2,Network components:0.1,Network performance evaluation:0.7,Network properties:0.3,Network protocols:0.4,Network services:0.2,Network types:0.1","Network algorithms,Network performance evaluation",Network algorithms is relevant for the packet classification methodology. Network performance evaluation is relevant for comparing classification speed and memory usage. Other categories are rejected as the paper focuses on algorithm design rather than protocols or architectures.,"Control path algorithms:0.0,Data path algorithms:0.8,Network economics:0.0,Network experimentation:0.0,Network measurement:0.0,Network performance analysis:1.0,Network performance modeling:0.5,Network simulations:0.0","Network performance analysis,Data path algorithms",Network performance analysis is highly relevant as the paper evaluates classification speed and memory usage. Data path algorithms are relevant for the packet classification scheme. Other categories like Network simulations are less relevant as the focus is on algorithm design.
355,An adaptive path planning algorithm for mobile-anchor based wireless sensor networks,"In wireless sensor networks, anchor positions play an important role for high localization accuracy. Planning an efficient path for mobile anchor nodes (MN) is critical for required anchor positions. In this work, an adaptive path planning algorithm is proposed for mobile anchors based on Gauss-Markov mobility model. The approach, which the velocity adjustment, the perpendicular bisector and the virtual repulsive strategies are combined based on Gauss-Markov mobility, is improved by developing a grid based virtual repulsive force strategy. To enhance the localization accuracy, the surveillance area is divided into grids and the grid based virtual repulsive force is applied to the MN if it is located in an anchor dense area. The simulation results show that defining a grid based virtual repulsive strategy increase the localization accuracy and cover more surveillance region using less energy.","General and reference:0,Hardware:0,Computer systems organization:0,Networks:1,Software and its engineering:0,Theory of computation:0,Mathematics of computing:0.5,Information systems:0,Security and privacy:0,Human-centered computing:0,Computing methodologies:0.5,Applied computing:0,Social and professional topics:0",Networks,"Networks is highly relevant because the paper focuses on path planning for mobile anchors in wireless sensor networks, a critical aspect of wireless network design. Computing methodologies and Mathematics of computing receive moderate scores for the algorithmic and mobility modeling components but are secondary to the primary networking focus.","Network algorithms:0.9,Network components:0.8,Network performance evaluation:0.85,Network architectures:0.2,Network properties:0.3,Network protocols:0.2,Network services:0.2,Network types:0.2","Network algorithms,Network components,Network performance evaluation","Network algorithms is highly relevant as the paper introduces an adaptive path planning algorithm. Network components is relevant as mobile anchors are key components in wireless sensor networks. Network performance evaluation is relevant as the paper evaluates localization accuracy and energy efficiency. Other categories like Network architectures, Network protocols, etc., are less relevant as the paper focuses on algorithm design rather than network structure or protocols.","Control path algorithms:1,Data path algorithms:0,End nodes:0,Intermediate nodes:0,Logical nodes:0,Middle boxes / network appliances:0,Network economics:0,Network experimentation:0,Network measurement:0,Network performance analysis:0.5,Network performance modeling:0.5,Network simulations:1,Physical links:0,Wireless access points, base stations and infrastructure:0.3","Control path algorithms,Network simulations","Control path algorithms: The paper proposes an adaptive path planning algorithm for mobile anchors. Network simulations: Results are validated via simulations. 'Wireless access points' is rejected as the focus is on path planning, not infrastructure."
1217,An Exponential Distribution Complex Network Model Constructed by Degree Sequence Length Iteration,"Many real world complex networks possess exponential degree distribution with the characteristics of small length and self-similar. To better understand the formation mechanism of exponential distribution network, and make it more direct to reveal the change of degree sequence and the dynamic relationship among nodes in the process of network formation, we propose a deterministic iterative algorithm based on increasing degree sequence length and network self-replication and generate a complex network model. We analyze the main attributes of the network model by both theoretical derivation and numerical simulation. The simulation results are well consistent with the theoretical predictions and show that our network model has a low average distance, an arithmetic degree sequence and follows an exponential degree distribution.","General and reference:0.1,Hardware:0.2,Computer systems organization:0.2,Networks:1.0,Software and its engineering:0.3,Theory of computation:0.1,Mathematics of computing:0.4,Information systems:0.2,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.3,Applied computing:0.2,Social and professional topics:0.1",Networks,Networks is highly relevant as the paper constructs a complex network model. Other categories like Mathematics are secondary to the network structure analysis focus.,"Network algorithms:0.8,Network architectures:0.3,Network components:0.4,Network performance evaluation:0.5,Network properties:0.9,Network protocols:0.2,Network services:0.1,Network types:0.4","Network properties,Network algorithms",Network properties: The paper models exponential degree distribution and self-similarity. Network algorithms: The iterative algorithm for network generation. Other categories are less central to the theoretical network analysis.,"Control path algorithms:0,Data path algorithms:0,Network dynamics:0.5,Network economics:0,Network manageability:0,Network mobility:0,Network privacy and anonymity:0,Network range:0,Network reliability:0,Network security:0,Network structure:1","Network structure,Network dynamics",Network structure is highly relevant for the proposed complex network model. Network dynamics is moderately relevant as the model's self-similar properties relate to dynamic network behavior. Other options are irrelevant to the network modeling focus.
2231,Disseminating Updates on Broadcast Disks,"Lately there has been increasing interest in the use of data dissemination as a means for delivering data from servers to clients in both wired and wireless environments. Using data dissemination, the transfer of data is initiated by servers, resulting in a reversal of the traditional relationship between clients and servers. In previous papers, we have proposed Broadcast Disks as a model for structuring the repetitive transmission of data in a broadcast medium. Broadcast Disks are intended for use in environments where, for either physical or application-dependent reasons, there is asymmetry in the communication capacity between clients and servers. Examples of such environments include wireless networks with mobile clients, cable and direct satellite broadcast, and information dispersal applications. Our initial studies of Broadcast Disks focused on the performance of the mechanism when the data being broadcast did not change. In this paper, we extend those results to incorporate the impact of updates. We first propose several alternative models for updates and examine the fundamental tradeoff that arises between the currency of data and performance. We then propose and analyze mechanisms for implementing these various models. The performance results show that, even in a model where updates must be transmitted immediately, the performance of the Broadcast Disks technique can be made quite mbust through the use of simple techniques for propagating and prefetching data items.","General and reference:0.2,Hardware:0.2,Computer systems organization:0.2,Networks:1.0,Software and its engineering:0.2,Theory of computation:0.2,Mathematics of computing:0.2,Information systems:0.2,Security and privacy:0.2,Human-centered computing:0.2,Computing methodologies:0.2,Applied computing:0.2,Social and professional topics:0.2",Networks,Networks is highly relevant as the paper focuses on data dissemination in broadcast environments. Other categories do not address network transmission strategies.,"Network algorithms:0.25,Network architectures:0.25,Network components:0.25,Network performance evaluation:0.9,Network properties:0.5,Network protocols:0.75,Network services:0.25,Network types:0.75","Network performance evaluation,Network protocols,Network types",Network performance evaluation is highly relevant as the paper evaluates the performance of Broadcast Disks with updates. Network protocols is relevant because Broadcast Disks are a specific protocol for data transmission. Network types is relevant since the paper discusses asymmetric communication environments. Other categories like Network algorithms or Network components are less directly related to the core contribution of performance evaluation and protocol design.,"Ad hoc networks:0.0,Application layer protocols:0.0,Cross-layer protocols:0.0,Cyber-physical networks:0.0,Data center networks:0.0,Home networks:0.0,Link-layer protocols:0.0,Mobile networks:1.0,Network File System (NFS) protocol:0.0,Network experimentation:0.0,Network layer protocols:0.0,Network measurement:0.0,Network on chip:0.0,Network performance analysis:1.0,Network performance modeling:0.0,Network protocol design:1.0,Network simulations:0.0,OAM protocols:0.0,Overlay and other logical network structures:0.0,Packet-switching networks:0.0,Presentation protocols:0.0,Protocol correctness:0.0,Public Internet:0.0,Session protocols:0.0,Storage area networks:0.0,Transport protocols:0.0,Wired access networks:0.0,Wireless access networks:1.0","Mobile networks,Network performance analysis,Wireless access networks","Mobile networks (1): The paper addresses wireless/mobile client environments. Network performance analysis (1): The study focuses on evaluating update mechanisms' performance. Wireless access networks (1): The Broadcast Disk model is applied in wireless communication. Other categories like Transport protocols are irrelevant as the paper focuses on broadcast dissemination, not specific transport protocols."
5011,Joint Adaptive Modulation and Combining for Hybrid FSO/RF Systems,"In this paper, we present and analyze a new transmission scheme for hybrid FSO/RF communication system based on joint adaptive modulation and adaptive combining. Specifically, the data rate on the FSO link is adjusted in discrete manner according to the FSO link's instantaneous received signal-to-noise-ratio (SNR). If the FSO link's quality is too poor to maintain the target bit-error-rate, the system activates the RF link along with the FSO link. When the RF link is activated, simultaneous transmission of the same modulated data takes place on both links, where the received signals from both links are combined using maximal ratio combining scheme. In this case, the data rate of the system is adjusted according to the instantaneous combined SNRs. Novel analytical expression for the cumulative distribution function (CDF) of the received SNR for the proposed adaptive hybrid system is obtained. This CDF expression is used to study the spectral and outage performances of the proposed adaptive hybrid FSO/RF system. Numerical examples are presented to compare the performance of the proposed adaptive hybrid FSO/RF system with that of switch-over hybrid FSO/RF and FSO-only systems employing the same adaptive modulation schemes.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:1.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.2,Applied computing:0.0,Social and professional topics:0.0",Networks,Networks is highly relevant as the paper focuses on hybrid FSO/RF communication systems. Computing methodologies is only marginally relevant due to algorithmic aspects.,"Network algorithms:0.8,Network architectures:0.3,Network components:0.2,Network performance evaluation:0.9,Network properties:0.4,Network protocols:0.7,Network services:0.1,Network types:0.2","Network performance evaluation,Network algorithms,Network protocols",Network performance evaluation: Analyzes spectral and outage performance metrics. Network algorithms: Adaptive modulation and combining are algorithmic solutions. Network protocols: Involves hybrid FSO/RF transmission protocols. Other categories: Less directly related to performance analysis or protocol design.,"Application layer protocols:0.1,Control path algorithms:0.1,Cross-layer protocols:1,Data path algorithms:0.1,Link-layer protocols:0.2,Network File System (NFS) protocol:0.1,Network economics:0.1,Network experimentation:0.1,Network layer protocols:0.2,Network measurement:0.1,Network performance analysis:1,Network performance modeling:0.5,Network protocol design:0.7,Network simulations:0.3,OAM protocols:0.1,Presentation protocols:0.1,Protocol correctness:0.2,Session protocols:0.1,Transport protocols:0.3","Cross-layer protocols,Network performance analysis","Cross-layer protocols is directly relevant due to the hybrid FSO/RF system design. Network performance analysis is central as the paper evaluates spectral and outage performance. Protocol design gets a moderate score as the system design involves protocol-level decisions, but it's secondary to the core cross-layer and performance analysis focus."
1520,Physical-Layer Network Coding for Wireless Cooperative Networks,"1Department of Electronic Engineering, Shanghai Jiao Tong University, Shanghai 200240, China 2Department of Electrical and Computer Engineering, University of Victoria, Victoria, BC, Canada V8W 2Y2 3Department of Electronic Engineering, Tsinghua University, Beijing 100084, China 4 Institute for Communications Engineering, Technische Universität München, 80290 Munich, Germany 5Department of Electrical and Computer Engineering, Lehigh University, Bethlehem, PA 18015, USA 6Department of Electronic Systems, Aalborg University, 9220 Aalborg, Denmark","General and reference:0.1,Hardware:0.1,Computer systems organization:0.2,Networks:0.9,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Networks,"Networks: The paper introduces physical-layer network coding in wireless cooperative networks, directly addressing core network communication protocols. Other categories are rejected because the work focuses on wireless communication rather than hardware design, software engineering, or other fields.","Network algorithms:0.8,Network architectures:0.3,Network components:0.4,Network performance evaluation:0.5,Network properties:0.6,Network protocols:0.3,Network services:0.2,Network types:0.3","Network algorithms,Network properties",Network algorithms is relevant for the physical-layer network coding technique. Network properties is relevant as the work involves wireless cooperative network dynamics. Other categories like Network architectures are not central to the core contribution.,"Control path algorithms:0,Data path algorithms:1,Network dynamics:0,Network economics:0,Network manageability:0,Network mobility:0,Network privacy and anonymity:0,Network range:0,Network reliability:0,Network security:0,Network structure:1","Data path algorithms,Network structure",Data path algorithms: Physical-layer network coding involves algorithms for combining and processing data signals at the physical layer. Network structure: The title suggests a structural approach to network coding in cooperative networks. Other categories like network dynamics or security are not addressed.
2602,A Reference Model for Monitoring IoT WSN-Based Applications,"The Internet of Things (IoT) is, at this moment, one of the most promising technologies that has arisen for decades. Wireless Sensor Networks (WSNs) are one of the main pillars for many IoT applications, insofar as they require to obtain context-awareness information. The bibliography shows many difficulties in their real implementation that have prevented its massive deployment. Additionally, in IoT environments where data producers and data consumers are not directly related, compatibility and certification issues become fundamental. Both problems would profit from accurate knowledge of the internal behavior of WSNs that must be obtained by the utilization of appropriate tools. There are many ad-hoc proposals with no common structure or methodology, and intended to monitor a particular WSN. To overcome this problem, this paper proposes a structured three-layer reference model for WSN Monitoring Platforms (WSN-MP), which offers a standard environment for the design of new monitoring platforms to debug, verify and certify a WSN’s behavior and performance, and applicable to every WSN. This model also allows the comparative analysis of the current proposals for monitoring the operation of WSNs. Following this methodology, it is possible to achieve a standardization of WSN-MP, promoting new research areas in order to solve the problems of each layer.","General and reference:0,Hardware:0,Computer systems organization:0.2,Networks:1,Software and its engineering:0.3,Theory of computation:0,Mathematics of computing:0,Information systems:0,Security and privacy:0,Human-centered computing:0,Computing methodologies:0,Applied computing:0,Social and professional topics:0",Networks,"Networks is directly relevant as the paper discusses wireless sensor network monitoring. Computer systems organization receives partial relevance for architectural aspects, but the core focus is on network monitoring.","Network algorithms:0.5,Network architectures:1.0,Network components:0.3,Network performance evaluation:0.8,Network properties:0.2,Network protocols:0.1,Network services:0.1,Network types:0.4","Network architectures,Network performance evaluation",Network architectures is highly relevant due to the proposed three-layer reference model. Network performance evaluation is relevant as the model supports monitoring and debugging. Other categories like types or protocols are less central to the architectural contribution.,"Network design principles:0.4,Network experimentation:0.6,Network measurement:0.9,Network performance analysis:0.8,Network performance modeling:0.7,Network simulations:0.5,Programming interfaces:0.3","Network performance analysis,Network measurement,Network experimentation","Network performance analysis (WSN behavior evaluation), Network measurement (monitoring platform design), Network experimentation (proposed reference model). Simulations and design principles are secondary."
6003,Analysis of a mobile cellular systems with hand-off priority and hysteresis control,"In this paper, we introduce and analyze a new cut-off priority scheme which provides a better grade-of-service to hand-off traffic while maintaining high throughput for the originating calls. A hysteresis control is used for additional robustness. The system model is found to have a general level-dependent quasi-birth-and-death (QBD) structure. An efficient solution methodology is used taking full advantage of the problem structure. Our model extends, generalizes and unifies the existing models for cut-off priority schemes in wireless cellular networks.","General and reference:0.2,Hardware:0.3,Computer systems organization:0.4,Networks:1.0,Software and its engineering:0.3,Theory of computation:0.3,Mathematics of computing:0.5,Information systems:0.3,Security and privacy:0.2,Human-centered computing:0.2,Computing methodologies:0.5,Applied computing:0.3,Social and professional topics:0.2",Networks,Networks: The paper focuses on wireless cellular network performance analysis with hand-off priority schemes. Other fields like Mathematics of computing and Computing methodologies have partial relevance but are secondary to the core network analysis.,"Network algorithms:1.0,Network performance evaluation:0.7,Network architectures:0.2,Network components:0.0,Network properties:0.0,Network protocols:0.0,Network services:0.0,Network types:0.0","Network algorithms,Network performance evaluation",Network algorithms: The paper introduces and analyzes a new cut-off priority scheme for hand-off traffic. Network performance evaluation: The algorithm's impact on grade-of-service and throughput is studied. Other network categories like architectures or protocols are not the focus.,"Control path algorithms:0.0,Data path algorithms:0.0,Network economics:0.0,Network experimentation:0.0,Network measurement:0.0,Network performance analysis:1.0,Network performance modeling:1.0,Network simulations:0.0","Network performance analysis,Network performance modeling",Network performance analysis: Analyzes hand-off priority impact on service quality. Network performance modeling: Introduces a general QBD model for cut-off priority schemes.
5234,Iterative Antenna Selection for Decode-and-Forward MIMO Relay Systems Under a Holistic Power Model,"In this letter, we investigate the antenna selection in decode-and-forward (DF) MIMO relay systems where the circuit power consumption is considered. The main problem is that the optimal active relay antenna subsets can only be selected by exhaustive search. To reduce the complexity, three iterative properties on the capacity bounds are first derived in this letter, which lay the foundation for our proposed algorithm. This algorithm can improve the performance of DF MIMO relay systems by the maximization of the upper and lower bounds on the capacity. Simulation results will show that the proposed low complexity algorithm has nearly the same performance as that of exhaustive search. In addition, it has a remarkable performance gain over the conventional DF MIMO relay protocol.","General and reference:0.1,Hardware:0.2,Computer systems organization:0.3,Networks:0.9,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Networks,"Networks is highly relevant because the paper focuses on MIMO relay systems, antenna selection, and capacity optimization in wireless communication networks. Other fields like Hardware or Computer systems organization are not central to the paper's core contribution of optimizing relay protocols.","Network algorithms:0.9,Network architectures:0.3,Network components:0.85,Network performance evaluation:0.4,Network properties:0.2,Network protocols:0.3,Network services:0.2,Network types:0.1","Network algorithms,Network components",Network algorithms is directly relevant for antenna selection optimization. Network components applies to MIMO relay systems. Other categories are less central to the technical approach.,"Control path algorithms:0.1,Data path algorithms:0.1,End nodes:0.1,Intermediate nodes:0.7,Logical nodes:0.1,Middle boxes / network appliances:0.1,Network economics:0.1,Physical links:0.6,Wireless access points, base stations and infrastructure:0.1","Intermediate nodes,Physical links",Intermediate nodes is relevant for MIMO relay systems as relay nodes. Physical links is relevant for antenna selection in wireless communication. Other categories like control path algorithms are not central to the antenna selection focus.
1984,Dynamically survivable WDM network design with p-cycle-based PWCE,The concept of a protected working capacity envelope (PWCE) is very attractive for designing a dynamically survivable WDM network. We consider a p-cycle-based PWCE approach for designing survivable WDM networks where the traffic demand is dynamically changing. We develop an integer linear programming formulation to determine the protected working capacity envelope and use simulations to evaluate the blocking performance for different network models.,"General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.9,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Networks,Networks is highly relevant for dynamic survivable WDM network design. Other categories like Hardware or Software are irrelevant as the focus is on network optimization and survivability modeling.,"Network algorithms:0.9,Network architectures:0.8,Network components:0.2,Network performance evaluation:0.3,Network properties:0.2,Network protocols:0.2,Network services:0.1,Network types:0.1","Network algorithms,Network architectures",Network algorithms is highly relevant for the integer linear programming formulation. Network architectures applies to the WDM network design. Other categories like network protocols or services are less central to the core contribution.,"Network design principles:1.0,Control path algorithms:0.2,Data path algorithms:0.2,Network economics:0.1,Programming interfaces:0.1",Network design principles,Network design principles is directly relevant for the dynamic survivable WDM network approach. Other categories are not addressed in the paper.
5655,Poster: Anti-jamming automatic gain control and packet detection for vehicular receiver,"In vehicular application, the wireless receiver not only considers the channel effect, like Doppler effect, Shadowing effect, Multipath fading, ... etc., but also the impulse jamming and continuous wave (CW) jamming from the device in the vehicle. This paper presents anti-jamming automatic gain control (AGC) and packet detection for vehicular receiver. In impulse jamming and CW jamming existence, the AGC responses a suitable gain in 3.2us when packet is arriving, and the packet detection hit rate exceeds 99% hit rate under multipath fading and shadowing effect.","General and reference:0.1,Hardware:0.3,Computer systems organization:0.2,Networks:1.0,Software and its engineering:0.5,Theory of computation:0.1,Mathematics of computing:0.2,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.3,Applied computing:0.2,Social and professional topics:0.1",Networks,Networks is highly relevant as the paper discusses wireless communication and signal processing. Hardware is moderately relevant but secondary to the network focus.,"Network algorithms:0.5,Network architectures:0.3,Network components:0.8,Network performance evaluation:0.6,Network properties:0.4,Network protocols:0.5,Network services:0.3,Network types:0.3",Network components,"Network components is highly relevant as the paper discusses receiver design (AGC, packet detection). Network performance evaluation is less central compared to the component focus.","End nodes:0.7,Intermediate nodes:0.1,Logical nodes:0.1,Middle boxes / network appliances:0.1,Physical links:0.8,Wireless access points, base stations and infrastructure:0.1","End nodes,Physical links",End nodes are relevant as the paper focuses on vehicular receivers (end devices). Physical links are relevant due to the discussion of wireless transmission and jamming mitigation. Other categories like Wireless access points are not directly addressed.
5416,An Evalutaion of Routing and Admission Control Algorithms for Real-Time Traffic in Packet-Switched Networks,"Quality of Service (QoS) guarantees must be supported in a network that intends to carry real-time multimedia traffic effectively. A key problem in providing QoS guarantees is routing which consists of finding a path in a network that satisfies several constraints such as bandwidth, delay, delay jitter, etc. This paper proposes a novel QoS routing scheme called AntNet-QoS which provides QoS by adaptively learning routing tables in packet switched networks. AntNet-QoS is based on the socalled AntNet routing, a swarm intelligence router developed by G. Di Caro and M. Dorigo, that only considers delay when calculating the probabilities used in its routing tables. The innovative AntNet-QoS proposal is based on the utilization of different types of ants (routing packets) for the different classes of services (e.g. premium, best effort and so on) with different QoS requirements. AntNet-QoS considers parameters such as guaranteed bandwidth and maximum delay, enabling the distribution of different packet-based services using a model derived from standard AntNet.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.9,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Networks,Networks is highly relevant as the paper focuses on QoS routing algorithms for packet-switched networks. Other categories like Software and its engineering are not the primary focus.,"Network algorithms:1.0,Network architectures:0.25,Network components:0.0,Network performance evaluation:0.5,Network properties:0.0,Network protocols:0.75,Network services:0.0,Network types:0.0","Network algorithms,Network protocols",Network algorithms is highly relevant as the paper introduces a novel QoS routing scheme (AntNet-QoS). Network protocols is relevant because the work addresses service-class routing mechanisms. Network architectures and performance evaluation are less central to the core algorithmic contribution.,"Application layer protocols:0.2,Control path algorithms:1.0,Cross-layer protocols:0.3,Data path algorithms:0.1,Link-layer protocols:0.1,Network File System (NFS) protocol:0.1,Network economics:0.2,Network layer protocols:1.0,Network protocol design:0.3,OAM protocols:0.1,Presentation protocols:0.1,Protocol correctness:0.1,Session protocols:0.1,Transport protocols:0.2","Control path algorithms,Network layer protocols",Control path algorithms are relevant for the adaptive routing mechanism based on swarm intelligence. Network layer protocols are relevant as the paper focuses on QoS routing in packet-switched networks. Application layer protocols and Transport protocols are less relevant as the paper focuses on routing rather than specific application-layer concerns.
2285,A New Practical Dirty-Paper Coding Strategy in MIMO System,"In this paper, we propose a new broadcast strategy for a MIMO system with N transmit antennas at the transmitter and M les N single antenna receivers. The proposed method spatially separates the M users but does not suffer from the power loss of classical SDMA. For the special case of M = N = 2 and when the two single antenna receivers are assumed to be co-located, the proposed scheme produces a 2 transmit, 2 receiver antenna MIMO scheme. This gives a MIMO transmission scheme that doubles the symbol rate of MIMO STBC systems (Alamouti scheme) from one to two symbol per transmission time. It is proved that it provides the same performance level as that of the Alamouti STBC for the first symbol, and the same performance as the BLAST system for the second symbol. When compared to the BLAST system, our scheme has the same symbol rate, but enjoys significant performance improvements, since it provides 2 level diversity per symbol on the first symbol while the BLAST system does not provide any diversity.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.8,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.2,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Networks,Networks is highly relevant for the MIMO broadcast strategy in wireless communication systems. Other categories like Mathematics of Computing are less central as the focus is on system design rather than theoretical analysis.,"Network algorithms:1.0,Network performance evaluation:0.75,Network types:0.2,Network protocols:0.2,Network architectures:0.2,Network components:0.2,Network properties:0.2,Network services:0.2","Network algorithms,Network performance evaluation",Network algorithms is highly relevant because the paper introduces a novel MIMO system broadcast strategy that directly relates to network algorithm design. Network performance evaluation is relevant due to the comparative analysis with Alamouti and BLAST systems. Other categories like network types or protocols are not discussed in the core contribution.,"Control path algorithms:0.0,Data path algorithms:0.2,Network economics:0.0,Network experimentation:0.1,Network measurement:0.1,Network performance analysis:1.0,Network performance modeling:0.6,Network simulations:0.2",Network performance analysis,Network performance analysis is highly relevant as the paper evaluates the performance of a MIMO broadcast strategy. Other categories like Network simulations are less directly addressed.
5773,Resilience Aspects of Autonomic Cooperative Communications in Context of Cloud Networking,"The desire for ubiquitous, seamless and resilient access to content and services builds up momentum for cutting edge technologies such as cooperative transmission, autonomic communications and cloud networking. Cooperative transmission, being one of the hottest research topics recently, capitalises on the exploitation of orthogonal resource in the spatio-temporal domain to provide diversity through selected relay nodes. At the same time, the paradigm of autonomic communications is based on the idea that a networked system should be able to act like a living organism and self-manage without any external intervention. Last but not least is the concept of wireless network cloud where equipment and software gets separated so new radio standards can be implemented without changes to the hardware. This paper presents a joint vision of using these concepts for service resilience in the network of the future.","General and reference:0.1,Hardware:0.2,Computer systems organization:0.3,Networks:0.9,Software and its engineering:0.4,Theory of computation:0.3,Mathematics of computing:0.2,Information systems:0.2,Security and privacy:0.3,Human-centered computing:0.1,Computing methodologies:0.5,Applied computing:0.3,Social and professional topics:0.1",Networks,"Networks: The paper focuses on cooperative communications, autonomic networks, and cloud networking as foundational infrastructure concepts. The core contribution is about network resilience through these paradigms. Other options like Software/Security are tangential as they don't address the primary network architecture focus.","Network algorithms:0.9,Network architectures:0.4,Network components:0.5,Network performance evaluation:0.6,Network properties:0.7,Network protocols:0.8,Network services:0.7,Network types:0.5","Network algorithms,Network protocols,Network services",Network algorithms is relevant for cooperative transmission methods. Network protocols applies to autonomic communication design. Network services is relevant for cloud networking resilience. Other fields are less central to the architectural discussion.,"Application layer protocols:0.1,Cloud computing:0.9,Control path algorithms:0.6,Cross-layer protocols:0.2,Data path algorithms:0.1,In-network processing:0.1,Link-layer protocols:0.1,Location based services:0.1,Naming and addressing:0.1,Network File System (NFS) protocol:0.1,Network economics:0.1,Network layer protocols:0.1,Network management:0.8,Network monitoring:0.2,Network protocol design:0.1,OAM protocols:0.1,Presentation protocols:0.1,Programmable networks:0.1,Protocol correctness:0.1,Session protocols:0.1,Transport protocols:0.1","Cloud computing,Network management",Cloud computing is relevant due to the integration of cloud networking concepts. Network management is relevant for autonomic communication self-management. Control path algorithms (0.6) are secondary but not central.
5054,Practical cooperative coding for Half-Duplex relay channels,Simple variations on rate-compatible channel codes are shown to achieve cooperative coding gains for the half-duplex relay channel without the complexity of capacity approaching codes. The simulated performance of an optimized irregular low density parity check code is provided.,"General and reference:0.0,Hardware:0.1,Computer systems organization:0.05,Networks:0.9,Software and its engineering:0.05,Theory of computation:0.05,Mathematics of computing:0.05,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.05,Applied computing:0.05,Social and professional topics:0.0",Networks,Networks: The paper presents cooperative coding techniques for relay channels. Other categories are rejected because the paper focuses specifically on communication network problems and solutions.,"Network algorithms:1.0,Network architectures:0.2,Network components:0.3,Network performance evaluation:0.1,Network properties:0.1,Network protocols:0.2,Network services:0.1,Network types:0.1",Network algorithms,Network algorithms is highly relevant as the paper introduces cooperative coding techniques for relay channels. Network components is moderately relevant for the LDPC code implementation but secondary to the algorithmic contribution.,"Control path algorithms:0.1,Data path algorithms:1.0,Network economics:0.2",Data path algorithms,Data path algorithms: The focus on cooperative coding for half-duplex channels is a data path optimization. 'Network economics' and 'Control path algorithms' are not relevant to the paper's technical contributions.
2036,Measuring interactions between transport protocols and middleboxes,"In this paper we explore the evolution of both the Internet's most heavily used transport protocol, TCP, and the current network environment with respect to how the network's evolution ultimately impacts end-to-end protocols. The traditional end-to-end assumptions about the Internet are increasingly challenged by the introduction of intermediary network elements (middleboxes) that intentionally or unintentionally prevent or alter the behavior of end-to-end communications. This paper provides measurement results showing the impact of the current network environment on a number of traditional and proposed protocol mechanisms (e.g., Path MTU Discovery, Explicit Congestion Notification, etc.). In addition, we investigate the prevalence and correctness of implementations using proposed TCP algorithmic and protocol changes (e.g., selective acknowledgment-based loss recovery, congestion window growth based on byte counting, etc.). We present results of measurements taken using an active measurement framework to study web servers and a passive measurement survey of clients accessing information from our web server. We analyze our results to gain further understanding of the differences between the behavior of the Internet in theory versus the behavior we observed through measurements. In addition, these measurements can be used to guide the definition of more realistic Internet modeling scenarios.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.3,Networks:0.9,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.2,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Networks,"Networks is highly relevant because the paper studies how network middleboxes interact with transport protocols like TCP, focusing on protocol mechanisms (Path MTU Discovery, ECN) and network behavior measurements. Other categories like Computer systems organization are less directly relevant as the paper focuses on protocol-environment interactions rather than system architecture.","Network algorithms:0.3,Network architectures:0.2,Network components:0.2,Network performance evaluation:0.85,Network properties:0.4,Network protocols:0.9,Network services:0.3,Network types:0.2","Network protocols,Network performance evaluation",Network protocols is central to analyzing TCP/middlebox interactions. Network performance evaluation addresses measurement of protocol behavior. Other categories like Network architectures are less relevant to the measurement-centric focus.,"Application layer protocols:0.2,Cross-layer protocols:0.1,Link-layer protocols:0.1,Network File System (NFS) protocol:0.1,Network experimentation:0.3,Network layer protocols:0.2,Network measurement:0.9,Network performance analysis:0.6,Network performance modeling:0.4,Network protocol design:0.5,Network simulations:0.3,OAM protocols:0.1,Presentation protocols:0.1,Protocol correctness:0.2,Session protocols:0.1,Transport protocols:0.8","Network measurement,Transport protocols",Network measurement: The paper presents measurement results of protocol behavior. Transport protocols: The focus is on TCP and middlebox interactions. Other children like network performance analysis are less central as the study emphasizes measurement over performance modeling.
3834,Implementation of Advanced IP Network Technology for IPTV Service,": It is absolutely essential to implement advanced IP network technologies such as QoS, Multicast, High Availability, and Security in order to provide real-time services like IPTV via IP backbone network. In reality, the existing commercial networks of internet service providers are subject to certain technical difficulties and limitations in embodying those technologies. On-going research efforts involve the experimental engineering works and implementation experience to trigger IPTV service on the premium-level IP backbone which has recently been developed. This paper introduces the core network technologies that will enable the deployment of a high-quality IPTV service, and then proposes a suitable methodology for application and deployment policies on each technology to lead the establishment and globalization of the IPTV service.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.95,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Networks,"Networks is highly relevant as the paper focuses on IP network technologies (QoS, multicast) for IPTV services. Other categories like Applied computing are secondary but less directly connected to the network infrastructure focus.","Network architectures:1.0,Network protocols:0.9,Network services:0.8,Network algorithms:0.3,Network components:0.4,Network performance evaluation:0.2,Network properties:0.3,Network types:0.5","Network architectures,Network protocols,Network services","Network architectures is primary as the paper discusses IP backbone design. Network protocols (QoS, multicast) and services (IPTV deployment) are directly addressed. Other categories are secondary or not discussed.","Application layer protocols:0.7,Cloud computing:0.2,Cross-layer protocols:0.5,In-network processing:0.1,Link-layer protocols:0.1,Location based services:0.2,Network File System (NFS) protocol:0.1,Network design principles:0.6,Network layer protocols:0.9,Network management:0.6,Network monitoring:0.3,Network protocol design:0.8,OAM protocols:0.2,Presentation protocols:0.1,Programmable networks:0.2,Programming interfaces:0.1,Protocol correctness:0.3,Session protocols:0.5,Transport protocols:0.7","Network layer protocols,Application layer protocols,Network protocol design",Network layer protocols are central to IP backbone implementation for IPTV. Application layer protocols are relevant for service delivery. Network protocol design applies to the proposed methodologies. Lower-layer protocols like Link-layer are less directly addressed in the paper's focus on deployment policies.
4084,Towards Service Continuity in Emerging Heterogeneous Mobile Networks,"There are indications that the next generation mobile communication systems shifts away from the traditional vertically integrated approach in system design and instead allow access to a set of services through a variety of heterogeneous access technologies. With the emergence of multimode mobile devices, the next step in the integration of heterogeneous mobile networks, i.e. inter-access service continuity, gains relevance. This paper reviews emerging heterogeneous access networks and their integration, and studies service continuity, including the relevance of mobile IPv4 and mobile IPv6 and other mobility solutions at, above, and below IP layer, in this context. We propose a new system architecture, which combines mobile IP and IPsec in a 3GPP operator environment. As part of the proposed framework we present new techniques for mobility security association bootstrapping and for enhancing the mobile device's NAPT traversal implementation. The selected architecture is evaluated using an experimental laboratory setup. The results are used to build up an analysis of the solution and draw up some conclusions","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.9,Software and its engineering:0.1,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Networks,Networks is highly relevant because the paper addresses service continuity in heterogeneous mobile networks and evaluates architectures for mobility solutions. Applied computing is less relevant as the focus is on network protocols rather than application-specific systems.,"Network algorithms:0.0,Network architectures:0.5,Network components:0.0,Network performance evaluation:0.0,Network properties:0.0,Network protocols:1.0,Network services:0.8,Network types:0.0","Network protocols,Network services",Network protocols is highly relevant for the mobile IP and IPsec integration. Network services is relevant for service continuity discussion. Network architectures is only partially relevant as the focus is more on protocols than architectural design.,"Application layer protocols:0,Cloud computing:0,Cross-layer protocols:1,In-network processing:0,Link-layer protocols:0,Location based services:1,Naming and addressing:0,Network File System (NFS) protocol:0,Network layer protocols:1,Network management:1,Network monitoring:0,Network protocol design:1,OAM protocols:0,Presentation protocols:0,Programmable networks:0,Protocol correctness:0,Session protocols:0,Transport protocols:0","Cross-layer protocols,Location based services,Network layer protocols",Cross-layer protocols: Mobile IP and IPsec integration spans multiple layers. Location based services: Trilateration for positioning is central to the system. Network layer protocols: Mobile IPv4/IPv6 and NAPT traversal are core to the solution. Other categories like 'Transport protocols' are irrelevant to mobility management.
813,Delay Performance Evaluation of Shared-Buffer Based All-Optical Multihop Networks,An analytical model is developed to evaluate the delay performance of slotted optical shared-buffer based all-optical networks. ShuffleNet is considered as an example of all-optical networks. The model uses the inclusion-exclusion technique for evaluating the probability of packet buffering and it uses Discrete Time Markov Chain (DTMC) to evaluate the end-to-end packet delay. The model enables dimensioning the switch's buffer to meet the target performance in terms of end-to-end delay.,"General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:1.0,Software and its engineering:0.0,Theory of computation:0.5,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Networks,Networks is highly relevant as the paper evaluates delay performance in all-optical networks. Theory of computation is marginally relevant due to analytical models but not central to the core contribution.,"Network algorithms:0.7,Network architectures:0.3,Network components:0.2,Network performance evaluation:0.8,Network properties:0.4,Network protocols:0.3,Network services:0.2,Network types:0.6","Network performance evaluation,Network types",Network performance evaluation: The paper develops analytical models for delay performance. Network types: Focuses on all-optical networks. Other categories like Network algorithms are secondary.,"Ad hoc networks:0,Cyber-physical networks:0,Data center networks:0,Home networks:0,Mobile networks:0,Network experimentation:0,Network measurement:0,Network on chip:0,Network performance analysis:1,Network performance modeling:1,Network simulations:0,Overlay and other logical network structures:0,Packet-switching networks:0,Public Internet:0,Storage area networks:0,Wired access networks:0,Wireless access networks:0","Network performance analysis,Network performance modeling",Network performance analysis: The paper evaluates delay performance using DTMC. Network performance modeling: The analytical model for buffer dimensioning is central. Other options like simulations or wireless networks are not addressed.
2486,Buffer-based smooth rate adaptation for dynamic HTTP streaming,"Recently, Dynamic Streaming over HTTP (DASH) has been widely deployed in the Internet. However, it is still a challenge to play back video smoothly with high quality in the time-varying Internet. In this paper, we propose a buffer based rate adaptation scheme, which is able to smooth bandwidth variations and provide a continuous video playback. Through analysis, we show that simply preventing buffer underflow/overflow in the greedy rate adaptation method may incur serious rate oscillations, which is poor quality-of-experience for users. To improve it, we present a novel control-theoretic approach to control buffering size and rate adaptation. We modify the buffered video time model by adding two thresholds: an overflow threshold and an underflow threshold, to filter the effect of short-term network bandwidth variations while keeping playback smooth. However, the modified rate adaptation system is nonlinear. By choosing operating point properly, we linearize the rate control system. By a Proportional-Derivative (PD) controller, we are able to adapt video rate with high responsiveness and stability. We carefully design the parameters for the PD controller. Moreover, we show that reserving a small positive/negative bandwidth margin can greatly decrease the opportunities of buffer underflow/overflow incurred by the bandwidth prediction error. At last, we demonstrate that our proposed control-theoretic approach are highly efficient through real network trace.","General and reference:0.0,Hardware:0.1,Computer systems organization:0.2,Networks:1.0,Software and its engineering:0.75,Theory of computation:0.1,Mathematics of computing:0.3,Information systems:0.1,Security and privacy:0.0,Human-centered computing:0.1,Computing methodologies:0.4,Applied computing:0.2,Social and professional topics:0.0",Networks,"Networks is highly relevant due to the paper's focus on DASH, bandwidth variation control, and HTTP streaming. Software and its engineering is relevant for the rate adaptation algorithm, but Networks is the primary domain. Other categories lack direct relevance to streaming or network management.","Network performance evaluation:0.9,Network algorithms:0.85,Network protocols:0.3,Network components:0.2,Network types:0.1,Network architectures:0.1,Network services:0.1,Network properties:0.1","Network performance evaluation,Network algorithms",Network performance evaluation is highly relevant as the paper analyzes buffer-based rate adaptation performance. Network algorithms is relevant for the control-theoretic approach. Other categories like network protocols are less relevant since the focus is on rate adaptation algorithms rather than protocol design.,"Control path algorithms:0.0,Data path algorithms:0.0,Network economics:0.0,Network experimentation:0.0,Network measurement:0.0,Network performance analysis:1.0,Network performance modeling:0.9,Network simulations:0.0","Network performance analysis,Network performance modeling","Network performance analysis: The paper introduces a control-theoretic approach for buffer management and rate adaptation in dynamic streaming, directly addressing network performance optimization. Network performance modeling: The modified buffered video time model with thresholds and PD controller linearization represents a modeling approach for network performance. Other options are irrelevant as the paper does not discuss network economics, simulations, or data/control path algorithms."
4182,Understanding TCP fairness over wireless LAN,"As local area wireless networks based on the IEEE 802.11 standard see increasing public deployment, it is important to ensure that access to the network by different users remains fair. While fairness issues in 802.11 networks have been studied before, this paper is the first to focus on TCP fairness in 802.11 networks in the presence of both mobile senders and receivers. In this paper, we evaluate extensively through analysis, simulation, and experimentation the interaction between the 802.11 MAC protocol and TCP. We identify four different regions of TCP unfairness that depend on the buffer availability at the base station, with some regions exhibiting significant unfairness of over 10 in terms of throughput ratio between upstream and downstream TCP flows. We also propose a simple solution that can be implemented at the base station above the MAC layer that ensures that different TCP flows share the 802.11 bandwidth equitably irrespective of the buffer availability at the base station.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:1.0,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Networks,Networks is highly relevant as the paper focuses on TCP fairness in 802.11 wireless networks. Other categories like Hardware or Software are secondary.,"Network performance evaluation:1.0,Network protocols:0.75,Network algorithms:0.5,Network architectures:0.3,Network components:0.2,Network properties:0.2,Network services:0.1,Network types:0.1","Network performance evaluation,Network protocols",Network performance evaluation is highly relevant as the paper analyzes TCP fairness metrics. Network protocols (0.75) addresses TCP behavior in 802.11. Other categories like network types are less central to the study.,"Application layer protocols:0.2,Cross-layer protocols:0.3,Link-layer protocols:0.1,Network File System (NFS) protocol:0.1,Network experimentation:0.6,Network layer protocols:0.1,Network measurement:0.6,Network performance analysis:1.0,Network performance modeling:0.8,Network protocol design:0.4,Network simulations:0.7,OAM protocols:0.1,Presentation protocols:0.1,Protocol correctness:0.1,Session protocols:0.1,Transport protocols:1.0","Transport protocols,Network performance analysis",Transport protocols: The paper investigates TCP fairness in wireless networks. Network performance analysis: The extensive evaluation of TCP performance is central. Other categories like Network simulations are mentioned but not the primary focus.
4684,Distributed load balancing by two-hop relaying in LTE-Advanced networks,"Next-generation Long Term Evolution - Advanced (LTE-A) constitutes a promising solution to withstand the exponential growth of cellular connections related to user equipment devices (UEs) and their increasing demands of bandwidth. Relaying is a technique contemplated in this type of cellular networks where multi-hop communications can be established between nodes (UEs and relay nodes [either fixed or mobile]) to improve link quality and balance the network load between neighboring evolved Node Base stations (eNodeBs). Particularly, due to the dynamic behavior of Mobile Relay Nodes (MRNs), relaying becomes more challenging because there is a need to implement effective methods to choose multi-hop links that can guarantee acceptable levels of Quality of Service (QoS), while balancing the data traffic between different eNodeBs (for improving the robustness of the network), and with the handicap of mobility. This work presents a distributed algorithm in which MRNs act as intermediate relays between underloaded eNodeBs and UEs previously attached to other overloaded cell by dynamically assigning them a portion of the Resource Blocks initially scheduled for the corresponding intermediate MRN (by its serving eNodeB). Results show that multi-hop forwarding can be beneficial for the network performance enhancement in terms of per-user throughput and network load balancing.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.95,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Networks,"Networks: The paper discusses load balancing in LTE-Advanced networks using relaying techniques, which is a core networking problem. 'Computer systems organization' is irrelevant as the focus is on network protocols, not hardware architecture.","Network algorithms:0.8,Network architectures:0.3,Network components:0.1,Network performance evaluation:0.7,Network properties:0.1,Network protocols:0.1,Network services:0.1,Network types:0.1","Network algorithms,Network performance evaluation",Network algorithms: The paper introduces a distributed load-balancing algorithm. Network performance evaluation: Focuses on throughput and load metrics. Other options like 'Network architectures' are less central.,"Control path algorithms:0.1,Data path algorithms:0.2,Network economics:0.1,Network experimentation:0.1,Network measurement:0.1,Network performance analysis:0.8,Network performance modeling:0.6,Network simulations:0.5","Network performance analysis,Network simulations",Network performance analysis is directly relevant for evaluating load balancing effectiveness. Network simulations applies to the algorithm testing in LTE-A networks. Other options like Data path algorithms are less relevant as the focus is on traffic distribution rather than data routing.
5459,Performance evaluation of IEEE 802.11p MAC protocol in VANETs safety applications,"VANETs are becoming more and more popular as a way to increase the traffic safety and comfort. The IEEE 802.11p standard, especially the 802.11p MAC protocol, has attracted much attention as part of the WAVE protocol in VANETs. Safety applications, as one of the main categories of applications in VANETs, is very challenging for the design of a MAC protocol due to their low latency and high reliability requirements. The CCH interval is also a key parameter for the 802.11p MAC protocol since it can affect the performance of safety message delivery significantly. In this paper, a simulation based evaluation is proposed to evaluate the performance of the 802.11p MAC protocol with various vehicle densities and CCH interval settings. The evaluation results indicate that the 802.11p MAC protocol can be improved via extending the CCH interval. However, the reliability is still very challenging due to high collision rates.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.2,Networks:1.0,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.2,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Networks,Networks is relevant as the paper evaluates IEEE 802.11p MAC protocol performance in VANETs. Computer systems organization is marginally relevant for system-level parameters.,"Network algorithms:0.1,Network architectures:0.1,Network components:0.1,Network performance evaluation:0.9,Network properties:0.1,Network protocols:0.8,Network services:0.1,Network types:0.1","Network performance evaluation,Network protocols","Network performance evaluation: Simulates and evaluates MAC protocol performance under varying conditions. Network protocols: Analyzes IEEE 802.11p MAC protocol design and reliability. Other options like Network architectures are irrelevant as the focus is on protocol evaluation, not infrastructure design.","Application layer protocols:0,Cross-layer protocols:0,Link-layer protocols:0.5,Network File System (NFS) protocol:0,Network experimentation:0,Network layer protocols:0,Network measurement:0,Network performance analysis:1,Network performance modeling:0,Network protocol design:0,Network simulations:1,OAM protocols:0,Presentation protocols:0,Protocol correctness:0,Session protocols:0,Transport protocols:0","Network performance analysis,Network simulations",Network performance analysis is central to the evaluation. Network simulations are used to test the protocol. Link-layer protocols are marginally relevant.
527,Performance tradeoffs in wireless scheduling with flow aggregration,"We develop Markov models for various schedulers in order to evaluate their performance tradeoffs at wireless links. While FIFO scheduling aggregates flows into a single flow just prior to the wireless link, channel-state dependent (or CSD) schedulers maintain a queue for each flow, and use predicted channel information to make scheduling decisions. We present results of tradeoffs in terms of overall throughput and per-flow QoS performance obtained with each scheduler under different channel conditions.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:1.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Networks,Networks is highly relevant as the paper focuses on wireless scheduling performance analysis. Other categories like 'Computer systems organization' or 'Software and its engineering' are not central to the core contribution of evaluating scheduling algorithms in wireless networks.,"Network algorithms:0.8,Network architectures:0.1,Network components:0.1,Network performance evaluation:0.8,Network properties:0.1,Network protocols:0.1,Network services:0.1,Network types:0.1","Network algorithms,Network performance evaluation",Network algorithms is relevant for modeling and evaluating wireless schedulers. Network performance evaluation is relevant for analyzing throughput and QoS tradeoffs. Other fields like Network protocols are not directly addressed in the study's focus on scheduling strategies.,"Control path algorithms:0,Data path algorithms:0,Network economics:0,Network experimentation:0,Network measurement:0,Network performance analysis:1,Network performance modeling:1,Network simulations:0","Network performance analysis,Network performance modeling",Network performance analysis and Network performance modeling are relevant as the paper evaluates scheduling tradeoffs using Markov models. Other categories like network measurement are not discussed.
1380,MoveRL: A Movement Region Based Location Discovery Protocol for Wireless Sensor Networks,"Location discovery is a challenging task in a wireless sensor network. In this paper, we propose a protocol so that the devices that are part of a wireless sensor network may compute their own locations. In the proposed protocol, each anchor selects a region of its movement. which is based on its initial location. During the movement of an anchor, beacons are transmitted so that unlocalized sensors may compute their own locations using the locations of anchors and the distance estimates. To evaluate the performance of the proposed protocol, simulations are carried out. We observed that the proposed protocol performs well in terms of the localization error and the fraction of localized sensors.","General and reference:0.0,Hardware:0.25,Computer systems organization:0.25,Networks:1.0,Software and its engineering:0.5,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Networks,"Networks is highly relevant because the paper introduces a novel protocol for location discovery in wireless sensor networks, which is a core networking problem. Other categories like Hardware or Software and its engineering are less directly relevant as the focus is on protocol design rather than implementation specifics.","Network algorithms:0.75,Network architectures:0.5,Network components:0.25,Network performance evaluation:1.0,Network properties:0.5,Network protocols:1.0,Network services:0.25,Network types:0.5","Network protocols,Network performance evaluation",Network protocols is core for the movement-based location discovery method. Network performance evaluation applies to the simulation results. Other categories like Network algorithms are less directly relevant.,"Network protocol design:1,Network simulations:1,Network performance analysis:0.7","Network protocol design,Network simulations",Network protocol design: The protocol for anchor-based localization is the primary contribution. Network simulations: Performance is evaluated through simulations. Performance analysis (0.7) is secondary as the focus is on protocol design rather than analysis.
3570,Performance evaluation of PUMA routing protocol for Manhattan mobility model on vehicular ad-hoc network,"The Quality of Service (QoS) on the traffic environment with high mobility, dynamic topology, and change trajectory is one of the challenges in the use of Vehicular Ad-Hoc Networks (VANET) Routing Protocols. PUMA routing protocol presents multicast work mechanism based on shared mesh topology. It always dynamically adapt to VANET traffic conditions. In this paper, we evaluated the PUMA routing protocol performance using the Manhattan Mobility Model with the effect of the Nakagami fading distribution in VANET. We also analyse the average node distribution in the tagged vehicle communication range using Poisson distribution. We use the NS 2.34 and VanetMobiSim for the simulation and evaluation of the performance. The performance results will be analyzed by varying the traffic parameters in the Manhattan mobility model from low to high traffic condition. The simulation result shows that the communication range under the Nakagami fading distribution affected the Quality of Service (QoS) in VANET. We found that the delay is higher compared with those in the condition without Nakagami fading. The throughput is also lower compared with those in the condition without Nakagami fading.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:1.0,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Networks,Networks is highly relevant as the paper evaluates a routing protocol in VANET. Other fields do not address mobile ad-hoc network performance analysis.,"Network algorithms:0.7,Network architectures:0.1,Network components:0.1,Network performance evaluation:0.9,Network properties:0.1,Network protocols:0.8,Network services:0.1,Network types:0.1","Network performance evaluation,Network protocols,Network algorithms","The paper evaluates the PUMA protocol under Manhattan mobility, making 'Network performance evaluation' primary. 'Network protocols' is central as the focus is on routing. 'Network algorithms' is included due to the protocol's algorithmic design. Other categories like Network types are not discussed.","Application layer protocols:0,Control path algorithms:0.3,Cross-layer protocols:0,Data path algorithms:0.4,Link-layer protocols:0,Network File System (NFS) protocol:0,Network economics:0,Network experimentation:0,Network layer protocols:1,Network measurement:0.5,Network performance analysis:1,Network performance modeling:0.7,Network protocol design:1,Network simulations:1,OAM protocols:0,Presentation protocols:0,Protocol correctness:0,Session protocols:0,Transport protocols:0","Network layer protocols,Network protocol design,Network simulations",Network layer protocols: PUMA is a routing protocol evaluated for VANETs. Network protocol design: The paper analyzes the protocol's multicast mechanism and adaptability. Network simulations: The study uses NS-2.34 and VanetMobiSim for evaluation. Rejected categories like Control/Data path algorithms are secondary; the focus is on protocol evaluation rather than specific algorithm design.
227,Overview of the DVB‐SH specifications,"This article provides an overview of the DVB‐SH specifications, and the context within which they have been developed. The main DVB‐SH configurations are introduced, with explanations of both the technical and frequency planning considerations that drive choices. Regulatory aspects in the European Union and United States of America are summarized. Then specific DVB‐SH issues are being addressed, involving reception conditions and channel characteristics, combining techniques for satellite and terrestrial signals, and synchronization and local content insertion. Some attention is paid to the reuse of DVB link and application layer features. Finally, a couple of likely service introduction scenarios are suggested. It is claimed that DVB‐SH fulfills the initial requirements of extending the present UHF‐based service offer to other frequencies below 3 GHz with a large area coverage, a reduced total network infrastructure cost and an expansion of the offer in terms of number of TV channels or multimedia services. Copyright © 2009 John Wiley & Sons, Ltd.","General and reference:0.1,Hardware:0.2,Computer systems organization:0.1,Networks:0.8,Software and its engineering:0.3,Theory of computation:0.1,Mathematics of computing:0.2,Information systems:0.2,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.2,Social and professional topics:0.1",Networks,Networks is highly relevant for the DVB-SH specification overview. Other categories like Hardware are less relevant as the focus is on communication standards.,"Network types:1.0,Network protocols:0.75,Network components:0.5,Network architectures:0.6,Network services:0.4","Network types,Network protocols",Network types is core as the paper discusses satellite/terrestrial broadcast configurations. Network protocols is relevant for synchronization and content insertion. Other categories like network components are secondary to the architectural focus.,"Ad hoc networks:0,Application layer protocols:0,Cross-layer protocols:0,Cyber-physical networks:0,Data center networks:0,Home networks:0,Link-layer protocols:0,Mobile networks:0,Network File System (NFS) protocol:0,Network layer protocols:0,Network on chip:0,Network protocol design:1,OAM protocols:0,Overlay and other logical network structures:0,Packet-switching networks:0,Presentation protocols:0,Protocol correctness:0,Public Internet:0,Session protocols:0,Storage area networks:0,Transport protocols:1,Wired access networks:0,Wireless access networks:1","Network protocol design,Transport protocols,Wireless access networks",Network protocol design is relevant for DVB-SH specification development. Transport protocols are relevant for content delivery. Wireless access networks are relevant for satellite-terrestrial integration. Other network categories are not central to the paper.
5806,Low Complexity Resource Allocation for Massive Carrier Aggregation,"Optimal resource allocation (RA) in massive carrier aggregation scenarios is a challenging combinatorial optimization problem whose dimension is proportional to the number of users, component carriers (CCs), and OFDMA resource blocks per CC. Toward scalable, near-optimal RA in massive CA settings, an iterative RA algorithm is proposed for joint assignment of CCs and OFDMA resource blocks to users. The algorithm is based on the principle of successive geometric programming approximations and has a complexity that scales only linearly with the problem dimension. Although its derivation is based on a relaxed formulation of the RA problem, the algorithm is shown to converge to integer-valued RA variables with probability 1 under mild assumptions on the distribution of user utilities. Simulations demonstrate improved performance of the proposed algorithm compared to commonly considered heuristic RA procedures of comparable complexity.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:1.0,Software and its engineering:0.2,Theory of computation:0.2,Mathematics of computing:0.3,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Networks,Networks is highly relevant as the paper proposes a resource allocation algorithm for massive carrier aggregation in wireless networks. Theory of computation (0.2) and Mathematics of computing (0.3) are secondary because the focus is on practical network optimization rather than theoretical models.,"Network algorithms:1.0,Network architectures:0.3,Network components:0.2,Network performance evaluation:0.8,Network properties:0.4,Network protocols:0.5,Network services:0.1,Network types:0.1","Network algorithms,Network performance evaluation",Network algorithms: The core contribution is a low-complexity resource allocation algorithm. Network performance evaluation: The paper validates the algorithm's performance against heuristics. Other categories like Network protocols are less central as the focus is on allocation rather than protocol design.,"Control path algorithms:0.2,Data path algorithms:1.0,Network economics:0.3,Network experimentation:0.4,Network measurement:0.5,Network performance analysis:1.0,Network performance modeling:0.6,Network simulations:0.7","Data path algorithms,Network performance analysis",Data path algorithms are relevant due to the focus on OFDMA resource allocation complexity. Network performance analysis is relevant for evaluating algorithm efficiency. Other categories like control path or economics are not central to the resource allocation optimization approach.
2320,WLC44-6: Interference Characterization in Shared Wireless Channels,"An investigation carried out for a generalized Nakagami fading channel in this paper, concerns the performance degradation introduced by a field of interferers surrounding a victim receiver in a shared wireless channel. A new mathematical model adopted assumes interferers are distributed according to a Poisson process in space and frequency. Based on this model, an expression derived for average bit error probability and employed to determine spectral efficiency of the shared wireless channel which emerges is not only exact but explicit. Numerical results obtained suggests that pulse waveshaping is vital to the overall performance of the wireless system. It is found that root raised cosine filters offer improved performance compared with Gaussian filters. For example, the enhancement is as much as 16.7% spectral efficiency when compared with Gaussian filter of time-bandwidth product of 0.3. Furthermore, single tone interference are less harmful than interference that has a waveshape that is identical to the useful signal.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.9,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.3,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.2,Social and professional topics:0.1",Networks,Networks is highly relevant as the paper investigates interference in wireless channels and proposes signal shaping techniques. Other categories like Mathematics of computing are less relevant as the focus is on practical communication system design rather than theoretical mathematics.,"Network algorithms:0.8,Network architectures:0.1,Network components:0.1,Network performance evaluation:0.7,Network properties:0.6,Network protocols:0.1,Network services:0.1,Network types:0.1","Network algorithms,Network performance evaluation,Network properties",Network algorithms: Mathematical modeling of interference. Network performance evaluation: Spectral efficiency testing. Network properties: Interference analysis.,"Control path algorithms:0.1,Data path algorithms:0.1,Network dynamics:0.9,Network economics:0.1,Network experimentation:0.3,Network manageability:0.1,Network measurement:0.1,Network mobility:0.1,Network performance analysis:0.8,Network performance modeling:0.7,Network privacy and anonymity:0.1,Network range:0.1,Network reliability:0.2,Network security:0.1,Network simulations:0.2,Network structure:0.1","Network dynamics,Network performance analysis",Network dynamics is central to the interference modeling. Network performance analysis is relevant for the derived bit error probability. Other categories like reliability are less directly addressed.
2195,On-Demand Cooperation MAC Protocols with Optimal Diversity-Multiplexing Tradeoff,"This paper presents access protocols with optimal Diversity-Multiplexing Tradeoff (DMT) performance in the context of IEEE 802.11-based mesh networks. The protocols are characterized by two main features: on-demand cooperation and selection of the best relay terminal. The on-demand characteristic refers to the ability of a destination terminal to ask for cooperation when it fails in decoding the message transmitted by a source terminal. This approach allows maximization of the spatial multiplexing gain. The selection of the best relay terminal allows maximization of the diversity order. Hence, the optimal DMT curve is achieved with these protocols.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:1.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Networks,"Networks: The paper presents MAC protocols for IEEE 802.11 mesh networks with optimal DMT performance, directly addressing network communication and optimization. Other categories like Theory of computation are irrelevant as the focus is on network protocols.","Network algorithms:0.2,Network architectures:0.1,Network components:0.1,Network performance evaluation:0.3,Network properties:0.1,Network protocols:0.9,Network services:0.1,Network types:0.1",Network protocols,Network protocols is highly relevant as the paper presents new MAC protocols for mesh networks. Other categories are less relevant as the paper focuses on protocol design rather than network performance evaluation or architecture.,"Application layer protocols:0,Cross-layer protocols:1,Link-layer protocols:1,Network File System (NFS) protocol:0,Network layer protocols:0,Network protocol design:1,OAM protocols:0,Presentation protocols:0,Protocol correctness:0,Session protocols:0,Transport protocols:0","Link-layer protocols,Cross-layer protocols,Network protocol design",The paper focuses on MAC (link-layer protocols) and cross-layer design for DMT optimization. Network protocol design encompasses the broader protocol framework. Higher-layer protocols are not discussed.
903,Stable user-defined priorities,"Network providers now want to enable users to define their own flow priorities, and commercial devices already implement this ability. However, it has been shown that directly applying arbitrary user-defined priorities can fundamentally destabilize a network. In this paper, we show that it is possible to apply user-defined priorities while keeping the network stable. We introduce U-BP, a scalable approach that extends backpressure-based scheduling techniques to service user-defined flow priorities and rates while maintaining throughput optimality and strong network performance. We explain how our approach relies on a dual-layer scheme with an exponential convergence to requested priorities. We further prove analytically the network stability of our solution, and show how it achieves a strong performance for high-priority flows.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.95,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Networks,"Networks: The paper addresses network stability with user-defined priorities using backpressure scheduling, a core networking problem in flow control and resource allocation.","Network algorithms:1.0,Network architectures:0.75,Network components:0.1,Network performance evaluation:0.3,Network properties:0.2,Network protocols:0.1,Network services:0.1,Network types:0.1","Network algorithms,Network architectures","Network algorithms is directly relevant as the paper introduces U-BP, a backpressure-based scheduling algorithm. Network architectures is relevant due to the dual-layer scheme design. Other categories like Network performance evaluation or protocols are less central to the core contribution.","Control path algorithms:1.0,Data path algorithms:0.2,Network design principles:0.6,Network economics:0.1,Programming interfaces:0.1","Control path algorithms,Network design principles","Control path algorithms: The paper introduces U-BP, a scheduling technique that directly addresses flow priority management, which is a core aspect of control path algorithms. Network design principles: The dual-layer scheme and stability proofs relate to network architecture and design. Data path algorithms: Less relevant as the focus is on scheduling rather than data forwarding. Network economics/Programming interfaces: Not discussed."
501,Multiple Active Queue Management within Input/Output Buffers,"The purpose of this research is to design and evaluate a new queue management algorithm effectively controlling the heavy-tailed traffics. Among active queue managements (AQM), Random Early Detection (RED), has been widely used to improve the TCP throughput. The heavy-tailed traffic, however, generates the large fluctuation on the queue causing the degradation of fairness in terms of jitter performance. In this paper, we proposed the Stable time RED (STRED) to improve the jitter and the fairness of jitter performance. STRED enables to stabilize the calculation timing for average queue length leading to suppress the queue fluctuations. We also implemented the STRED mechanism on the ns-2 simulator. In addition, we propose the pseudo model network consisting of multiple input / output buffers. The experiments of different pattern of AQM algorithms were conducted. As the results of simulation, we extracted the following features. Firstly, STRED could sustain the jitter values and large fluctuations of each file size and Pareto distribution. Secondly, STRED at the first incoming buffer made lower and smoother jitter values than the RED method. Thirdly, STRED could improve the fairness of jitter for all file sizes, and especially for heavy-tailed traffics. Finally, STRED creates low jitters and the high fairness in term of jitter meaning that STRED can enhance the quality of mutual communication applications such as Internet telephony.","General and reference:0,Hardware:0,Computer systems organization:0,Networks:1,Software and its engineering:0.5,Theory of computation:0,Mathematics of computing:0.5,Information systems:0,Security and privacy:0,Human-centered computing:0,Computing methodologies:0.5,Applied computing:0,Social and professional topics:0",Networks,'Networks' is directly relevant as the paper focuses on queue management for network traffic. 'Software and its engineering' and 'Computing methodologies' are secondary to the core network optimization contribution.,"Network algorithms:1.0,Network architectures:0.2,Network components:0.2,Network performance evaluation:0.9,Network properties:0.2,Network protocols:0.2,Network services:0.2,Network types:0.2","Network algorithms,Network performance evaluation",Network algorithms is central to the STRED AQM design and optimization. Network performance evaluation is relevant due to the simulation-based analysis of jitter and fairness. Categories like 'Network protocols' are less relevant as the focus is on queue management rather than protocol design.,"Control path algorithms:0.1,Data path algorithms:0.4,Network economics:0.1,Network experimentation:0.3,Network measurement:0.3,Network performance analysis:0.6,Network performance modeling:0.5,Network simulations:0.7","Network performance analysis,Network simulations",Network performance analysis and simulations are central to evaluating the proposed STRED algorithm and its impact on jitter and fairness.
3128,An algebraic approach to practical and scalable overlay network monitoring,"Overlay network monitoring enables distributed Internet applications to detect and recover from path outages and periods of degraded performance within seconds. For an overlay network with n end hosts, existing systems either require O(n2) measurements, and thus lack scalability, or can only estimate the latency but not congestion or failures. Our earlier extended abstract [1] briefly proposes an algebraic approach that selectively monitors k linearly independent paths that can fully describe all the O(n2) paths. The loss rates and latency of these k paths can be used to estimate the loss rates and latency of all other paths. Our scheme only assumes knowledge of the underlying IP topology, with links dynamically varying between lossy and normal.In this paper, we improve, implement and extensively evaluate such a monitoring system. We further make the following contributions: i) scalability analysis indicating that for reasonably large n (e.g., 100), the growth of k is bounded as O(n log n), ii) efficient adaptation algorithms for topology changes, such as the addition or removal of end hosts and routing changes, iii) measurement load balancing schemes, and iv) topology measurement error handling. Both simulation and Internet experiments demonstrate we obtain highly accurate path loss rate estimation while adapting to topology changes within seconds and handling topology errors.","General and reference:0,Hardware:0,Computer systems organization:0,Networks:1,Software and its engineering:0,Theory of computation:0.5,Mathematics of computing:0,Information systems:0,Security and privacy:0,Human-centered computing:0,Computing methodologies:0.5,Applied computing:0,Social and professional topics:0",Networks,Networks: The paper focuses on overlay network monitoring techniques. 'Theory of computation' and 'Computing methodologies' are partially relevant for the algebraic approach but less central than the network-specific application.,"Network algorithms:0.9,Network architectures:0.1,Network components:0.1,Network performance evaluation:0.8,Network properties:0.1,Network protocols:0.1,Network services:0.1,Network types:0.1","Network algorithms,Network performance evaluation",Network algorithms is relevant for the algebraic approach to scalable monitoring. Network performance evaluation is relevant due to the focus on latency and loss rate estimation. Other options like Network architectures are secondary to the paper's core contribution.,"Control path algorithms:0.0,Data path algorithms:0.0,Network economics:0.0,Network experimentation:0.0,Network measurement:1.0,Network performance analysis:1.0,Network performance modeling:0.0,Network simulations:0.0","Network measurement,Network performance analysis",Network measurement is relevant as the paper introduces an algebraic approach to estimate path loss and latency. Network performance analysis is relevant due to the focus on scalable overlay monitoring and performance degradation detection. Other categories like Network simulations are not primary to the proposed methodology.
1671,Delay-aware massive random access: Adaptive framing and successive decoding,"In Internet of Things, wireless access networks need to support a large number of user equipments (UEs) in real time, where UEs frequently arrive and leave and plenty of packet collisions occur. In this paper, we propose a protocol termed adaptive framing with successive decoding (AFSD), that not only supports vast connectivity but also involves little signalling overhead. By using AFSD, the frame length can be adaptively adjusted based on the present UE number to deal with the fluidity of UEs, and successive decoding is adopted to recover signal from the mixed received signal to alleviate the packet collision. We explicitly analyze the average delay of our protocol. Numerical results verify the accuracy of our analysis, as well as the effectiveness of the proposed protocol on handling massive access.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.9,Software and its engineering:0.3,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Networks,Networks is highly relevant as the paper proposes a wireless access protocol for IoT. Software engineering has moderate relevance for protocol implementation but is secondary to the network protocol design. Other categories like Mathematics are not central.,"Network protocols:0.9,Network performance evaluation:0.8,Network algorithms:0.7,Network architectures:0.2,Network components:0.1,Network services:0.3,Network types:0.1","Network protocols,Network performance evaluation",Network protocols is central to the AFSD protocol design. Network performance evaluation is relevant for delay analysis. Other categories are secondary.,"Application layer protocols:1,Cross-layer protocols:0,Link-layer protocols:0,Network File System (NFS) protocol:0,Network experimentation:0,Network layer protocols:0,Network measurement:0.5,Network performance analysis:1,Network performance modeling:0.5,Network protocol design:1,Network simulations:0.5,OAM protocols:0,Presentation protocols:0,Protocol correctness:0,Session protocols:0,Transport protocols:0","Application layer protocols,Network performance analysis,Network protocol design","Application layer protocols is relevant for the AFSD protocol design. Network performance analysis is relevant for delay analysis. Network protocol design is relevant for the protocol's structure. Other categories like Link-layer protocols or Transport protocols are not directly addressed, while Network measurement and simulations are secondary."
1622,"Report of the DIMACS working group on abstractions for network services, architecture, and implementation","A workshop on Abstractions for Network Services, Architecture, and Implementation brought together researchers interested in creating better abstractions for creating and analyzing networked services and network architectures. The workshop took place at DIMACS on May 21-23, 2012. This report summarizes the presentations and discussions that took place at the workshop, organized by areas of abstractions such as layers, domains, and graph properties.","General and reference:0,Hardware:0,Computer systems organization:0,Networks:1,Software and its engineering:0.75,Theory of computation:0,Mathematics of computing:0,Information systems:0,Security and privacy:0,Human-centered computing:0,Computing methodologies:0,Applied computing:0,Social and professional topics:0",Networks,Networks: The paper focuses on abstractions for network services and architecture. Software and its engineering: Only marginally relevant as the paper discusses design rather than implementation.,"Network algorithms:0.3,Network architectures:0.9,Network components:0.4,Network performance evaluation:0.3,Network properties:0.3,Network protocols:0.3,Network services:0.9,Network types:0.4","Network architectures,Network services",Network architectures is relevant as the workshop focuses on abstractions for network architecture design. Network services is relevant due to the emphasis on service abstractions. Network algorithms and protocols are less directly addressed as the core is about structural abstractions rather than algorithmic details.,"Cloud computing:0.0,In-network processing:0.0,Location based services:0.0,Naming and addressing:0.0,Network design principles:1.0,Network management:0.0,Network monitoring:0.0,Programmable networks:0.2,Programming interfaces:0.0","Network design principles,Programmable networks","Network design principles (1.0): The paper focuses on abstractions for network architecture, including layers and domains. Programmable networks (0.2): The abstract mentions architectural design considerations that could imply programmable aspects. Other options are irrelevant as the paper does not address cloud computing, location-based services, or management/monitoring."
2762,Unified Mobility Manager: enabling efficient SIP/UMTS mobile network control,"Internet telephony is viewed as an emerging technology not only for wireline networks, but also for third-generation wireless networks. Although IP end to end is considered the ultimate approach to future wireless voice services, there is still a long way to go before IP voice packets can be effectively transported over the air. Therefore, Internet telephony and today's circuit-switched wireless network will coexist for years to come, and it is essential to effectively perform interworking between these networks. This article proposes the Unified Mobility Manager (UMM) that achieves efficient interworking between traditional wireless networks and Internet telephony networks. The main characteristic of the UMM is that it combines UMTS HLR and SIP proxy functionality in one logical entity, which helps eliminate the performance degradation due to interworking between SIP and UMTS. This article identifies seven potential network architectures with and without the UMM and with varying degrees of IP penetration in the wireless core networks, and performs comparative analysis in terms of their call setup signaling latency. Our performance results show that for SIP originated calls, the architecture with the UMM can achieve better performance than existing UMTS networks without the UMM. Our results further show that when the backbone network is fully IP-enabled, dramatic performance gains can be accomplished with the UMM for PSTN originated calls as well as for SIP originated calls. The article also demonstrates that the UMM allows graceful migration from today's circuit-switched wireless networks to hybrid SIP/circuit-switched wireless networks, and toward the IMS architecture for all-IP UMTS networks in the future.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.9,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Networks,Networks: The paper discusses SIP/UMTS network interworking and mobility management in wireless networks. Other categories like Applied computing or Security are not central to the network protocol optimization focus.,"Network algorithms:0.0,Network architectures:1.0,Network components:0.0,Network performance evaluation:1.0,Network properties:0.0,Network protocols:0.0,Network services:0.0,Network types:0.0","Network architectures,Network performance evaluation",Network architectures is relevant for proposing the Unified Mobility Manager. Network performance evaluation is relevant for latency analysis. Other network categories are not central to the paper's contributions.,"Network design principles:0.5,Network experimentation:0,Network measurement:0,Network performance analysis:1,Network performance modeling:0.5,Network simulations:0,Programming interfaces:0","Network design principles,Network performance analysis",Network design principles are relevant for the UMM architecture. Network performance analysis is central to evaluating interworking efficiency. Modeling is secondary but related.
726,Dynamic Replica Location Service Supporting Data Grid Systems,"Data grid is a kind of wide-area, distributed storage system. It can provide huge storage capacity. There are millions of files in data grid, distributed on hundreds of geographically dispersed storage resources. Given a unique logical identifier for desired content, replica location service can be used to determine the physical locations of one or more copies of this content. It is one of the most important components of data grid. Grid users are dynamic and widely distributed. To provide good performance to all the users, we proposed a design of dynamic replica location service. New replica location service providers can be easily constructed at any time and any location. Users that are away from any of the existing replica location service providers are allowed to construct their own replica location service providers. In this way, good performance can always be achieved. Such design can also enhance the stability of the data grid system and simplify the recovery from replica location service failure.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.8,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.0,Information systems:0.3,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.2,Applied computing:0.0,Social and professional topics:0.0",Networks,Networks: The dynamic replica location service is a core networking/distributed system problem. Information systems is secondary as the focus is on infrastructure rather than data management. Other categories are less relevant.,"Network algorithms:0.3,Network architectures:0.1,Network components:0.1,Network performance evaluation:0.2,Network properties:0.1,Network protocols:0.1,Network services:1.0,Network types:0.1",Network services,Network services is directly relevant as the paper discusses replica location services in data grids.,"Cloud computing:0,In-network processing:0,Location based services:1,Naming and addressing:0,Network management:1,Network monitoring:0,Programmable networks:0","Location based services,Network management",Location based services is relevant as the paper discusses dynamic replica location in data grids. Network management is relevant due to the focus on enhancing system stability and recovery. Other options like Cloud computing are irrelevant as the paper is about data grid systems rather than cloud infrastructure.
5114,A cross-layer parallel handover optimization scheme for WiMAX networks,"The handover performance plays a crucial role in guaranteeing the quality of real-time applications in WiMAX networks. In general, a handover process can be divided into four stages: i) cell reselection, ii) handover preparation, iii) link layer handover, and iv) IP layer handover. A cross-layer parallel handover optimization (CPHO) scheme is proposed in this paper to reduce the handover signaling overhead and latency in each stage. The key idea of our proposed scheme is that uses the knowledge achieved from the backhaul inter-BS communications to reduce the HO control message load in wireless links and overlaps the executions of the link layer and the network layer handover process. Therefore the mean of the handover interruption time can be significantly reduced. The numerical analysis and simulation results show that the proposed approach significantly enhances the handover performance.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:1.0,Software and its engineering:0.3,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Networks,Networks is directly relevant as the paper focuses on handover optimization in WiMAX networks. Other categories like Computer systems organization are less relevant as the focus is on network protocols rather than system architecture.,"Network algorithms:0.9,Network architectures:0.1,Network components:0.1,Network performance evaluation:0.8,Network properties:0.1,Network protocols:0.6,Network services:0.1,Network types:0.1","Network algorithms,Network performance evaluation",Network algorithms: Proposes a cross-layer handover optimization technique. Network performance evaluation: Evaluates the reduction in handover latency and signaling overhead. Other categories like 'Network protocols' are less relevant as the focus is on optimization methods rather than protocol design.,"Control path algorithms:0.6,Data path algorithms:0.5,Network economics:0.1,Network experimentation:0.2,Network measurement:0.2,Network performance analysis:1.0,Network performance modeling:1.0,Network simulations:0.3","Network performance analysis,Network performance modeling",Network performance analysis and modeling are central as the paper proposes optimization for handover latency and signaling overhead. Control path algorithms are secondary due to cross-layer coordination. Other categories like Network economics are irrelevant to the technical focus.
4853,Public network integrity-avoiding a crisis in trust,"Public telecommunications networks have historically been designed with high levels of integrity in mind. Network architects have recognized that public communications facilities play a critical role in society and that a disruption in service could cause severe economic dislocation or even the loss of life. Given this sensitivity, how could the recent series of massive network failures occur? What went wrong with our thinking? What should we change in the future to prevent the recurrence of such failures and thereby avoid a possible crisis in the public's trust of our stewardship? The paper addresses these questions and offers a set of recommendations based on new network integrity measures and design objectives. >","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.9,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.2,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.3,Social and professional topics:0.1",Networks,Networks is highly relevant because the paper focuses on public telecommunications network design and integrity. Other categories like Applied computing (0.3) are less relevant as the paper discusses network reliability rather than specific applications.,"Network algorithms:0.3,Network architectures:0.7,Network components:0.2,Network performance evaluation:0.4,Network properties:0.6,Network protocols:0.5,Network services:0.3,Network types:0.2","Network architectures,Network properties,Network protocols",Network architectures: The paper discusses network design principles for integrity. Network properties: Focuses on critical characteristics like reliability. Network protocols: Mentions measures to prevent failures. Other categories like Network algorithms are less central to the discussion of structural integrity.,"Application layer protocols:0,Cross-layer protocols:0,Link-layer protocols:0,Network File System (NFS) protocol:0,Network design principles:1,Network dynamics:0.3,Network layer protocols:0,Network manageability:0.2,Network mobility:0,Network privacy and anonymity:0,Network protocol design:0,Network range:0,Network reliability:1,Network security:0,Network structure:0.3,OAM protocols:0,Presentation protocols:0,Programming interfaces:0,Protocol correctness:0,Session protocols:0,Transport protocols:0","Network design principles,Network reliability",Network design principles and reliability are directly addressed in the paper's recommendations. Other categories like dynamics or manageability are tangential.
5878,In-Network Local Distributed Estimation for Power-Constrained Wireless Sensor Networks,"In this paper, we consider the problem of power-efficient distributed estimation of a localized event in the large-scale Wireless Sensor Networks (WSNs). In order to increase the power efficiency in these networks, we develop a joint optimization problem that involves both selecting a subset of active sensors and the routing structure so that the quality of estimation at a given querying node is the best possible subject to a total imposed communication cost. We first formulate our problem as an optimization problem and show that it is NP-Hard. Then, we propose a local distributed optimization algorithm that is based on an Estimate-and-Forward (EF) strategy, which allows to perform sequentially this joint optimization in an efficient way. We also provide a lower bound for our optimization problem and show that our local distributed optimization algorithm provides a performance that is close to this bound. Although there is no guarantee that the gap between this lower bound and the optimal solution of the main problem is always small, our numerical experiments support that this gap is actually very small in many cases. An important result from our work is that because of the interplay between the communication cost over the links and the gains in estimation accuracy obtained by choosing certain sensors, the traditional Shortest Path Tree (SPT) routing structure, widely used in practice, is no longer optimal, that is, our routing structures provide a better trade-off between the overall power efficiency and the final estimation accuracy obtained at the querying node. Our experimental results show that our algorithms yield a significant energy saving.","General and reference:0.25,Hardware:0.25,Computer systems organization:0.25,Networks:1.0,Software and its engineering:0.25,Theory of computation:0.5,Mathematics of computing:0.25,Information systems:0.25,Security and privacy:0.25,Human-centered computing:0.25,Computing methodologies:0.25,Applied computing:0.25,Social and professional topics:0.25",Networks,"The paper addresses wireless sensor networks (Networks). Applied computing is marginally relevant as an application domain, but the core contribution is network optimization. Other categories are irrelevant.","Network algorithms:0.9,Network performance evaluation:0.8,Network architectures:0.7,Network types:0.5,Network protocols:0.6,Network services:0.4,Network components:0.3,Network properties:0.4","Network algorithms,Network performance evaluation",Network algorithms: Proposes a distributed optimization algorithm. Network performance evaluation: Demonstrates energy savings through experiments. 'Network architectures' is secondary to the algorithmic contribution.,"Control path algorithms:0.5,Data path algorithms:0.5,Network economics:0.2,Network experimentation:0.3,Network measurement:0.4,Network performance analysis:1.0,Network performance modeling:1.0,Network simulations:0.75","Network performance analysis,Network performance modeling",Network performance analysis and modeling are directly addressed through the optimization of power efficiency and estimation quality. Network simulations are moderately relevant due to numerical experiments. Other categories lack direct relevance to the core problem of resource-constrained routing.
2666,Link-Adaptive MAC Protocol for Wireless Multicast,"Previous researches on ad-hoc networks did not consider the dynamic rate adaptation for wireless multicast. Instead, they statically use the lowest data rate for multicast transmission. The MAC protocol proposed in this paper utilizes the OFDMA mechanism, so that all members can report their rate preference at one time. As a result, the best rate for each member is dynamically selected.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.9,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.3,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.4,Applied computing:0.1,Social and professional topics:0.1",Networks,Networks is highly relevant for the MAC protocol design in wireless multicast. Other categories like Software and its engineering are not central to this protocol-level contribution.,"Network algorithms:0.9,Network architectures:0.0,Network components:0.0,Network performance evaluation:0.0,Network properties:0.0,Network protocols:0.9,Network services:0.0,Network types:0.0","Network algorithms,Network protocols",Network algorithms is relevant for the OFDMA mechanism used in the MAC protocol. Network protocols is relevant as the paper introduces a new MAC protocol for wireless multicast. Other categories like Network components are less relevant as the focus is on protocol design rather than specific hardware components.,"Application layer protocols:0,Control path algorithms:0.4,Cross-layer protocols:0.6,Data path algorithms:0.3,Link-layer protocols:1,Network File System (NFS) protocol:0,Network economics:0,Network layer protocols:0,Network protocol design:0.8,OAM protocols:0,Presentation protocols:0,Protocol correctness:0.5,Session protocols:0,Transport protocols:0","Link-layer protocols,Network protocol design",Link-layer protocols is directly relevant to MAC protocol design. Network protocol design is relevant to the overall protocol development. Cross-layer is secondary but mentioned in the OFDMA mechanism context.
1198,QoS provisioning in beyond 3G heterogeneous wireless systems through common radio resource management algorithms,"Beyond 3G mobile communication systems are being defined as the integration of diverse Radio Access Technologies (RATs) into what is generally known as heterogeneous wireless systems. One of the main challenges that such systems must overcome is the ability to guarantee the interoperability and efficient management of the different RATs in order to provide the user with a suitable and consistent Quality of Service (QoS) level. To this end, one of the key elements that must be considered by the network provider is the Common Radio Resource Management (CRRM) of the different RATs. The work reported in this paper is focused on the development of new CRRM techniques designed to efficiently distribute traffic across the diverse RATs of a heterogeneous wireless network. The main objective of this work is to provide solutions that are able to fulfil user QoS requirements and to optimally exploit overall system resources","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:1.0,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.75,Applied computing:0.1,Social and professional topics:0.1",Networks,"Networks: The paper addresses resource management in heterogeneous wireless systems, a core networking topic. Computing methodologies is partially relevant for the algorithm design but less central than networking. Other fields are irrelevant as the focus is on QoS provisioning in communication systems.","Network algorithms:1.0,Network protocols:0.9,Network performance evaluation:0.8,Network architectures:0.4,Network components:0.3,Network properties:0.3,Network services:0.3,Network types:0.3","Network algorithms,Network protocols,Network performance evaluation",Network algorithms is highly relevant because the paper proposes new CRRM techniques for traffic distribution. Network protocols is relevant as resource management involves protocol design. Network performance evaluation is relevant for assessing QoS fulfillment. Other categories like Network architectures or Network components are less central to the core algorithmic contribution.,"Application layer protocols:0,Control path algorithms:0.3,Cross-layer protocols:0.7,Data path algorithms:0,Link-layer protocols:0,Network File System (NFS) protocol:0,Network economics:0,Network experimentation:0.2,Network layer protocols:0.2,Network measurement:0.5,Network performance analysis:1,Network performance modeling:0.8,Network protocol design:1,Network simulations:0.4,OAM protocols:0,Presentation protocols:0,Protocol correctness:0,Session protocols:0,Transport protocols:0","Network performance analysis,Network protocol design",Network performance analysis is relevant because the paper focuses on optimizing QoS through resource allocation. Network protocol design is relevant due to the development of CRRM techniques. Cross-layer protocols (0.7) is secondary as the work involves integrating multiple RATs. Other categories like transport protocols or link-layer protocols are not core to the paper's focus on CRRM and QoS.
140,BER performance of linear STBC from orthogonal designs over MIMO correlated Nakagami-m fading channels,"This paper presents the evaluation of the average bit error rate (BER) performance of linear space-time block codes (STBC) from orthogonal designs over correlated identically distributed Nakagami-m fading channels. Starting from the moment-generating function (MGF) of the multipath component signals at the antenna array elements, analytical expressions of the BER performance for both integral and nonintegral Nakagami-m fading parameters are derived. Closed-form expressions of the spatial cross-correlation function for mobile nonfrequency selective Nakagami-m fading multiple-input-multiple-output (MIMO) channels are obtained, which are valid for small angle-of-arrival (AOA) spread. In this expressions, various parameters of interest, such as the mean AOA of the signal, AOA spread, and array configurations, are all taken into account. The effects of antenna array configuration and the operating environment (mean AOA, AOA spread, Nakagami fading parameter) on the BER performance of the system are illustrated by several numerical examples.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.9,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Networks,"Networks: The paper analyzes the performance of space-time block codes in MIMO systems with Nakagami-m fading, which is a wireless communication topic. Other fields: The paper doesn't focus on hardware design, theory of computation, software engineering, information systems, etc.","Network performance evaluation:0.9,Network algorithms:0.6,Network protocols:0.3,Network types:0.2",Network performance evaluation,Network performance evaluation is relevant for the BER analysis of STBC codes. Network algorithms receives moderate relevance for code evaluation aspects. Network protocols and types are irrelevant as the focus is on performance metrics rather than protocol design or network architectures.,"Network experimentation:0.2,Network measurement:0.2,Network performance analysis:1.0,Network performance modeling:1.0,Network simulations:0.8","Network performance analysis,Network performance modeling",Network performance analysis: The paper evaluates BER performance analytically. Network performance modeling: Closed-form expressions for spatial cross-correlation are derived. Simulations are mentioned but secondary to the analytical approach.
1015,IP multicast for mobile hosts,"We present alternative designs for efficiently supporting multicast for mobile hosts on the Internet. Methods for separately supporting multicasting and mobility along with their possible interactions are briefly described, and then various solutions to the combined problem are explored. We examine three different multicast delivery mechanisms and compare them based on their efficiency and impact on host protocol software. We describe proposals for integrating multicasting and mobility in the Internet architecture. We first present IP extensions for host mobility and other extensions for multicasting. We then examine local multicasting mechanisms, focusing on a group membership protocol that is optimized for wireless point-to-point links. Next, we examine the problems of sending and receiving multicast datagrams in a wide area network. For multicast reception, we describe three alternative proposals and compare them by examining both their applicability and their performance, as well as possible tradeoffs among the two.","General and reference:0.1,Hardware:0.25,Computer systems organization:0.15,Networks:0.95,Software and its engineering:0.35,Theory of computation:0.1,Mathematics of computing:0.2,Information systems:0.1,Security and privacy:0.15,Human-centered computing:0.1,Computing methodologies:0.25,Applied computing:0.15,Social and professional topics:0.1",Networks,Networks is highly relevant for IP multicast and mobility solutions. Other categories like Software and its engineering are less central to the protocol design focus.,"Network algorithms:0.0,Network architectures:0.5,Network components:0.0,Network performance evaluation:0.0,Network properties:0.0,Network protocols:1.0,Network services:0.0,Network types:0.5",Network protocols,"Network protocols is relevant because the paper focuses on multicast delivery mechanisms for mobile hosts, a protocol-level design. Network architectures and types are less directly relevant as the paper emphasizes protocol design over system structure or classification.","Application layer protocols:0.0,Cross-layer protocols:1.0,Link-layer protocols:0.0,Network File System (NFS) protocol:0.0,Network layer protocols:1.0,Network protocol design:0.5,OAM protocols:0.0,Presentation protocols:0.0,Protocol correctness:0.0,Session protocols:0.0,Transport protocols:0.0","Cross-layer protocols,Network layer protocols",Cross-layer protocols is relevant because the paper integrates mobility and multicasting across layers. Network layer protocols is relevant due to the focus on IP extensions and multicast delivery mechanisms. Other options like Transport protocols or Application layer protocols are not discussed.
3859,Making Packet Erasures to Improve Quality of FEC-Protected Video,"Media delivery over lossy packet networks is a challenging problem, and forward error correction (FEC) based techniques are an important technique for overcoming packet loss. Conventional FEC-based media delivery techniques protect all packets equally, or protect a subset of the packets, or protect different subsets of packets with different levels of protection, e.g., scalable coding with unequal error protection (UEP). This paper proposes an FEC-based technique to maximize the expected received media quality by explicitly discarding packets, when beneficial, in order to provide additional room for FEC. Given knowledge of the importance of each packet, we show that there is a simple and intuitive criterion for the optimal selection of which packets to discard and which to protect, as well as the level of protection, to minimize the expected distortion experienced at the receiver. The proposed approach provides significant gains over the conventional approaches, and these gains are illustrated for the case of sending H.264 coded video data over a packet erasure channel with known packet loss rate.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.8,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Networks,"Networks: The paper focuses on FEC-based optimization for video delivery over packet erasure channels, which is a core networking problem. No other categories directly address the network protocol design focus.","Network algorithms:1.0,Network architectures:0.4,Network components:0.3,Network performance evaluation:0.75,Network properties:0.3,Network protocols:0.5,Network services:0.3,Network types:0.3","Network algorithms,Network performance evaluation",Network algorithms is central to the FEC optimization approach. Network performance evaluation is relevant for analyzing quality gains. Other categories are not primary to the algorithmic contribution.,"Control path algorithms:0,Data path algorithms:0,Network economics:0,Network experimentation:0,Network measurement:0,Network performance analysis:1,Network performance modeling:1,Network simulations:0","Network performance analysis,Network performance modeling","The paper analyzes FEC-based media delivery under packet loss, directly addressing performance analysis and modeling. Other network categories are unrelated."
5656,On determining cluster size of randomly deployed heterogeneous WSNs,"Clustering is an efficient method to solve scalability problems and energy consumption challenges. For this reason it is widely exploited in Wireless Sensor Network (WSN) applications. It is very critical to determine the number of required clusterheads and thus the overall cost of WSNs while satisfying the desired level of coverage. Our objective is to study cluster size, i.e., how much a clusterhead together with sensors can cover a region when all the devices in a WSN are deployed randomly. Therefore, it is possible to compute the required number of nodes of each type for given network parameters.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:1.0,Software and its engineering:0.3,Theory of computation:0.1,Mathematics of computing:0.2,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.3,Applied computing:0.3,Social and professional topics:0.1",Networks,Networks is highly relevant as the paper focuses on WSN clustering and energy efficiency. Computing methodologies is less relevant as the focus is on network structure rather than algorithms.,"Network algorithms:1,Network performance evaluation:0.75,Network properties:0.5,Network protocols:0.25,Network services:0.2,Network architectures:0.15,Network components:0.1,Network types:0.05","Network algorithms,Network performance evaluation,Network properties","Network algorithms: The paper focuses on clustering algorithms for WSNs. Network performance evaluation: It evaluates coverage and energy efficiency. Network properties: Discusses random deployment and coverage properties. Other children like Network components, Network types, etc., are less relevant as the paper does not discuss specific components or network types.","Control path algorithms:0.1,Data path algorithms:0.1,Network dynamics:0.1,Network economics:0.1,Network experimentation:0.1,Network manageability:0.1,Network measurement:0.1,Network mobility:0.1,Network performance analysis:0.9,Network performance modeling:0.8,Network privacy and anonymity:0.1,Network range:0.1,Network reliability:0.1,Network security:0.1,Network simulations:0.1,Network structure:0.8","Network performance analysis,Network structure",Network performance analysis is relevant for optimizing cluster size to improve coverage and energy efficiency. Network structure is relevant as the paper studies clustering in WSNs. Other categories like Network security are not directly addressed.
4029,Frequency-domain iterative inter-code interference combined with parallel interference cancellation for cooperative-MIMO multi-code DS-CDMA,"In future wireless communications, it is severe frequency-selective fading channel to support high data rate; meanwhile the available bandwidth becomes limited. To improve the spectrum-efficiency, one of the promising techniques is multiple input multiple output (MIMO) multiplexing technique. However, the space of one user terminal is limited, and it is difficult to accommodate more antennas in one terminal. Thus, cooperative communication is proposed, to achieve diversity and multiplexing gain via different user terminals. In this paper, a cooperative-MIMO based on multi-code direct sequence code-division multiple access (DS-CDMA) system is presented, which improves the spectrum efficiency a lot. However, the severely fading channel degrades the bit error rate (BER) performance of the proposed system significantly. The paper provided a frequency-domain iterative inter-code interference (ICI) combined with parallel interference cancellation (PIC) technology (ICP), which makes it possible to eliminate the interference between antennas and the residual inter-code interference between codes. From the simulation results, it is derived that the proposed method improves BER performance and the throughput of the system significantly.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.9,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.2,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.3,Applied computing:0.1,Social and professional topics:0.1",Networks,Networks: The paper introduces a cooperative-MIMO communication system for DS-CDMA in wireless networks. Other categories like Hardware are secondary as the focus is on communication protocols and network performance optimization.,"Network algorithms:0.9,Network architectures:0.3,Network components:0.2,Network performance evaluation:0.8,Network properties:0.1,Network protocols:0.4,Network services:0,Network types:0.2","Network algorithms,Network performance evaluation",Network algorithms is relevant due to the proposed frequency-domain interference cancellation technique. Network performance evaluation is relevant as the paper evaluates BER and throughput improvements. Other categories like Network architectures are less central.,"Control path algorithms:0.1,Data path algorithms:0.1,Network economics:0.1,Network experimentation:0.1,Network measurement:0.1,Network performance analysis:0.7,Network performance modeling:0.8,Network simulations:0.7","Network performance analysis,Network performance modeling",Network performance analysis: The paper evaluates BER performance improvements. Network performance modeling: The proposed ICI-PIC method involves algorithmic modeling for performance optimization. Other children like Network simulations were rejected as the focus is on algorithm design rather than simulation frameworks.
4139,Subband adaptive array for MIMO-CDMA space-time block coded system,"This paper presents an interference suppression using subband adaptive array for space-time block coding (STBC) code division multiple access (CDMA) under the frequency selective fading (FSF) channel. The proposed scheme utilizes CDMA with STBC and receive array antenna with subband adaptive array (SBAA) processing at receiver. The received signal is converted into the frequency domain before despreading and adaptive processing is done at each subband. A novel construction of SBAA is introduced to process CDMA signal based on STBC. Furthermore, to improve the performance of proposed scheme, we also introduce STBC-SBAA adopting spreading codes cyclic prefix (CP). Simulation results demonstrate improved performance of the proposed system in the case of single and multiusers environment compare to competing relatives.","General and reference:0.1,Hardware:0.4,Computer systems organization:0.3,Networks:0.8,Software and its engineering:0.5,Theory of computation:0.3,Mathematics of computing:0.4,Information systems:0.2,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.3,Applied computing:0.2,Social and professional topics:0.1",Networks,Networks is directly relevant as the paper focuses on interference suppression in MIMO-CDMA systems. Hardware and Software are less central compared to the communication system's core. Other categories like Theory of computation are not primary domains here.,"Network algorithms:0.9,Network architectures:0.1,Network components:0.1,Network performance evaluation:0.7,Network properties:0.1,Network protocols:0.8,Network services:0.1,Network types:0.1","Network algorithms,Network protocols,Network performance evaluation",Network algorithms is relevant due to the interference suppression method in MIMO-CDMA systems. Network protocols is also relevant as the paper discusses STBC and CDMA protocols. Network performance evaluation is included because the paper evaluates system performance through simulations. Other categories like Network architectures are less directly connected to the technical contributions.,"Data path algorithms:1,Link-layer protocols:0.7,Cross-layer protocols:0.5,Application layer protocols:0.2,Network layer protocols:0.3,Control path algorithms:0.2,Network performance analysis:0.2,Network protocol design:0.2,Network simulations:0.2,Network measurement:0.2,Transport protocols:0.2","Data path algorithms,Link-layer protocols","Data path algorithms: The paper introduces a subband adaptive array algorithm for CDMA. Link-layer protocols: The work addresses signal processing in the physical layer. Other categories like 'Application layer protocols' were rejected because the focus is on low-level signal processing, not higher-layer protocols."
1476,A spectrum access strategy for cloud-based cognitive radio networks,"Traditional cognitive radio network is considerably constrained in the spectrum sensing and the access by its limited power, memory and computational capacity. Fortunately, the advent of cloud computing bring the potential to mitigate these constraints due to its vast storage and computational capacity. This paper focuses on the cloud-based cognitive radio spectrum access strategy and provides an optimal spectrum access operation strategy, which maximizes the transmission efficiency of the secondary users. Simulations results show that the maximum benefits can be achieved at the same cost.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.8,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Networks,Networks is highly relevant because the paper focuses on cloud-based cognitive radio spectrum access strategies. Other fields are irrelevant as the focus is on network resource allocation.,"Network algorithms:1.0,Network architectures:0.75,Network components:0.25,Network performance evaluation:0.75,Network properties:0.25,Network protocols:0.25,Network services:0.25,Network types:0.75","Network algorithms,Network architectures",Network algorithms (1.0): Proposes a cloud-based spectrum access algorithm. Network architectures (0.75): Discusses cloud-integrated cognitive radio network design. Network performance evaluation (0.75): Includes simulation results. Other categories like components or protocols are not central to the algorithmic and architectural contributions.,"Control path algorithms:0.5,Data path algorithms:0.2,Network design principles:0.7,Network economics:0.4,Programming interfaces:0.1","Network design principles,Network economics",Network design principles are relevant for cloud-based cognitive radio architecture. Network economics applies to spectrum access optimization. Categories like Control path algorithms are secondary to the design focus.
5554,Network localization using angle of arrival,"In this paper, we propose two localization methods using angle of arrival (AoA) information. We assume that nodespsila axis orientations are unknown. Therefore, all AoA measurements are employed to calculate the angle differences of two different nodes viewed by the third one. Distance measurements between two nodes within the communication range are also utilized in the first method. For the second method, only AoA information is required. As all distance and angle measurements are accurate enough, the localization problem can be formulated as a linear program (LP). Otherwise, by introducing auxiliary variables, it can be cast as a quadratic program (QP). Simulation examples are presented to illustrate the effectiveness of the proposed methods.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:1.0,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.3,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Networks,"The paper proposes network localization methods using angle of arrival, directly aligning with the 'Networks' category. Other categories like 'Mathematics of computing' are secondary as the paper focuses on network-specific algorithms.","Network algorithms:0.95,Network architectures:0.4,Network components:0.2,Network performance evaluation:0.5,Network properties:0.3,Network protocols:0.1,Network services:0.1,Network types:0.1",Network algorithms,Network algorithms receives high score as the paper proposes new localization algorithms using AoA. Network architectures is less relevant as the focus is on the algorithm rather than network structure design.,"Control path algorithms:0.1,Data path algorithms:0.7,Network economics:0.1",Data path algorithms,"Data path algorithms: The paper addresses network localization using mathematical optimization (LP/QP), which relates to data flow in networked systems. Other options: Control path algorithms/Network economics are not relevant to the problem formulation."
5615,A new TCP for persistent packet reordering,"Most standard implementations of TCP perform poorly when packets are reordered. In this paper, we propose a new version of TCP that maintains high throughput when reordering occurs and yet, when packet reordering does not occur, is friendly to other versions of TCP. The proposed TCP variant, or TCP-PR, does not rely on duplicate acknowledgments to detect a packet loss. Instead, timers are maintained to keep track of how long ago a packet was transmitted. In case the corresponding acknowledgment has not yet arrived and the elapsed time since the packet was sent is larger than a given threshold, the packet is assumed lost. Because TCP-PR does not rely on duplicate acknowledgments, packet reordering (including out-or-order acknowledgments) has no effect on TCP-PR's performance. Through extensive simulations, we show that TCP-PR performs consistently better than existing mechanisms that try to make TCP more robust to packet reordering. In the case that packets are not reordered, we verify that TCP-PR maintains the same throughput as typical implementations of TCP (specifically, TCP-SACK) and shares network resources fairly. Furthermore, TCP-PR only requires changes to the TCP sender side making it easier to deploy.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.9,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Networks,Networks is highly relevant as the paper presents a new TCP protocol variant for handling packet reordering. Other categories like Software engineering are not central to the networking protocol design.,"Network protocols:1.0,Network performance evaluation:0.9,Network algorithms:0.7,Network properties:0.3,Network services:0.2","Network protocols,Network performance evaluation",Network protocols is directly relevant as the paper introduces a new TCP variant. Network performance evaluation is relevant for demonstrating TCP-PR's performance improvements. Network algorithms is less central since the focus is on protocol design rather than algorithmic analysis.,"Application layer protocols:0.2,Cross-layer protocols:0.3,Link-layer protocols:0.1,Network File System (NFS) protocol:0,Network experimentation:0.6,Network layer protocols:0.2,Network measurement:0.7,Network performance analysis:1,Network performance modeling:0.8,Network protocol design:1,Network simulations:0.9,OAM protocols:0.1,Presentation protocols:0.1,Protocol correctness:0.4,Session protocols:0.1,Transport protocols:1","Transport protocols,Network protocol design,Network performance analysis",Transport protocols are central to the TCP-PR proposal. Network protocol design is relevant as the paper introduces a new TCP variant. Network performance analysis is key to evaluating its effectiveness. Cross-layer protocols and Link-layer protocols are rejected as the focus is purely on transport-layer modifications.
106,RiaS: overlay topology creation on a PlanetLab infrastructure,"The PlanetLab testbed was originally built to develop new technologies for distributed storage, network mapping, peer-to-peer systems, distributed hash tables and query processing in a live-traffic environment. This allowed researchers to construct their own overlay network topologies on top of IP without any need for direct Layer2 access.
 While this structure is easy to use for many purposes, it does not lend itself directly to experiments with new routing protocols, which need a finer-grained control of where packets flow. Enabling such tests of new protocols and architectures on the PlanetLab infrastructure is the objective of this paper. To this end, a researcher must be able to build Layer2 topologies upon the PlanetLab infrastructure and to have routing and forwarding protocols execute in such a defined infrastructure. Currently this is impossible due to the nature of PlanetLab's network virtualization.
 This paper describes RiaS, a tool to create customized network topologies inside of PlanetLab slices. This enables researchers to evaluate and test new routing protocols on PlanetLab. We analyze the existing shortcomings of PlanetLab, identify the prerequisites to enable routing experiments, and propose our Routing-in-a-Slice (RiaS) system to overcome this impasse.","General and reference:0.1,Hardware:0.2,Computer systems organization:0.3,Networks:0.9,Software and its engineering:0.4,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Networks,Networks is highly relevant for overlay topology design. Other categories like Computer systems organization are secondary to the core network infrastructure focus.,"Network algorithms:0.7,Network architectures:1.0,Network components:0.2,Network performance evaluation:0.6,Network protocols:0.8,Network services:0.3,Network types:0.4","Network architectures,Network protocols,Network algorithms",Network architectures (1.0) is directly relevant as the paper focuses on creating customized overlay topologies for PlanetLab. Network protocols (0.8) is relevant since the tool enables routing protocol experiments. Network algorithms (0.7) is relevant due to the algorithmic approach to topology creation. Other options like Network types are less relevant as the focus is on architectural design rather than specific network categorization.,"Application layer protocols:0,Control path algorithms:0,Cross-layer protocols:0,Data path algorithms:0,Link-layer protocols:0,Network File System (NFS) protocol:0,Network design principles:1,Network economics:0,Network layer protocols:1,Network protocol design:0,OAM protocols:0,Presentation protocols:0,Programming interfaces:0,Protocol correctness:0,Session protocols:0,Transport protocols:0","Network design principles,Network layer protocols",Network design principles is relevant because the paper addresses the limitations of PlanetLab and proposes a system to enable routing experiments. Network layer protocols is relevant as the focus is on creating Layer2 topologies for routing protocol testing. Other categories like application layer protocols are irrelevant.
3599,Distributed power on-off optimisation for heterogeneous networks - A comparison of autonomous and cooperative optimisation,"The design of distributed algorithms and techniques allowing for an efficient utilisation of infrastructure in terms of energy consumption is one of the key challenges in heterogeneous networks or HetNets. In this study the energy efficiency in the HetNet scenario is formulated as an optimisation problem and an iterative improvement algorithmic approach to power on-off of network cells is devised and evaluated. The network is divided into clusters of cells, and the algorithm, which employs simulated annealing search, is executed by the clusters in a distributed manner. Four different variations of autonomous and cooperative optimisation are devised and the obtained network configuration solutions are compared to a baseline configuration scenario where all cells are powered on. The optimization search is guided by an objective function which is defined on outage throughput and energy efficiency. Simulation results show that the distributed cooperative algorithms perform better than the baseline in nearly all cases both in terms of outage throughput and energy efficiency. Furthermore, coordination among clusters has a strong impact on the speed and the performance of the obtained solutions.","General and reference:0.2,Hardware:0.3,Computer systems organization:0.5,Networks:1.0,Software and its engineering:0.3,Theory of computation:0.2,Mathematics of computing:0.4,Information systems:0.2,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.3,Social and professional topics:0.1",Networks,"Networks: The paper focuses on distributed power optimization in heterogeneous networks (HetNets), which directly aligns with network design and resource management. Other categories like 'Computer systems organization' and 'Hardware' are less central as the core contribution is network-level energy efficiency rather than system architecture or hardware design.","Network algorithms:0.9,Network architectures:0.3,Network components:0.2,Network performance evaluation:0.8,Network properties:0.2,Network protocols:0.3,Network services:0.1,Network types:0.1","Network algorithms,Network performance evaluation",Network algorithms: The paper devises a distributed simulated annealing algorithm for power on-off optimization. Network performance evaluation: The study evaluates energy efficiency and outage throughput as performance metrics. Other categories like Network protocols are less relevant as the focus is on algorithmic optimization rather than protocol design.,"Control path algorithms:0.3,Data path algorithms:0.2,Network economics:0.1,Network experimentation:0.5,Network measurement:0.4,Network performance analysis:0.7,Network performance modeling:0.8,Network simulations:0.6","Network performance modeling,Network simulations",Network performance modeling: The paper formulates energy efficiency as an optimization problem. Network simulations: The algorithm is evaluated via simulations. Other options like Control path algorithms are not discussed in the abstract.
1473,Adaptive actuator failure compensation design for networked control systems with state-dependent disturbances,"In this paper, adaptive state feedback actuator failure compensator for networked control systems (NCSs) with state dependent disturbances are developed. The obstacles of NCSs such as; transmission delays and data-packets dropout induced by the insertion of data networks in the feedback adaptive control loops are also considered. Adaptive laws are designed for updating the controller parameters. Closed-loop stability is ensured. Simulation results show that the desired performance is achieved with the developed adaptive actuator failure compensation control designs. Simulation results are given to illustrate the effectiveness of our design approach.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.75,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.5,Applied computing:0.5,Social and professional topics:0.0",Networks,Networks: The paper discusses adaptive control in networked control systems. Applied computing and computing methodologies are partially relevant due to control system design.,"Network algorithms:0.8,Network architectures:0.0,Network components:0.0,Network performance evaluation:0.7,Network properties:0.0,Network protocols:0.0,Network services:0.0,Network types:0.0","Network algorithms,Network performance evaluation",Network algorithms is relevant for the adaptive control algorithm design. Network performance evaluation is relevant for analyzing system stability under network constraints. Other network categories are less relevant as the paper focuses on control rather than network infrastructure.,"Control path algorithms:0.8,Data path algorithms:0.2,Network economics:0.3,Network experimentation:0.1,Network measurement:0.1,Network performance analysis:0.7,Network performance modeling:0.4,Network simulations:0.2","Control path algorithms,Network performance analysis",Control path algorithms are relevant for adaptive compensation in NCS. Network performance analysis applies to addressing transmission delays and packet loss. Categories like Network economics are less relevant to the technical control focus.
4185,Ferry-based linear wireless sensor networks,"Many environmental, commercial, military, and structural monitoring applications of wireless sensor networks (WSNs) involve lining up the sensors in a linear form, and making a special class of these networks; we defined these in a previous paper as Linear Sensor Networks (LSNs), and provided a classification of the different types of LSNs. A multihop approach to routing the data from the individual sensor nodes to the sink can be used in an LSN. However, this can result in a rapid depletion of the sensor energy, due to the frequent transmissions performed by the sensors to transmit their own, as well as other sensor data. In addition, in many applications, the distance between the sensors deployed to monitor the linear structure might be much greater than the communication range leading to a disconnected network where the multihop approach cannot be used. This paper presents a framework for monitoring linear infrastructures using ferry-based LSNs (FLSNs). The data that is collected by the sensors is assumed to be delay-tolerant. In such a system, a moving robot, vehicle, or any other mobile node (named a ferry), can move back and forth along the linear network, and collect data from the individual sensors when it comes within their communication range; The ferry can deliver the collected sensor data when it reaches the sink. It can also perform other functions, such as data processing, and aggregation, and can also transport messages from the sink to the sensor nodes (SNs). Four different ferry movement approaches are presented, simulated, and analyzed.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:1.0,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Networks,Networks is highly relevant as the paper discusses ferry-based data collection in linear wireless sensor networks. Other categories like Applied computing are secondary.,"Network architectures:1.0,Network types:0.8,Network algorithms:0.5,Network protocols:0.3,Network components:0.2,Network performance evaluation:0.2,Network services:0.1,Network properties:0.1","Network architectures,Network types",Network architectures is directly relevant for the ferry-based design framework. Network types (0.8) addresses linear sensor networks. Other categories like network protocols are less central to the structural design focus.,"Ad hoc networks:1,Mobile networks:1,Wireless access networks:1,Cyber-physical networks:0,Data center networks:0,Home networks:0,Network design principles:0,Network on chip:0,Overlay and other logical network structures:0,Packet-switching networks:0,Programming interfaces:0,Public Internet:0,Storage area networks:0,Wired access networks:0","Ad hoc networks,Mobile networks","Ad hoc networks: The paper discusses wireless sensor networks, which are a type of ad hoc network. Mobile networks: The ferry's movement introduces mobility into the network structure. Wireless access networks: While wireless communication is mentioned, the focus is on ferry-based data collection rather than access network design."
1284,Iterative Receiver with Enhanced Spatial Covariance Matrix Estimation in Asynchronous Interference Environment for 3GPP LTE MIMO-OFDMA System,"To mitigate the asynchronous ICI (Inter-Cell Interference), SCM (Spatial Covariance Matrix) of the asynchronous ICI plus background noise should be accurately estimated for MIMO-OFDMA (Multiple-input Multiple-output-Orthogonal Frequency Division Multiple Access) system. Generally, it is assumed that the SCM of the asynchronous ICI plus background noise is estimated by using training symbols. However, it is difficult to measure the interference statistics for a long time and considering that training symbols are not appropriate for OFDMA system such as LTE (3GPP Long Term Evolution). Therefore, noise reduction method is required to improve the estimation accuracy. Although the conventional time-domain low-pass type weighting method can be effective for noise reduction, it causes significant estimation error due to the spectral leakage in practical OFDM system. Therefore, we propose a time-domain sinc type weighing method which can not only reduce noise effectively minimizing estimation error caused by the spectral leakage but also can be implemented using frequency-domain weighted moving average filter easily. We also consider the iterative CFR (Channel Frequency Response) and SCM estimation method which can effectively reduce the estimation error of both CFR and SCM, and improve the performance for LTE system. By using computer simulation, we show that the proposed method can provide up to 2.5dB SIR (Signal to Interference Ratio) gain compared with the conventional method, and verify that the proposed method is attractive and suitable for implementation with stable operation.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.9,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Networks,"Networks: The paper focuses on signal processing techniques for MIMO-OFDMA systems in LTE, a core topic in communication networks. Other categories do not address the technical focus on wireless communication protocols.","Network algorithms:0.6,Network architectures:0.1,Network components:0.9,Network performance evaluation:0.7,Network properties:0.1,Network protocols:0.1,Network services:0.1,Network types:0.1","Network components,Network performance evaluation,Network algorithms",Network components: The paper focuses on receiver design with spatial covariance matrix estimation. Network performance evaluation: The study quantifies SIR gain improvements through simulations. Network algorithms: The iterative estimation methods involve algorithmic innovation. Other categories like Network types are not central to the technical contribution.,"Control path algorithms:0.1,Data path algorithms:0.1,End nodes:0.2,Intermediate nodes:0.2,Logical nodes:0.1,Middle boxes / network appliances:0.1,Network economics:0.1,Network experimentation:0.4,Network measurement:0.5,Network performance analysis:1.0,Network performance modeling:0.8,Network simulations:0.9,Physical links:0.3,Wireless access points, base stations and infrastructure:0.9","Network performance analysis,Wireless access points, base stations and infrastructure,Network simulations","Network performance analysis is highly relevant as the paper focuses on performance analysis. Wireless access points, base stations and infrastructure is relevant as the paper is about MIMO-OFDMA systems. Network simulations is relevant as the paper uses simulations. Other categories like Data path algorithms are less relevant."
1033,Asynchronous algorithms for network utility maximisation with a single bit,We present a convergence result for a nonhomogeneous Markov chain that arises in the study of networks employing the additive-increase multiplicative decrease (AIMD) algorithm. We then use this result to solve the network utility maximisation (NUM) problem.,"General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.8,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Networks,Networks is highly relevant for the asynchronous NUM algorithms and AIMD. Theory of computation is less central to the core application in network optimization.,"Network algorithms:0.9,Network architectures:0.1,Network components:0.1,Network performance evaluation:0.1,Network properties:0.1,Network protocols:0.1,Network services:0.1,Network types:0.1",Network algorithms,Network algorithms: Core contribution is asynchronous algorithms for NUM. Other options are not directly relevant to the algorithmic analysis.,"Control path algorithms:1,Data path algorithms:0.2,Network economics:0",Control path algorithms,Control path algorithms are directly relevant as the paper addresses network utility maximisation via AIMD algorithms. Data path algorithms and Network economics are not core to the paper's contributions.
3527,DREEM-ME: Distributed Regional Energy Efficient Multi-hop Routing Protocol Based on Maximum Energy in WSNs,Wireless distributed sensor network consists of randomly deployed sensors having low energy assets. These networks can be used for monitoring a variety of environments. Major problems of these networks are energy constraints and their finite lifetimes. To overcome these problems different routing protocols and clustering techniques are introduced. We propose DREEMME which uses a unique technique for clustering to overcome these two problems efficiently. DREEM-ME elects a fix number of cluster heads (CHs) in each round instead of probabilistic selection of CHs. Packet Drop Technique is also implemented in our protocol to make it more comprehensive and practical. In DREEM-ME confidence interval is also shown in each graph which helps in visualising the maximum deviation from original course. Our simulations and results show that DREEM-ME is much better than existing protocols of the same nature.,"General and reference:0.1,Hardware:0.2,Computer systems organization:0.3,Networks:0.8,Software and its engineering:0.2,Theory of computation:0.3,Mathematics of computing:0.3,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.2,Computing methodologies:0.3,Applied computing:0.5,Social and professional topics:0.1",Networks,"Networks is highly relevant for the energy-efficient routing protocol in wireless sensor networks. Applied computing is partially relevant due to the practical application in monitoring environments, but the core contribution is protocol design.","Network algorithms:0.0,Network architectures:0.0,Network components:0.0,Network performance evaluation:1.0,Network properties:0.0,Network protocols:1.0,Network services:0.0,Network types:0.0","Network performance evaluation,Network protocols",Network performance evaluation is relevant for the energy efficiency analysis. Network protocols is relevant for the multi-hop routing protocol design. Other categories like Network components are not the focus.,"Application layer protocols:0.1,Cross-layer protocols:0.1,Link-layer protocols:0.1,Network File System (NFS) protocol:0.1,Network experimentation:0.1,Network layer protocols:0.8,Network measurement:0.1,Network performance analysis:0.7,Network performance modeling:0.6,Network protocol design:0.1,Network simulations:0.1,OAM protocols:0.1,Presentation protocols:0.1,Protocol correctness:0.1,Session protocols:0.1,Transport protocols:0.1","Network layer protocols,Network performance analysis",Network layer protocols is relevant as the paper introduces a routing protocol for WSNs. Network performance analysis is relevant for evaluating the protocol's effectiveness. Other categories like Transport protocols are not focused on the network layer.
1474,Future internets escape the simulator,Future Internet testbeds permit experiments not possible in today's public Net or commercial cloud services.,"General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.9,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Networks,Networks is highly relevant because the paper discusses Future Internet testbeds for experimental networking research. Other fields are irrelevant as the focus is on network infrastructure and experimentation.,"Network algorithms:0.75,Network architectures:1.0,Network components:0.25,Network performance evaluation:0.75,Network properties:0.25,Network protocols:0.25,Network services:0.25,Network types:1.0","Network architectures,Network types",Network architectures (1.0): Discusses future internet testbeds as novel architectural designs. Network types (1.0): Focuses on experimental network environments distinct from current public/cloud networks. Network performance evaluation (0.75): Mentions experimental evaluation in testbeds. Other categories lack direct relevance to the paper's core focus on testbed design and network paradigms.,"Ad hoc networks:0.1,Cyber-physical networks:0.2,Data center networks:0.1,Home networks:0.1,Mobile networks:0.1,Network design principles:0.7,Network on chip:0.1,Overlay and other logical network structures:0.1,Packet-switching networks:0.2,Programming interfaces:0.1,Public Internet:0.3,Storage area networks:0.1,Wired access networks:0.1,Wireless access networks:0.1","Network design principles,Public Internet",Network design principles are relevant for Future Internet testbeds. Public Internet is mentioned as a baseline for comparison. Other categories like Ad hoc or Mobile networks are not explicitly discussed in the abstract.
1374,OCTOPUS: A Hybrid Scheduling Strategy for P2P VoD Services,"Peer-to-Peer (P2P) networks have been shown to be a promising approach to provide large-scale Video on Demand (VoD) services over Internet for its potential high scalability. However, for a normal peer, how to efficiently schedule media data to multiple asynchronous peers for VoD services in such networks remains a major challenge. Hereby we propose a hybrid scheduling scheme for data dissemination in P2P VoD system in this paper, called OCTOPUS, which is combined with adaptive scheduling scheme at normal time and special scheduling strategy emergently. Usually each peer is given a value, meaning scheduling bandwidth to represent service capacity, which is an integrated evaluation for peer's bandwidth and services ability, also including statistical characteristics about historical data transferring. According to the value, one peer can adjust the expectation of each neighbor's abilities and send scheduling request of media data to all partners. In case of failure scheduling, emergent scheduling scheme will be called and limited data requests will be sent to media servers. OCTOPUS scheme helps to make sufficient utilization of end-peers' resource, alleviate the load of source server and improve their scalability. Experimental result from a P2P VoD system, GridCast, based on OCTOPUS schemes, shows that OCTOPUS achieves obvious effect.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.9,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Networks,"Networks: The paper presents a hybrid scheduling strategy for P2P VoD networks, focusing on data dissemination and network resource utilization. Other categories are less relevant as the core contribution is about network protocols and systems.","Network algorithms:1.0,Network architectures:0.4,Network components:0.3,Network performance evaluation:0.7,Network properties:0.5,Network protocols:0.8,Network services:0.2,Network types:0.3","Network algorithms,Network protocols,Network performance evaluation",Network algorithms is relevant because the paper introduces a hybrid scheduling strategy for P2P VoD. Network protocols is relevant as the algorithm is part of a P2P communication protocol. Network performance evaluation is relevant due to the experimental results demonstrating scalability improvements. Other categories are less relevant as the focus is on algorithmic design for scheduling rather than broader network infrastructure.,"Application layer protocols:1,Control path algorithms:0.5,Network performance analysis:1,Network protocol design:0.7,Network simulations:0.5","Application layer protocols,Network performance analysis","Application layer protocols: The paper proposes a scheduling strategy for P2P VoD, which operates at the application layer for data dissemination. Network performance analysis: The work evaluates OCTOPUS's effectiveness in improving scalability and resource utilization, directly addressing performance metrics. Other categories like Control path algorithms (0.5) and Network protocol design (0.7) are secondary but relevant to scheduling and protocol design aspects."
1561,Reducing Localisation Overhead: A Ranging Protocol and an Enhanced Algorithm for UWB-Based WSNs,"The ability for the nodes in a Wireless Sensor Network to determine their position is a desirable trait. Routing as well as other client applications can benefit from this information. In this paper, we introduce the results obtained from our UWB-based prototype. We implemented two adaptations of the Symmetric Double-Sided Two-Way Ranging (SDSTWR) protocol, namely Sequential Symmetric Double-Sided Two- Way Ranging (SSDS-TWR), and Parallel Double-Sided Two-Way Ranging (PDS-TWR), the latter being one of our contributions. PDS-TWR significantly reduces the overhead associated with ranging. We also introduce the enhanced version of our localisation algorithm, inter-Ring Localisation Algorithm (iRingLA), which is a good alternative for conventional trilateration. This new version improves the ability to compute the position when thin rings are used by focusing on the exact intersection: the number of test points remains small and the algorithm can be implemented on computationally constrained platforms. Using PDS-TWR and 2 anchors, we obtained a 2D localisation error of 79cm in an indoor environment.","General and reference:0.0,Hardware:0.2,Computer systems organization:0.0,Networks:1.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Networks,Networks is directly relevant as the paper introduces a UWB-based ranging protocol and localization algorithm for WSNs. Other fields are irrelevant as the focus is on networking protocols and algorithms.,"Network algorithms:0.75,Network architectures:0.0,Network components:0.0,Network performance evaluation:0.25,Network properties:0.0,Network protocols:0.75,Network services:0.0,Network types:0.0","Network protocols,Network algorithms","Network protocols is relevant as the paper introduces new UWB-based ranging protocols (SSDS-TWR, PDS-TWR). Network algorithms is relevant for the enhanced localisation algorithm (iRingLA). Network performance evaluation is less relevant as the focus is on protocol design rather than performance metrics.","Application layer protocols:0.1,Control path algorithms:0.1,Cross-layer protocols:0.1,Data path algorithms:0.1,Link-layer protocols:1.0,Network File System (NFS) protocol:0.1,Network economics:0.1,Network layer protocols:0.2,Network protocol design:1.0,OAM protocols:0.1,Presentation protocols:0.1,Protocol correctness:0.1,Session protocols:0.1,Transport protocols:0.1","Link-layer protocols,Network protocol design",Link-layer protocols: The paper introduces a UWB-based ranging protocol. Network protocol design: It proposes a new protocol design (PDS-TWR) for localization. Other layers like transport are not addressed.
5820,Making the Best out of Spectral Efficiency; Studies on The Introduction of Open-Spectrum Policy,"Open spectrum policy mostly deals with the shared-use of spectrum resources which again utilizes the orthogonality in different spectral domains - geography, space, power, frequency and time. Among these, temporal share of the spectrum, known as CR or cognitive radio technology, allows secondary licensees to opportunistically access the spectrum during the absence of the first. This paper provides the analysis on the performance of CR implementations in IEEE 802.22 WRAN system, in terms of the spectrum utilization rate, along with simulation results. Also discussed are the regulatory issues in introducing open spectrum policies into the market, centered on the reuse of spectrum resources.","General and reference:0.1,Hardware:0.2,Computer systems organization:0.2,Networks:1.0,Software and its engineering:0.3,Theory of computation:0.1,Mathematics of computing:0.2,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.2,Social and professional topics:0.1",Networks,Networks is the primary category as the paper focuses on cognitive radio technology and spectrum utilization in communication systems. Other categories like Software and its engineering are not the main focus.,"Network algorithms:0.0,Network architectures:0.0,Network components:0.0,Network performance evaluation:0.85,Network properties:0.0,Network protocols:0.75,Network services:0.0,Network types:0.0","Network performance evaluation,Network protocols",Network performance evaluation is relevant for CR implementation analysis. Network protocols is relevant for spectrum sharing mechanisms. Other categories like Network components are rejected as the focus is on policy and performance metrics rather than hardware.,"Application layer protocols:0.0,Cross-layer protocols:0.8,Link-layer protocols:0.0,Network File System (NFS) protocol:0.0,Network experimentation:0.0,Network layer protocols:0.0,Network measurement:0.0,Network performance analysis:0.0,Network performance modeling:0.0,Network protocol design:1.0,Network simulations:0.0,OAM protocols:0.0,Presentation protocols:0.0,Protocol correctness:0.0,Session protocols:0.0,Transport protocols:0.0","Network protocol design,Cross-layer protocols",Network protocol design: The paper analyzes CR implementations for spectrum sharing. Cross-layer protocols: Open-spectrum policy involves cross-layer coordination. Other fields like Transport protocols are not directly relevant.
3184,End-to-End Reliability-Aware Scheduling for Wireless Sensor Networks,"Wireless sensor networks (WSNs) are gaining popularity as a flexible and economical alternative to field-bus installations for monitoring and control applications. For mission-critical applications, communication networks must provide end-to-end reliability guarantees, posing substantial challenges for WSN. Reliability can be improved by redundancy, and is often addressed on the MAC layer by resubmission of lost packets, usually applying slotted scheduling. Recently, researchers have proposed a strategy to optimally improve the reliability of a given schedule by repeating the most rewarding slots in a schedule incrementally until a deadline. This Incrementer can be used with most scheduling algorithms but has scalability issues which narrows its usability to offline calculations of schedules, for networks that are rather static. In this paper, we introduce SchedEx, a generic heuristic scheduling algorithm extension, which guarantees a user-defined end-to-end reliability. SchedEx produces competitive schedules to the existing approach, and it does that consistently more than an order of magnitude faster. The harsher the end-to-end reliability demand of the network, the better the SchedEx performs compared to the Incrementer. We further show that SchedEx has a more evenly distributed improvement impact on the scheduling algorithms, whereas the Incrementer favors schedules created by certain scheduling algorithms.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.2,Networks:0.8,Software and its engineering:0.5,Theory of computation:0.1,Mathematics of computing:0.2,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.3,Applied computing:0.1,Social and professional topics:0.1",Networks,Networks is highly relevant for reliability-aware scheduling in wireless sensor networks. Software and its engineering is partially relevant for scheduling algorithms but less central to the core contribution.,"Network algorithms:0.9,Network architectures:0.3,Network components:0.3,Network performance evaluation:0.7,Network properties:0.5,Network protocols:0.4,Network services:0.2,Network types:0.2","Network algorithms,Network performance evaluation",Network algorithms is highly relevant as the paper introduces a scheduling algorithm for reliability. Network performance evaluation is relevant due to the focus on optimizing data transfer. Other network categories are less central to the core contribution.,"Control path algorithms:0.4,Data path algorithms:0.3,Network economics:0,Network experimentation:0.5,Network measurement:0.4,Network performance analysis:0.9,Network performance modeling:0.8,Network simulations:0.7","Network performance analysis,Network performance modeling",Network performance analysis is central to the scheduling algorithm's end-to-end reliability optimization. Network performance modeling is relevant as the paper proposes a method to model and improve reliability. Simulations and experimentation are secondary but still relevant for validation.
4178,Multimedia streaming via TCP: an analytic performance study,"TCP is widely used in commercial media streaming systems, with recent measurement studies indicating that a significant fraction of Internet streaming media is currently delivered over HTTP/TCP. These observations motivate us to develop analytic performance models to systematically investigate the performance of TCP for both live and stored media streaming. We validate our models via ns simulations and experiments conducted over the Internet. Our models provide guidelines indicating the circumstances under which TCP streaming leads to satisfactory performance, showing, for example, that TCP generally provides good streaming performance when the achievable TCP throughput is roughly twice the media bitrate, with only a few seconds of startup delay.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.9,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Networks,Networks is relevant because the paper analyzes TCP performance for multimedia streaming. Other categories are less directly related.,"Network algorithms:0.1,Network architectures:0.1,Network components:0.1,Network performance evaluation:0.8,Network properties:0.1,Network protocols:0.7,Network services:0.1,Network types:0.1","Network performance evaluation,Network protocols",Network performance evaluation is directly relevant as the paper develops analytic models for TCP streaming performance. Network protocols is relevant for the TCP protocol analysis. Other categories like network architectures are secondary.,"Application layer protocols:0.3,Cross-layer protocols:0.2,Link-layer protocols:0.1,Network File System (NFS) protocol:0.1,Network experimentation:0.4,Network layer protocols:0.1,Network measurement:0.4,Network performance analysis:1.0,Network performance modeling:1.0,Network protocol design:0.5,Network simulations:1.0,OAM protocols:0.1,Presentation protocols:0.1,Protocol correctness:0.1,Session protocols:0.1,Transport protocols:1.0","Transport protocols,Network performance analysis,Network simulations",Transport protocols: The paper focuses on TCP performance for streaming. Network performance analysis: The analytic models and validation of performance metrics are central. Network simulations: The ns simulations and Internet experiments are key contributions. Other categories like Application layer protocols are less relevant as the focus is on TCP (transport layer).
1668,Challenges in Inferring Internet Interdomain Congestion,"We introduce and demonstrate the utility of a method to localize and quantify inter-domain congestion in the Internet. Our Time Sequence Latency Probes (TSLP) method depends on two facts: Internet traffic patterns are typically diurnal, and queues increase packet delay through a router during periods of adjacent link congestion. Repeated round trip delay measurements from a single test point to the two edges of a congested link will show sustained increased latency to the far (but not to the near) side of the link, a delay pattern that differs from the typical diurnal pattern of an uncongested link. We describe our technique and its surprising potential,carefully analyze the biggest challenge with the methodology (interdomain router-level topology inference), describe other less severe challenges, and present initial results that are sufficiently promising to motivate further attention to overcoming the challenges.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.9,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.3,Applied computing:0.1,Social and professional topics:0.1",Networks,Networks is directly relevant for interdomain congestion analysis. 'Computing methodologies' is secondary as the focus is on network measurement rather than algorithm design.,"Network performance evaluation:0.9,Network algorithms:0.8,Network protocols:0.2,Network architectures:0.1,Network components:0.1,Network services:0.3,Network types:0.1","Network performance evaluation,Network algorithms",Network performance evaluation is critical for congestion detection methods. Network algorithms are relevant for the TSLP methodology. Other categories like Network protocols are less relevant.,"Network measurement:1.0,Network performance analysis:0.8,Network simulations:0.3,Network performance modeling:0.2,Network experimentation:0.1,Control path algorithms:0.0,Data path algorithms:0.0,Network economics:0.0","Network measurement,Network performance analysis",Network measurement is central to latency probing techniques. Network performance analysis is relevant for congestion quantification. Network simulations are only peripherally relevant for methodology validation.
4276,Enhancement of AODV Routing Protocol by Using Large Vehicles in VANETs on Highway,"In recent years, many routing protocols that take into account particular vehicular ad hoc network (VANET) characteristics such as higher vehicle speeds and mobility patterns have been proposed. Most of the existing VANET routing protocols largely neglect vehicles as obstacles in VANETs. Obstacle shadowing caused by large vehicles such as trucks or buses more negatively affects packet transmission performance than some other type of fading affects the performance in highway scenarios because the direction of packet forwarding is limited only to that along the highway, i.e., the forward and backward directions on the highway. Large vehicles have not only the negative effect of obstacle shadowing but also a positive effect due to their higher antenna position. Antennas of large vehicles are placed at higher positions than those of standard-size vehicles. A higher antenna position can achieve a longer transmission distance. In this paper, we propose a new Ad hoc On-Demand Distance Vector (AODV) routing protocol using large vehicles (AODV-LV) that exploits the positive effect of large vehicles. AODV-LV preferentially selects large vehicles as nodes for rebroadcasting route request (RREQ) packets. To investigate the performance of AODV-LV, we compare it with an existing improved AODV routing protocol. Our simulation results show that AODV-LV outperforms the existing protocol from the standpoints of packet delivery ratio and delay.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.9,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Networks,Networks is highly relevant as the paper focuses on VANET routing protocol enhancements. Other categories are irrelevant since the core contribution is a network-specific routing algorithm rather than systems or software.,"Network algorithms:0.8,Network architectures:0.0,Network components:0.0,Network performance evaluation:0.3,Network properties:0.0,Network protocols:0.9,Network services:0.0,Network types:0.5","Network protocols,Network algorithms","Network protocols is highly relevant as the paper modifies AODV for VANETs. Network algorithms is relevant due to the routing strategy. Network types is less relevant because the focus is on protocol enhancement, not network categorization.","Application layer protocols:0.6,Control path algorithms:0.5,Cross-layer protocols:0.3,Data path algorithms:0.4,Link-layer protocols:0.2,Network File System (NFS) protocol:0.1,Network economics:0.1,Network layer protocols:1.0,Network protocol design:0.9,OAM protocols:0.1,Presentation protocols:0.1,Protocol correctness:0.2,Session protocols:0.1,Transport protocols:0.3","Network layer protocols,Network protocol design","Network layer protocols is highly relevant as the paper modifies AODV, a routing protocol. Network protocol design is relevant for the proposed enhancements. Application layer protocols and Transport protocols are less directly related."
3170,Broadcasting in multichannel cognitive radio ad hoc networks,"Cognitive radio network technology is regarded as a new way to improve the spectral efficiency of wireless networks. It has been well studied for more than a decade and numerous precious works have been proposed. However, very few existing works consider how to broadcast messages in cognitive radio networks that operate in multichannel environments and none of these provides a full broadcast mechanism. Therefore, in this paper, we propose a broadcasting mechanism for multichannel cognitive radio ad hoc networks. Then, we analyze the mechanism regarding the speed of message dissemination, number of transmissions, portion of the users that receive the broadcast message and so forth.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.2,Networks:0.9,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Networks,"Networks is directly relevant because the paper focuses on broadcasting mechanisms in cognitive radio ad hoc networks, which is a core networking research problem. Other categories like Information systems or Applied computing are not the primary focus.","Network algorithms:0.1,Network architectures:0.1,Network components:0.1,Network performance evaluation:0.1,Network properties:0.1,Network protocols:0.9,Network services:0.1,Network types:0.1",Network protocols,"Network protocols is relevant because the paper proposes a broadcasting mechanism for multichannel cognitive radio ad hoc networks. Other fields are irrelevant as the paper focuses specifically on protocol design rather than algorithms, architectures, or other network aspects.","Application layer protocols:1.0,Cross-layer protocols:0.3,Link-layer protocols:0.2,Network File System (NFS) protocol:0.1,Network layer protocols:0.2,Network protocol design:0.4,OAM protocols:0.1,Presentation protocols:0.1,Protocol correctness:0.2,Session protocols:0.1,Transport protocols:0.2",Application layer protocols,Application layer protocols are highly relevant as the paper proposes a broadcasting mechanism in cognitive radio networks. Other protocol layers are not central to the broadcast mechanism discussed.
5827,Pilot Design for Sparse Channel Estimation in OFDM-Based Cognitive Radio Systems,"In this correspondence, sparse channel estimation is first introduced in orthogonal frequency-division multiplexing (OFDM)-based cognitive radio systems. Based on the results of spectrum sensing, the pilot design is studied by minimizing the coherence of the dictionary matrix used for sparse recovery. Then, it is formulated as an optimal column selection problem where a table is generated and the indexes of the selected columns of the table form a pilot pattern. A novel scheme using constrained cross-entropy optimization is proposed to obtain an optimized pilot pattern, where it is modeled as an independent Bernoulli random process. The updating rule for the probability of each active subcarrier selected as a pilot subcarrier is derived. A projection method is proposed so that the number of pilots during the optimization is fixed. Simulation results verify the effectiveness of the proposed scheme and show that it can achieve 11.5% improvement in spectrum efficiency with the same channel estimation performance compared with the least squares (LS) channel estimation.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.9,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.2,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Networks,Networks is relevant because the paper focuses on pilot design for OFDM-based cognitive radio systems. Other categories like Mathematics of computing are less directly relevant as the focus is on communication systems rather than pure mathematical theory.,"Network algorithms:1.0,Network architectures:0.3,Network components:0.2,Network performance evaluation:0.8,Network properties:0.3,Network protocols:0.9,Network services:0.1,Network types:0.2","Network algorithms,Network protocols,Network performance evaluation",Network algorithms is directly relevant for pilot pattern optimization. Network protocols is relevant for TLM communication design. Network performance evaluation is relevant for demonstrating efficiency improvements. Other categories like Network types are less relevant as the focus is on algorithm design rather than network categorization.,"Application layer protocols:0.3,Control path algorithms:0,Cross-layer protocols:0.5,Data path algorithms:0.5,Link-layer protocols:0.3,Network File System (NFS) protocol:0,Network economics:0,Network experimentation:0.5,Network layer protocols:0.3,Network measurement:0.5,Network performance analysis:1,Network performance modeling:0.5,Network protocol design:1,Network simulations:1,OAM protocols:0.3,Presentation protocols:0,Protocol correctness:0.3,Session protocols:0.3,Transport protocols:0.3","Network performance analysis,Network protocol design,Network simulations",Network performance analysis: The study analyzes the performance of the pilot design. Network protocol design: The pilot pattern is part of the protocol design in OFDM systems. Network simulations: Uses simulations to validate the proposed scheme. Categories like Application layer protocols are not the focus.
5626,Information Model and XML Data Model for Traceroute Measurements,"This document describes a standard way to store the configuration and
the results of traceroute measurements. This document first of all
describes the tool itself; afterwards, the common information model is
defined dividing the information elements in two semantically
separated groups (configuration elements and results ones). Moreover
an additional element is defined to relate configuration elements and
results ones by means of a common unique identifier. On the basis of
the information model a data model based on XML is defined to store
the results of traceroute measurements.","General and reference:0.2,Hardware:0.1,Computer systems organization:0.3,Networks:1.0,Software and its engineering:0.4,Theory of computation:0.1,Mathematics of computing:0.2,Information systems:0.3,Security and privacy:0.1,Human-centered computing:0.2,Computing methodologies:0.3,Applied computing:0.2,Social and professional topics:0.1",Networks,"Networks is highly relevant as the paper focuses on standardizing traceroute measurement data storage, a core networking task. Other categories like Software and Hardware are less directly related to the primary domain of network measurement modeling.","Network algorithms:0.8,Network architectures:0.1,Network components:0.1,Network performance evaluation:0.6,Network properties:0.2,Network protocols:0.7,Network services:0.3,Network types:0.1","Network algorithms,Network protocols",Network algorithms is relevant for the interference detection methodology. Network protocols is relevant for defining the XML data model for traceroute measurements. Other categories are rejected as the paper focuses on network data standardization rather than performance evaluation or architectures.,"Application layer protocols:0.0,Control path algorithms:0.0,Cross-layer protocols:0.0,Data path algorithms:0.0,Link-layer protocols:0.0,Network File System (NFS) protocol:0.0,Network economics:0.0,Network layer protocols:0.0,Network protocol design:1.0,OAM protocols:0.8,Presentation protocols:0.0,Protocol correctness:0.0,Session protocols:0.0,Transport protocols:0.0","Network protocol design,OAM protocols",Network protocol design is directly relevant as the paper defines a data model for network measurements. OAM protocols are relevant for operations and management aspects. Other categories like Application or Transport protocols are not addressed here.
2849,Coded full-duplex MIMO with iterative detection and decoding,"In this paper, we investigate the performance of a coded full-duplex multiple-input multiple-output (FD-MIMO) bi-directional transceiver. To mitigate self-interference (SI), iterative detection and decoding (IDD) are proposed that utilize soft parallel interference cancellation (SPIC) with adaptive Minimum Mean-Squared-Error (MMSE) filtering. Furthermore, Space-Time Bit-Interleaved Coded Modulation (ST-BICM) based channel coding schemes are considered, i.e. convolutional or turbo code based, and the system performance is evaluated in the presence of additive white Gaussian noise (AWGN) over MIMO Rayleigh fading channels. The proposed near-optimal IDD scheme performs cancellation of SI by iteratively exchanging the soft information of the desired and SI signals between the detector, which comprises adaptive MMSE filtering with log-likelihood ratio (LLR) demapping, and the soft-in soft-out (SISO) decoder implementing linear to logarithmic approximation of the maximum a posteriori algorithm. Performance results are presented to demonstrate the Bit-Error-Rate (BER) performance of the proposed coded FD-MIMO as a function of the signal to noise ratio (SNR). Furthermore, a comparison of the proposed IDD system with relevant state-of-the-art approaches is given which shows a significant improvement in performance as a result of the introduced iterative processing that reduces SI considerably.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:1.0,Software and its engineering:0.0,Theory of computation:0.5,Mathematics of computing:0.25,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Networks,Networks: Full-duplex MIMO is a core communication technology. Theory of computation has limited relevance to iterative algorithms. Other categories: No direct relevance to security or systems organization.,"Network algorithms:1.0,Network architectures:0.3,Network components:0.75,Network performance evaluation:0.2,Network properties:0.2,Network protocols:0.2,Network services:0.2,Network types:0.2","Network algorithms,Network components",Network algorithms is central to the paper's contribution on iterative detection in full-duplex MIMO. Network components is relevant due to the transceiver design. Other options are less aligned with the algorithmic and hardware focus.,"Control path algorithms:0,Data path algorithms:1,End nodes:0,Intermediate nodes:0,Logical nodes:0,Middle boxes / network appliances:0,Network economics:0,Physical links:1,Wireless access points, base stations and infrastructure:0","Data path algorithms,Physical links",Data path algorithms is relevant for the iterative detection and decoding techniques discussed. Physical links is relevant due to the focus on MIMO wireless communication. Other categories like wireless infrastructure or network economics are not central to the paper's technical contributions.
1439,An application-level multicast framework for large scale VOD services,"This paper proposes an application-level multicast framework based on peer-to-peer communications to improve performance of large scale VOD services. Since the deployment of multicast-enabled network is still not popular nowadays, developing an application-level multicast infrastructure to support multicast delivery would be a good alternative. We address the problems associated with heterogeneous clients and formulate the model of multicast streaming in a VOD service. The proposed multicast framework takes these results into account to reduce server load and network bandwidth. The simulation results demonstrate that our framework can satisfy at lest 40,000 viewers concurrently by utilizing only 70 MB of buffer on each peer node.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:1.0,Software and its engineering:0.3,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Networks,"Networks: The paper focuses on application-level multicast infrastructure for VOD services, a core networking problem. Other fields like Hardware or Security are not central to the peer-to-peer multicast framework.","Network algorithms:0.6,Network architectures:0.8,Network components:0.1,Network performance evaluation:0.6,Network properties:0.1,Network protocols:0.7,Network services:0.1,Network types:0.1","Network architectures,Network protocols,Network performance evaluation",Network architectures: Designs a peer-to-peer multicast framework. Network protocols: Implements application-level multicast. Network performance evaluation: Validates simulation results. Other categories are not central to the network design and evaluation focus.,"Application layer protocols:1,Cross-layer protocols:0,Link-layer protocols:0,Network File System (NFS) protocol:0,Network design principles:0.5,Network experimentation:0,Network layer protocols:0,Network measurement:0,Network performance analysis:1,Network performance modeling:1,Network protocol design:0.5,Network simulations:1,OAM protocols:0,Presentation protocols:0,Programming interfaces:0,Protocol correctness:0,Session protocols:0,Transport protocols:0","Application layer protocols,Network simulations","Application layer protocols: The framework is explicitly application-level, leveraging peer-to-peer communication for multicast delivery. Network simulations: The paper validates performance via simulations. Network performance analysis/modeling were rejected as the framework's design, not performance modeling, is the core contribution."
5685,ProgME: Towards Programmable Network MEasurement,"Traffic measurements provide critical input for a wide range of network management applications, including traffic engineering, accounting, and security analysis. Existing measurement tools collect traffic statistics based on some predetermined, inflexible concept of “flows.” They do not have sufficient built-in intelligence to understand the application requirements or adapt to the traffic conditions. Consequently, they have limited scalability with respect to the number of flows and the heterogeneity of monitoring applications. We present ProgME, a Programmable MEasurement architecture based on a novel concept of flowset-an arbitrary set of flows defined according to application requirements and/or traffic conditions. Through a simple flowset composition language, ProgME can incorporate application requirements, adapt itself to circumvent the scalability challenges posed by the large number of flows, and achieve a better application-perceived accuracy. The modular design of ProgME enables it to exploit the surging popularity of multicore processors to cope with 7-Gb/s line rate. ProgME can analyze and adapt to traffic statistics in real time. Using sequential hypothesis test, ProgME can achieve fast and scalable heavy hitter identification.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.9,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Networks,"Networks is relevant for the programmable measurement architecture. Other categories are irrelevant as the focus is on network traffic analysis, not hardware or applications.","Network algorithms:0.8,Network architectures:0.9,Network components:0.1,Network performance evaluation:0.7,Network properties:0.1,Network protocols:0.1,Network services:0.1,Network types:0.1","Network architectures,Network algorithms",Network architectures: The paper introduces a programmable network measurement architecture. Network algorithms: The paper discusses flowset composition and traffic analysis algorithms. Other categories are not directly relevant.,"Control path algorithms:0.1,Data path algorithms:0.05,Network design principles:0.8,Network economics:0.2,Programming interfaces:0.7","Network design principles,Programming interfaces",Network design principles: The paper introduces a programmable measurement architecture for networks. Programming interfaces: Flowset composition language as an interface for application requirements. Other children like Control path algorithms are less central to the architecture.
2563,Connection admission control for capacity-varying networks with stochastic capacity change times,"Many connection-oriented networks, such as low Earth orbit satellite (LEOS) systems and networks providing multipriority service using advance reservations, have capacities which vary over time. Connection admission control (CAC) policies which only use current capacity information may lead to intolerable dropping of admitted connections whenever network capacity decreases. We present the admission limit curve (ALC) for capacity-varying networks with random capacity change times. We prove the ALC is a constraint limiting the conditions under which any connection-stateless CAC policy may admit connections and still meet dropping guarantees on an individual connection basis. The ALC also leads to a lower bound on the blocking performance achievable by any connection-stateless CAC policy which provides dropping guarantees to individual connections. In addition, we describe a CAC policy for stochastic capacity change times which uses knowledge about future capacity changes to provide dropping guarantees on an individual connection basis and which achieves blocking performance close to the lower bound.","General and reference:0.1,Hardware:0.2,Computer systems organization:0.3,Networks:1.0,Software and its engineering:0.4,Theory of computation:0.1,Mathematics of computing:0.3,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Networks,"Networks is highly relevant as the paper focuses on connection admission control in capacity-varying networks. Other categories like Computer systems organization or Software and its engineering are less central since the work is about network resource allocation policies, not system architecture or software design.","Network algorithms:0.9,Network architectures:0.1,Network components:0.1,Network performance evaluation:0.8,Network properties:0.1,Network protocols:0.1,Network services:0.1,Network types:0.7","Network algorithms,Network performance evaluation,Network types","Network algorithms (0.9) is central to CAC policy design. Network performance evaluation (0.8) assesses blocking and dropping guarantees. Network types (0.7) refers to capacity-varying networks. Network components (0.1) is irrelevant as the focus is on policy, not hardware.","Ad hoc networks:0.0,Control path algorithms:0.0,Cyber-physical networks:0.0,Data center networks:0.0,Data path algorithms:0.0,Home networks:0.0,Mobile networks:0.0,Network economics:0.0,Network experimentation:0.0,Network measurement:0.0,Network on chip:0.0,Network performance analysis:1.0,Network performance modeling:1.0,Network simulations:0.0,Overlay and other logical network structures:0.0,Packet-switching networks:0.0,Public Internet:0.0,Storage area networks:0.0,Wired access networks:0.0,Wireless access networks:0.0","Network performance analysis,Network performance modeling",Network performance analysis and modeling are directly addressed in the CAC policy for capacity-varying networks. Other categories like Mobile networks are not discussed.
399,Models for pre-emption of packet data by voice in slotted cellular radio networks,"Cellular radio networks supporting both circuit switched voice and packet switched data services aim to maximise the resource utilisation through efficient sharing of the resource. These sharing algorithms can use pre-emption to give voice service priority over data, thus preserving the grade of service for voice at the expense of delaying data calls. This paper develops models for a pre-emptive voice and data system, with a particular focus on GPRS. The model is based on an assumption of independence between the voice and data traffic. These models allow estimates of (i) the probability that a voice call causes pre-emption of a data call, as well as (ii) the probability that a data call will be delayed due to either being pre-empted by a voice call while in progress or due to not finding free resources at its initial attempt. The results show that a Poisson model for data traffic gives the best results for pre-emption probability when compared with measurements taken from a GSM/GPRS network.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.8,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Networks,Networks: The paper focuses on resource allocation and pre-emption models in cellular networks. Other categories are irrelevant as the work is specifically about communication network design and analysis.,"Network algorithms:1.0,Network performance evaluation:0.9,Network architectures:0.6,Network protocols:0.7,Network properties:0.5,Network components:0.4,Network services:0.3,Network types:0.3","Network algorithms,Network performance evaluation",Network algorithms and Network performance evaluation are highly relevant as the paper develops models for pre-emption in cellular networks. Other categories like network architectures are less directly related.,"Control path algorithms:0.1,Data path algorithms:0.1,Network economics:0.1,Network experimentation:0.1,Network measurement:0.2,Network performance analysis:0.9,Network performance modeling:1.0,Network simulations:0.3","Network performance modeling,Network performance analysis",The paper focuses on pre-emption modeling and performance analysis. Simulations are not the core focus. Other categories like economics are irrelevant.
1319,Binary searches on multiple small trees for IP address lookup,"Rapid growth of Internet traffic requires more Internet bandwidth and high-speed packet processing in Internet routers. IP address lookup in routers is an essential operation that should be performed in real-time for routers where hundreds of million packets arrive per second. In this paper, we propose a new software-based architecture for efficient IP address lookup. In the proposed scheme, a large routing table is divided into multiple balanced trees, and sequential binary searches are performed on those trees, and hence the number of memory accesses depends on the number of routing entries not on the length of routing prefixes. Performance evaluation results show that the proposed architecture requires a single 301.7 Kbyte SRAM to store about 41000 routing entries, and an address lookup is achieved by 11 memory accesses in average.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.9,Software and its engineering:0.3,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Networks,Networks is highly relevant as the paper focuses on IP address lookup in routers. Software and its engineering is secondary for the software architecture. Other categories are rejected as the focus is not on algorithms or applied systems.,"Network algorithms:0.9,Network architectures:0.2,Network components:0.2,Network performance evaluation:0.8,Network properties:0.3,Network protocols:0.1,Network services:0.1,Network types:0.1","Network algorithms,Network performance evaluation",Network algorithms is directly relevant for the binary search architecture. Network performance evaluation applies to the memory access metrics. Other categories like network types are not central to the paper's contribution.,"Control path algorithms:0,Data path algorithms:1,Network economics:0,Network experimentation:0,Network measurement:0,Network performance analysis:0,Network performance modeling:0,Network simulations:0",Data path algorithms,Data path algorithms are relevant due to the focus on routing table tree structures for IP address lookup. Other categories like Network economics are not discussed in the paper.
3951,Realizing IP-based Services over an Information-Centric Networking Transport Network,"Information-centric networking (ICN) has been actively studied as a promising alternative to the IP-based Internet architecture with potential benefits in terms of network efficiency, privacy, security, and novel applications. However, it is difficult to adopt such wholesale replacement of the IP-based Internet to a new routing and service infrastructure due to the conflict among existing stakeholders, market players, and solution providers. To overcome these difficulties, we provide an evolutionary approach by which we enable the expected benefits of ICN for existing services. The demonstration shows that these benefits can be efficiently introduced and work with existing IP end-systems.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.9,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Networks,Networks is highly relevant as the paper discusses information-centric networking architecture and its integration with IP-based services. Other categories are irrelevant as the paper does not focus on hardware or software engineering.,"Network algorithms:0.2,Network architectures:1.0,Network components:0.1,Network performance evaluation:0.3,Network properties:0.2,Network protocols:0.8,Network services:0.4,Network types:0.1","Network architectures,Network protocols",Network architectures is directly relevant as the paper discusses ICN as an alternative to IP-based Internet architecture. Network protocols are relevant for enabling compatibility with existing IP systems. 'Network services' and 'Network performance evaluation' are secondary but not central to the core contribution.,"Application layer protocols:0.1,Cross-layer protocols:0.2,Link-layer protocols:0.1,Network File System (NFS) protocol:0.1,Network design principles:0.8,Network layer protocols:0.3,Network protocol design:0.2,OAM protocols:0.1,Presentation protocols:0.1,Programming interfaces:0.1,Protocol correctness:0.1,Session protocols:0.1,Transport protocols:0.1",Network design principles,Network design principles is highly relevant because the paper proposes an evolutionary approach to integrating ICN with IP-based services. Other children like Network layer protocols are less relevant as the focus is on architectural design rather than specific protocols.
180,Rate-1/2 component codes for nonbinary turbo codes,"The iterative decoding structure and component maximum a posteriori decoders used for decoding binary concatenated codes can be extended to the nonbinary domain. This paper considers turbo codes over nonbinary rings, specifically ternary, quaternary, penternary, hexernary, and octernary codes. The best rate-1/2 component codes are determined using a practical search algorithm. The performance of the resulting rate-1/3 turbo codes on an additive white Gaussian noise channel using q-ary phase-shift keying modulation is given.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.8,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Networks,Networks is relevant for the discussion of nonbinary turbo codes and communication systems. Other categories like Hardware or Software are not directly addressed.,"Network algorithms:1.0,Network protocols:0.75,Network components:0.5,Network performance evaluation:0.0,Network properties:0.0,Network services:0.0,Network types:0.0","Network algorithms,Network protocols",Network algorithms and protocols are relevant to the design and analysis of nonbinary turbo codes. Components are tangentially related but less central than algorithmic and protocol-level contributions.,"Application layer protocols:0.1,Control path algorithms:0.2,Cross-layer protocols:0.1,Data path algorithms:0.1,Link-layer protocols:0.1,Network File System (NFS) protocol:0,Network economics:0,Network layer protocols:0.1,Network protocol design:0.3,OAM protocols:0.1,Presentation protocols:0.1,Protocol correctness:0.2,Session protocols:0.1,Transport protocols:0.1","Network protocol design,Protocol correctness",Network protocol design is relevant due to the focus on nonbinary turbo codes. Protocol correctness is secondary but relevant for decoding algorithms. Other fields like Transport protocols are not addressed.
2823,Performance improvement of congestion avoidance mechanism for TCP Vegas,"In this paper, we propose a router-based congestion avoidance mechanism (RoVegas) for TCP Vegas. TCP Vegas detects network congestion in the early stage and successfully prevents periodic packet loss that usually occurs in TCP Reno. It has been demonstrated that TCP Vegas outperforms TCP Reno in many aspects. However, TCP Vegas suffers several problems that inhere in its congestion avoidance mechanism, these include issues of rerouting, persistent congestion, fairness, and network asymmetry. By performing the proposed scheme in routers along the round-trip path, RoVegas can solve the problems of rerouting and persistent congestion, enhance the fairness among the competitive connections, and improve the throughput when congestion occurs on the backward path. Through the results of both analysis and simulation, we demonstrate the effectiveness of RoVegas.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.2,Networks:0.8,Software and its engineering:0.3,Theory of computation:0.1,Mathematics of computing:0.2,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.3,Applied computing:0.2,Social and professional topics:0.1",Networks,Networks is directly relevant for the TCP Vegas congestion control improvement. Other categories do not address the core networking protocol focus.,"Network algorithms:0.9,Network architectures:0.7,Network components:0.2,Network performance evaluation:0.4,Network properties:0.3,Network protocols:0.6,Network services:0.1,Network types:0.2","Network algorithms,Network protocols",Network algorithms is relevant for congestion control improvements; Network protocols is relevant for TCP Vegas mechanism. Other categories like Network performance evaluation are secondary to algorithm design.,"Application layer protocols:0.1,Control path algorithms:0.7,Cross-layer protocols:0.3,Data path algorithms:0.5,Link-layer protocols:0.1,Network File System (NFS) protocol:0.1,Network economics:0.1,Network layer protocols:0.3,Network protocol design:0.2,OAM protocols:0.1,Presentation protocols:0.1,Protocol correctness:0.2,Session protocols:0.1,Transport protocols:1.0","Transport protocols,Control path algorithms",Transport protocols (1.0) is directly relevant as TCP Vegas operates at the transport layer. Control path algorithms (0.7) applies to the router-based congestion management. Other categories like Application layer protocols (0.1) and Data path algorithms (0.5) are less relevant as the focus is on transport-level control rather than application data or low-level routing.
1916,Distributed Congestion Control for Packet Switched Networks on Chip,c © 2006 by John von Neumann Institute for Computing Permission to make digital or hard copies of portions of this work for personal or classroom use is granted provided that the copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise requires prior specific permission by the publisher mentioned above.,"General and reference:0.1,Hardware:0.2,Computer systems organization:0.2,Networks:1.0,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Networks,"Networks is highly relevant as the paper discusses congestion control in packet-switched networks on chip, a core networking topic. Other categories like Hardware or Computer systems organization are tangential and do not capture the primary focus on network protocols.","Network algorithms:1.0,Network architectures:0.0,Network components:0.0,Network performance evaluation:0.9,Network properties:0.0,Network protocols:0.0,Network services:0.0,Network types:0.0","Network algorithms,Network performance evaluation",Network algorithms: The paper addresses congestion control algorithms for NoC. Network performance evaluation: The focus is on optimizing performance through distributed control. Other categories were rejected as the paper does not discuss network structure or protocols.,"Control path algorithms:0.8,Data path algorithms:0.7,Network performance analysis:0.4,Network performance modeling:0.5,Network simulations:0.3,Network experimentation:0.2,Network measurement:0.2,Network economics:0.1","Control path algorithms,Data path algorithms",Control path algorithms and Data path algorithms are relevant as congestion control involves managing data flow and path decisions. Other categories like Network performance analysis are less relevant as the abstract is incomplete and lacks details on performance modeling.
5372,Small-World Phenomena and the Dynamics of Information,"The problem of searching for information in networks like the World Wide Web can be approached in a variety of ways, ranging from centralized indexing schemes to decentralized mechanisms that navigate the underlying network without knowledge of its global structure. The decentralized approach appears in a variety of settings: in the behavior of users browsing the Web by following hyperlinks; in the design of focused crawlers [4, 5, 8] and other agents that explore the Web’s links to gather information; and in the search protocols underlying decentralized peer-to-peer systems such as Gnutella [10], Freenet [7], and recent research prototypes [21, 22, 23], through which users can share resources without a central server. In recent work, we have been investigating the problem of decentralized search in large information networks [14, 15]. Our initial motivation was an experiment that dealt directly with the search problem in a decidedly pre-Internet context: Stanley Milgram’s famous study of the small-world phenomenon [16, 17]. Milgram was seeking to determine whether most pairs of people in society were linked by short chains of acquaintances, and for this purpose he recruited individuals to try forwarding a letter to a designated “target” through people they knew on a first-name basis. The starting individuals were given basic information about the target — his name, address, occupation, and a few other personal details — and had to choose a single acquaintance to send the letter to, with goal of reaching the target as quickly as possible; subsequent recipients followed the same procedure, and the chain closed in on its destination. Of the chains that completed, the median number of steps required was six — a result that has since entered popular culture as the “six degrees of separation” principle [11]. Milgram’s experiment contains two striking discoveries — that short chains are pervasive, and that people are able to find them. This latter point is concerned precisely with a type of decentralized navigation in a social network, consisting of people as nodes and links joining","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.8,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Networks,Networks is highly relevant as the paper studies decentralized search in information networks. Other categories like Social and professional topics are less relevant as the focus is on technical network analysis rather than social implications.,"Network algorithms:0.8,Network architectures:0.5,Network components:0.2,Network performance evaluation:0.3,Network properties:0.7,Network protocols:0.2,Network services:0.1,Network types:0.3","Network algorithms,Network properties",Network algorithms is relevant for decentralized search mechanisms. Network properties is relevant for small-world phenomena analysis.,"Network dynamics:1.0,Network structure:1.0,Network manageability:0.3,Network mobility:0.2,Network privacy and anonymity:0.2,Network range:0.2,Network reliability:0.3,Network security:0.3,Control path algorithms:0.1,Data path algorithms:0.1","Network dynamics,Network structure",Network dynamics is directly relevant as the paper studies decentralized search in information networks. Network structure is relevant due to the focus on small-world phenomena. Other categories like Network security are rejected as the work does not address security mechanisms.
5333,Reconfigurable Modular Computer Networks for Spacecraft On-Board Processing,Standardized fault-tolerant microcomputers in a reconfigurable distributed network promise to meet spacecraft reliability requirements at low cost.,"General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.9,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Networks,"Networks is highly relevant for the reconfigurable distributed network design in spacecraft. Other categories are irrelevant as the paper focuses on network architecture for fault tolerance, not systems, algorithms, or applications.","Network algorithms:0.1,Network architectures:0.8,Network components:0.1,Network performance evaluation:0.1,Network properties:0.1,Network protocols:0.1,Network services:0.1,Network types:0.1",Network architectures,Network architectures is highly relevant for reconfigurable modular networks. Other categories like Network algorithms are secondary.,"Network design principles:0.9,Programming interfaces:0.0",Network design principles,Network design principles is relevant for reconfigurable distributed networks in spacecraft. Programming interfaces are not discussed.
3880,TDMA service for sensor networks,"Sensors networks are often constrained by limited power and limited communication range. If a sensor receives two messages simultaneously then they collide and both messages become incomprehensible. In this paper, we present a simple time division multiple access (TDMA) algorithms for assigning time slots to sensors and show that it provides a significant reduction in the number of collisions incurred during communication. We present TDMA algorithms customized for different communication patterns, namely, broadcast, convergecast and local gossip, that occur commonly in sensor networks. Our algorithms are self-stabilizing, i.e., TDMA is restored even if the system reaches an arbitrary state where the sensors are corrupted or improperly initialized.","General and reference:0.1,Hardware:0.2,Computer systems organization:0.2,Networks:0.9,Software and its engineering:0.4,Theory of computation:0.1,Mathematics of computing:0.3,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.3,Applied computing:0.2,Social and professional topics:0.1",Networks,"Networks is directly relevant because the paper focuses on TDMA algorithms for sensor networks, addressing communication protocols and collision avoidance. Other fields like Software and its engineering are less relevant as the core contribution is protocol design, not software implementation.","Network algorithms:0.8,Network architectures:0.2,Network components:0.1,Network performance evaluation:0.3,Network properties:0.2,Network protocols:1.0,Network services:0.4,Network types:0.5","Network protocols,Network algorithms","Network protocols is highly relevant as the paper introduces TDMA algorithms for sensor networks. Network algorithms is relevant for the algorithm design. Other categories like Network types are less central as the focus is on protocol design, not network structure.","Application layer protocols:0.0,Control path algorithms:0.0,Cross-layer protocols:0.0,Data path algorithms:0.0,Link-layer protocols:1.0,Network File System (NFS) protocol:0.0,Network economics:0.0,Network layer protocols:0.0,Network protocol design:0.0,OAM protocols:0.0,Presentation protocols:0.0,Protocol correctness:0.0,Session protocols:0.0,Transport protocols:0.0",Link-layer protocols,Link-layer protocols is relevant because TDMA operates at the link layer for collision avoidance. Other categories like Network layer protocols are irrelevant as the paper does not discuss routing or higher-layer communication.
2482,Stochastic modeling of Scouting Switching for adaptively-routed mesh networks,"Talking out the network issues, the switching techniques specify the connection activities performed by the switching elements when a message is received at the input port. Traditional switching mechanisms such as Wormhole Switching (WS) realize high performance, but prone to deadlock in the vicinity of faults. While some techniques such as adaptive routing can alleviate the problem, it cannot solve the problem by itself. This has motivated the development of different switching techniques. The Scouting Switching (SS) has been suggested as an efficient switching mechanism for reconciling the confliction demands on communication performance and fault-tolerance in interconnection networks. Although SS has been around for years and it can greatly benefit from adaptive routing as it reduces blocking in the network, there has been hardly any attempt to provide an analytical model for SS when fully adaptive routing along with virtual channels is used. Besides, mesh is one of the most desirable topologies regarding to characteristics which can offer very good scalability. In an effort to fill this gap, this paper proposes the first analytical model for 2-D mesh networks using SS augmented with virtual channels. Experimental results show that this model is able to predict message latency with a good degree of accuracy.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.3,Networks:0.9,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.3,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Networks,"Networks is highly relevant as the paper focuses on analytical modeling of Scouting Switching in mesh networks. Computer systems organization is marginally relevant for system-level design, but the primary domain is network performance analysis.","Network performance evaluation:0.9,Network algorithms:0.85,Network protocols:0.3,Network components:0.2,Network types:0.1,Network architectures:0.1,Network services:0.1,Network properties:0.1","Network performance evaluation,Network algorithms",Network performance evaluation is highly relevant as the paper presents an analytical model for predicting message latency. Network algorithms is relevant for the stochastic modeling approach. Other categories like network protocols are less relevant since the focus is on analytical modeling rather than protocol design.,"Control path algorithms:0.2,Data path algorithms:0.1,Network economics:0.1,Network experimentation:0.3,Network measurement:0.2,Network performance analysis:1.0,Network performance modeling:1.0,Network simulations:0.7","Network performance analysis,Network performance modeling",Network performance analysis and modeling (analytical model for SS) are central. Simulations are mentioned but secondary to the analytical focus.
1838,Rate Adaptation with CDM versus Adaptive Coding for Next Generation OFDM Systems,"This paper proposes data rate adaptation based on code division multiplexing (CDM). Under investigation are OFDM systems with cyclic delay diversity (CDD) and with spatial phase coding (SPC). These schemes are promising candidates for future broadband wireless systems. As lower bound, maximum ratio transmission (MRT) is chosen as reference. Imperfections due to channel estimation are taken into account in the investigations. The comparison of adaptive CDM versus adaptive coding shows that adaptive CDM outperforms adaptive coding, especially when spatial transmit diversity schemes are applied. This result is of interest for the design of future broadband adaptive transmission systems like IMT-Advanced.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.75,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Networks,Networks is relevant as the paper focuses on OFDM communication systems and rate adaptation techniques. Other categories like Computing methodologies are less central as the work is about communication protocols rather than general algorithm design.,"Network algorithms:0.9,Network architectures:0.0,Network components:0.0,Network performance evaluation:0.7,Network properties:0.0,Network protocols:0.0,Network services:0.0,Network types:0.0","Network algorithms,Network performance evaluation",Network algorithms is highly relevant because the paper proposes a data rate adaptation algorithm for OFDM systems. Network performance evaluation is relevant due to the comparison of adaptive CDM versus adaptive coding. Other categories like 'Network protocols' are less directly relevant to the core contribution of algorithm design.,"Control path algorithms:0.1,Data path algorithms:0.1,Network economics:0.1,Network experimentation:0.2,Network measurement:0.1,Network performance analysis:0.9,Network performance modeling:0.8,Network simulations:0.3","Network performance analysis,Network performance modeling",Network performance analysis is central as the paper compares CDM and adaptive coding for OFDM systems. Network performance modeling is relevant as it establishes theoretical bounds (MRT) and evaluates performance under channel estimation imperfections. Other categories like network experimentation or simulations are less directly addressed since the focus is on theoretical analysis rather than empirical testing.
4505,SCFDE with space-time coding for IM/DD optical wireless communication,"Recently, we proposed a novel transmission scheme termed decomposed-quadrature optical single carrier frequency domain equalization (DQO-SCFDE) suitable for intensity-modulated direct-detection (IM/DD) diffuse optical wireless (DOW) communication. DQO-SCFDE has been shown to perform better than the conventional asymmetrically clipped optical OFDM (ACO-OFDM) when the non-linearity characteristics of the light emitting diodes (LED) are considered. In this paper we present a detail comparison of DQO-SCFDE and ACO-OFDM. We then propose a space-time coding scheme for DQO-SCFDE using two LED's at the transmitter to improve the bit-error rate performance.","General and reference:0.1,Hardware:0.2,Computer systems organization:0.2,Networks:0.8,Software and its engineering:0.3,Theory of computation:0.1,Mathematics of computing:0.2,Information systems:0.2,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.3,Social and professional topics:0.1",Networks,Networks is highly relevant for the optical wireless communication system design. Other categories like Hardware were downweighted because the focus is on communication protocols rather than physical components.,"Network algorithms:0.25,Network architectures:0.0,Network components:1.0,Network performance evaluation:0.5,Network properties:0.0,Network protocols:0.75,Network services:0.0,Network types:0.5","Network components,Network protocols","Network components is highly relevant as the paper focuses on space-time coding implementation in SCFDE transmitters. Network protocols receives a moderate score due to the protocol-level design considerations. Network types is relevant due to the specific optical wireless communication context, but Network algorithms is only peripherally relevant as the focus is on component-level innovation rather than algorithmic improvements.","Application layer protocols:0.1,Cross-layer protocols:0.2,End nodes:0.1,Intermediate nodes:0.1,Link-layer protocols:0.1,Logical nodes:0.1,Middle boxes / network appliances:0.1,Network File System (NFS) protocol:0.1,Network layer protocols:0.1,Network protocol design:0.1,OAM protocols:0.1,Physical links:1,Presentation protocols:0.1,Protocol correctness:0.1,Session protocols:0.1,Transport protocols:0.1,Wireless access points, base stations and infrastructure:0.1",Physical links,"Physical links: The paper focuses on space-time coding for optical transmission, a physical layer technique. Other protocol categories are not relevant to this transmission scheme."
1241,Design and Analysis of Mobility-Aware Clustering Algorithms for Wireless Mesh Networks,"One of the major concerns in wireless mesh networks (WMNs) is the radio resource utilization efficiency, which can be enhanced by efficiently managing the mobility of users. To achieve this, we propose in this paper the use of mobility-aware clustering. The main idea behind WMN clustering is to restrict a major part of the exchanged signaling messages due to the mobility of users to a local area (i.e., inside a cluster). As such, less wireless links are used by the signaling messages, which reduces the resources occupied by a mobile user during its service and thus improves the total network capacity. Through analytical models and simulations, this work first identifies the cases where clustering is helpful. Building on these results, we propose two clustering schemes that take into consideration the mobility properties of the users in order to improve the WMN performance. We prove that both schemes can achieve significant gains in terms of radio resource utilization. Specifically, we show that the first scheme fits better both large and low-connected wireless mesh networks, whereas the second scheme is more suitable for both small to moderate and highly connected networks.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.2,Networks:0.9,Software and its engineering:0.2,Theory of computation:0.3,Mathematics of computing:0.3,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.2,Social and professional topics:0.1",Networks,"Networks: The paper proposes mobility-aware clustering algorithms for wireless mesh networks, directly addressing network resource utilization and performance. Other categories are less relevant as the focus is on network protocols rather than theoretical models or software engineering practices.","Network algorithms:1.0,Network architectures:0.3,Network components:0.2,Network performance evaluation:0.75,Network properties:0.2,Network protocols:0.1,Network services:0.2,Network types:0.1","Network algorithms,Network performance evaluation",Network algorithms is highly relevant as the paper proposes mobility-aware clustering algorithms. Network performance evaluation is relevant due to the analysis of resource utilization gains. Other categories like Network protocols are less relevant as the focus is on clustering strategies rather than protocol design.,"Control path algorithms:0.2,Data path algorithms:0.1,Network economics:0.1,Network experimentation:0.3,Network measurement:0.3,Network performance analysis:0.9,Network performance modeling:0.7,Network simulations:0.8","Network performance analysis,Network simulations","Network performance analysis: Evaluates clustering's impact on resource utilization. Network simulations: Uses simulations to validate schemes. Other options like 'Data path algorithms' are less relevant as the focus is on performance evaluation, not routing protocols."
1908,An adaptive frequency-domain interference coordination approach for different deployment of smallcell in heterogeneous network,"In the Long Term Evolution Advanced (LTE-A) heterogeneous networks (Het-Nets), smallcell is an emerging and key item, providing improved service coverage and high spectral efficiency with low power. However, the number and location arrangement of smallcell is always changing in reality, the interference scenario is also different. In order to reduce the layer-to-layer interference in uncertain deployment of smallcell, a method of frequency domain self-organizing interference coordination technology is proposed in this paper. In this method, the total spectrum bandwidth is divided into two parts according to a frequency partition ratio. One part of the bandwidth is called protected band where macrocell's transmission power will be reduced in an adaptively controlled way to schedule the edge UEs of smallcell and the center UEs of macrocell, another is named unprotected band to schedule the macro edge UEs and smallcells' center UEs. Thus, the edge UEs of smallcell could be protected under the condition of not losing too much macrocells' performance. The system-level simulation results show that the proposed scheme can achieve superior system performance improvements, and higher cell-edge user data rate.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:1.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Networks,Networks receives 1.0 as the paper's core contribution is a frequency-domain interference coordination method for LTE-A heterogeneous networks. No other categories address the primary domain of wireless network optimization.,"Network algorithms:0.8,Network architectures:0.3,Network components:0.1,Network performance evaluation:0.5,Network properties:0.2,Network protocols:0.9,Network services:0.1,Network types:0.4","Network protocols,Network algorithms",Network protocols is directly relevant for the interference coordination method. Network algorithms is relevant for the frequency-domain approach. Other categories like Network performance evaluation are secondary to protocol design.,"Application layer protocols:0,Control path algorithms:1,Cross-layer protocols:0,Data path algorithms:0,Link-layer protocols:0,Network File System (NFS) protocol:0,Network economics:0,Network layer protocols:1,Network protocol design:0,OAM protocols:0,Presentation protocols:0,Protocol correctness:0,Session protocols:0,Transport protocols:0","Network layer protocols,Control path algorithms",Network layer protocols is relevant for interference coordination in LTE-A. Control path algorithms is relevant for adaptive frequency domain coordination. Other protocol layers are not discussed.
4270,On Downlink Network MIMO under a Constrained Backhaul and Imperfect Channel Knowledge,"Next generation mobile communications systems will most likely employ network MIMO in order to mitigate inter-cell interference and improve system fairness and spectral efficiency. Critical issues of such schemes are, however, the large extent of backhaul infrastructure required for the information exchange between cooperating base stations, and the availability of channel knowledge at transmitter and receiver. In this paper, we consider a cooperative downlink transmission under a constrained backhaul, limited channel knowledge at base station and terminal side, and a per-antenna power constraint. We derive inner capacity bounds for different cooperation schemes through uplink/downlink duality and provide numerical results showing the superiority of certain cooperation schemes in terms of rate/backhaul tradeoff for different interference scenarios.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.3,Networks:0.9,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Networks,"Networks is highly relevant as the paper focuses on network MIMO, backhaul constraints, and wireless communication systems. Other categories like Hardware or Computer systems organization are less relevant since the core contribution is theoretical analysis of network performance.","Network algorithms:0.9,Network architectures:0.0,Network components:0.0,Network performance evaluation:0.8,Network properties:0.0,Network protocols:0.0,Network services:0.0,Network types:0.0","Network algorithms,Network performance evaluation",Network algorithms is relevant for the cooperation schemes and capacity bounds analysis. Network performance evaluation is relevant due to the rate/backhaul tradeoff studies. Other categories like network types are not addressed.,"Control path algorithms:0.0,Data path algorithms:0.0,Network economics:0.0,Network experimentation:0.0,Network measurement:0.5,Network performance analysis:1.0,Network performance modeling:1.0,Network simulations:0.5","Network performance analysis,Network performance modeling",Network performance analysis: The paper evaluates rate/backhaul tradeoffs in interference scenarios. Network performance modeling: The derivation of inner capacity bounds is core. Simulations and measurement are secondary for validation.
1849,Adaptive connection admission and flow control: quality of service with high utilisation,"The authors use adaptive feedback and adaptive feedforward control methodologies to avoid congestion at high server utilisation. This addresses the combined connection admission and flow control problem (generic functions for managing and controlling traffic and congestion in BISDN). Using a novel control concept, based on only two groups of traffic (controllable and uncontrollable) they formulate a problem aimed at high (unity) utilisation of resources while maintaining quality of service at prescribed levels. Bounds on operating conditions are derived, and using simulation they show that high utilisation can be achieved as suggested by the theory, together with robustness for unforeseen traffic connections and disconnections. Even with such a high efficiency and strong properties on the quality of service, the only traffic descriptor required from the user is that of the peak rate of the uncontrollable traffic.<<ETX>>","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.9,Software and its engineering:0.2,Theory of computation:0.3,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Networks,Networks: The paper addresses congestion control and quality-of-service (QoS) mechanisms in BISDN networks. Theory of computation (0.3) is secondary as the focus is on network protocols rather than computational theory.,"Network algorithms:1.0,Network performance evaluation:0.8,Network components:0.2,Network protocols:0.3,Network services:0.2,Network types:0.1,Network architectures:0.3,Network properties:0.4","Network algorithms,Network performance evaluation",Network algorithms is highly relevant because the paper introduces adaptive feedback/feedforward control methodologies for traffic management. Network performance evaluation is relevant due to simulation-based validation of high utilization claims. Other categories like Network types or Network services are only tangentially related to the paper's core congestion control focus.,"Control path algorithms:0.3,Data path algorithms:0.1,Network economics:0.1,Network experimentation:0.4,Network measurement:0.5,Network performance analysis:1,Network performance modeling:0.8,Network simulations:1","Network performance analysis,Network simulations",Network performance analysis and Network simulations are directly relevant as the paper addresses QoS modeling and validation. Experimentation and measurement are secondary but still relevant.
5053,Resource Allocation of noncooperative multi-cell for cognitive radio networks,"Cognitive Radio (CR) is proposed to improve frequency efficiency while avoiding interference with licensed users. In this paper, we adopt a Resource Allocation (RA) algorithm to perform uplink frequency allocation and power control between noncooperative multi-cell for CR network. The proposed RA algorithm is an extension of Iterative Water-Filling Algorithm (IWFA) applied to single-cell. The maximization of total information rates of multiple users in one cell is considered for Gaussian channel with pathloss, subject to power constraint on each user. By using Multiple Access Channel (MAC) capacity region, the optimization problem is formulated as a decentralized game, in which each base station, trying to maximize the sum-rate of its own users, is a player, and Nash Equilibrium (NE) can be achieved to guarantee global convergence. Our algorithm shows higher sum-rate than the traditional FDMA by sharing one and more subcarriers for different users. Final simulation results show that our algorithm can achieve more information rate than fixed channel allocation algorithm.","General and reference:0.0,Hardware:0.05,Computer systems organization:0.05,Networks:0.9,Software and its engineering:0.05,Theory of computation:0.05,Mathematics of computing:0.1,Information systems:0.05,Security and privacy:0.05,Human-centered computing:0.0,Computing methodologies:0.05,Applied computing:0.05,Social and professional topics:0.0",Networks,Networks: The paper addresses resource allocation in cognitive radio networks. Other categories are rejected because the paper focuses specifically on network communication and resource allocation problems.,"Network algorithms:1.0,Network architectures:0.2,Network components:0.1,Network performance evaluation:0.7,Network properties:0.1,Network protocols:0.3,Network services:0.1,Network types:0.1",Network algorithms,Network algorithms is highly relevant as the paper proposes an extension of the IWFA algorithm for cognitive radio resource allocation. Network performance evaluation is relevant for the simulations but secondary to the algorithm design.,"Control path algorithms:0.1,Data path algorithms:1.0,Network economics:0.7","Data path algorithms,Network economics","Data path algorithms: The paper introduces a resource allocation algorithm for uplink frequency and power control, a data path concern. Network economics: The decentralized game-theoretic approach aligns with network economics principles. 'Control path algorithms' are not directly addressed."
2173,Managing availability in wireless inter domain access,"This paper deals with how the availability of wireless/cellular access networks depend on the cooperation between the operators as well as with transmission network operators and professional land lords. The forthcoming 4G network will consist of diverse sets of wireless/cellular networks integrated into IP-based networks. Mobility, QoS and seamless handover between the networks are key features. In the work of Gustafsson and Jonsson (2003), the concept of always best connected is defined as means that the user is connected through the best available device and access technology at all times. Availability of the access is fundamental for QoS and critical services (e.g. emergency services, health services) are more and more dependent of wireless/mobile access. Availability can be increased by utilization of several accesses through seamless handover and/or multihoming. Usually, the availability has been calculated assuming independencies between the access network operators, but in this paper we show that the actual cooperation between the market actors has significant impact on the availability. We propose a solution by usage of the IMH 802.21 framework to build and distribute network topology information with availability estimates allowing to predict the overall availability for the access networks accessible as one of the criteria for a handover decision.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.9,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.4,Applied computing:0.3,Social and professional topics:0.1",Networks,Networks is relevant for the wireless/cellular network availability analysis and handover protocols. Other categories like Applied computing are secondary to the network-centric focus.,"Network algorithms:0.7,Network architectures:0.5,Network components:0.4,Network performance evaluation:0.8,Network properties:0.6,Network protocols:0.9,Network services:0.5,Network types:0.4","Network protocols,Network performance evaluation",Network protocols is relevant for the IMH 802.21 framework and handover mechanisms discussed. Network performance evaluation is relevant for analyzing availability metrics. Network algorithms and properties are partially relevant but not as central as the protocol and performance aspects.,"Application layer protocols:0,Cross-layer protocols:0,Link-layer protocols:0,Network File System (NFS) protocol:0,Network experimentation:0,Network layer protocols:1,Network measurement:0,Network performance analysis:1,Network performance modeling:0,Network protocol design:0,Network simulations:0,OAM protocols:0,Presentation protocols:0,Protocol correctness:0,Session protocols:0,Transport protocols:0","Network layer protocols,Network performance analysis",Network layer protocols is relevant for the 4G inter-domain access discussion. Network performance analysis is relevant for availability estimation. Other categories lack direct protocol or performance focus.
415,Network selection in heterogeneous wireless networks. (Sélection de Réseau dans les Réseaux Sans Fil Hétérogènes),"Dans le contexte de la tendance actuelle vers l'ubiquite des reseaux et la mobilite des services, nous observons que l'acces de reseau est fourni par une grande diversite de technologies d'acces, avec des recouvrements, ce qui compose un environnement de reseaux sans fil heterogenes. Dans cet environnement, les terminaux mobiles ont besoin de toujours selectionner le meilleur reseau. Dans cette these, nous faisons tout d'abord une etude de schemas existants de la selection de reseau qui utilisent differents modeles mathematiques. Ensuite, nous etablissons un simulateur en matlab. Dans notre premiere proposition, le probleme en utilisant les facteurs de mobilite est etudie. Nous expliquons que la selection du meilleur reseau devient la selection de la meilleure permutation des reseaux lorsque les proprietes de handover verticale sont prises en compte. A la fin, nous proposons un schema pour trouver la meilleure permutation. Deuxiemement, l'exigence d'une nouvelle methode de ponderation subjective est analysee. Nous proposons un nouveau mecanisme automatique, fonde sur les declencheurs, qui est capable de calculer efficacement les poids subjectifs des attributs divers, considerant les proprietes de reseau et terminal mobile. Finalement, plusieurs autres problemes de selection de reseau sont analyses et les solutions possibles sont proposees. Base sur toutes les etudes ci-dessus, nous proposons une strategie integree.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.9,Software and its engineering:0.2,Theory of computation:0.3,Mathematics of computing:0.5,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Networks,"The paper focuses on heterogeneous wireless network selection strategies. Mathematics of computing receives moderate relevance for optimization models, but the primary domain is networking.","Network algorithms:0.9,Network protocols:0.4,Network properties:0.5",Network algorithms,Network algorithms is directly relevant as the paper focuses on network selection algorithms. Other categories are secondary or not discussed.,"Control path algorithms:1.0,Data path algorithms:0.0,Network economics:0.0",Control path algorithms,Control path algorithms is directly relevant for network selection strategies. Data path algorithms and Network economics are not discussed in the context of this paper.
2422,Energy Aware Dynamic Routing Using SDN for a Campus Network,"Increase in traffic over internet by large number of mobile devices results into high energy consumption on network devices. In this paper, we address the necessity to reduce the energy consumption in a campus network that includes both wired and wireless network devices. Specifically, we propose dynamic link rate adaptation mechanism on Software Defined Network (SDN) switches and control power consumption on Access Points (APs) by using users association to an AP. Moreover, our online flow routing approach dynamically routes user traffic in order to decrease the overall energy consumption of whole network while taking in consideration Quality of Service (QoS), acceptance ratio on forwarding table and link BandWidth (BW) constraints. Our simulation result shows that our approach results in 3.69% less power consumption, and furthermore, there is no congested links nor overflow in flow table as compared to Dijkstra Ant colony Power (DAPower) that has 2 congested links for 150 users in a network, each generating 34 flows.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.9,Software and its engineering:0.5,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.3,Applied computing:0.1,Social and professional topics:0.1",Networks,Networks (0.9) is the primary category for SDN-based energy optimization. Software and its engineering (0.5) is secondary for the implementation aspects. Other categories like Hardware (0.1) are irrelevant as the focus is on routing algorithms.,"Network algorithms:0.9,Network architectures:0.3,Network components:0.2,Network performance evaluation:0.7,Network properties:0.1,Network protocols:0.2,Network services:0.1,Network types:0.1","Network algorithms,Network performance evaluation",Network algorithms is highly relevant for the dynamic routing approach. Performance evaluation is key to measuring energy savings. Network architectures are less central.,"Control path algorithms:1.0,Data path algorithms:0.2,Network economics:0.1,Network experimentation:0.1,Network measurement:0.2,Network performance analysis:1.0,Network performance modeling:0.3,Network simulations:0.5","Control path algorithms,Network performance analysis",Control path algorithms is relevant because SDN control plane mechanisms for dynamic routing are discussed. Network performance analysis is relevant as energy consumption and QoS optimization are central to the paper. Network simulations received moderate score since they are mentioned in the results but not the core contribution.
508,Performance analysis of IR-UWB TR receiver using cooperative dual hop AF strategy,"Non-Coherent receivers are attractive for IR-UWB systems and are preferred over its coherent counterpart, because of its implementation simplicity and non-requirement of channel state information (CSI). UWB systems have a low Power Spectral Density (PSD) which limits it from achieving a wide coverage and high data rate despite, providing an adequate system performance. Hence, cooperative diversity technology is introduced which helps in expansion of coverage area, improvement in Quality of Service (QoS) and BER performance. The paper evaluates the BER performance of IR-UWB TR receiver using dual hop cooperative strategy with Amplify and Forward (AF) relaying analytically and compares it with the simulation results. Simulations clearly show that with increase in number of frames from 1 to 2, the BER performance degrades.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:1.0,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Networks,The paper evaluates cooperative diversity techniques for IR-UWB communication networks. Networks is directly relevant for the wireless communication analysis. Other categories like Hardware are not discussed.,"Network algorithms:0.2,Network architectures:0.1,Network components:0.1,Network performance evaluation:0.8,Network properties:0.3,Network protocols:0.1,Network services:0.1,Network types:0.7","Network performance evaluation,Network types","Network performance evaluation is relevant as the paper evaluates BER performance of a UWB receiver. Network types is relevant because the focus is on IR-UWB systems. Other categories like Network properties are less directly relevant, as the study is more about performance than intrinsic network characteristics.","Ad hoc networks:0.1,Cyber-physical networks:0.0,Data center networks:0.0,Home networks:0.0,Mobile networks:0.0,Network experimentation:0.2,Network measurement:0.0,Network on chip:0.0,Network performance analysis:1.0,Network performance modeling:0.6,Network simulations:0.4,Overlay and other logical network structures:0.0,Packet-switching networks:0.0,Public Internet:0.0,Storage area networks:0.0,Wired access networks:0.0,Wireless access networks:1.0","Network performance analysis,Wireless access networks",Network performance analysis is relevant as the paper evaluates BER performance through analytical and simulation methods. Wireless access networks is relevant because the study focuses on IR-UWB technology used in wireless communication. Other options like 'Network simulations' are less directly relevant as the paper uses simulations as a tool rather than being about simulation techniques.
1759,Joint power and bandwidth allocation in downlink transmission,"We formulate and analyze the problem of optimal downlink scheduling with instantaneous channel and queue size information when both power and bandwidth may be adaptively split among multiple users. We derive optimal solutions of low computational complexity, as well as faster and simpler approximations, to various versions of this problem when the power, rate, and bandwidth allocations to the users can all take continuous values. For this case, we show that the optimal scheme requires transmission to no more than two users during each time slot when users can receive at arbitrary rates, even when the user rate per unit of bandwidth is upper bounded by the best available modulation scheme. Our methods also extend easily to incorporate other intuitive constraints such as upper limits on user rates to improve frame fill efficiency. Simulation results suggest that the simple approximations work nearly as well as the throughput optimal schemes when continuous bandwidth and power partitions are allowed. In practice, the rate and bandwidth assignments to users take discrete values, and we present heuristic methods motivated by the continuous optimum to this discrete case.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:1.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Networks,Networks is highly relevant as the paper addresses downlink resource allocation in communication systems. Other categories are irrelevant as the focus is on network optimization and transmission strategies.,"Network algorithms:1.0,Network architectures:0.0,Network components:0.0,Network performance evaluation:0.5,Network properties:0.0,Network protocols:0.0,Network services:0.0,Network types:0.0",Network algorithms,Network algorithms is directly relevant as the paper focuses on power and bandwidth allocation algorithms. Network performance evaluation receives a moderate score due to simulation results.,"Control path algorithms:0.9,Data path algorithms:0.3,Network economics:0.1",Control path algorithms,"The paper focuses on algorithmic methods for optimizing power and bandwidth allocation in downlink transmission, which aligns with 'Control path algorithms'. 'Data path algorithms' relates to data flow management, which is not the primary focus. 'Network economics' is irrelevant as no economic models are discussed."
4818,Musical engagement that is predicated on intentional activity of the performer with noisa instruments,"This paper presents our current research in which we study the notion of performer engagement within the variance and diversities of the intentional activities of the performer in musical interaction. We introduce a user-test study with the aim to evaluate our system's engagement prediction capability and to understand in detail the system's response behaviour. The quantitative results indicate that our system recognises and monitors performer's engagement successfully, although we found that the system's response to maintain and deepen the performer's engagement is perceived differently among participants. The results reported in this paper can be used to inform the design of interactive systems that enhance the quality of performer's engagement in musical interaction with new interfaces.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.9,Computing methodologies:0.4,Applied computing:0.3,Social and professional topics:0.1",Human-centered computing,Human-centered computing is highly relevant as the paper focuses on performer engagement in interactive systems. Applied computing is secondary to the human interaction focus.,"Accessibility:0.1,Collaborative and social computing:0.1,Human computer interaction (HCI):0.9,Interaction design:0.8,Ubiquitous and mobile computing:0.1,Visualization:0.1","Human computer interaction (HCI),Interaction design","Human computer interaction (HCI): The paper examines performer engagement with interactive music systems, a key HCI topic. Interaction design: The study involves designing systems to monitor and enhance engagement. Other fields like ubiquitous computing are not relevant to the focus on performer-system interaction.","Empirical studies in HCI:0.9,Empirical studies in interaction design:0.8,HCI design and evaluation methods:0.7,HCI theory, concepts and models:0.3,Interaction design process and methods:0.4,Interaction design theory, concepts and paradigms:0.3,Interaction devices:0.2,Interaction paradigms:0.2,Interaction techniques:0.3,Interactive systems and tools:1.0,Systems and tools for interaction design:0.6","Interactive systems and tools,Empirical studies in interaction design",Interactive systems and tools is relevant for the engagement prediction system described. Empirical studies in interaction design applies to the user testing methodology. Other categories like interaction paradigms are less focused.
2372,An empirical user-based study of text drawing styles and outdoor background textures for augmented reality,"A challenge in presenting augmenting information in outdoor augmented reality (AR) settings lies in the broad range of uncontrollable environmental conditions that may be present, specifically large-scale fluctuations in natural lighting and wide variations in likely backgrounds or objects in the scene. In this paper, we present a user-based study which examined the effects of outdoor background textures, changing outdoor illuminance values, and text drawing styles on user performance of a text identification task with an optical, see-through augmented reality system. We report significant effects for all of these variables, and discuss design guidelines and ideas for future work.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.2,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:1.0,Computing methodologies:0.6,Applied computing:0.1,Social and professional topics:0.1",Human-centered computing,Human-centered computing is relevant for the user study of AR text presentation. Computing methodologies is only peripherally relevant as the paper focuses on user experience evaluation rather than computational methods.,"Accessibility:0.2,Collaborative and social computing:0.1,Human computer interaction (HCI):0.9,Interaction design:0.8,Ubiquitous and mobile computing:0.75,Visualization:0.3","Human computer interaction (HCI),Interaction design,Ubiquitous and mobile computing",Human computer interaction (HCI) is central as the study examines user performance in AR systems. Interaction design is relevant for evaluating text drawing styles. Ubiquitous and mobile computing applies to AR's mobile context. Accessibility and Visualization are only incidentally related. Collaborative computing is irrelevant.,"Empirical studies in HCI:0.9,Empirical studies in interaction design:0.7,Empirical studies in ubiquitous and mobile computing:0.8,HCI design and evaluation methods:0.3,HCI theory, concepts and models:0.2,Interaction design process and methods:0.4,Interaction design theory, concepts and paradigms:0.3,Interaction devices:0.1,Interaction paradigms:0.2,Interaction techniques:0.3,Interactive systems and tools:0.2,Systems and tools for interaction design:0.1,Ubiquitous and mobile computing design and evaluation methods:0.4,Ubiquitous and mobile computing systems and tools:0.5,Ubiquitous and mobile computing theory, concepts and paradigms:0.3,Ubiquitous and mobile devices:0.2","Empirical studies in HCI,Empirical studies in ubiquitous and mobile computing",Empirical studies in HCI and ubiquitous computing are directly relevant to the user-based evaluation of AR text display. Other categories like 'Interaction devices' are less central as the focus is on study outcomes rather than specific hardware.
4950,Measurement and Outcomes of Identity Communication in Virtual Teams,"Virtual teams play an important role in the modern economy, and many organizations struggle to overcome the weaknesses inherent in technology-mediated work. Drawing from a strong empirical foundation for identity-related outcomes in non-mediated settings, we propose that perceived virtual identity communication accuracy positively impacts virtual team trust and performance. We further propose that capabilities of the communication medium can either support or hinder perceived virtual identity communication accuracy. In three studies with a cumulative sample of N=410, this research-in-progress paper reports on the first phase of a two-phase study. We develop survey scales for a medium's identity communication capabilities and users' perceived virtual identity communication accuracy, and then outline an in-progress experiment that has been pilot-tested to examine the outcomes of virtual identity communication. The conclusion of the research will make contributions to the virtual teams literature, as well as provide actionable guidelines for increasing the effectiveness of virtual teams.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.9,Computing methodologies:0.1,Applied computing:0.2,Social and professional topics:0.1",Human-centered computing,"The study examines human communication dynamics in virtual teams, aligning with human-centered computing. Applied computing is marginally relevant as it addresses practical team performance, but the primary focus is on human factors.","Accessibility:0.1,Collaborative and social computing:0.8,Human computer interaction (HCI):0.4,Interaction design:0.3,Ubiquitous and mobile computing:0.2,Visualization:0.1",Collaborative and social computing,Collaborative and social computing is highly relevant as the paper studies identity communication in virtual teams. HCI is secondary as the focus is on social dynamics rather than interface design.,"Collaborative and social computing design and evaluation methods:1.0,Collaborative and social computing devices:0.1,Collaborative and social computing systems and tools:0.3,Collaborative and social computing theory, concepts and paradigms:0.2,Empirical studies in collaborative and social computing:1.0","Collaborative and social computing design and evaluation methods,Empirical studies in collaborative and social computing",Design and evaluation methods address survey scale development and experiment design. Empirical studies support the research-in-progress focus. Other options like systems or theory are less directly relevant.
2235,DiscoverySpace: Suggesting Actions in Complex Software,"Complex software offers power for experts, yet overwhelms new users. Novices often do not know how to execute tasks, what they want to achieve, or even what is possible. To address this, we introduce the DiscoverySpace interface for executable action suggestions. DiscoverySpace is a prototype extension panel for Adobe Photoshop that suggests task-level action macros to apply to photographs based on visual features. DiscoverySpace harvests these one-click actions from the online Photoshop user community. A between-subjects study indicated that action suggestions may help novices maintain confidence, accomplish tasks, and discover features. This work demonstrates how interfaces can leverage user-generated content to help novices navigate complex software.","General and reference:0.2,Hardware:0.2,Computer systems organization:0.2,Networks:0.2,Software and its engineering:0.2,Theory of computation:0.2,Mathematics of computing:0.2,Information systems:0.2,Security and privacy:0.2,Human-centered computing:1.0,Computing methodologies:0.2,Applied computing:0.2,Social and professional topics:0.2",Human-centered computing,Human-centered computing is highly relevant for the interface design to assist novices in complex software. Other categories lack focus on user interaction or experience.,"Accessibility:0.25,Collaborative and social computing:0.25,Human computer interaction (HCI):0.9,Interaction design:0.75,Ubiquitous and mobile computing:0.25,Visualization:0.25","Human computer interaction (HCI),Interaction design",Human computer interaction (HCI) is highly relevant as the paper focuses on interface design for complex software. Interaction design is relevant due to the design of the DiscoverySpace panel. Other categories like Collaborative and social computing are less relevant as the focus is on individual user interaction.,"Empirical studies in HCI:1.0,Empirical studies in interaction design:0.0,HCI design and evaluation methods:1.0,HCI theory, concepts and models:0.0,Interaction design process and methods:0.0,Interaction design theory, concepts and paradigms:0.0,Interaction devices:0.0,Interaction paradigms:0.0,Interaction techniques:0.0,Interactive systems and tools:1.0,Systems and tools for interaction design:0.0","Empirical studies in HCI,HCI design and evaluation methods,Interactive systems and tools",Empirical studies in HCI (1): The paper includes a between-subjects study on user experience. HCI design and evaluation methods (1): DiscoverySpace is evaluated for usability. Interactive systems and tools (1): The interface is a prototype tool for Photoshop. Other options like Interaction techniques are irrelevant as the focus is on interface design evaluation.
1958,Metaphoric Iconicity in Signed and Spoken Languages,"Metaphoric Iconicity in Signed and Spoken Languages Defu Yap (yapd@uchicago.edu) Laura Staum Casasanto (lcasasanto@uchicago.edu) Daniel Casasanto (casasanto@uchicago.edu) Department of Psychology, University of Chicago 5848 S. University Avenue, Chicago, IL 60637 USA Abstract Metaphoric iconicity in gestures Since Saussure, the idea that the forms of words are arbitrarily related to their meanings has been widely accepted. Yet, implicit metaphorical mappings may provide opportunities for iconicity throughout the lexicon. We hypothesized that vertical spatial metaphors for emotional valence are manifested in language through space in signed languages and through the spatialized dimension of pitch in spoken languages. In Experiment 1, we analyzed the directions of the hand motions constituting words in three signed languages, and related them to the valence of their English translation equivalents. The vertical direction of signs predicted their valences. On average, signs with upward movements were the most positive in valence, and signs with downward movements the most negative. Signs with non-vertical movements were intermediate in valence. Experiment 2 extended this type of analysis to a tonal language, Mandarin Chinese. The pitch contours of Chinese words predicted the valence of their English translation equivalents. These results demonstrate a previously unrecognized source of non-arbitrariness in language, revealing that implicit space-valence metaphors are encoded in the forms of words in both signed and spoken languages. Keywords: Metaphoric iconicity; Conceptual metaphor; Valence Introduction Since Saussure (1959), the idea that words’ forms are arbitrarily related to their meanings has been widely accepted. According to Saussure, the meaning of “tree” is unmotivated by the letters a-r-b-r-e in French, since in principle it can be represented by any other letters in other languages, such as t-r-e-e in English. The documented exceptions to arbitrariness tend to fall into a narrow range of categories, such as ideophones (e.g., Bang! and tinkle sound like their referents; Nuckolls, 2004), phonaesthemes (e.g., words having to do with noses like snout and sniffle tend to start with the sound /sn/; Bergen, 2004), the bouba-kiki phenomenon (Maurer et al., 2006) and iconic signs in signed languages (e.g., the sign for two in American Sign Language is two extended fingers). These kinds of iconic relationships rely on concrete qualities of the referent being echoed in the form of the word, so only certain meanings are eligible to participate in them. Beyond these special exceptions, are form-meaning relationships in languages truly arbitrary? If not, what are the sources of non-arbitrariness in language? Are there constraints that influence form-meaning relationships systematically throughout our lexicons? Iconic form-meaning relationships are common in the gestures we produce when we speak (McNeill, 1992). Iconic gestures depict some concrete aspect of the referents of the words they accompany (e.g., raising the hand to indicate that a rocket went higher). In a special class of iconics called metaphoric gestures (McNeill, 1992), concrete objects or relationships are depicted with the hands in order to represent some aspect of an abstract idea (e.g., raising the hand to indicate that a students’ grades went “higher”; Cienki, 1998; Sweetser, 1998). In metaphoric gestures, abstract ideas that we can never see or touch can nevertheless be represented with the hands via conceptual metaphor (Lakoff & Johnson, 1980). People often talk about abstract, non-spatial entities using spatial words (e.g., a long time, a high price, or a close friendship). Beyond talking in linguistic metaphors, there is a growing body of evidence suggesting that people also think in mental metaphors (Casasanto & Bottini, 2013, for review): implicit associations between non-linguistic representations in abstract target domains and relatively concrete source domains like space, force, and motion (Lakoff & Johnson, 1980). Although target domains like time are nearly impossible to depict gesturally, per se, their source domains can often be depicted: A long time can be indicated by a long-distance sweep of the hand; a distant time can be represented by gesturing toward a far-away point in space (see Cienki & Muller, 2008, for numerous examples of metaphorical gestures). Metaphoric iconicity in languages Although the evidence for metaphoric iconicity in gestures is strong, this type of iconicity is generally assumed not to extend to language. Even signed languages, which share a modality with hand gestures and therefore have the potential to express spatial iconicity, have been characterized as exhibiting largely arbitrary form-meaning mappings, in part for historical reasons having to do with establishing American Sign Language (ASL) as a full-fledged language, and not a simple system of pantomimes (Klima & Bellugi, 1979). Taub (2001) noted that signed languages’ potential for iconicity is expanded by their ability to depict aspects of metaphoric source domains in sign, as in gesture. She and others have shown metaphoric iconicity in a number of ASL signs (see also Emmorey, 2001). In a multiple-choice test, non-signers were able to match the meanings of some metaphoric signs in ASL to their English glosses (O’Brien,","General and reference:0.3,Hardware:0.3,Computer systems organization:0.3,Networks:0.3,Software and its engineering:0.3,Theory of computation:0.3,Mathematics of computing:0.3,Information systems:0.6,Security and privacy:0.3,Human-centered computing:0.7,Computing methodologies:0.3,Applied computing:0.3,Social and professional topics:0.3",Human-centered computing,Human-centered computing is relevant for the study of user interaction with language metaphors. Information systems is less central as the focus is on linguistic analysis rather than data systems.,"Accessibility:0.0,Collaborative and social computing:0.3,Human computer interaction (HCI):0.0,Interaction design:0.0,Ubiquitous and mobile computing:0.0,Visualization:0.0",Collaborative and social computing,Collaborative and social computing is marginally relevant for analyzing social communication patterns. All other options are unrelated to the linguistic research on metaphoric iconicity.,"Collaborative and social computing theory, concepts and paradigms:1,Empirical studies in collaborative and social computing:1,Collaborative and social computing design and evaluation methods:0.2,Collaborative and social computing devices:0.1,Collaborative and social computing systems and tools:0.1","Collaborative and social computing theory, concepts and paradigms,Empirical studies in collaborative and social computing","Collaborative and social computing theory, concepts and paradigms: The paper explores metaphoric iconicity as a conceptual framework in language. Empirical studies in collaborative and social computing: The study includes experiments analyzing valence in signed and spoken languages. Other categories are rejected as the paper focuses on linguistic theory rather than systems, tools, or design."
384,Influencing group participation with a shared display,"During face-to-face interactions, groups frequently overly rely on the dominant viewpoint to lead the group in its decision-making process. We begin with a discussion of this phenomenon and the possibility for technology to assist in addressing it. We then present findings from a behavioral study that examines how a shared display of individual speaker-participation rates can impact the behavior of the group during a collaboration task. The results from the study indicate that the presence of such a display influences the behavior of group participants in the extremes of over and under participation. While influencing the quantity of time someone speaks is not directly equivalent to influencing the topics discussed, we suggest that this approach of providing peripheral displays of social information is promising for improving certain types of group interactions.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.2,Information systems:0.2,Security and privacy:0.1,Human-centered computing:0.8,Computing methodologies:0.3,Applied computing:0.2,Social and professional topics:0.1",Human-centered computing,Human-centered computing is relevant for the study of group behavior and shared displays in collaborative settings. Other categories like Information systems or Applied computing are less directly tied to the human interaction focus.,"Accessibility:0.0,Collaborative and social computing:0.8,Human computer interaction (HCI):0.7,Interaction design:0.1,Ubiquitous and mobile computing:0.0,Visualization:0.0","Collaborative and social computing,Human computer interaction (HCI)",Collaborative and social computing is highly relevant as the paper studies group interaction dynamics with shared displays. Human computer interaction (HCI) is relevant as it examines how technology affects group behavior. Interaction design receives a low score as the focus is on behavioral impact rather than design methodologies.,"Collaborative and social computing design and evaluation methods:1,Empirical studies in collaborative and social computing:1,Empirical studies in HCI:0.8,HCI design and evaluation methods:0.6,Interaction devices:0.3,Interaction paradigms:0.2,Interaction techniques:0.3,Interactive systems and tools:0.5,Collaborative and social computing systems and tools:0.7,Collaborative and social computing theory, concepts and paradigms:0.4","Collaborative and social computing design and evaluation methods,Empirical studies in collaborative and social computing",Collaborative and social computing design and evaluation methods is relevant because the paper evaluates a shared display design for group interactions. Empirical studies in collaborative and social computing is relevant due to the behavioral study on group dynamics. Other categories like 'Empirical studies in HCI' and 'HCI design and evaluation methods' are partially relevant but less focused on collaboration-specific aspects.
235,"Six-oof haptic interaction with fluids, solids, and their transitions","Haptic interaction with different types of materials in the same scene is a challenging task, mainly due to the specific coupling mechanisms that are usually required for either fluid, deformable or rigid media. Dynamically-changing materials, such as melting or freezing objects, present additional challenges by adding another layer of complexity in the interaction between the scene and the haptic proxy. In this paper, we address these issues through a common simulation framework, based on Smoothed-Particle Hydrodynamics, and enable haptic interaction simultaneously with fluid, elastic and rigid bodies, as well as their melting or freezing. We introduce a mechanism to deal with state changes, allowing the perception of haptic feedback during the process, and a set of dynamic mechanisms to enrich the interaction through the proxy. We decouple the haptic and visual loops through a dual GPU implementation. An initial evaluation of the approach is performed through performance and feedback measurements, as well as a small user study assessing the capability of users to recognize the different states of matter they interact with.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.0,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.2,Security and privacy:0.0,Human-centered computing:0.9,Computing methodologies:0.3,Applied computing:0.2,Social and professional topics:0.1",Human-centered computing,"Human-centered computing is highly relevant as the paper presents a haptic interaction system for different materials. Computing methodologies (0.3) receives a moderate score for the simulation framework, but the core contribution is in human-computer interaction.","Accessibility:0.1,Collaborative and social computing:0.2,Human computer interaction (HCI):1.0,Interaction design:0.4,Ubiquitous and mobile computing:0.1,Visualization:0.3",Human computer interaction (HCI),Human computer interaction (HCI) is highly relevant as the paper develops a haptic interaction framework for materials. Other categories like Visualization are less central since the focus is on tactile feedback rather than visual representation.,"Empirical studies in HCI:0,HCI design and evaluation methods:0,HCI theory, concepts and models:0,Interaction devices:1,Interaction paradigms:0,Interaction techniques:1,Interactive systems and tools:0","Interaction devices,Interaction techniques",Interaction devices are relevant due to the haptic proxy implementation. Interaction techniques are relevant for the state-change handling and dynamic mechanisms. Other categories are less relevant as the focus is on technical implementation rather than theoretical models or evaluation methods.
482,Pushy versus meek - using avatars to influence turn-taking behaviour,"The flow of spoken interaction between human interlocutors is a widely studied topic. Amongst other things, studies have shown that we use a number of facial gestures to improve this flow - for example to control the taking of turns. This type of gestures ought to be useful in systems where an animated talking head is used, be they systems for computer mediated human-human dialogue or spoken dialogue systems, where the computer itself uses speech to interact with users. In this article, we show that a small set of simple interaction control gestures and a simple model of interaction can be used to influence users' behaviour in an unobtrusive manner. The results imply that such a model may improve the flow of computer mediated interaction between humans under adverse circumstances, such as network latency, or to create more human-like spoken human-computer interaction.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.3,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.9,Computing methodologies:0.2,Applied computing:0.3,Social and professional topics:0.1",Human-centered computing,Human-centered computing is highly relevant as the paper focuses on using avatars to influence human interaction patterns in spoken dialogue systems. Other categories are less relevant because the paper is primarily about interaction design rather than technical systems implementation or mathematical theory.,"Accessibility:0.0,Collaborative and social computing:0.0,Human computer interaction (HCI):1.0,Interaction design:1.0,Ubiquitous and mobile computing:0.0,Visualization:0.0","Human computer interaction (HCI),Interaction design",Human computer interaction (HCI): The paper studies how avatars influence human-computer interaction through turn-taking behavior. Interaction design: The design of interaction control gestures for avatars is central to the research. Other categories like accessibility or ubiquitous computing are not directly addressed as the focus is on interaction design principles rather than accessibility or mobility.,"Empirical studies in HCI:1,Empirical studies in interaction design:0,HCI design and evaluation methods:1,HCI theory, concepts and models:0,Interaction design process and methods:0,Interaction design theory, concepts and paradigms:0,Interaction devices:0,Interaction paradigms:0,Interaction techniques:1,Interactive systems and tools:0,Systems and tools for interaction design:0","Empirical studies in HCI,HCI design and evaluation methods,Interaction techniques","Empirical studies in HCI are relevant as the paper presents user studies on avatar behavior. HCI design and evaluation methods are relevant for the design of interaction models. Interaction techniques are relevant as the paper discusses how avatars influence user behavior. Other categories like Interaction devices or Systems are irrelevant as the focus is on behavioral models, not hardware or tools."
3952,Exploration environments: concept and empirical evaluation,"Exploration environments support users to learn groupware in a self-directed way. They allow users to perceive the way a groupware function works by simulating the impacts of the function's execution on other users' interfaces. This paper motivates and describes the concept of exploration environments and presents an exemplary implementation. Moreover, it reports about the results of an experimental study, which evaluated the effects of this concept on users' learning. The results of this study are discussed and compared to findings from a field study which evaluated the concept, as well.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.9,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Human-centered computing,"Human-centered computing is relevant as the paper evaluates exploration environments for user learning in groupware. Other categories are irrelevant as the paper does not focus on hardware, networks, or theoretical computation.","Accessibility:0.0,Collaborative and social computing:0.4,Human computer interaction (HCI):0.8,Interaction design:1.0,Ubiquitous and mobile computing:0.1,Visualization:0.0","Interaction design,Human computer interaction (HCI)",Interaction design is central to the concept of exploration environments for groupware learning. Human computer interaction (HCI) is relevant due to the focus on user learning and interface simulation. Other options like 'Collaborative and social computing' are less directly tied to the paper's core contribution.,"Empirical studies in HCI:0.9,Empirical studies in interaction design:0.7,HCI design and evaluation methods:0.6,HCI theory, concepts and models:0.2,Interaction design process and methods:0.8,Interaction design theory, concepts and paradigms:0.3,Interaction devices:0.1,Interaction paradigms:0.2,Interaction techniques:0.4,Interactive systems and tools:0.7,Systems and tools for interaction design:0.5","Empirical studies in HCI,Interaction design process and methods",Empirical studies in HCI is highly relevant as the paper presents an experimental evaluation of exploration environments. Interaction design process and methods is relevant due to the description and implementation of the concept. Other children like Interaction devices are irrelevant as the focus is on software evaluation.
3847,Usability metrics and methodologies,The British Computer Society's human-computer interaction specialist group sponsored a one-day seminar on usability metrics and methodologies in connection with its 1990 annual meeting in London. The seminar was organized by Nigel Bevan from the National Physical Laboratory and attracted an audience of about 80 people. The lively discussions during the seminar and the tea breaks indicated substantial interest in the issues of usability engineering.,"General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.9,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Human-centered computing,Human-centered computing is relevant because the paper focuses on usability metrics and methodologies for human-computer interaction. Other categories like 'Social and professional topics' are less relevant as the paper specifically addresses technical usability engineering rather than broader social issues.,"Accessibility:0.6,Collaborative and social computing:0.3,Human computer interaction (HCI):0.9,Interaction design:0.7,Ubiquitous and mobile computing:0.4,Visualization:0.3","Human computer interaction (HCI),Interaction design",Human computer interaction (HCI) is directly relevant as the paper focuses on usability engineering and metrics. Interaction design is secondary as it relates to the design aspects of usability. Other categories like Collaborative computing or Ubiquitous computing are less relevant as the paper does not address multi-user systems or mobile contexts.,"Empirical studies in HCI:0.5,Empirical studies in interaction design:0.5,HCI design and evaluation methods:1,HCI theory, concepts and models:0.5,Interaction design process and methods:0.5,Interaction design theory, concepts and paradigms:0.5,Interaction devices:0,Interaction paradigms:0,Interaction techniques:0,Interactive systems and tools:0,Systems and tools for interaction design:0",HCI design and evaluation methods,HCI design and evaluation methods is directly relevant as the seminar focused on usability metrics. Empirical studies in HCI is secondary as the abstract mentions discussions but lacks explicit empirical methods.
4456,Subjective ratings of instantaneous and gradual transitions from narrowband to wideband active speech,"In advanced heterogeneous telecommunication networks, network resources can dynamically dictate the type of speech coding that is used. An increase in resources allows for lower coding distortion or it might also be used to provide wideband speech instead of narrowband speech. Existing studies have demonstrated that wideband speech is preferred to narrowband speech, but they have also demonstrated that an abrupt transition from narrowband to wideband is perceived as an impairment, even though it is a transition to a higher quality signal. We describe our recent work that resulted in subjective scores for abrupt and gradual transitions from narrowband to wideband at the midpoint of a six-second segment of active speech. On average, signals that start narrowband and end wideband are rated slightly lower than constant narrowband signals and results are nearly the same for abrupt and gradual (2.5 second) transitions. Scores from 20 listeners show a wide range of individual opinions so we conclude that studies of bandwidth transitions may be quite sensitive to the listener population sample.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.8,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Human-centered computing,"Human-centered computing: The paper analyzes human perception of speech quality transitions. Other categories: The work is not about algorithms, systems, or pure computing methodologies.","Accessibility:0.0,Collaborative and social computing:0.0,Human computer interaction (HCI):0.75,Interaction design:0.5,Ubiquitous and mobile computing:0.25,Visualization:0.0",Human computer interaction (HCI),Human computer interaction is highly relevant as the paper studies subjective ratings of speech transitions in telecommunications. Interaction design is somewhat relevant but less central. Ubiquitous computing is only tangentially relevant due to the telecom context.,"Empirical studies in HCI:1,HCI design and evaluation methods:1,HCI theory, concepts and models:0.1,Interaction devices:0.1,Interaction paradigms:0.1,Interaction techniques:0.1,Interactive systems and tools:0.1","Empirical studies in HCI,HCI design and evaluation methods",Empirical studies in HCI: The paper presents subjective evaluation data from 20 listeners. HCI design and evaluation methods: Discusses evaluation of speech transition interfaces. Other children have minimal relevance as the paper focuses on perceptual evaluation rather than interaction techniques or paradigms.
4913,Facial Orientation During Multi-party Interaction with Information Kiosks,"We hypothesize that the performance of multimodal perceptive user interfaces during multi-party interaction may be improved by using facial orientation of users as a cue for identifying the addressee of a user utterance. Multi-party interactions were collected in a user test where one participant would both interact with an information kiosk and negotiate with another person about the information to be obtained. It was found that users indeed look at the system when they speak to the system, but that they also look at the system most of the time when they negotiate with the other person. It is concluded that facial orientation by itself does not fully identify the addressee of a user utterance, but there are promising results for a combination of facial orientation and","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:1.0,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Human-centered computing,"Human-centered computing: The study analyzes facial orientation as a cue for multi-party interaction with kiosks. Other categories are irrelevant as the focus is on user interface design and behavioral patterns, not software, hardware, or mathematical models.","Accessibility:0.1,Collaborative and social computing:0.7,Human computer interaction (HCI):1.0,Interaction design:0.5,Ubiquitous and mobile computing:0.3,Visualization:0.1","Human computer interaction (HCI),Collaborative and social computing",Human computer interaction (HCI) is highly relevant for analyzing multi-party interaction with kiosks. Collaborative and social computing is relevant for studying negotiation dynamics. Other categories are rejected as the focus is on user behavior rather than visualization or interaction design specifics.,"Collaborative and social computing design and evaluation methods:0.6,Collaborative and social computing devices:0.7,Collaborative and social computing systems and tools:1.0,Collaborative and social computing theory, concepts and paradigms:0.5,Empirical studies in HCI:0.6,Empirical studies in collaborative and social computing:1.0,HCI design and evaluation methods:0.6,HCI theory, concepts and models:0.5,Interaction devices:0.4,Interaction paradigms:0.5,Interaction techniques:1.0,Interactive systems and tools:0.7","Collaborative and social computing systems and tools,Interaction techniques,Empirical studies in collaborative and social computing",Collaborative and social computing systems and tools is relevant for the information kiosk interaction system. Interaction techniques is relevant for analyzing facial orientation as an interaction cue. Empirical studies in collaborative and social computing is relevant for the user study methodology. Other categories like HCI design methods are less directly connected to the paper's focus on multi-party interaction analysis.
569,Integrating knowledge management systems with everyday work: design principles leveraging user practice,"Much research argues that information technology can have a positive influence on knowledge application. However, practical results from research on knowledge management systems indicate that such systems often fail when implemented in contemporary organizations. Whilst maintenance of knowledge management systems has been recognized as an important research area, imbalance between additional workload and accurate content still appears to be a critical factor, resulting in systems of little use for organisations in their knowledge application processes. The objective of this paper is to demonstrate how knowledge management systems can be designed to better support knowledge application in organizational knowledge work processes. Building on lessons learned from three knowledge management systems, this paper contributes general design principles describing how knowledge management systems can be integrated with everyday work to leverage user practices.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.75,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Human-centered computing,Human-centered computing is relevant for the user-centered design principles in knowledge management systems. Other categories like Information systems are less relevant as the focus is on user practices rather than technical systems.,"Human computer interaction (HCI):1.0,Interaction design:1.0,Collaborative and social computing:0.4,Accessibility:0.1,Ubiquitous and mobile computing:0.2,Visualization:0.1","Human computer interaction (HCI),Interaction design",Human computer interaction (HCI) and Interaction design are relevant as the paper focuses on design principles for knowledge systems integrated with user practices. Collaborative computing is less central as the abstract doesn't emphasize collaboration.,"Empirical studies in HCI:0.7,Empirical studies in interaction design:0.7,HCI design and evaluation methods:1.0,HCI theory, concepts and models:0.5,Interaction design process and methods:1.0,Interaction design theory, concepts and paradigms:0.5,Interaction devices:0.0,Interaction paradigms:0.0,Interaction techniques:0.0,Interactive systems and tools:0.0,Systems and tools for interaction design:0.0","HCI design and evaluation methods,Interaction design process and methods",HCI design and evaluation methods are relevant as the paper derives design principles from case studies. Interaction design process and methods are relevant for the integration framework. Empirical studies categories are partially relevant but secondary to the design focus.
5135,Predictors of Drop-Out in an Internet Study of Men Who Have Sex with Men,"Researchers have suggested the rising use of Internet to look for sexual partners is an important contributor to the resurgence in the incidence of sexually transmitted diseases (STDs) and HIV among men who have sex with men (MSM). Web-based samples of this population reflect good geographical diversity for research, but a high drop-out rate creates a significant potential for bias, misrepresentation of population, and misinterpretation of data. This study aims to describe the demographics of an Internet sample of MSM and the differences between the completers and drop-outs. We analyzed data obtained from a cross-sectional study using an online self-administered questionnaire for males, 18 years or older, who were current U.S. residents, and who had sex with men. Of 850 eligible participants, 404 (47.5%) were labeled as drop-outs. The completers and drop-outs differed significantly in age, education, country of birth, health insurance, time spent on Internet, location of computer access, types of sites visited, profile information, last homosexual experience, methods used to determine safety of partners, and type of sexual activities on real meets. Almost half of the participants dropped out before completion. Although the two groups did not differ in many of the items, high drop-out rates are a threat to the validity of such data. Drop-out constitutes a significant bias in Internet sexuality research and must be considered while interpreting the results of such studies.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:1.0,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.5",Human-centered computing,Human-centered computing is relevant for analyzing user behavior in online surveys. Social and professional topics is tangentially related but not the core focus. Other categories are irrelevant.,"Accessibility:0.0,Collaborative and social computing:0.0,Human computer interaction (HCI):0.5,Interaction design:0.0,Ubiquitous and mobile computing:0.25,Visualization:0.0",Human computer interaction (HCI),"Human computer interaction (HCI) is relevant as the study examines user behavior in online research, a key aspect of HCI. Ubiquitous and mobile computing is marginally relevant due to the focus on internet-based data collection, but the core contribution centers on understanding drop-out behavior, not the mobility aspect.","Empirical studies in HCI:1,HCI design and evaluation methods:0.2,HCI theory, concepts and models:0.2,Interaction devices:0.1,Interaction paradigms:0.1,Interaction techniques:0.2,Interactive systems and tools:0.3",Empirical studies in HCI,Empirical studies in HCI is highly relevant as the paper analyzes user behavior and drop-out patterns in an online study. Other categories like HCI design/methods are less relevant since the focus is on analyzing existing data rather than designing new interfaces.
3446,A robust integrated system for selecting and commanding multiple mobile robots,"We describe a system whereby multiple humans and mobile robots interact robustly using a combination of sensing and signalling modalities. Extending our previous work on selecting an individual robot from a population by face-engagement, we show that reaching toward a robot - a specialization of pointing - can be used to designate a particular robot for subsequent one-on-one interaction. To achieve robust operation despite frequent sensing problems, the robots use three phases of human detection and tracking, and emit audio cues to solicit interaction and guide the behaviour of the human. A series of real-world trials demonstrates the practicality of our approach.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.5,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:1.0,Computing methodologies:0.25,Applied computing:0.0,Social and professional topics:0.0",Human-centered computing,Human-centered computing is highly relevant as the paper focuses on human-robot interaction using multimodal signals. Software and its engineering receives a moderate score for system implementation. Computing methodologies gets a lower score as the paper's primary contribution is interaction design rather than algorithm development.,"Accessibility:0.0,Collaborative and social computing:1.0,Human computer interaction (HCI):1.0,Interaction design:0.0,Ubiquitous and mobile computing:0.0,Visualization:0.0","Human computer interaction (HCI),Collaborative and social computing",Human computer interaction is directly relevant as the paper discusses human-robot interaction. Collaborative and social computing is relevant as the system supports interaction between humans and robots. Other fields are not central.,"Collaborative and social computing design and evaluation methods:0,Collaborative and social computing devices:0,Collaborative and social computing systems and tools:0,Collaborative and social computing theory, concepts and paradigms:0,Empirical studies in HCI:0,Empirical studies in collaborative and social computing:0,HCI design and evaluation methods:0,HCI theory, concepts and models:0,Interaction devices:0,Interaction paradigms:0,Interaction techniques:1,Interactive systems and tools:1","Interactive systems and tools,Interaction techniques",The paper describes an interactive system for human-robot interaction using interaction techniques. Other categories like HCI theory are not the focus.
5648,Learning human behaviour patterns in work environments,"In this paper, we propose a flexible, human-oriented framework for learning the behaviour pattern of the users in work environments from visual sensors. The knowledge of human behaviour pattern enables the ambient environment to communicate with the user in a seamless way and make anticipatory decisions, from the automation of appliances and personal schedule reminder to the detection of unhealthy habits. Our learning method is general and learns from a set of activity sequences, where the granularity of activities can vary for different applications. Algorithms to extract the activity information from the videos are described. We evaluate our method on video sequences captured in a real office, where the user's daily routine is recorded over a month. The results show that our approach is capable of not only identifying the frequent behaviour of the user, but also the time relations and conditions of the activities.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.9,Computing methodologies:0.1,Applied computing:0.2,Social and professional topics:0.1",Human-centered computing,Human-centered computing: The paper studies human behavior pattern learning for ambient environments. Applied computing has limited relevance compared to the core human interaction focus.,"Accessibility:0.2,Collaborative and social computing:0.3,Human computer interaction (HCI):0.8,Interaction design:0.4,Ubiquitous and mobile computing:0.7,Visualization:0.5","Human computer interaction (HCI),Ubiquitous and mobile computing",Human computer interaction (HCI) is highly relevant as the paper focuses on ambient environments adapting to human behavior. Ubiquitous and mobile computing is relevant for its emphasis on seamless environmental interaction. Other categories like Accessibility or Visualization are less central to the core contribution.,"Empirical studies in HCI:0.7,Empirical studies in ubiquitous and mobile computing:1.0,HCI design and evaluation methods:0.5,HCI theory, concepts and models:0.3,Interaction devices:0.2,Interaction paradigms:0.4,Interaction techniques:0.3,Interactive systems and tools:0.6,Ubiquitous and mobile computing design and evaluation methods:1.0,Ubiquitous and mobile computing systems and tools:1.0,Ubiquitous and mobile computing theory, concepts and paradigms:0.8,Ubiquitous and mobile devices:0.4","Empirical studies in ubiquitous and mobile computing,Ubiquitous and mobile computing systems and tools",Empirical studies in ubiquitous and mobile computing (real-world behavior pattern analysis). Ubiquitous and mobile computing systems and tools (framework implementation for ambient environments). Other categories relate to general HCI or interaction techniques not central to the paper.
4902,A wearable 3D augmented reality workspace,"Describes our work to build a wearable augmented reality (AR) system that supports true stereoscopic 3D graphics. Through a pen and pad interface, well known 2D user interfaces can be presented to the user whereas the tracking of the pen allows us to use direct interaction with virtual objects. The system is assembled from off-the-shelf hardware components and serves as a basic test bed for user interface experiments related to collaboration between stationary and mobile AR users.","General and reference:0.1,Hardware:0.2,Computer systems organization:0.2,Networks:0.1,Software and its engineering:0.3,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.8,Computing methodologies:0.3,Applied computing:0.2,Social and professional topics:0.1",Human-centered computing,Human-centered computing is highly relevant as the paper describes a wearable AR system with a user interface. Other fields like Hardware or Software engineering are not central to the core contribution focused on human-computer interaction.,"Accessibility:0.2,Collaborative and social computing:0.3,Human computer interaction (HCI):0.8,Interaction design:0.7,Ubiquitous and mobile computing:0.7,Visualization:0.1","Human computer interaction (HCI),Ubiquitous and mobile computing","Human computer interaction is highly relevant for the wearable AR interface design. Ubiquitous and mobile computing is relevant due to the focus on mobile AR systems. Collaborative and social computing is marginally relevant for the collaboration aspect, but the core contribution is in interface design.","Empirical studies in HCI:0,Empirical studies in ubiquitous and mobile computing:0,HCI design and evaluation methods:0,HCI theory, concepts and models:0,Interaction devices:1,Interaction paradigms:0,Interaction techniques:0,Interactive systems and tools:1,Ubiquitous and mobile computing design and evaluation methods:0,Ubiquitous and mobile computing systems and tools:0,Ubiquitous and mobile computing theory, concepts and paradigms:0,Ubiquitous and mobile devices:0","Interaction devices,Interactive systems and tools",Interaction devices are relevant as the system uses a pen and pad interface. Interactive systems and tools are relevant for the wearable AR system's design. Other categories are unrelated to the hardware and interface focus.
2199,"It's not all about Green: energy use in low-income communities""","Personal energy consumption, specifically home energy consumption such as heating, cooling, and electricity, has been an important environmental and economic topic for decades. Despite the attention paid to this area, few researchers have specifically explored these issues within a community that makes up approximately 30% of U.S. households -- those below the federal poverty line. We present a study of 26 low-income households in two very different locations -- a small town in the Southern U.S. and a northerly metropolitan area. Through a photo-elicitation study and directed interviews, we explore the relationship between energy saving behaviors, external factors, and users' intrinsic values and beliefs. Most of our participants are committed to saving energy for non-financial reasons, even when not responsible for paying bills. Challenges to saving energy include safety and lack of control over the environment. We discuss how Ubicomp technologies for saving energy can address some of these challenges.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.2,Software and its engineering:0.15,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.2,Security and privacy:0.1,Human-centered computing:0.9,Computing methodologies:0.3,Applied computing:0.3,Social and professional topics:0.1",Human-centered computing,"Human-centered computing: The study focuses on energy-saving behaviors in low-income communities and their interaction with Ubicomp technologies. Other categories are less relevant as the paper is not about networks, software engineering, or computing methodologies.","Accessibility:0.2,Collaborative and social computing:0.4,Human computer interaction (HCI):0.8,Interaction design:0.6,Ubiquitous and mobile computing:0.9,Visualization:0.3","Ubiquitous and mobile computing,Human computer interaction (HCI)",Ubiquitous and mobile computing is highly relevant as the paper discusses Ubicomp technologies for energy savings. Human computer interaction (HCI) is relevant due to the study of user behaviors and interactions with energy systems. Collaborative and social computing is less relevant as the focus is not on social interactions but individual behaviors.,"Empirical studies in HCI:0.7,Empirical studies in ubiquitous and mobile computing:1.0,HCI design and evaluation methods:0.4,HCI theory, concepts and models:0.2,Interaction devices:0.1,Interaction paradigms:0.1,Interaction techniques:0.1,Interactive systems and tools:0.2,Ubiquitous and mobile computing design and evaluation methods:0.6,Ubiquitous and mobile computing systems and tools:0.3,Ubiquitous and mobile computing theory, concepts and paradigms:0.5,Ubiquitous and mobile devices:0.1","Empirical studies in ubiquitous and mobile computing,Empirical studies in HCI","Empirical studies in ubiquitous and mobile computing is central as the paper presents a field study of energy-saving behaviors using photo-elicitation and interviews. Empirical studies in HCI is relevant through behavioral analysis of low-income users. Other options like Interaction devices or Systems are not discussed, while the focus is on human factors rather than technical system design."
5232,Engaging the Periphery for Visual Communication on Mobile Phones,"While mobile phones have become ubiquitous instruments of communication and social interaction, they still require explicit interaction, placing high demands on attention. Engaging the periphery of users' attention offers opportunities for awareness and interaction while reducing demands on attention and risks of disruption. We explore the mobile peripheral design space with Emotipix, an application for camera phones that turns the background of the phone's display into a place for visual conversations. We conducted an exploratory 2-week user study with 6 pairs and one 4-person group, and found that Emotipix facilitated ongoing social practices. Our study shows that there is an unexploited opportunity to use mobile phones for peripheral awareness. We provide recommendations for managing users' expectations, desires for control, and privacy in mobile peripheral display design.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.9,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Human-centered computing,"The study explores peripheral awareness in mobile phone design and user interaction, directly aligning with 'Human-centered computing.' Other categories are irrelevant as the focus is on user behavior and interface design.","Accessibility:0.3,Collaborative and social computing:0.75,Human computer interaction (HCI):0.9,Interaction design:0.65,Ubiquitous and mobile computing:0.85,Visualization:0.5","Human computer interaction (HCI),Collaborative and social computing,Ubiquitous and mobile computing",Human computer interaction (HCI) is central for mobile interface design. Collaborative and social computing addresses social practices. Ubiquitous and mobile computing is relevant for mobile phone context. Visualization is secondary to the interaction focus.,"Collaborative and social computing design and evaluation methods:0.6,Collaborative and social computing devices:0.3,Collaborative and social computing systems and tools:0.5,Collaborative and social computing theory, concepts and paradigms:0.2,Empirical studies in HCI:0.2,Empirical studies in collaborative and social computing:0.6,Empirical studies in ubiquitous and mobile computing:0.7,HCI design and evaluation methods:0.4,HCI theory, concepts and models:0.2,Interaction devices:0.3,Interaction paradigms:0.2,Interaction techniques:0.3,Interactive systems and tools:0.5,Ubiquitous and mobile computing design and evaluation methods:0.4,Ubiquitous and mobile computing systems and tools:0.8,Ubiquitous and mobile computing theory, concepts and paradigms:0.3,Ubiquitous and mobile devices:0.6","Empirical studies in ubiquitous and mobile computing,Ubiquitous and mobile computing systems and tools",Empirical studies in ubiquitous and mobile computing is relevant for the 2-week user study. Ubiquitous and mobile computing systems and tools is relevant for the Emotipix application design. Other categories like social computing theory or interaction devices are less central than the mobile-specific contributions.
1488,System for Presenting and Creating Smell Effects to Video,"Olfaction has recently been gaining attention in information and communication technology, as shown by attempts in theaters to screen videos while emitting scents. However, because there is no current infrastructure to communicate and synchronize odor information with visual information, people cannot enjoy this experience at home. Therefore, we have constructed a system of smell videos which could be applied to television (TV), allowing viewers to experience scents while watching their videos. To solve the abovementioned technical problems, we propose using the existing system for broadcasting closed caption. Our system's implementation is mindful of both video viewers and producers, allowing the system on the viewer end to disperse odorants in synchronization with videos, and allowing producers to add odor information to videos. We finally verify the system's feasibility. We expect that this study will make smell videos become common, and people will enjoy ones in daily life in the near future.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.9,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Human-centered computing,Human-centered computing: Focuses on integrating olfactory feedback with video for user experience. Other categories like Software or Networks are secondary to the system's design for human interaction.,"Accessibility:0.1,Collaborative and social computing:0.1,Human computer interaction (HCI):0.9,Interaction design:0.1,Ubiquitous and mobile computing:0.8,Visualization:0.1","Human computer interaction (HCI),Ubiquitous and mobile computing",Human computer interaction (HCI) is relevant for the smell-video system enhancing user experience. Ubiquitous and mobile computing is relevant as the system enables home-based interaction. Categories like Visualization are irrelevant as the focus is on olfactory rather than visual interfaces.,"Empirical studies in HCI:0.4,Empirical studies in ubiquitous and mobile computing:0.3,HCI design and evaluation methods:1.0,HCI theory, concepts and models:0.6,Interaction devices:0.8,Interaction paradigms:0.5,Interaction techniques:0.7,Interactive systems and tools:0.9,Ubiquitous and mobile computing design and evaluation methods:0.5,Ubiquitous and mobile computing systems and tools:0.7,Ubiquitous and mobile computing theory, concepts and paradigms:0.4,Ubiquitous and mobile devices:0.8","HCI design and evaluation methods,Interactive systems and tools,Interaction devices",HCI design and evaluation methods: System design and testing described. Interactive systems and tools: New interactive system developed. Interaction devices: Smell emission hardware involved. Other options: Ubiquitous computing aspects are secondary.
1052,Brain activity measurement during program comprehension with NIRS,"Near infrared spectroscopy (NIRS) has been used as a low cost, noninvasive method to measure brain activity. In this paper, we experiment to measure the effects of variables and controls in a source code to the brain activity in program comprehension. The measurement results are evaluated after noise reduction and normalization to statistical analysis. As the result of the experiment, significant differences in brain activity were observed at a task that requires memorizing variables to understand a code snippet. On the other hand, no significant differences between different levels of mental arithmetic tasks were observed. We conclude that the frontal pole reflects workload to short-term memory caused by variables without affected from calculation.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.85,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Human-centered computing,Human-centered computing is highly relevant as the paper studies brain activity during programming tasks using NIRS. Other categories like Information systems are less relevant as the focus is on cognitive processes rather than data management.,"Accessibility:0.1,Collaborative and social computing:0.1,Human computer interaction (HCI):0.9,Interaction design:0.2,Ubiquitous and mobile computing:0.1,Visualization:0.5",Human computer interaction (HCI),"Human computer interaction (HCI) is highly relevant as the paper studies brain activity during program comprehension, a core aspect of user interaction with computing systems. Other children like Accessibility or Interaction design are not addressed in the paper's focus on measuring cognitive load rather than interface design or accessibility features.","Empirical studies in HCI:1.0,HCI design and evaluation methods:0.6,HCI theory, concepts and models:0.5,Interaction devices:0.7,Interaction paradigms:0.4,Interaction techniques:0.6,Interactive systems and tools:0.7",Empirical studies in HCI,"Empirical studies in HCI: The paper presents an empirical study using NIRS to measure brain activity during program comprehension. Other categories like Interaction devices are less relevant as the focus is on cognitive evaluation, not interaction design."
861,Clarification dialogues in human-augmented mapping,An approach to dialogue based interaction for resolution of ambiguities encountered as part of Human-Augmented Mapping (HAM) is presented. The paper focuses on issues related to spatial organisation and localisation. The dialogue pattern naturally arises as robots are introduced to novel environments. The paper discusses an approach based on the notion of Questions under Discussion (QUD). The presented approach has been implemented on a mobile platform that has dialogue capabilities and methods for metric SLAM. Experimental results from a pilot study clearly demonstrate that the system can resolve problematic situations.,"General and reference:0.2,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.9,Computing methodologies:0.3,Applied computing:0.3,Social and professional topics:0.1",Human-centered computing,"Human-centered computing is directly relevant because the paper focuses on dialogue systems for human-robot interaction in mapping tasks. Applied computing is somewhat relevant due to the practical implementation, but the core contribution is in human-robot interaction design.","Accessibility:0.1,Collaborative and social computing:0.2,Human computer interaction (HCI):0.8,Interaction design:0.7,Ubiquitous and mobile computing:0.9,Visualization:0.3","Human computer interaction (HCI),Ubiquitous and mobile computing",Human computer interaction (HCI) is relevant for the dialogue-based interaction in HAM. Ubiquitous and mobile computing is relevant for the implementation on a mobile platform. Collaborative computing and visualization are less central to the core contribution in dialogue design.,"Empirical studies in HCI:0.3,Empirical studies in ubiquitous and mobile computing:0.5,HCI design and evaluation methods:0.4,HCI theory, concepts and models:0.6,Interaction devices:0.2,Interaction paradigms:0.5,Interaction techniques:1.0,Interactive systems and tools:0.8,Ubiquitous and mobile computing design and evaluation methods:0.7,Ubiquitous and mobile computing systems and tools:1.0,Ubiquitous and mobile computing theory, concepts and paradigms:0.7,Ubiquitous and mobile devices:0.6","Interaction techniques,Ubiquitous and mobile computing systems and tools",Interaction techniques apply to dialogue-based clarification. Ubiquitous systems are relevant for mobile platform implementation. Other categories are secondary.
2242,A Review of Ubiquitous Applications and Technologies Oriented Towards Disabled and Senior People,"Forthcoming wireless networks will combine high capacity heterogeneous networks, with almost universal availability, with the property of roaming and interaction aimed to mobile users. In this context of ubiquitous networking, applied research will undoubtedly bring the attention to the enduser, who will become the center of services and applications. People-aware scenarios will offer services according to the user profile. Indeed, specific collectives, such as disabled/handicapped or senior people, may benefit specially from these smart wireless environments, due to their property of adaptivity. In this paper we aim to review the most important projects and technologies in this field. We provide a brief taxonomy of the most prominent developments in the area, including a description of envisioned and current systems, and a discussion on wireless technology issues and profile management and standardization.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.9,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Human-centered computing,Human-centered computing is highly relevant as the paper focuses on ubiquitous applications tailored for disabled and senior users. Other categories lack direct connection to the user-centric design focus.,"Accessibility:0.9,Collaborative and social computing:0.2,Human computer interaction (HCI):0.6,Interaction design:0.4,Ubiquitous and mobile computing:0.8,Visualization:0.1","Accessibility,Ubiquitous and mobile computing",Accessibility is directly addressed through technologies for disabled/senior users. Ubiquitous and mobile computing applies to the wireless environment context. Human computer interaction receives a lower score as the paper focuses on technology overview rather than specific interaction design.,"Accessibility technologies:1,Ubiquitous and mobile computing systems and tools:0.9,Ubiquitous and mobile computing theory, concepts and paradigms:0.7,Empirical studies in ubiquitous and mobile computing:0.6,Accessibility design and evaluation methods:0.5,Empirical studies in accessibility:0.4,Ubiquitous and mobile devices:0.3","Accessibility technologies,Ubiquitous and mobile computing systems and tools",Accessibility technologies is primary due to focus on systems for disabled/senior users. Ubiquitous systems are relevant for the wireless environment context. Other categories like design methods are secondary.
3000,On the Prototyping of an ICT-Enhanced Toilet System for Assisting Older Persons Living Independently and Safely at Home,"Standard toilets often do not meet the needs of a significant number of older persons and persons with disabilities. The EU funded iToilet project aims at design and development of a new type of ICT enhanced modular toilet system which shall be able to support autonomy, dignity and safety of older persons living at home. Methodologically the project started with gathering user requirements by means of questionnaires, interviews and focus group discussion involving a total of 74 persons, thereof 41 subjects with movement disorders (primary users), 21 caregivers (secondary users) and 12 healthcare managers (tertiary users). Most important wishes were bilateral removable handrails, height and tilt adjustment, emergency detection, simplicity. In parallel to the ongoing technical development participatory design activities have been carried out at user test sites in order to continuously involve users into the design process and to allow quick feedback with regards to early prototype parts. The project currently is working on the finalization of the first prototype ready to enter the lab trial stage in spring 2017. The experiences will be used for redesigning a prototype 2 which is planned to be tested in real life settings early 2018.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.9,Computing methodologies:0.3,Applied computing:0.1,Social and professional topics:0.1",Human-centered computing,"Human-centered computing: The paper describes an ICT-enhanced toilet system designed through participatory design with older users, focusing on human-centered design principles. Applied computing and Computing methodologies were rejected because the primary contribution is the user-centered design process, not computational techniques or applications.","Accessibility:0.9,Collaborative and social computing:0.2,Human computer interaction (HCI):0.8,Interaction design:0.7,Ubiquitous and mobile computing:0.3,Visualization:0.1","Accessibility,Human computer interaction (HCI)",Accessibility and Human computer interaction are central to ICT-enhanced toilet design for elderly users. Other categories are less relevant to the core user-centered design focus.,"Accessibility design and evaluation methods:0.9,Accessibility systems and tools:0.7,Accessibility technologies:0.8,Accessibility theory, concepts and paradigms:0.6,Empirical studies in HCI:0.2,Empirical studies in accessibility:0.9,HCI design and evaluation methods:0.3,HCI theory, concepts and models:0.2,Interaction devices:0.4,Interaction paradigms:0.2,Interaction techniques:0.3,Interactive systems and tools:0.4","Accessibility design and evaluation methods,Empirical studies in accessibility",Accessibility design and evaluation methods is relevant because the paper describes user-centered design and evaluation of an ICT-enhanced toilet. Empirical studies in accessibility is relevant because the paper reports on user requirements gathered through questionnaires and focus groups. Other categories like HCI design or Interaction devices are less relevant as the focus is specifically on accessibility for older persons.
2326,WebQuilt: a framework for capturing and visualizing the web experience,"is a web logging and visualization system that helps web design teams run usability tests (both local and remote) and analyze the collected data. Logging is done through a proxy, overcoming many of the problems with server-side and client-side logging. Captured usage traces can be aggregated and visualized in a zooming interface that shows the web pages people viewed. The visualization also shows the most common paths taken through the website for a given task, as well as the optimal path for that task as designated by the designer. This paper discusses the architecture of WebQuilt and also describes how it can be extended for new kinds of analyses and visualizations.","General and reference:0.0,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.8,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Human-centered computing,"Human-centered computing: WebQuilt is a tool for usability testing and visualization of web interactions. Other categories like Networks are irrelevant as the focus is on user experience and interface design, not communication protocols.","Accessibility:0.2,Collaborative and social computing:0.1,Human computer interaction (HCI):0.9,Interaction design:0.2,Ubiquitous and mobile computing:0.3,Visualization:0.8","Human computer interaction (HCI),Visualization","Human computer interaction (HCI) is central to usability testing tools like WebQuilt. Visualization is critical for the system's interface. Accessibility is less relevant as the focus is on logging/analysis, not accessibility features.","Empirical studies in HCI:0,Empirical studies in visualization:0,HCI design and evaluation methods:1,HCI theory, concepts and models:0,Interaction devices:0,Interaction paradigms:0,Interaction techniques:0,Interactive systems and tools:1,Visualization application domains:0,Visualization design and evaluation methods:0,Visualization systems and tools:1,Visualization techniques:0,Visualization theory, concepts and paradigms:0","HCI design and evaluation methods,Interactive systems and tools,Visualization systems and tools","HCI design and evaluation methods: The paper discusses a system (WebQuilt) designed for usability testing and data analysis. Interactive systems and tools: The framework is implemented as a tool for capturing and visualizing web experiences. Visualization systems and tools: The system includes a visualization interface for analyzing user paths and optimal tasks. Other children are irrelevant as the focus is on system design and usability testing, not interaction techniques or theory."
3525,A relation between voice quality and acoustic features,". The present study deals with a voice quality that is referred to as “intelligible” or “articulate” voice. The purpose of the present study is to find a relation between the voice quality and acoustic features. The speech material is a read version of a simple short Japanese sentence “a o i so ra” in order to eliminate these linguistic variations. A set of speech samples uttered by fifteen adult male speakers including TV/Radio professional casters has been used. A perceptual similarity judgment on voice quality has been performed first and then a correlation between the result of the similarity and acoustic distances has been taken to find out how the acoustic features relate to the voice quality. Two types of acoustic features, time-varying dynamic parameter and static parameter, have been considered for each of the source and resonance characteristics. From source characteristics, 1) pitch contour (dynamic parameter), 2) mean pitch frequency (static parameter) are chosen. From the resonance characteristics, 3) formant trajectory (dynamic parameter), 4) mean spectrum envelope (static parameter), are chosen. The highest correlation is observed for the formant trajectory followed by the pitch contour. The third highest is the mean spectrum envelope and the lowest is, as it is expected, the mean pitch frequency. This result reveals that the dynamic parameters, especially formant movement, do contribute to the “clear” or “articulate” voice quality.","General and reference:0.1,Hardware:0.2,Computer systems organization:0.3,Networks:0.2,Software and its engineering:0.3,Theory of computation:0.2,Mathematics of computing:0.4,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.85,Computing methodologies:0.4,Applied computing:0.3,Social and professional topics:0.1",Human-centered computing,"Human-centered computing is highly relevant as the study examines perceptual voice quality and its relation to acoustic features. Other categories like Mathematics of computing are less central, as the paper focuses on human perception rather than mathematical theory.","Accessibility:0.0,Collaborative and social computing:0.0,Human computer interaction (HCI):1.0,Interaction design:0.0,Ubiquitous and mobile computing:0.0,Visualization:0.0",Human computer interaction (HCI),Human computer interaction is relevant for studying voice quality perception. Other categories like Interaction design or Accessibility are not directly addressed in the acoustic feature analysis.,"Empirical studies in HCI:0.8,HCI design and evaluation methods:0.7,HCI theory, concepts and models:0.1,Interaction devices:0.1,Interaction paradigms:0.1,Interaction techniques:0.1,Interactive systems and tools:0.1","Empirical studies in HCI,HCI design and evaluation methods",Empirical studies in HCI is relevant as the paper uses perceptual similarity judgments and correlation analysis. HCI design and evaluation methods is relevant for the evaluation of acoustic features in relation to voice quality. Other interaction categories are not directly related.
2979,Do Variations in Agency Indirectly Affect Behavior with Others? An Analysis of Gaze Behavior,"In a group setting, it is possible for attributes of one group member to indirectly affect how other group members are perceived. In this paper, we explore whether one group member's agency (e.g. if they are real or virtual) can indirectly affect behavior with other group members. We also consider whether variations in the agency of a group member directly affects behavior with that group member. To do so, we examined gaze behavior during a team training exercise, in which sixty-nine nurses worked with a surgeon and an anesthesiologist to prepare a simulated patient for surgery. The agency of the surgeon and the anesthesiologist were varied between conditions. Nurses' gaze behavior was coded using videos of their interactions. Agency was observed to directly affect behavior, such that participants spent more time gazing at virtual teammates than human teammates. However, participants continued to obey polite gaze norms with virtual teammates. In contrast, agency was not observed to indirectly affect gaze behavior. The presence of a second human did not affect participants' gaze behavior with virtual teammates.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.95,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Human-centered computing,"Human-centered computing: The paper studies how variations in agency (human vs. virtual agents) affect human behavior and gaze norms in collaborative settings, which is central to human-computer interaction and social computing. Other categories are irrelevant as the focus is on behavioral analysis, not technical systems or algorithms.","Accessibility:0.1,Collaborative and social computing:0.9,Human computer interaction (HCI):0.7,Interaction design:0.1,Ubiquitous and mobile computing:0.1,Visualization:0.1",Collaborative and social computing,Collaborative and social computing receives 0.9 for analyzing group behavior with mixed human/virtual agents. Human computer interaction scores 0.7 for the interaction context. Other fields are irrelevant.,"Collaborative and social computing design and evaluation methods:0,Collaborative and social computing devices:0,Collaborative and social computing systems and tools:0,Collaborative and social computing theory, concepts and paradigms:0,Empirical studies in collaborative and social computing:1",Empirical studies in collaborative and social computing,Empirical studies in collaborative and social computing: The paper presents an empirical analysis of gaze behavior in group settings. Other categories like systems/tools or theory are irrelevant as the focus is on behavioral analysis rather than system design or theoretical frameworks.
629,Toward Disambiguating Multiple Selections for Frustum-Based Pointing,"These environments often simulate or augment the real world, and a part of that simulation is the ability to select objects for observation and manipulation. Many user interfaces for these applications depend on six-degree-of-freedom tracking devices. Such devices have limited accuracy and are susceptible to noise, giving an imprecision that makes object selections difficult and hard to repeat. This difficulty is amplified when the user’s viewpoint is also tracked, meaning the user must compensate for noise from both the head tracker and the pointing device when performing object selection. Also, users may experience fatigue when using handheld pointing devices for extended periods, creating error even if the tracking technology were perfect. This paper presents a pointing-based probabilistic selection algorithm that addresses some of the ambiguities associated with tracking and user imprecision. It performs multiple selections by considering a frustum along the user’s pointing direction and the hierarchical structure of the database. It assigns probabilities that the user has selected particular objects using a set of low-level 3D intersection-based selection techniques and the relationship of the objects in a hierarchical database, and makes the final selection using one of several weighting schemes. We performed several experiments to evaluate the low-level selection techniques, tested several weighting schemes for the integration algorithm, and we show that the algorithm is effective at disambiguating multiple selections.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.2,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.9,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Human-centered computing,Human-centered computing is relevant for 3D user interface design. Other categories lack interaction design focus.,"Accessibility:0.1,Collaborative and social computing:0.1,Human computer interaction (HCI):0.9,Interaction design:0.7,Ubiquitous and mobile computing:0.1,Visualization:0.1","Human computer interaction (HCI),Interaction design",Human computer interaction (HCI) is central to addressing selection ambiguities in 3D interfaces. Interaction design is relevant for the probabilistic selection algorithm. Other categories like Accessibility are not directly discussed.,"Empirical studies in HCI:0.6,Empirical studies in interaction design:0.5,HCI design and evaluation methods:0.8,HCI theory, concepts and models:0.7,Interaction design process and methods:0.4,Interaction design theory, concepts and paradigms:0.3,Interaction devices:0.9,Interaction paradigms:1.0,Interaction techniques:1.0,Interactive systems and tools:0.7,Systems and tools for interaction design:0.5","Interaction techniques,Interaction paradigms",Interaction techniques: The paper introduces a probabilistic selection algorithm for 3D interaction. Interaction paradigms: The frustum-based approach represents a new interaction paradigm for disambiguating selections. 'HCI design and evaluation methods' is secondary as the focus is on technique rather than evaluation methodology.
1620,Human-computer interaction: introduction and overview,"The objective of this special introductory seminar is to provide newcomers to Human-Computer Interaction (HCI) with an introduction and overview of the field. The material will begin with a brief history of the field, followed by presentation and discussion of how good application development methods pull on the interdisciplinary technologies of HCI. The topics will include the psychology of human-computer interaction, psychologically-based design methods and tools, user interface media and tools, and introduction to user interface architecture.","General and reference:0,Hardware:0,Computer systems organization:0,Networks:0,Software and its engineering:0,Theory of computation:0,Mathematics of computing:0,Information systems:0,Security and privacy:0,Human-centered computing:1,Computing methodologies:0,Applied computing:0,Social and professional topics:0",Human-centered computing,"Human-centered computing: The paper is an introduction to Human-Computer Interaction (HCI), a core area of human-centered computing.","Human computer interaction (HCI):1.0,Interaction design:0.6,Visualization:0.3",Human computer interaction (HCI),"Human computer interaction (HCI): The paper provides an introductory overview of the HCI field, its history, and interdisciplinary approaches. Interaction design/Visualization: Less relevant as the focus is on foundational concepts rather than specific design or visualization techniques.","Empirical studies in HCI:0.3,HCI design and evaluation methods:0.8,HCI theory, concepts and models:0.7,Interaction devices:0.2,Interaction paradigms:0.6,Interaction techniques:0.5,Interactive systems and tools:0.4","HCI theory, concepts and models,HCI design and evaluation methods","HCI theory, concepts and models: The paper provides an overview of foundational HCI concepts and psychology. HCI design and evaluation methods: It discusses application development methods and design principles. Other children are less directly relevant as the focus is on theoretical foundations rather than specific interaction techniques or devices."
2411,Demonstrating generation Y interactions through interactive prototyping,"With Generation Y entering the workforce, for the first time IT tools at home are richer in interaction than tools at work. This study aims to demonstrate novel Generation Y interactions by mapping three interaction qualities identified in private and work contexts. In an interactive prototyping course three prototypes were built in which these qualities are demonstrated. From these prototypes, guidelines for supporting Generation Y interactions in future office contexts, were subsequently deducted.","General and reference:0.2,Hardware:0.3,Computer systems organization:0.4,Networks:0.3,Software and its engineering:0.5,Theory of computation:0.3,Mathematics of computing:0.4,Information systems:0.3,Security and privacy:0.2,Human-centered computing:0.9,Computing methodologies:0.5,Applied computing:0.4,Social and professional topics:0.2",Human-centered computing,"Human-centered computing is highly relevant as the paper focuses on interaction design for Generation Y. Applied computing is secondary if the focus were on implementation, but the core is user interaction.","Accessibility:0.1,Collaborative and social computing:0.1,Human computer interaction (HCI):0.85,Interaction design:0.95,Ubiquitous and mobile computing:0.1,Visualization:0.1","Human computer interaction (HCI),Interaction design",Human computer interaction (HCI) is relevant for studying user interactions. Interaction design is central as the paper focuses on prototyping interaction qualities. Other fields like Accessibility or Visualization are not directly addressed.,"Empirical studies in HCI:0.5,Empirical studies in interaction design:0.5,HCI design and evaluation methods:0.3,HCI theory, concepts and models:0.3,Interaction design process and methods:1,Interaction design theory, concepts and paradigms:0.3,Interaction devices:0.3,Interaction paradigms:0.3,Interaction techniques:1,Interactive systems and tools:0.3,Systems and tools for interaction design:0.3","Interaction design process and methods,Interaction techniques",Interaction design process is core for prototyping. Interaction techniques demonstrate specific qualities. Empirical studies are secondary as the focus is on prototyping and guidelines.
5392,Acquisition of Human Feelings in Music Arrangements,"We often make decisions based on our feelings, which are implicit and very difficult to express as knowledge. This paper details an attempt to acquire feelings automatically. We assume that some relations or constraints exist between impressions felt and situations, which consist of an object and its environment. For example, in music arrangement, the object is a music score and its environment contains listeners, etc. Our project validates this assumption through three levels of experiments. At the first level, a program simply mimics human arrangements in order to transfer their impressions to another arrangement. This implies that the program is capable of distinguishing patterns that result in some impressions. At the second level, in order to produce a music recognition model, the program locates relations and constraints between a music score and its impressions, by which we show that machine learning techniques may provide a powerful tool for composing music and analyzing human feelings. Finally, we examine the generality of the model by modifying some arrangements to provide the subjects with a specified impression.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.2,Security and privacy:0.1,Human-centered computing:0.8,Computing methodologies:0.2,Applied computing:0.2,Social and professional topics:0.1",Human-centered computing,"Human-centered computing: The paper focuses on modeling human emotions and feelings in music through computational methods. Other categories like Information systems or Applied computing are less central to the core contribution, which is about human perception and interaction with music.","Accessibility:0.2,Collaborative and social computing:0.6,Human computer interaction (HCI):0.8,Interaction design:0.9,Ubiquitous and mobile computing:0.3,Visualization:0.1","Interaction design,Human computer interaction (HCI)",Interaction design and HCI are relevant as the study models human feelings in music through interaction. Collaborative computing is somewhat relevant due to user studies. Other categories like Accessibility or Visualization are not directly addressed.,"Empirical studies in HCI:1,Empirical studies in interaction design:0,HCI design and evaluation methods:0,HCI theory, concepts and models:1,Interaction design process and methods:0,Interaction design theory, concepts and paradigms:0,Interaction devices:0,Interaction paradigms:0,Interaction techniques:0,Interactive systems and tools:0,Systems and tools for interaction design:0","Empirical studies in HCI,HCI theory, concepts and models",Empirical studies in HCI is relevant due to the experimental validation of human feelings in music. HCI theory is relevant for modeling human-computer interaction concepts. Other categories do not align with the paper's focus on empirical analysis and theoretical modeling.
2291,Using Stories to Teach Human Values to Artificial Agents,"Value alignment is a property of an intelligent agent indicating that it can only pursue goals that are beneficial to humans. Successful value alignment should ensure that an artificial general intelligence cannot intentionally or unintentionally perform behaviors that adversely affect humans. This is problematic in practice since it is difficult to exhaustively enumerated by human programmers. In order for successful value alignment, we argue that values should be learned. In this paper, we hypothesize that an artificial intelligence that can read and understand stories can learn the values tacitly held by the culture from which the stories originate.We describe preliminary work on using stories to generate a value-aligned reward signal for reinforcement learning agents that prevents psychotic-appearing behavior.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.9,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Human-centered computing,Human-centered computing is relevant because the paper focuses on aligning AI with human values through cultural stories. Categories like Software engineering or Theory of computation are not central to the core contribution of value alignment in AI.,"Human computer interaction (HCI):1.0,Collaborative and social computing:0.5,Interaction design:0.2,Accessibility:0.2,Ubiquitous and mobile computing:0.2,Visualization:0.2",Human computer interaction (HCI),Human computer interaction is relevant because the paper focuses on teaching values to AI through story-based interaction. Collaborative computing is tangentially relevant but not central to the core contribution.,"Empirical studies in HCI:0.3,HCI design and evaluation methods:0.5,HCI theory, concepts and models:0.8,Interaction devices:0.2,Interaction paradigms:0.4,Interaction techniques:0.3,Interactive systems and tools:0.4","HCI theory, concepts and models",HCI theory is relevant for the conceptual framework of value alignment. Other categories like design methods are less directly applicable. The paper focuses on theoretical aspects rather than empirical studies or specific interaction techniques.
93,Realising aura for initiating interactions in real environments,"Information overload and spam are serious problems that plague our society today. In the advent of context aware systems and applications, these problems will inevitably continue to cause anguish among users. The problems stem primarily from the lack of user control over the interactions and services they receive. In search of a feasible solution to enable user controllable trade-offs over interactions, we study how the spatial model of interaction from the virtual environment domain addresses these interaction issues and find that recent research that uses similar concepts from the spatial model of interactions does not fully make use of the concept of Aura from the spatial model. This paper presents a modified spatial model of interactions focused on the abstract concept of Aura and tailored for implementation in real environments to resolve the interaction issues in context aware systems. We also conceptualise a system architecture to implement the modified spatial model of interaction and develop a fully functional prototype application as a proof of concept to demonstrate the model's operational feasibility.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.9,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Human-centered computing,"Human-centered computing: The paper focuses on user interactions and control in context-aware systems. Other categories are irrelevant as the paper doesn't focus on software engineering, networks, or theory.","Human computer interaction (HCI):1.0,Interaction design:0.75,Ubiquitous and mobile computing:0.5,Accessibility:0.2,Collaborative and social computing:0.2,Visualization:0.2",Human computer interaction (HCI),Human computer interaction (HCI) is highly relevant as the paper addresses user-controllable interaction models in context-aware systems. Other categories are less central to the core contribution.,"Empirical studies in HCI:0,HCI design and evaluation methods:0,HCI theory, concepts and models:1,Interaction devices:0,Interaction paradigms:0,Interaction techniques:0,Interactive systems and tools:0","HCI theory, concepts and models","HCI theory, concepts and models is relevant for the modified spatial model of interactions. Other categories like Interaction paradigms or devices are not central to the paper's contribution."
4114,Afraid to ask: proactive assistance with healthcare documents using eye tracking,"We investigate gaze patterns and other nonverbal behavior that people use when providing and receiving explanations of complex healthcare documents, and use a model of this behavior as the basis of a system that provides automated, proactive assistance. We present the results of the human analog study along with results from a preliminary evaluation of the automated system. We also demonstrate the feasibility of using eye tracking to automatically assess the health literacy of people reading healthcare documents.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.2,Security and privacy:0.1,Human-centered computing:1.0,Computing methodologies:0.1,Applied computing:0.3,Social and professional topics:0.1",Human-centered computing,"The paper uses eye tracking for healthcare document assistance, a core Human-Centered Computing topic. Applied Computing is secondary but relevant.","Human computer interaction (HCI):0.9,Accessibility:0.6,Collaborative and social computing:0.3,Interaction design:0.5,Ubiquitous and mobile computing:0.4,Visualization:0.3",Human computer interaction (HCI),Human computer interaction (HCI) (0.9) - The paper focuses on eye tracking for proactive assistance in healthcare document interaction. Accessibility (0.6) is partially relevant but less central than HCI. Other categories like Ubiquitous computing are not directly addressed.,"Empirical studies in HCI:0.8,HCI design and evaluation methods:1,HCI theory, concepts and models:0.5,Interaction devices:0.4,Interaction paradigms:0.4,Interaction techniques:0.7,Interactive systems and tools:0.6","HCI design and evaluation methods,Empirical studies in HCI",HCI design and evaluation methods is highly relevant as the paper designs and evaluates an interactive system. Empirical studies in HCI is relevant due to the empirical evaluation of the system. Interaction techniques and devices are moderately relevant. Other options are less relevant as the focus is on system design and evaluation rather than general paradigms or devices.
2843,Network Sharing beyond Knowledge Sharing: The Mediating Role of Tertius Iungens Orientation in Social Media Contexts,"This study examines the influences of knowledge-sharing activities and tertius iungens orientation on job performance within social media environments. Through empirical analysis, it finds that knowledge self-efficacy, social interaction ties, and the norm of reciprocity positively influence knowledge-sharing activities in social media environments, and that social interaction ties and the norm of reciprocity contribute to the establishment of an individual's tertius iungens orientation only through knowledge-sharing activities. In terms of job performance, it shows that tertius iungens orientation plays a mediating role in the relationship between knowledge-sharing activities and individual job performance. The results are expected to provide important theoretical and practical implications for researchers and practitioners respectively.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.9,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Human-centered computing,Human-centered computing: The study examines social media interactions and their impact on job performance. Other categories are irrelevant as the focus is on human behavior rather than algorithms or hardware.,"Accessibility:0.2,Collaborative and social computing:1.0,Human computer interaction (HCI):0.3,Interaction design:0.4,Ubiquitous and mobile computing:0.3,Visualization:0.2",Collaborative and social computing,Collaborative and social computing is highly relevant as the study focuses on knowledge-sharing and tertius iungens orientation in social media contexts. Other options like HCI or Interaction design are less central as the paper emphasizes organizational and social dynamics over technical interaction design.,"Collaborative and social computing design and evaluation methods:0.1,Collaborative and social computing devices:0.1,Collaborative and social computing systems and tools:0.2,Collaborative and social computing theory, concepts and paradigms:0.5,Empirical studies in collaborative and social computing:1.0","Empirical studies in collaborative and social computing,Collaborative and social computing theory, concepts and paradigms",Empirical studies is directly relevant as the paper presents an empirical analysis of social media effects. The theory category applies to the conceptual framework of tertius iungens. Other categories are rejected as the paper does not focus on system design or devices.
5227,The Effects of Dynamic Transparency on Targeting Performance,"Transparency can be used to increase the visibility of a user’s workspace in situations where the space is obscured by floating windows and tool palettes. Dynamic transparency takes this approach further by making components more transparent when the user’s cursor is far away. However, dynamic transparency may make palettes and floating windows more difficult to target. We carried out a study to test the effects of different types of dynamic transparency on targeting performance. We found that although targeting time does increase as targets become more transparent, the increases are small – often less than ten percent. Our study suggests reasonable maximum, minimum, and default transparency levels for designers of dynamic transparency schemes.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.9,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Human-centered computing,"The study investigates dynamic transparency in user interfaces and its impact on targeting performance, which directly relates to human-computer interaction and user interface design under 'Human-centered computing.'","Accessibility:0.2,Collaborative and social computing:0.1,Human computer interaction (HCI):0.9,Interaction design:0.85,Ubiquitous and mobile computing:0.3,Visualization:0.1","Human computer interaction (HCI),Interaction design",Human computer interaction (HCI) is directly relevant as the paper studies user interface performance. Interaction design is relevant for exploring transparency in UI. Other categories lack core focus on interface interaction.,"Empirical studies in HCI:1.0,Empirical studies in interaction design:0.0,HCI design and evaluation methods:1.0,HCI theory, concepts and models:0.0,Interaction design process and methods:0.0,Interaction design theory, concepts and paradigms:0.0,Interaction devices:0.0,Interaction paradigms:0.0,Interaction techniques:1.0,Interactive systems and tools:1.0,Systems and tools for interaction design:0.0","Empirical studies in HCI,HCI design and evaluation methods",Empirical studies in HCI: The paper is an empirical evaluation of a UI technique. HCI design and evaluation methods: The study evaluates the effectiveness of a design (dynamic transparency).
3264,Highbrow Films Gather Dust: Time-Inconsistent Preferences and Online DVD Rentals,"We report on a field study demonstrating systematic differences between the preferences people anticipate they will have over a series of options in the future and their subsequent revealed preferences over those options. Using a novel panel data set, we analyze the film rental and return patterns of a sample of online DVD rental customers over a period of four months. We predict and find that should DVDs (e.g., documentaries) are held significantly longer than want DVDs (e.g., action films) within customer. Similarly, we also predict and find that people are more likely to rent DVDs in one order and return them in the reverse order when should DVDs are rented before want DVDs. Specifically, a 1.3% increase in the probability of a reversal in preferences (from a baseline rate of 12%) ensues if the first of two sequentially rented movies has more should and fewer want characteristics than the second film. Finally, we find that as the same customers gain more experience with online DVD rentals, the extent to which they hold should films longer than want films decreases. Our results suggest that present bias has a meaningful impact on choice in the field, and that people may learn about their present bias with experience and, as a result, gain the capacity to curb its influence.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.2,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.9,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Human-centered computing,"Human-centered computing: The paper analyzes time-inconsistent user preferences in online DVD rentals, focusing on human behavior and decision-making. Other categories like information systems or mathematics are secondary as the core contribution is about understanding user psychology.","Accessibility:0.0,Collaborative and social computing:0.9,Human computer interaction (HCI):0.9,Interaction design:0.2,Ubiquitous and mobile computing:0.1,Visualization:0.1","Collaborative and social computing,Human computer interaction (HCI)",Collaborative and social computing is relevant because the paper analyzes user behavior patterns in an online rental system. Human computer interaction is relevant as it examines how users interact with digital rental platforms. Accessibility and Interaction design are less relevant since the study focuses on behavioral economics rather than interface design. Ubiquitous computing and Visualization are not central to the study's focus on preference prediction.,"Collaborative and social computing design and evaluation methods:0.4,Collaborative and social computing systems and tools:0.3,Collaborative and social computing theory, concepts and paradigms:0.3,Empirical studies in collaborative and social computing:0.7,Empirical studies in HCI:1,HCI design and evaluation methods:0.5,HCI theory, concepts and models:0.4,Interaction devices:0.3,Interaction paradigms:0.3,Interaction techniques:0.4,Interactive systems and tools:0.5",Empirical studies in HCI,"The paper is an empirical study analyzing time-inconsistent user preferences in online DVD rentals, making 'Empirical studies in HCI' highly relevant. 'Empirical studies in collaborative and social computing' is moderately relevant as it involves user behavior, but the study lacks collaborative/social computing elements. Other categories are irrelevant as the paper focuses on individual choice patterns."
5658,OPOSSUM: Desk-Top Schema Management through Customizable Visualization,"Several recent trends have changed the usage and users of schemas beyond those of a database administrator’s tool for describing database contents. Distribution of computing power to the desk-top and increasing needs for data management have broadened the base of schema users to include people who are not database experts. The advent of graphical user interfaces has brought schemas into use as templates for a variety of database operations such as query specification and browsing. Such changes demand advanced schema management techniques, primarily schema visualization, in order to support productive interaction between increasingly novice users and increasingly complex schemas. In this paper, we present OPOSSUM, a flexible, customizable, and extensible schema management system. Working within the established paradigm of schema editing through direct manipulation, OPOSSUM employs several novel techniques to offer the following capabilities: enhancement of schema visualizations with user-specific information; exploration of schemas through choice of visual representations; and creation of new visual representation styles when existing ones prove unsatisfactory. We discuss the architecture of the system and the methodology that guided its development, and illustrate its most important features through examples of how it has been used. OPOSSUM is operational and is in use by three groups of experimental scientists on the University of Wisconsin campus as a tool for experiment and database design.","General and reference:0.1,Hardware:0.2,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.6,Theory of computation:0.1,Mathematics of computing:0.2,Information systems:0.2,Security and privacy:0.1,Human-centered computing:1.0,Computing methodologies:0.4,Applied computing:0.3,Social and professional topics:0.1",Human-centered computing,Human-centered computing is highly relevant for the customizable visualization techniques and user-focused schema management. Software and its engineering is less relevant as the focus is on user interaction rather than software development.,"Human computer interaction (HCI):1,Visualization:0.9,Accessibility:0.2,Collaborative and social computing:0.1,Interaction design:0.05,Ubiquitous and mobile computing:0.05","Human computer interaction (HCI),Visualization",Human computer interaction (HCI): OPOSSUM focuses on user interaction with schemas. Visualization: Customizable visual representations are central. Other children like Accessibility and Collaborative computing are not relevant as the paper does not discuss accessibility or collaboration.,"Empirical studies in HCI:0.1,Empirical studies in visualization:0.1,HCI design and evaluation methods:0.1,HCI theory, concepts and models:0.1,Interaction devices:0.1,Interaction paradigms:0.1,Interaction techniques:0.2,Interactive systems and tools:0.8,Visualization application domains:0.9,Visualization design and evaluation methods:0.7,Visualization systems and tools:0.8,Visualization techniques:0.7,Visualization theory, concepts and paradigms:0.1","Visualization application domains,Interactive systems and tools",Visualization application domains are relevant for using visualization in schema management. Interactive systems and tools are relevant as OPOSSUM is a customizable schema management system. Other categories like Interaction paradigms are less directly addressed.
5802,The Expressions of Colours,"A whole world is presented in front of the eyes of a gamer. This world has shapes and colours. What do colours transmit? Is this considered in digital games? These are the main issues of this work. We are part of a culture which has been decanting meanings for ages, including those of colours. These coded meanings are shared connotations of feelings, sensations, atmosphere, thoughts, and moods. Repeatedly, this symbolism is used without realizing how colours communicate. So, if colours can express, gamers would get messages which contain reactions and sensations. Diverse elements of digital games will be taken into account: characters, landscapes, clothes, objects, interfaces. A better understanding of the uses of colours would improve game experience.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:1.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Human-centered computing,Human-centered computing is relevant for analyzing color symbolism in games and its impact on user experience. Other categories like 'Information systems' are not discussed.,"Accessibility:0.1,Collaborative and social computing:0.2,Human computer interaction (HCI):1.0,Interaction design:0.3,Ubiquitous and mobile computing:0.2,Visualization:0.4",Human computer interaction (HCI),"Human computer interaction (HCI): The paper analyzes color symbolism in digital games, which directly impacts user experience and perception. Visualization is tangential as the focus is on color semantics rather than data visualization techniques.","Empirical studies in HCI:0,HCI design and evaluation methods:1,HCI theory, concepts and models:1,Interaction devices:0,Interaction paradigms:0,Interaction techniques:0,Interactive systems and tools:0","HCI design and evaluation methods,HCI theory, concepts and models","HCI design and evaluation methods is relevant for analyzing color use in game design. HCI theory, concepts and models is included as the paper explores color symbolism in human-computer interaction."
1238,Practical strategies for integrating a conversation analyst in an iterative design process,"We present a case study of an iterative design process that includes a conversation analyst. We discuss potential benefits of conversation analysis for design, and we describe our strategies for integrating the conversation analyst in the design process. Since the analyst on our team had no previous exposure to design or engineering, and none of the other members of our team had any experience with conversation analysis, we needed to build a foundation for our interaction. One of our key strategies was to pair the conversation analyst with a designer in a highly interactive collaboration. Our tactics have been effective on our project, leading to valuable results that we believe we could not have obtained using another method. We hope that this paper can serve as a practical guide to those interested in establishing a productive and efficient working relationship between a conversation analyst and the other members of a design team.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:1.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Human-centered computing,"Human-centered computing: The paper focuses on integrating a conversation analyst into an iterative design process to improve human-centric design outcomes. Other categories (e.g., Applied computing) are less relevant as the primary domain is human interaction and collaboration.","Accessibility:0.1,Collaborative and social computing:0.3,Human computer interaction (HCI):0.75,Interaction design:1.0,Ubiquitous and mobile computing:0.2,Visualization:0.1","Interaction design,Human computer interaction (HCI)",Interaction design is highly relevant as the paper discusses strategies for integrating a conversation analyst in design processes. Human computer interaction (HCI) is relevant due to the focus on collaborative design practices. Other categories like Accessibility and Visualization are less relevant as the paper does not focus on these aspects.,"Empirical studies in HCI:0.2,Empirical studies in interaction design:0.1,HCI design and evaluation methods:0.9,HCI theory, concepts and models:0.3,Interaction design process and methods:0.9,Interaction design theory, concepts and paradigms:0.4,Interaction devices:0.1,Interaction paradigms:0.2,Interaction techniques:0.1,Interactive systems and tools:0.3,Systems and tools for interaction design:0.3","HCI design and evaluation methods,Interaction design process and methods",HCI design and evaluation methods: The paper evaluates strategies for integrating conversation analysis in design. Interaction design process and methods: Focuses on collaborative design processes with conversation analysts. Other options are less relevant as the paper emphasizes process integration over empirical studies or tools.
3045,The Effect of Positive Emotion and Perceived Risk on Usage Intention to Online Decision Aids,"Although perceived risk has a negative effect on usage intention toward new information technology, both perceived risk and usage intention are the results of cognitive processes, so they are inevitably influenced by emotion. Based on positive mood theory and the appraisal-tendency framework (ATF), a laboratory experiment using online decision aids with 126 participants was conducted. The results indicate that positive emotion (happy emotion in the current study) can increase usage intention and decrease perceived risk, while perceived risk decreases usage intention. Further investigation finds that perceived risk is a mediator between emotion and usage intention.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:1.0,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Human-centered computing,Human-centered computing is directly relevant as the paper studies emotion and perceived risk in technology adoption. Other categories like Networks or Applied computing are not central to the psychological framework analysis.,"Accessibility:0.2,Collaborative and social computing:0.3,Human computer interaction (HCI):1.0,Interaction design:0.75,Ubiquitous and mobile computing:0.4,Visualization:0.2","Human computer interaction (HCI),Interaction design","Human computer interaction (HCI) is relevant as the study examines how emotional states influence user interaction with online tools. Interaction design is moderately relevant due to the focus on user behavior patterns. Accessibility, Collaborative computing, and Ubiquitous computing are less relevant as the paper does not focus on assistive technologies, collaboration, or mobile contexts.","Empirical studies in HCI:1,Empirical studies in interaction design:0,HCI design and evaluation methods:0,HCI theory, concepts and models:1,Interaction design process and methods:0,Interaction design theory, concepts and paradigms:0,Interaction devices:0,Interaction paradigms:0,Interaction techniques:0,Interactive systems and tools:0,Systems and tools for interaction design:0","Empirical studies in HCI,HCI theory, concepts and models","Empirical studies in HCI: The paper presents a lab experiment on user behavior. HCI theory, concepts and models: The focus on emotion's impact on usage intention relates to theoretical models. Other categories like Interaction design or devices are unrelated to the study's focus on psychological factors."
1885,Computational and Experimental Approaches to Visual Aesthetics,"Aesthetics has been the subject of long-standing debates by philosophers and psychologists alike. In psychology, it is generally agreed that aesthetic experience results from an interaction between perception, cognition, and emotion. By experimental means, this triad has been studied in the field of experimental aesthetics, which aims to gain a better understanding of how aesthetic experience relates to fundamental principles of human visual perception and brain processes. Recently, researchers in computer vision have also gained interest in the topic, giving rise to the field of computational aesthetics. With computing hardware and methodology developing at a high pace, the modeling of perceptually relevant aspect of aesthetic stimuli has a huge potential. In this review, we present an overview of recent developments in computational aesthetics and how they relate to experimental studies. In the first part, we cover topics such as the prediction of ratings, style and artist identification as well as computational methods in art history, such as the detection of influences among artists or forgeries. We also describe currently used computational algorithms, such as classifiers and deep neural networks. In the second part, we summarize results from the field of experimental aesthetics and cover several isolated image properties that are believed to have a effect on the aesthetic appeal of visual stimuli. Their relation to each other and to findings from computational aesthetics are discussed. Moreover, we compare the strategies in the two fields of research and suggest that both fields would greatly profit from a joined research effort. We hope to encourage researchers from both disciplines to work more closely together in order to understand visual aesthetics from an integrated point of view.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:1.0,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Human-centered computing,"The paper explores computational models of visual aesthetics in relation to human perception, aligning with human-centered computing. Other categories like Networks are not central to the core contribution.","Human computer interaction (HCI):1.0,Visualization:0.75,Collaborative and social computing:0.2,Accessibility:0.2,Interaction design:0.2,Ubiquitous and mobile computing:0.2","Human computer interaction (HCI),Visualization",Human computer interaction (HCI) (1.0) - The paper studies visual aesthetics as part of human perception and interaction. Visualization (0.75) - Computational methods analyze visual stimuli properties. Other categories like accessibility are less directly relevant.,"Empirical studies in HCI:0,Empirical studies in visualization:1,HCI design and evaluation methods:0,HCI theory, concepts and models:0,Interaction devices:0,Interaction paradigms:0,Interaction techniques:0,Interactive systems and tools:0,Visualization application domains:1,Visualization design and evaluation methods:1,Visualization systems and tools:0,Visualization techniques:1,Visualization theory, concepts and paradigms:1","Empirical studies in visualization,Visualization design and evaluation methods,Visualization techniques",Empirical studies in visualization and design/evaluation methods are central to analyzing aesthetic principles. Visualization techniques and application domains are relevant for modeling visual stimuli. Theory categories are less directly addressed.
4354,Speech and speech recognition during dictation corrections,"A natural way to correct errors made while dictating to a computer is to respeak portions of the original sentence. But often spoken corrections are themselves misrecognized, costing the user time and testing their patience. To better understand how users behave while correcting, I created a simulated dictation interface and fooled users into believing they were correcting errors by respeaking. I found that users not only hyperarticulate during corrections, but they do so preemptively before any misrecognition. Depending on the recognizer, hyperarticulation was found to cause relatively minor changes in error rate. The correction of isolated words or phrases was more troublesome, causing substantial recognition problems for an HTK recognizer. Dragon Naturally Speaking, on the other hand, performed slightly better on hyperarticulated speech and only degraded slightly on isolated corrections.","General and reference:0.1,Hardware:0.2,Computer systems organization:0.3,Networks:0.2,Software and its engineering:0.3,Theory of computation:0.4,Mathematics of computing:0.5,Information systems:0.4,Security and privacy:0.1,Human-centered computing:0.9,Computing methodologies:0.6,Applied computing:0.5,Social and professional topics:0.1",Human-centered computing,Human-centered computing is central for user behavior in speech correction. Computing methodologies is secondary for recognition algorithms but less primary than user studies.,"Accessibility:0.2,Collaborative and social computing:0.1,Human computer interaction (HCI):1.0,Interaction design:0.75,Ubiquitous and mobile computing:0.3,Visualization:0.3",Human computer interaction (HCI),"Human computer interaction (HCI): The study analyzes user behavior in speech-based dictation corrections. Interaction design: Secondary relevance for interface design. Other categories are rejected as the focus is on user interaction with speech recognition, not accessibility or visualization.","Empirical studies in HCI:1,HCI design and evaluation methods:0.2,HCI theory, concepts and models:0.3,Interaction devices:0,Interaction paradigms:0.1,Interaction techniques:1,Interactive systems and tools:0.4","Empirical studies in HCI,Interaction techniques",Empirical studies in HCI: The paper presents an empirical study of user correction behavior. Interaction techniques: Discusses speech-based correction techniques. Other options are tangential to the study's focus on user behavior.
2483,Interaction between real and virtual humans in augmented reality,Interaction between real and virtual humans covers a wide range of topics from creation and animation of virtual actors to computer vision techniques for data acquisition from real world. We discuss the design and implementation of an augmented reality system which allows investigation of different real/virtual interaction aspects. As an example we present an application to create real time interactive drama with real and virtual actors.,"General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.9,Computing methodologies:0.5,Applied computing:0.1,Social and professional topics:0.1",Human-centered computing,"Human-centered computing is highly relevant as the paper discusses augmented reality systems for real-virtual human interaction, a core HCC topic. Computing methodologies is moderately relevant for the system design, but the primary focus is interaction design.","Human computer interaction (HCI):0.9,Visualization:0.7,Ubiquitous and mobile computing:0.3,Collaborative and social computing:0.2,Interaction design:0.2,Accessibility:0.1",Human computer interaction (HCI),Human computer interaction is highly relevant as the paper focuses on real/virtual human interaction in AR systems. Visualization receives moderate relevance for rendering virtual actors. Other categories like mobile computing are less relevant as the focus is on interaction design rather than mobility.,"Empirical studies in HCI:0.3,HCI design and evaluation methods:0.6,HCI theory, concepts and models:0.4,Interaction devices:0.2,Interaction paradigms:0.5,Interaction techniques:1.0,Interactive systems and tools:1.0","Interaction techniques,Interactive systems and tools",Interaction techniques (real/virtual interaction) and Interactive systems and tools (AR system implementation) are primary. Other options like Empirical studies in HCI are less relevant.
2934,A model for investigating the effects of machine autonomy on human behavior,"As autonomous machines become more pervasive, situations arise when human decision-makers receive advice from both machines and other humans. When these instructions conflict, a new social situation is defined for which we have little precedent. The authors propose a model for investigating these situations. The model synthesizes research from several different fields, including machine autonomy, affect, initial trust, individual differences, and training. The model is explained, and a set of propositions is described. The model is used to analyze the case of an air collision in which machines and humans provided conflicting advice. The model is also applied to situations in which unmanned aerial vehicles and piloted aircraft seek to avoid collisions with each other. Ways of testing the model through human subject experiments are discussed.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.2,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.2,Mathematics of computing:0.3,Information systems:0.1,Security and privacy:0.1,Human-centered computing:1.0,Computing methodologies:0.4,Applied computing:0.2,Social and professional topics:0.1",Human-centered computing,Human-centered computing is directly relevant as the paper focuses on human behavior in machine autonomy interactions. Other categories like Applied computing or Social topics are secondary to the human-centered focus.,"Human computer interaction (HCI):0.9,Collaborative and social computing:0.3,Interaction design:0.2",Human computer interaction (HCI),Human computer interaction (HCI) is central to analyzing human-machine conflict resolution. Other categories are less focused on behavioral modeling.,"Empirical studies in HCI:1,HCI design and evaluation methods:0.2,HCI theory, concepts and models:1,Interaction devices:0.1,Interaction paradigms:0.1,Interaction techniques:0.1,Interactive systems and tools:0.1","HCI theory, concepts and models,Empirical studies in HCI","HCI theory, concepts and models: The paper proposes a theoretical model for analyzing human-machine interaction dynamics. Empirical studies in HCI: The model is tested through case studies (air collision analysis) and discusses human subject experiments. Other categories like interaction devices/paradigms are irrelevant as the paper focuses on theoretical frameworks rather than specific interaction mechanisms."
963,Visualizing Temporal Uncertainty in 3D Virtual Reconstructions,"Uncertainty in various forms is prevalent throughout Archaeology. With archaeological site data in particular, the dating regularly has significant uncertainty. In this paper we present an application that enables integrating and visualizing the temporal uncertainty for multiple 3D archaeological data sets with different dating. We introduce a temporal time window for dealing with the uncertainty and review various visual cues appropriate for revealing the uncertainty within the time window. The interactive animation of the time window allows a unique exploration of the temporal uncertainty.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.9,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Human-centered computing,Human-centered computing is highly relevant as the paper focuses on visualizing uncertainty for archaeological data interaction. Other categories like Applied Computing are secondary.,"Accessibility:0.1,Collaborative and social computing:0.1,Human computer interaction (HCI):0.3,Interaction design:0.1,Ubiquitous and mobile computing:0.1,Visualization:0.9",Visualization,Visualization is directly relevant as the paper focuses on visualizing temporal uncertainty in 3D reconstructions. Human computer interaction has limited relevance to the technical visualization methods.,"Empirical studies in visualization:0,Visualization application domains:1,Visualization design and evaluation methods:0,Visualization systems and tools:0,Visualization techniques:1,Visualization theory, concepts and paradigms:0","Visualization application domains,Visualization techniques",Visualization application domains: The work applies visualization to archaeology (temporal uncertainty in 3D reconstructions). Visualization techniques: The paper introduces a temporal time window for uncertainty visualization. Other categories are less central.
4482,Usability analysis of 3D rotation techniques,"We report results from a formal user study of interactive 3D rotation using the mouse-driven Virtual Sphere and Arcball techniques, as well as multidimensional input techniques based on magnetic orientation sensors. MultidimensionaI input is often assumed to allow users to work quickly, but at the cost of precision, due to the instability of the hand moving in the open air. We show that, at least for the orientation matching task used in this experiment, users can take advantage of the integrated degrees of freedom provided by multidimensional input without necessarily sacrificing precision: using multidimensional input, users completed the experimental task up to 36% faster without any statistically detectable loss of accuracy. We also report detailed observations of common usability problems when first encountering the techniques. Our observations suggest some design issues for 3D input devices. For example, the physical form-factors of the 3D input device significantly influenced user acceptance of otherwise identical input sensors. The device should afford some tactile cues, so the user can feel its orientation without looking at it. In the absence of such cues, some test users were unsure of how to use the device.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.8,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Human-centered computing,Human-centered computing is relevant for the usability study of 3D input techniques. Other categories do not address user interaction and interface design.,"Accessibility:0.3,Collaborative and social computing:0.0,Human computer interaction (HCI):1.0,Interaction design:0.7,Ubiquitous and mobile computing:0.1,Visualization:0.2","Human computer interaction (HCI),Interaction design",Human computer interaction (HCI) is directly relevant for the 3D input device evaluation. Interaction design is moderately relevant for the usability-focused device design analysis. Visualization is less relevant as the study focuses on interaction rather than data representation.,"Empirical studies in HCI:1.0,Empirical studies in interaction design:0.8,HCI design and evaluation methods:0.7,HCI theory, concepts and models:0.2,Interaction design process and methods:0.6,Interaction design theory, concepts and paradigms:0.3,Interaction devices:0.9,Interaction paradigms:0.4,Interaction techniques:1.0,Interactive systems and tools:0.5,Systems and tools for interaction design:0.5","Empirical studies in HCI,Interaction techniques,Interaction devices",Empirical studies in HCI is relevant due to the user study. Interaction techniques is relevant for evaluating 3D rotation methods. Interaction devices is relevant as the paper examines physical input devices. Other categories like HCI theory are less directly connected.
1766,Supporting Freeform Modelling in Spatial Augmented Reality Environments with a New Deformable Material,"This paper describes how a new free-form modelling material, Quimo (Quick Mock-up), can be used by industrial designers in spatial augmented reality environments. Quimo is a white malleable material that can be sculpted and deformed with bare hands into an approximate model. The material is white in colour, retains its shape once sculpted, and allows for later modification. Projecting imagery onto the surface of the low-fidelity mock-up allows for detailed prototype visualisations to be presented. This ability allows the designer to create design concept visualisations and re-configure the physical shape and projected appearance rapidly. 
 
We detail the construction techniques used to create the Quimo material and present the modelling techniques employed during mock-up creation. We then extend the functionality of the material by integrating low-visibility retro-reflective fiducial markers to capture the surface geometry. The surface tracking allows the combined physical and virtual modelling techniques to be integrated. This is advantageous compared to the traditional prototyping process that requires a new mock-up to be built whenever a significant change of the shape or visual appearance is desired. We demonstrate that Quimo, augmented with projected imagery, supports interactive changes of an existing prototype concept for advanced visualisation.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.9,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Human-centered computing,Human-centered computing is highly relevant as the paper addresses AR design tools for industrial users. Other categories are irrelevant as the focus is on interaction design rather than hardware or algorithms.,"Accessibility:0.0,Collaborative and social computing:0.0,Human computer interaction (HCI):1.0,Interaction design:1.0,Ubiquitous and mobile computing:0.0,Visualization:0.5","Human computer interaction (HCI),Interaction design",Human computer interaction (HCI) is directly relevant due to the focus on spatial augmented reality environments. Interaction design is relevant for the material's use in prototyping. Visualization is only tangentially relevant as it's about projecting imagery onto the material.,"Empirical studies in HCI:0.2,Empirical studies in interaction design:0.2,HCI design and evaluation methods:0.6,HCI theory, concepts and models:0.1,Interaction design process and methods:0.8,Interaction design theory, concepts and paradigms:0.3,Interaction devices:0.2,Interaction paradigms:0.4,Interaction techniques:0.7,Interactive systems and tools:0.9,Systems and tools for interaction design:0.9","Interactive systems and tools,Systems and tools for interaction design,Interaction techniques",Interactive systems and tools: The paper introduces Quimo as a physical-digital interaction system. Systems and tools for interaction design: The material is explicitly designed for interaction design workflows. Interaction techniques: The paper describes physical manipulation and projection interaction techniques. Other options like HCI theory or empirical studies have lower scores as the paper focuses on system development rather than theoretical models or formal studies.
5149,Ghosts in the network: distributed troubleshooting in a shared working environment,"The implementation of networking technology in work settings offers numerous opportunities for improving the transmission of information and the sharing of resources within and between organizations. Its success in integrating distributed working activities, however, rests on how well the users of a network can coordinate their activities with respect to each other. This paper examines the communicative and interactive processes that take place when a typical breakdown occurs in a networked environment. A detailed analysis is presented which interprets the events that unfold in relation to the sociocognitive issues of shared understanding, the transmission of knowledge and distributed problem-solving.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.5,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:1.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Human-centered computing,Relevant to 'Human-centered computing' for studying sociocognitive aspects of network troubleshooting. 'Networks' is rejected as the paper focuses on human interaction rather than network protocols.,"Accessibility:0.1,Collaborative and social computing:0.85,Human computer interaction (HCI):0.8,Interaction design:0.3,Ubiquitous and mobile computing:0.25,Visualization:0.15","Collaborative and social computing,Human computer interaction (HCI)",Collaborative and social computing addresses distributed problem-solving in networks. Human computer interaction relates to user coordination in troubleshooting. Other fields like Ubiquitous computing are less focused on the core sociocognitive analysis.,"Collaborative and social computing design and evaluation methods:0.3,Collaborative and social computing devices:0.1,Collaborative and social computing systems and tools:1.0,Collaborative and social computing theory, concepts and paradigms:0.5,Empirical studies in HCI:0.4,Empirical studies in collaborative and social computing:0.7,HCI design and evaluation methods:0.2,HCI theory, concepts and models:0.2,Interaction devices:0.1,Interaction paradigms:0.1,Interaction techniques:0.1,Interactive systems and tools:0.6","Collaborative and social computing systems and tools,Empirical studies in collaborative and social computing",Collaborative and social computing systems and tools: The paper analyzes distributed troubleshooting in networks. Empirical studies: Includes case studies on communication processes. Other options like HCI are less relevant.
5731,Perceptual Fusion Tendency of Speech Sounds,"To discriminate and to recognize sound sources in a noisy, reverberant environment, listeners need to perceptually integrate the direct wave with the reflections of each sound source. It has been confirmed that perceptual fusion between direct and reflected waves of a speech sound helps listeners recognize this speech sound in a simulated reverberant environment with disrupting sound sources. When the delay between a direct sound wave and its reflected wave is sufficiently short, the two waves are perceptually fused into a single sound image as coming from the source location. Interestingly, compared with nonspeech sounds such as clicks and noise bursts, speech sounds have a much larger perceptual fusion tendency. This study investigated why the fusion tendency for speech sounds is so large. Here we show that when the temporal amplitude fluctuation of speech was artificially time reversed, a large perceptual fusion tendency of speech sounds disappeared, regardless of whether the speech acoustic carrier was in normal or reversed temporal order. Moreover, perceptual fusion of normal-order speech, but not that of time-reversed speech, was accompanied by increased coactivation of the attention-control-related, spatial-processing-related, and speech-processing-related cortical areas. Thus, speech-like acoustic carriers modulated by speech amplitude fluctuation selectively activate a cortical network for top–down modulations of speech processing, leading to an enhancement of perceptual fusion of speech sounds. This mechanism represents a perceptual-grouping strategy for unmasking speech under adverse conditions.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.2,Information systems:0.1,Security and privacy:0.1,Human-centered computing:1.0,Computing methodologies:0.3,Applied computing:0.2,Social and professional topics:0.1",Human-centered computing,Human-centered computing is relevant due to the study of perceptual mechanisms in speech processing. Other fields like Applied computing are not directly addressed in the paper.,"Accessibility:0.1,Collaborative and social computing:0.1,Human computer interaction (HCI):0.8,Interaction design:0.3,Ubiquitous and mobile computing:0.1,Visualization:0.1",Human computer interaction (HCI),"Human computer interaction (HCI): The study of perceptual fusion in speech relates to how humans process auditory information, which is core to understanding human-system interaction. Other categories like 'Accessibility' are less relevant as the paper focuses on cognitive processes rather than assistive technologies.","Empirical studies in HCI:1,HCI design and evaluation methods:0,HCI theory, concepts and models:0,Interaction devices:0,Interaction paradigms:0,Interaction techniques:0,Interactive systems and tools:0",Empirical studies in HCI,Empirical studies in HCI is the only relevant category as the paper investigates perceptual fusion through controlled auditory experiments. Other HCI categories focus on interface design rather than perceptual psychology studies.
1104,When Gaze Turns into Grasp,"Previous research has provided evidence for a neural system underlying the observation of another person's hand actions. Is the neural system involved in this capacity also important in inferring another person's motor intentions toward an object from their eye gaze? In real-life situations, humans use eye movements to catch and direct the attention of others, often without any accompanying hand movements or speech. In an event-related functional magnetic resonance imaging study, subjects observed videos showing a human model either grasping a target object (grasping condition) or simply gazing (gaze condition) at the same object. These two conditions were contrasted with each other and against a control condition in which the human model was standing behind the object without performing any gazing or grasping action. The results revealed activations within the dorsal premotor cortex, the inferior frontal gyrus, the inferior parietal lobule, and the superior temporal sulcus in both grasping and gaze conditions. These findings suggest that signaling the presence of an object through gaze elicits in an observer a similar neural response to that elicited by the observation of a reach-to-grasp action performed on the same object.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.9,Computing methodologies:0.1,Applied computing:0.0,Social and professional topics:0.0",Human-centered computing,"Human-centered computing: The study investigates neural responses to gaze and grasping actions, a key topic in human factors and neuroscience. Other categories are irrelevant as the paper does not involve computing methodologies, applied systems, or technical implementation.","Accessibility:0.1,Collaborative and social computing:0.3,Human computer interaction (HCI):0.2,Interaction design:0.1,Ubiquitous and mobile computing:0.1,Visualization:0.1",Collaborative and social computing,Collaborative and social computing is relevant as the study explores human social interaction through gaze and grasp. Other categories like HCI or Visualization are less directly aligned with the paper's focus on neural responses and behavioral observation.,"Collaborative and social computing design and evaluation methods:0.1,Collaborative and social computing devices:0.1,Collaborative and social computing systems and tools:0.2,Collaborative and social computing theory, concepts and paradigms:0.3,Empirical studies in collaborative and social computing:1.0",Empirical studies in collaborative and social computing,"Empirical studies in collaborative and social computing is the only relevant category as the paper presents an fMRI study on human neural responses. Other categories were rejected as the paper does not discuss design methods, devices, or theoretical frameworks."
4897,Assessing neuromuscular mechanisms in human-exoskeleton interaction,"In this study, we propose to evaluate a 7 DOF exoskeleton in terms of motion control. Using criteria from the human motor control literature, inverse optimization was performed to assess an industrial screwing movement. The results of our study show that the hybrid composition of the free arm movement was accurately determined. At contrary, when wearing the exoskeleton, which produces an arbitrary determined torque compensation, the motion is different from the naturally adopted one. This study is part of the evaluation and comprehension of the complex neuromuscular mechanism resulting in wearing an exoskeleton several hours per day for industrial tasks assistance.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.8,Computing methodologies:0.2,Applied computing:0.3,Social and professional topics:0.1",Human-centered computing,Human-centered computing is relevant as the paper studies human-exoskeleton interaction and neuromuscular adaptation. Other fields like Applied computing or Software engineering are not central to the core contribution focused on human factors.,"Accessibility:0.3,Collaborative and social computing:0.1,Human computer interaction (HCI):1.0,Interaction design:0.5,Ubiquitous and mobile computing:0.1,Visualization:0.1",Human computer interaction (HCI),Human computer interaction (HCI) is highly relevant as the paper evaluates human-exoskeleton interaction. Interaction design is secondary due to ergonomic considerations.,"Empirical studies in HCI:1,HCI design and evaluation methods:1,HCI theory, concepts and models:0,Interaction devices:0,Interaction paradigms:0,Interaction techniques:0,Interactive systems and tools:0","Empirical studies in HCI,HCI design and evaluation methods",Empirical studies in HCI are relevant as the paper evaluates human interaction with the exoskeleton. HCI design and evaluation methods are relevant for assessing the system's impact on natural motion. Other categories are unrelated to the study's focus.
268,Analyzing speech rate entrainment and its relation to therapist empathy in drug addiction counseling,"A key quality index in drug addiction counseling such as Motivational Interviewing is the degree of therapist’s empathy towards the client. Empathy ratings are meant to evaluate the therapist’s understanding of the patient’s feelings, through their sensitivity and care of response. Empathy is also associated with the manifestation of behavioral entrainment in the interaction. In this paper, we compute a measure of entrainment in speech rate during dyadic interactions, and investigate its relation to perceived empathy. We show that the averaged absolute difference of turn-level speech rates between the therapist and the patient correlates with the ratings of therapist empathy. We also present the correlation of empathy to the statistics of speech and silence durations. Finally we show that in the task of automatically predicting high or low empathy, speech rate cues provide complementary information to previously proposed prosodic cues. These findings suggest speech rate as an important behavioral cue that is modulated by entrainment and contributes to empathy modeling.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:1.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Human-centered computing,"Human-centered computing is highly relevant because the paper analyzes speech patterns in human interactions (therapist-patient) to model empathy, a key human-centric computing application. Other categories lack direct relevance to behavioral modeling in human interactions.","Accessibility:0.1,Collaborative and social computing:1.0,Human computer interaction (HCI):0.3,Interaction design:0.2,Ubiquitous and mobile computing:0.1,Visualization:0.1",Collaborative and social computing,"Collaborative and social computing: The paper analyzes speech rate entrainment in dyadic interactions as a behavioral cue for empathy, directly relating to social interaction dynamics. Other categories like HCI or mobile computing are not discussed in the core contribution.","Collaborative and social computing design and evaluation methods:0,Collaborative and social computing devices:0,Collaborative and social computing systems and tools:0,Collaborative and social computing theory, concepts and paradigms:0,Empirical studies in collaborative and social computing:1,Empirical studies in HCI:0",Empirical studies in collaborative and social computing,Empirical studies in collaborative and social computing is relevant as the paper investigates therapist-patient interaction patterns related to empathy. No other categories in the list are relevant to this study.
1416,Learning through ICT-enabled social networks,"Utilising Information and Communication Technology (ICT) to build effective learning networks in organisations has received attention from both practitioners and researchers in recent years. Going beyond the limited focus on the structural characteristics of networks in prior research, this paper develops a theoretical model that specifies how the structural properties of network ties and the quality of interaction influence the occurrence of single- and double-loop learning within ICT-enabled social networks. It provides insights into how ICT can be utilised to bring people together and form a productive environment conducive to learning. The model is expected to have theoretical and practical implications for the development of ICT-enabled social networks for individual learning.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.9,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Human-centered computing,Human-centered computing is highly relevant because the paper focuses on how ICT-enabled social networks influence learning behaviors and organizational outcomes. Other categories like Applied computing are less central as the focus is on human interaction and social dynamics rather than technical systems.,"Accessibility:0.2,Collaborative and social computing:0.9,Human computer interaction (HCI):0.7,Interaction design:0.3,Ubiquitous and mobile computing:0.2,Visualization:0.2","Collaborative and social computing,Human computer interaction (HCI)",Collaborative and social computing is relevant as the paper discusses learning through ICT-enabled social networks. Human computer interaction (HCI) is relevant because it addresses the interaction design for learning. Other children are rejected as they are not central to the study of social network-based learning.,"Collaborative and social computing design and evaluation methods:0,Collaborative and social computing devices:0,Collaborative and social computing systems and tools:0,Collaborative and social computing theory, concepts and paradigms:1,Empirical studies in collaborative and social computing:1,Empirical studies in HCI:0,HCI design and evaluation methods:0,HCI theory, concepts and models:0,Interaction devices:0,Interaction paradigms:0,Interaction techniques:0,Interactive systems and tools:0","Collaborative and social computing theory, concepts and paradigms,Empirical studies in collaborative and social computing",The paper presents a theoretical model for ICT-enabled social networks (Collaborative and social computing theory) and uses empirical analysis (Empirical studies). Other categories like systems or devices are not discussed.
3816,The Digital Pathfinder - A Plug-and-Play Approach to Local History Instruction in the Field,"The Digital Pathfinder is an open source “plug-and-play” mobile electronic guide in field trials as an interdisciplinary project in the Hudson River Valley National Heritage Area. This paper introduces the project, its motivations, operational, instructional, and technical facets, and experimental results.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.4,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.7,Computing methodologies:0.2,Applied computing:0.3,Social and professional topics:0.1",Human-centered computing,Human-centered computing is highly relevant as the paper presents an educational technology application for history instruction. Software and its engineering receives moderate relevance due to the system's implementation aspects. Applied computing is less relevant as the focus is on educational application rather than technical methodology.,"Accessibility:0.2,Collaborative and social computing:0.3,Human computer interaction (HCI):0.4,Interaction design:0.3,Ubiquitous and mobile computing:0.9,Visualization:0.2",Ubiquitous and mobile computing,Ubiquitous and mobile computing: The paper introduces a mobile electronic guide for field instruction. Other categories lack direct relevance to the technical implementation of a mobile field tool.,"Empirical studies in ubiquitous and mobile computing:1,Ubiquitous and mobile computing design and evaluation methods:0,Ubiquitous and mobile computing systems and tools:1,Ubiquitous and mobile computing theory, concepts and paradigms:0,Ubiquitous and mobile devices:0","Empirical studies in ubiquitous and mobile computing,Ubiquitous and mobile computing systems and tools",Empirical studies are relevant for the experimental results. Systems and tools are relevant for the mobile guide implementation. Other categories like Theory are not the focus.
1905,Compensation for Changing Motor Uncertainty,"When movement outcome differs consistently from the intended movement, errors are used to correct subsequent movements (e.g., adaptation to displacing prisms or force fields) by updating an internal model of motor and/or sensory systems. Here, we examine changes to an internal model of the motor system under changes in the variance structure of movement errors lacking an overall bias. We introduced a horizontal visuomotor perturbation to change the statistical distribution of movement errors anisotropically, while monetary gains/losses were awarded based on movement outcomes. We derive predictions for simulated movement planners, each differing in its internal model of the motor system. We find that humans optimally respond to the overall change in error magnitude, but ignore the anisotropy of the error distribution. Through comparison with simulated movement planners, we found that aimpoints corresponded quantitatively to an ideal movement planner that updates a strictly isotropic (circular) internal model of the error distribution. Aimpoints were planned in a manner that ignored the direction-dependence of error magnitudes, despite the continuous availability of unambiguous information regarding the anisotropic distribution of actual motor errors.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.2,Information systems:0.0,Security and privacy:0.0,Human-centered computing:1.0,Computing methodologies:0.3,Applied computing:0.0,Social and professional topics:0.0",Human-centered computing,Human-centered computing receives 1.0 as the paper's core contribution is modeling human motor adaptation mechanisms. Computing methodologies (0.3) is partially relevant for the simulation techniques but not primary. Mathematics of computing (0.2) is relevant for the statistical analysis.,"Accessibility:0.1,Collaborative and social computing:0.2,Human computer interaction (HCI):0.9,Interaction design:0.8,Ubiquitous and mobile computing:0.3,Visualization:0.1","Human computer interaction (HCI),Interaction design",Human computer interaction (HCI) applies to motor adaptation studies. Interaction design is relevant for modeling human behavior. Other categories like Accessibility or Ubiquitous computing are not central to the paper's focus on internal motor models.,"Empirical studies in HCI:1,Empirical studies in interaction design:0,HCI design and evaluation methods:0,HCI theory, concepts and models:1,Interaction design process and methods:0,Interaction design theory, concepts and paradigms:0,Interaction devices:0,Interaction paradigms:0,Interaction techniques:0,Interactive systems and tools:0,Systems and tools for interaction design:0","Empirical studies in HCI,HCI theory, concepts and models",Empirical studies in HCI are relevant for the experimental analysis of motor behavior. HCI theory is relevant for modeling internal representations of motor uncertainty. Other interaction design topics are not discussed.
3727,An investigation into three visual characteristics of complex scenes that evoke human emotion,"Prior computational studies have examined hundreds of visual characteristics related to color, texture, and composition in an attempt to predict human emotional responses. Beyond those myriad features examined in computer science, roundness, angularity, and visual complexity have also been found to evoke emotions in human perceivers, as demonstrated in psychological studies of facial expressions, dance poses, and even simple synthetic visual patterns. Capturing these characteristics algorithmically to incorporate in computational studies, however, has proven difficult. Here we expand the scope of previous computer vision work by examining these three visual characteristics in computer analysis of complex scenes, and compare the results to the hundreds of visual qualities previously examined. A large collection of ecologically valid stimuli (i.e., photos that humans regularly encounter on the web), named the EmoSet and containing more than 40,000 images crawled from web albums, was generated using crowd-sourcing and subjected to human subject emotion ratings. We developed computational methods to the separate indices of roundness, angularity, and complexity, thereby establishing three new computational constructs. Critically, these three new physically interpretable visual constructs achieve comparable classification accuracy to the hundreds of shape, texture, composition, and facial feature characteristics previously examined. In addition, our experimental results show that color features related most strongly with the positivity of perceived emotions, the texture features related more to calmness or excitement, and roundness, angularity, and simplicity related similarly with both of these emotions dimensions.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.9,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Human-centered computing,"Human-centered computing: The paper investigates how visual characteristics evoke human emotions, focusing on human perception and interaction with computational systems. Other categories like Information systems or Software engineering are less relevant as the core contribution is about human factors.","Accessibility:0.05,Collaborative and social computing:0.05,Human computer interaction (HCI):0.7,Interaction design:0.6,Ubiquitous and mobile computing:0.05,Visualization:0.8","Human computer interaction (HCI),Visualization",Human computer interaction (HCI) is relevant for analyzing how visual features affect human emotion. Visualization is relevant as the paper develops computational methods for visual characteristics. Other categories like Accessibility are irrelevant as the focus is not on accessibility features.,"Empirical studies in HCI:0.0,Empirical studies in visualization:0.0,HCI design and evaluation methods:0.0,HCI theory, concepts and models:0.0,Interaction devices:0.0,Interaction paradigms:0.0,Interaction techniques:0.0,Interactive systems and tools:0.0,Visualization application domains:0.6,Visualization design and evaluation methods:1.0,Visualization systems and tools:0.3,Visualization techniques:0.8,Visualization theory, concepts and paradigms:0.5","Visualization design and evaluation methods,Visualization techniques",Visualization design and evaluation methods is relevant for developing computational constructs for visual characteristics. Visualization techniques is relevant for the algorithmic approach to analyzing scenes. Other options like Application domains (0.6) are secondary to the methodological focus.
3697,Towards multi-domain speech understanding using a two-stage recognizer,"This paper describes our e(cid:11)orts in designing a two-stage recognizer with the objective of developing a multi-domain speech understanding system. We envisage one (cid:12)rst-stage recognition engine that is domain-independent, and multiple second-stage systems specializing in individual domains. A major novelty in our initial two-stage design is a front-end that incorporates angie -based hierarchical sublexical probability models encapsulated within a (cid:12)nite-state transducer (FST) paradigm. This (cid:12)rst stage is a context-dependentsyllable-level recognizer which outputs acoustic-phonetic networks to be processed in a second pass. The second stage incorporates higher order linguistic knowledge, from phonological to syntactic and semantic, in a tightly coupled search. This system has yielded up to a 28.5% reduction in understanding error, compared with a single stage context-dependent recognizer which does not use angie -based probabilities.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.2,Security and privacy:0.1,Human-centered computing:0.8,Computing methodologies:0.3,Applied computing:0.2,Social and professional topics:0.1",Human-centered computing,"Human-centered computing is highly relevant for speech understanding systems, which are core to human-computer interaction. Computing methodologies receives moderate score for the two-stage recognition architecture, but the primary focus is human-centered computing.","Accessibility:0.0,Collaborative and social computing:0.0,Human computer interaction (HCI):1.0,Interaction design:0.7,Ubiquitous and mobile computing:0.5,Visualization:0.0",Human computer interaction (HCI),"Human computer interaction (HCI) is highly relevant as the paper designs a multi-domain speech understanding system. Interaction design and Ubiquitous and mobile computing receive moderate scores for system design aspects, but the primary focus is on HCI.","Empirical studies in HCI:0.1,HCI design and evaluation methods:0.2,HCI theory, concepts and models:0.1,Interaction devices:0.1,Interaction paradigms:0.1,Interaction techniques:0.6,Interactive systems and tools:0.8","Interactive systems and tools,Interaction techniques",Interactive systems and tools: The paper describes a two-stage speech understanding system with specific technical implementation. Interaction techniques: The FST-based hierarchical probability models represent interaction techniques between stages. Other options are rejected as the paper focuses on algorithmic systems rather than empirical HCI studies or device-specific interactions.
914,Reorientation during Body Turns,"Immersive virtual environment (IVE) systems allow users to control their virtual viewpoint by moving their tracked head and by walking through the real world, but usually the virtual space which can be explored by walking is restricted to the size of the tracked space of the laboratory. However, as the user approaches an edge of the tracked walking area, reorientation techniques can be applied to imperceptibly turn the user by manipulating the mapping between real-world body turns and virtual camera rotations. With such reorientation techniques, users can walk through large-scale IVEs while physically remaining in a reasonably small workspace. 
 
In psychophysical experiments we have quantified how much users can unknowingly be reoriented during body turns. We tested 18 subjects in two different experiments. First, in a just-noticeable difference test subjects had to perform two successive body turns between which they had to discriminate. In the second experiment subjects performed body turns that were mapped to different virtual camera rotations. Subjects had to estimate whether the visually perceived rotation was slower or faster than the physical rotation. Our results show that the detection thresholds for reorientation as well as the point of subjective equality between real movement and visual stimuli depend on the virtual rotation angle.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.9,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Human-centered computing,Human-centered computing: The study involves human perception and interaction in virtual environments. Other categories lack direct relevance to user interface design or human factors.,"Accessibility:0.1,Collaborative and social computing:0.1,Human computer interaction (HCI):0.9,Interaction design:0.7,Ubiquitous and mobile computing:0.2,Visualization:0.3","Human computer interaction (HCI),Interaction design",Human computer interaction (HCI) is directly relevant as the study focuses on user reorientation in virtual environments. Interaction design is relevant for designing the mapping between physical and virtual movements. Visualization is less central as the paper focuses on perception rather than visual design.,"Empirical studies in HCI:1.0,Empirical studies in interaction design:0.0,HCI design and evaluation methods:0.0,HCI theory, concepts and models:0.0,Interaction design process and methods:0.0,Interaction design theory, concepts and paradigms:0.0,Interaction devices:0.0,Interaction paradigms:0.0,Interaction techniques:0.0,Interactive systems and tools:0.0,Systems and tools for interaction design:0.0",Empirical studies in HCI,"Empirical studies in HCI is highly relevant because the paper reports psychophysical experiments with 18 subjects to quantify reorientation thresholds. Other categories are irrelevant as the paper does not focus on design methods, interaction techniques, or systems."
1293,Evaluation of Fast-Forward Video Visualization,"We evaluate and compare video visualization techniques based on fast-forward. A controlled laboratory user study (n = 24) was conducted to determine the trade-off between support of object identification and motion perception, two properties that have to be considered when choosing a particular fast-forward visualization. We compare four different visualizations: two representing the state-of-the-art and two new variants of visualization introduced in this paper. The two state-of-the-art methods we consider are frame-skipping and temporal blending of successive frames. Our object trail visualization leverages a combination of frame-skipping and temporal blending, whereas predictive trajectory visualization supports motion perception by augmenting the video frames with an arrow that indicates the future object trajectory. Our hypothesis was that each of the state-of-the-art methods satisfies just one of the goals: support of object identification or motion perception. Thus, they represent both ends of the visualization design. The key findings of the evaluation are that object trail visualization supports object identification, whereas predictive trajectory visualization is most useful for motion perception. However, frame-skipping surprisingly exhibits reasonable performance for both tasks. Furthermore, we evaluate the subjective performance of three different playback speed visualizations for adaptive fast-forward, a subdomain of video fast-forward.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.9,Computing methodologies:0.3,Applied computing:0.2,Social and professional topics:0.1",Human-centered computing,Human-centered computing is relevant because the paper focuses on user interface visualization techniques and user studies for fast-forward video. Other categories like Software and Applied computing are less directly relevant as the core contribution is about human perception and interaction design.,"Accessibility:0.25,Collaborative and social computing:0.25,Human computer interaction (HCI):1.0,Interaction design:1.0,Ubiquitous and mobile computing:0.25,Visualization:0.75","Human computer interaction (HCI),Interaction design,Visualization",Human computer interaction (HCI) is relevant as the study evaluates user interaction with fast-forward visualizations. Interaction design is relevant due to the design of new visualization techniques. Visualization is relevant as the paper compares different visualization methods. Accessibility and mobile computing are irrelevant as these are not primary focus areas.,"Empirical studies in HCI:0.3,Empirical studies in interaction design:0.3,Empirical studies in visualization:1,HCI design and evaluation methods:0.5,HCI theory, concepts and models:0,Interaction design process and methods:0.3,Interaction design theory, concepts and paradigms:0,Interaction devices:0,Interaction paradigms:0,Interaction techniques:0.5,Interactive systems and tools:0.3,Systems and tools for interaction design:0,Visualization application domains:0.5,Visualization design and evaluation methods:1,Visualization systems and tools:0.3,Visualization techniques:1,Visualization theory, concepts and paradigms:0.5","Empirical studies in visualization,Visualization design and evaluation methods,Visualization techniques",Empirical studies in visualization: The paper presents a controlled user study. Visualization design and evaluation methods: The paper compares multiple visualization techniques. Visualization techniques: The study introduces new visualization variants. Interaction techniques is only marginally relevant.
3698,Mobile AR live annotation for printed materials,"Nowadays, mobile devices has become a prevalent platform for Augment Reality (AR). And in the field of publication, annotation plays an important role in solving the problem of connection between virtual 3D objects and books. However, most related annotation programs are operated on PC systems. So we propose a novel application for document based AR on mobile devices. Compared with programs in PC, the app would face the challenge of the slower microprocessor. In order to solve this problem, we applied the method of LLAH in PC into the system of mobile devices. This application allows users to annotate text documents through mobile devices. Users can utilize the devices' camera to capture the document, add some annotations, and register them to our system. When reviewing the document again, the annotation stored in the system will be retrieved, and overlay at the corresponding position by semitransparent rectangle.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.2,Security and privacy:0.1,Human-centered computing:0.8,Computing methodologies:0.2,Applied computing:0.2,Social and professional topics:0.1",Human-centered computing,"Human-centered computing is highly relevant as mobile AR annotation involves user interaction and experience design. Applied computing receives moderate score for real-world application, but the primary domain is human-computer interaction.","Accessibility:0.0,Collaborative and social computing:0.0,Human computer interaction (HCI):1.0,Interaction design:0.8,Ubiquitous and mobile computing:1.0,Visualization:0.0","Human computer interaction (HCI),Ubiquitous and mobile computing","Human computer interaction (HCI) and Ubiquitous and mobile computing are highly relevant as the paper focuses on mobile AR for printed materials. Interaction design is moderately relevant for user annotation features, but the core contribution is in mobile computing and HCI.","Empirical studies in HCI:0.1,Empirical studies in ubiquitous and mobile computing:0.1,HCI design and evaluation methods:0.2,HCI theory, concepts and models:0.1,Interaction devices:0.1,Interaction paradigms:0.2,Interaction techniques:0.6,Interactive systems and tools:0.7,Ubiquitous and mobile computing design and evaluation methods:0.2,Ubiquitous and mobile computing systems and tools:0.8,Ubiquitous and mobile computing theory, concepts and paradigms:0.2,Ubiquitous and mobile devices:0.7","Ubiquitous and mobile computing systems and tools,Ubiquitous and mobile devices",Ubiquitous and mobile computing systems and tools: The paper presents a mobile AR annotation system specifically for mobile devices. Ubiquitous and mobile devices: The focus is on adapting LLAH methods to mobile device limitations. Other options like Interaction techniques are partially relevant but secondary to the primary system/device focus.
5402,Navigation of Pitch Space on a Digital Musical Instrument with Dynamic Tactile Feedback,"We present a study investigating the impact of dynamic tactile feedback on performer navigation of a continuous pitch space on a digital musical instrument. Ten musicians performed a series of blind pitch selection and melodic tasks on a self-contained digital musical instrument with audio-frequency tactile feedback that was generated in response to their interaction. Results from the study show that tactile feedback can positively impact a performer's ability to play in tune when the instrument is hidden from sight, however with a temporal impact on performance. Furthermore, several playing techniques were observed that emerged from the performer's engagement with the tactile feedback conditions. We discuss the implications of our findings in the context of tangible interface design and non-visual interface navigation. We also discuss how our implementation suggests guidelines for future instruments and interfaces incorporating dynamic tactile feedback and present a novel tactile feedback technique that uses tactile 'beating'.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.9,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Human-centered computing,Human-centered computing is directly relevant as the paper examines tactile feedback for non-visual musical instrument interaction. Other categories like Applied computing are less central.,"Accessibility:0.4,Collaborative and social computing:0.1,Human computer interaction (HCI):1.0,Interaction design:0.75,Ubiquitous and mobile computing:0.2,Visualization:0.1",Human computer interaction (HCI),Human computer interaction (HCI) is highly relevant because the paper studies how tactile feedback affects musical performance on a digital instrument. Interaction design is moderately relevant due to the interface design focus.,"Empirical studies in HCI:1,HCI design and evaluation methods:0.8,HCI theory, concepts and models:0.7,Interaction devices:0.6,Interaction paradigms:0.5,Interaction techniques:1,Interactive systems and tools:0.6","Empirical studies in HCI,Interaction techniques",Empirical studies are central to the performance evaluation. Interaction techniques apply to tactile feedback implementation. Other categories have weaker relevance to the study's core focus.
3114,Comparing Information Visualization Tools Focusing on the Temporal Dimensions,"Empirical comparisons and categorizations of information visualization tools lack important considerations: the former undervalue the need for a theoretical background, and the latter tend to have too much distance from the user because they do not consider definite user tasks. Therefore, our work combines these approaches and presents the results of both a qualitative evaluation and a recently published categorization. We focus on the visualization of temporal data and reveal that current tools realize only a small part of the visualization possibilities in this field.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.5,Security and privacy:0.0,Human-centered computing:1.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Human-centered computing,Human-centered computing: The paper evaluates visualization tools for temporal data. Information systems is partially relevant but less central than human-centered design.,"Accessibility:0.0,Collaborative and social computing:0.0,Human computer interaction (HCI):0.2,Interaction design:0.3,Ubiquitous and mobile computing:0.1,Visualization:0.9",Visualization,Visualization is highly relevant because the paper focuses on temporal data visualization techniques and categorization. Other children like Interaction design are less relevant as the paper emphasizes evaluation of tools rather than interface design specifics.,"Empirical studies in visualization:0.8,Visualization application domains:0.3,Visualization design and evaluation methods:1.0,Visualization systems and tools:0.5,Visualization techniques:0.6,Visualization theory, concepts and paradigms:0.4","Visualization design and evaluation methods,Empirical studies in visualization",Visualization design and evaluation methods: The paper presents a qualitative evaluation framework for visualization tools. Empirical studies in visualization: The work includes comparative analysis of tools. Other fields like Visualization systems and tools are less central than the design and evaluation focus.
2628,Designing Participant-Generated Context into Guided Tours,"This article presents an interdisciplinary framework for designing participant-generated context into guided tours. The framework has been developed in parallel to practice-led research in the design of mobile learning tours with young people based in London. The article draws on art, architecture and urbanism to outline productive concepts, ‘seeding’ and ‘threading’, which support mobilized learning in tours of the built environment. In this, context is explored as an active and dynamic idea in developing attributes of the mobilized learner in the design of tours around buildings and the built environment.","General and reference:0,Hardware:0,Computer systems organization:0,Networks:0,Software and its engineering:0,Theory of computation:0,Mathematics of computing:0,Information systems:0,Security and privacy:0,Human-centered computing:1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.3",Human-centered computing,Human-centered computing is directly relevant as the paper focuses on designing mobile learning experiences for young people. Social and professional topics is less relevant as the focus is on design frameworks rather than societal issues.,"Accessibility:0.3,Collaborative and social computing:0.2,Human computer interaction (HCI):0.7,Interaction design:0.8,Ubiquitous and mobile computing:0.6,Visualization:0.1","Human computer interaction (HCI),Interaction design,Ubiquitous and mobile computing",Human computer interaction: The paper explores designing mobile learning tours with user participation. Interaction design: It focuses on frameworks for dynamic learner engagement. Ubiquitous and mobile computing: The study involves mobile learning in urban environments. Accessibility is secondary as the focus is on general design rather than disability-specific needs.,"Empirical studies in HCI:0.4,Empirical studies in interaction design:0.3,Empirical studies in ubiquitous and mobile computing:0.5,HCI design and evaluation methods:0.6,HCI theory, concepts and models:0.3,Interaction design process and methods:0.7,Interaction design theory, concepts and paradigms:0.4,Interaction devices:0.1,Interaction paradigms:0.2,Interaction techniques:0.2,Interactive systems and tools:0.3,Systems and tools for interaction design:0.2,Ubiquitous and mobile computing design and evaluation methods:0.8,Ubiquitous and mobile computing systems and tools:0.6,Ubiquitous and mobile computing theory, concepts and paradigms:0.7,Ubiquitous and mobile devices:0.1","Interaction design process and methods,Ubiquitous and mobile computing design and evaluation methods",Interaction design process and methods: The paper discusses design frameworks for mobile learning tours. Ubiquitous and mobile computing design and evaluation methods: The research focuses on designing mobile experiences in urban environments. Other categories like 'Empirical studies in HCI' are less relevant as the paper emphasizes design theory over empirical evaluation.
3950,Sorry and I Didn’t Catch That! - An Investigation of Non-understanding Errors and Recovery Strategies,"We present results from an extensive empirical analysis of non-understanding errors and ten non-understanding recovery strategies, based on a corpus of dialogs collected with a spoken dialog system that handles conference room reservations. More specifically, the issues we investigate are: what are the main sources of non-understanding errors? What is the impact of these errors on global performance? How do various strategies for recovery from non-understandings compare to each other? What are the relationships between these strategies and subsequent user response types, and which response types are more likely to lead to successful recovery? Can dialog performance be improved by using a smarter policy for engaging the non-understanding recovery strategies? If so, can we learn such a policy from data? Whenever available, we compare and contrast our results with other studies in the literature. Finally, we summarize the lessons learned and present our plans for future work inspired by this analysis.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.9,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Human-centered computing,"Human-centered computing is relevant as the paper investigates non-understanding errors in spoken dialog systems and user interaction strategies. Other categories are irrelevant as the paper does not focus on hardware, networks, or theoretical computation.","Accessibility:0.0,Collaborative and social computing:0.3,Human computer interaction (HCI):1.0,Interaction design:0.8,Ubiquitous and mobile computing:0.1,Visualization:0.0","Human computer interaction (HCI),Interaction design",Human computer interaction (HCI) is central to analyzing spoken dialog systems and non-understanding recovery strategies. Interaction design is relevant for evaluating user response types and interface strategies. Other options like 'Collaborative and social computing' are marginally relevant but not the primary focus.,"Empirical studies in HCI:0.9,Empirical studies in interaction design:0.7,HCI design and evaluation methods:0.3,HCI theory, concepts and models:0.1,Interaction design process and methods:0.4,Interaction design theory, concepts and paradigms:0.2,Interaction devices:0.1,Interaction paradigms:0.2,Interaction techniques:0.3,Interactive systems and tools:0.5,Systems and tools for interaction design:0.2","Empirical studies in HCI,Interaction design process and methods",Empirical studies in HCI is highly relevant as the paper presents an extensive empirical analysis of non-understanding errors. Interaction design process and methods is relevant due to the investigation of recovery strategies. Other children like Interaction devices are irrelevant as the paper does not discuss hardware.
57,Classification of gaze preference decision for human-machine interaction using eye tracking device,"This paper aims at classifying a gaze preference decision an operator made taking individual difference into account for purpose of a smooth human-machine interaction. A proposed method, inspired from a visual-psychophysical experiment of gaze preference decision-making when 2 faces are compared, focuses on a likelihood of a chosen face to the subject's gaze shift using an inferential statistical theory. A hypothesis in this psychophysical experiments is well known as the gaze cascade effect, i.e., there is a positive feed back as “more the human look at a face, more they like it” and “more the human like it, more they look at it”. A system developed for implementing the proposed method, comprises an eye tracking device, analyzes the operator's eye movement data measured by it, and classifies the decision he/she made using its likelihood curve fit by a logistic model sigmoid function as a criterion. Moreover, the system is designed as changing displayed visual stimuli flexibly based on programming using the measured eye movement data as an image switcher. In order to discuss the proposed method using the likelihood curve, different types of experiments using the image switcher are done. 2 conditions are compared in experiments, i.e., one is when 2 faces are displayed side by side statically, and another is when the face the subject tries to look at disappears dynamically. Thus, the proposed classification method is enhanced by discussing interesting results of the visual-psychophysical experiments to supplement and progress the gaze cascade effect.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.9,Computing methodologies:0.3,Applied computing:0.1,Social and professional topics:0.1",Human-centered computing,Human-centered computing is highly relevant for gaze preference classification in interaction. Computing methodologies gets moderate relevance for statistical models.,"Accessibility:0.1,Collaborative and social computing:0.1,Human computer interaction (HCI):1.0,Interaction design:0.3,Ubiquitous and mobile computing:0.2,Visualization:0.1",Human computer interaction (HCI),Human computer interaction (HCI) is relevant as the paper focuses on gaze preference classification for interaction systems. Interaction design is secondary as the system design is not the core focus.,"Empirical studies in HCI:1,HCI design and evaluation methods:1,HCI theory, concepts and models:0,Interaction devices:1,Interaction paradigms:0,Interaction techniques:0,Interactive systems and tools:0","HCI design and evaluation methods,Empirical studies in HCI,Interaction devices","HCI design and evaluation methods: The paper proposes a method for classifying gaze preferences in human-machine interaction. Empirical studies in HCI: The method is validated through visual-psychophysical experiments. Interaction devices: The system uses an eye tracking device. Other options like 'Interaction paradigms' are irrelevant as the focus is on device-based interaction analysis, not abstract paradigms."
1667,Interface design strategies for computer-assisted speech transcription,"A set of user interface design techniques for computer-assisted speech transcription are presented and evaluated with respect to task performance and usability. These techniques include error-correction mechanisms which originated in dictation systems and audio editors as well as new techniques developed by us which exploit specific characteristics of existing speech recognition technologies in order to facilitate transcription in settings that typically yield considerable recognition inaccuracy, such as when the speech to be transcribed was produced by different speakers. In particular, we describe a mechanism for dynamic propagation of user feedback which progressively adapts the system to different speakers and lexical contexts. Results of usability and performance evaluation trials indicate that feedback propagation, menu-based correction coupled with keyboard interaction and text-driven audio playback are positively perceived by users and result in improved transcript accuracy.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.3,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.9,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Human-centered computing,Human-centered computing is directly relevant for user interface design in speech transcription. Other categories like 'Software' are less central as the focus is on interaction design rather than software development.,"Human computer interaction (HCI):0.9,Interaction design:0.8,Accessibility:0.2,Collaborative and social computing:0.1,Ubiquitous and mobile computing:0.3,Visualization:0.1","Human computer interaction (HCI),Interaction design","Human computer interaction (HCI) is central to the user interface techniques. Interaction design is relevant for the novel correction mechanisms. Accessibility is not explicitly addressed, and other categories are tangential.","HCI design and evaluation methods:1.0,Interaction design process and methods:0.9,Empirical studies in interaction design:0.6,Interaction techniques:0.5,Systems and tools for interaction design:0.4,Empirical studies in HCI:0.3,Interaction paradigms:0.2,Interaction devices:0.1,HCI theory, concepts and models:0.0,Interaction design theory, concepts and paradigms:0.0","HCI design and evaluation methods,Interaction design process and methods",HCI design and evaluation methods are relevant for the usability trials. Interaction design process is relevant for interface strategies. Other options like empirical studies are less directly tied to the technical interface design focus.
2983,Design of Web Agents Inspired by Brain Research,"The paper presents an approach to combine knowledge from memory and brain sciences with information retrieval research in the design of Web agents. An information retrieval agent for classification of Web pages based on genre features is used. In developing the agent to adapt to users' search preferences, a neuro-cognitive model of human episodic memory is employed. Our studies show that neuro-realistic models, capable of abstraction of meaningful fragments of knowledge, rather than snapshots of the retrieved Web pages, are closer to the human way of interacting with the Web and can be used for optimization of agent performance.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.9,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Human-centered computing,"Human-centered computing: The paper integrates neuro-cognitive models into web agents to simulate human-like information retrieval behavior, directly addressing human-computer interaction design. Other categories are not central to the behavioral modeling focus.","Accessibility:0.2,Collaborative and social computing:0.3,Human computer interaction (HCI):0.9,Interaction design:0.8,Ubiquitous and mobile computing:0.4,Visualization:0.2","Human computer interaction (HCI),Interaction design",HCI and Interaction design are relevant as the paper integrates neuro-cognitive models into Web agent design for user interaction. Other fields like Visualization are less central to the agent's cognitive modeling focus.,"Empirical studies in HCI:0.2,Empirical studies in interaction design:0.2,HCI design and evaluation methods:0.7,HCI theory, concepts and models:0.6,Interaction design process and methods:0.3,Interaction design theory, concepts and paradigms:0.3,Interaction devices:0.0,Interaction paradigms:0.2,Interaction techniques:0.1,Interactive systems and tools:0.4,Systems and tools for interaction design:0.1","HCI design and evaluation methods,HCI theory, concepts and models","HCI design and evaluation methods: The paper presents a novel design approach for Web agents using neuro-cognitive models. HCI theory, concepts and models: The integration of brain research into agent design aligns with theoretical HCI models. Other categories like Interaction devices or Interaction techniques are less relevant as the focus is on design theory and evaluation methods rather than specific tools or paradigms."
3268,Mental registration of 2D and 3D visualizations (an empirical study),"2D and 3D views are used together in many visualization domains, such as medical imaging, flow visualization, oceanographic visualization, and computer aided design (CAD). Combining these views into one display can be done by: (1) orientation icon (i.e., separate windows), (2) in-place methods (e.g., clip and cutting planes), and (3) a new method called ExoVis. How 2D and 3D views are displayed affects ease of mental registration (understanding the spatial relationship between views), an important factor influencing user performance. This paper compares the above methods in terms of their ability to support mental registration. Empirical results show that mental registration is significantly easier with in-place displays than with ExoVis, and significantly easier with ExoVis than with orientation icons. Different mental transformation strategies can explain this result. The results suggest that ExoVis may be a better alternative to orientation icons when in-place displays are not appropriate (e.g., when in-place methods hide data or cut the 3D view into several pieces).","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.2,Security and privacy:0.1,Human-centered computing:0.9,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Human-centered computing,Human-centered computing: The paper presents an empirical study on user interaction with 2D/3D visualizations. Other options are irrelevant as the focus is on user interface evaluation rather than security or software systems.,"Accessibility:0.0,Collaborative and social computing:0.0,Human computer interaction (HCI):0.9,Interaction design:0.6,Ubiquitous and mobile computing:0.0,Visualization:0.9","Human computer interaction (HCI),Visualization",Human computer interaction (HCI) is highly relevant as the study focuses on user interaction with 2D/3D visualizations. Visualization is directly addressed as the core domain of the research. Other categories like Interaction design are less central as the paper emphasizes empirical evaluation of visualization methods rather than design principles.,"Empirical studies in HCI:0.9,Empirical studies in visualization:1,HCI design and evaluation methods:0.5,HCI theory, concepts and models:0.4,Interaction devices:0.3,Interaction paradigms:0.3,Interaction techniques:0.4,Interactive systems and tools:0.5,Visualization application domains:0.6,Visualization design and evaluation methods:1,Visualization systems and tools:0.7,Visualization techniques:0.8,Visualization theory, concepts and paradigms:0.6","Empirical studies in visualization,Visualization design and evaluation methods","The paper is an empirical study comparing visualization methods for mental registration, directly aligning with 'Empirical studies in visualization'. It also evaluates design techniques (e.g., orientation icons vs. in-place displays), making 'Visualization design and evaluation methods' relevant. 'Visualization techniques' is secondary as the paper discusses specific methods but not their technical implementation."
3784,Enhancing the explanatory power of usability heuristics,"Several published sets of usability heuristics were compared with a database of existing usability problems drawn from a variety of projects in order to determine what heuristics best explain actual usability problems. Based on a factor analysis of the explanations as well as an analysis of the heuristics providing the broadest explanatory coverage of the problems, a new set of nine heuristics were derived: visibility of system status, match between system and the real world, user control and freedom, consistency and standards, error prevention, recognition rather than recall, flexibility and efficiency of use, aesthetic and minimalist design, and helping users recognize, diagnose, and recover from errors.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.9,Computing methodologies:0.3,Applied computing:0.3,Social and professional topics:0.1",Human-centered computing,Human-centered computing is directly relevant as the paper focuses on usability heuristics for user experience. Other categories like Applied computing and Computing methodologies are secondary because the core contribution is about human-computer interaction principles rather than algorithms or software systems.,"Accessibility:0.2,Collaborative and social computing:0.1,Human computer interaction (HCI):0.9,Interaction design:0.8,Ubiquitous and mobile computing:0.1,Visualization:0.1","Human computer interaction (HCI),Interaction design","Human computer interaction (HCI): The paper directly addresses usability heuristics, a core topic in HCI. Interaction design: The derived heuristics focus on design principles for user interaction. Other categories are irrelevant as the paper doesn't address accessibility, social computing, mobile computing, or visualization.","Empirical studies in HCI:0.3,Empirical studies in interaction design:0.2,HCI design and evaluation methods:1.0,HCI theory, concepts and models:0.3,Interaction design process and methods:0.2,Interaction design theory, concepts and paradigms:0.3,Interaction devices:0.1,Interaction paradigms:0.1,Interaction techniques:0.1,Interactive systems and tools:0.1,Systems and tools for interaction design:0.1","HCI design and evaluation methods,Empirical studies in HCI",HCI design and evaluation methods is highly relevant as the paper focuses on usability heuristic evaluation methods. Empirical studies in HCI is relevant due to the analysis of usability problems from multiple projects. Other interaction design categories are less directly related to the paper's focus on heuristic evaluation frameworks.
1243,Situated Plan Attribution for Intelligent Tutoring,"Plan recognition techniques frequently make rigid assumptions about the student's plans, and invest substantial effort to infer unobservable properties of the student. The pedagogical benefits of plan recognition analysis are not always obvious. We claim that these difficulties can be overcome if greater attention is paid to the situational context of the student's activity and the pedagogical tasks which plan recognition is intended to support. This paper describes an approach to plan recognition called situated plan attribution that takes these factors into account. It devotes varying amounts of effort to the interpretation process, focusing the greatest effort on interpreting impasse points, i.e., points where the student encounters some difficulty completing the task. This approach has been implemented and evaluated in the context of the REACT tutor, a trainer for Operators of deep space communications stations.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.2,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.9,Computing methodologies:0.4,Applied computing:0.3,Social and professional topics:0.1",Human-centered computing,"Human-centered computing: The paper addresses plan recognition in intelligent tutoring systems, focusing on situational context and pedagogical tasks. Other categories are less relevant as the work emphasizes user modeling and educational interaction rather than software engineering or network design.","Accessibility:0.1,Collaborative and social computing:0.3,Human computer interaction (HCI):0.75,Interaction design:1.0,Ubiquitous and mobile computing:0.2,Visualization:0.1","Interaction design,Human computer interaction (HCI)",Interaction design is highly relevant as the paper discusses situated plan attribution in tutoring systems. Human computer interaction (HCI) is relevant due to the focus on student-plan interpretation. Other categories like Accessibility and Visualization are less relevant as the paper does not address these areas.,"Empirical studies in HCI:0.1,Empirical studies in interaction design:0.1,HCI design and evaluation methods:0.8,HCI theory, concepts and models:0.4,Interaction design process and methods:0.9,Interaction design theory, concepts and paradigms:0.5,Interaction devices:0.1,Interaction paradigms:0.2,Interaction techniques:0.2,Interactive systems and tools:0.3,Systems and tools for interaction design:0.4","HCI design and evaluation methods,Interaction design process and methods",HCI design and evaluation methods: Evaluates situated plan attribution in tutoring. Interaction design process and methods: Focuses on context-aware plan recognition strategies. Other options like 'Empirical studies' are less relevant as the paper emphasizes design methodology over data collection.
3425,Toward Standard Usability Questionnaires for Handheld Augmented Reality,"Usability evaluations are important to improving handheld augmented reality (HAR) systems. However, no standard questionnaire considers perceptual and ergonomic issues found in HAR. The authors performed a systematic literature review to enumerate these issues. Based on these issues, they created a HAR usability scale that consists of comprehensibility and manipulability scales. These scales measure general system usability, ease of understanding the information presented, and ease of handling the device. The questionnaires' validity and reliability were evaluated in four experiments, and the results show that the questionnaires consistently correlate with other subjective and objective measures of usability. The questionnaires also have good reliability based on the Cronbach's alpha. Researchers and professionals can directly use these questionnaires to evaluate their own HAR applications or modify them with the insights presented in this article.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:1.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Human-centered computing,"The paper develops usability questionnaires for AR systems, aligning with Human-centered computing. Other categories like Applied computing are not primary here.","Human computer interaction (HCI):0.9,Ubiquitous and mobile computing:0.8,Interaction design:0.7,Accessibility:0.4,Collaborative and social computing:0.2,Visualization:0.5","Human computer interaction (HCI),Ubiquitous and mobile computing,Interaction design","HCI is core to usability evaluation. Ubiquitous and mobile computing applies to handheld AR devices. Interaction design connects to the usability scales. Accessibility is secondary, and collaborative computing is not emphasized.","Empirical studies in HCI:1.0,Empirical studies in interaction design:0.8,Empirical studies in ubiquitous and mobile computing:1.0,HCI design and evaluation methods:0.9,HCI theory, concepts and models:0.3,Interaction design process and methods:0.4,Interaction design theory, concepts and paradigms:0.3,Interaction devices:0.2,Interaction paradigms:0.2,Interaction techniques:0.2,Interactive systems and tools:0.3,Systems and tools for interaction design:0.4,Ubiquitous and mobile computing design and evaluation methods:1.0,Ubiquitous and mobile computing systems and tools:0.6,Ubiquitous and mobile computing theory, concepts and paradigms:0.5,Ubiquitous and mobile devices:0.7","Empirical studies in HCI,Empirical studies in ubiquitous and mobile computing,Ubiquitous and mobile computing design and evaluation methods","Empirical studies in HCI is relevant because the paper develops and validates a usability questionnaire. Empirical studies in ubiquitous and mobile computing is relevant because the research focuses on handheld AR, a subset of ubiquitous/mobile computing. Ubiquitous and mobile computing design and evaluation methods is relevant as the paper creates evaluation methods for this domain. Other categories are less relevant as they don't directly address the empirical validation of AR usability or the specific domain of handheld AR."
847,QuickReview: A Novel Data-Driven Mobile User Interface for Reporting Problematic App Features,"User-reviews of mobile applications provide information that benefits other users and developers. Even though reviews contain feedback about an app's performance and problematic features, users and app developers need to spend considerable effort reading and analyzing the feedback provided. In this work, we introduce and evaluate QuickReview, an intelligent user interface for reporting problematic app features. Preliminary user evaluations show that QuickReview facilitates users to add reviews swiftly with ease, and also helps developers with quick interpretation of submitted reviews by presenting a ranked list of commonly reported features.","General and reference:0.0,Hardware:0.1,Computer systems organization:0.1,Networks:0.0,Software and its engineering:0.3,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:1.0,Computing methodologies:0.0,Applied computing:0.3,Social and professional topics:0.0",Human-centered computing,"Human-centered computing is highly relevant because the paper introduces QuickReview, a user interface focused on improving user experience and developer feedback in mobile applications. Applied computing and Software and its engineering receive partial relevance due to the application of computing techniques in real-world systems and software design.","Accessibility:0.2,Collaborative and social computing:0.3,Human computer interaction (HCI):0.9,Interaction design:0.8,Ubiquitous and mobile computing:0.7,Visualization:0.4","Human computer interaction (HCI),Interaction design,Ubiquitous and mobile computing","Human computer interaction (HCI) is directly relevant as the paper introduces a novel user interface for mobile app reviews. Interaction design is relevant due to the focus on user interface evaluation. Ubiquitous and mobile computing is relevant because the system is mobile-centric. Accessibility, Collaborative and social computing, and Visualization are less central as the core contribution is interface design, not accessibility, social features, or data visualization.","Empirical studies in ubiquitous and mobile computing:1,Ubiquitous and mobile computing design and evaluation methods:1,Interaction techniques:0.8,Ubiquitous and mobile computing systems and tools:0.6,Empirical studies in HCI:0.4","Empirical studies in ubiquitous and mobile computing,Ubiquitous and mobile computing design and evaluation methods",Empirical studies in ubiquitous and mobile computing: Focuses on mobile app reviews and user behavior. Ubiquitous and mobile computing design and evaluation methods: Involves designing and evaluating a mobile UI. Interaction techniques are secondary; others lack primary focus on mobile-specific design.
5888,Visualizing queries and querying visualizations,"Suppose you are flipping channels on a TV set and flip into the middle of a film that you have seen before. Most people can, within a few seconds, identify the film, recall the title and main actors, and predict what is going to happen next. Alan Kay [Kay91] gives this example to show the remarkable powers of storage and retrieval of visual information that human beings have. We may perhaps one day be able to duplicate some of this power in our computing machines, although the challenges are enormous; in the meantime, computer systems that manage, retrieve and manipulate information should be designed to play to the strengths of human information processing capabilities.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.8,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Human-centered computing,Human-centered computing: Focuses on designing systems aligned with human cognitive strengths. Other categories are irrelevant as the focus is on human-computer interaction principles.,"Accessibility:0.1,Collaborative and social computing:0.1,Human computer interaction (HCI):0.9,Interaction design:0.2,Ubiquitous and mobile computing:0.1,Visualization:0.85","Human computer interaction (HCI),Visualization",Human computer interaction (HCI) is relevant because the paper discusses designing systems that leverage human visual processing capabilities. Visualization is relevant as it focuses on visual information retrieval concepts. Other categories are rejected as the focus is on theoretical HCI concepts rather than accessibility or mobile computing.,"Empirical studies in HCI:0.1,Empirical studies in visualization:0.1,HCI design and evaluation methods:0.1,HCI theory, concepts and models:0.8,Interaction devices:0.1,Interaction paradigms:0.1,Interaction techniques:0.1,Interactive systems and tools:0.1,Visualization application domains:0.1,Visualization design and evaluation methods:0.1,Visualization systems and tools:0.1,Visualization techniques:0.7,Visualization theory, concepts and paradigms:0.1","HCI theory, concepts and models,Visualization techniques","HCI theory, concepts and models is relevant for understanding human visual processing capabilities. Visualization techniques is relevant for designing systems that leverage these capabilities. Other options like Empirical studies in visualization are less directly addressed here."
4475,Phone in the Pocket: Pervasive Self-Tracking of Physical Activity Levels,"Mobile (smart) phones prevail in our daily life activities, and in our research we aim for it to provide pervasive services for wellness. Therefore, we assess the phone’s feasibility to unobtrusively, continuously and in real-time track its user’s physical activity and the resulting energy expenditure (EE). Activity Level Estimator (ALE) is an Android OS application developed for that purpose.We have assessed the accuracy of ALE against the BodyMedia SenseWear (SW) device and the gold standard for EE estimation, i.e., an indirect calorimetry (IC) method. ALE has mean accuracy of 86% (vs. SW) to 93% (vs. IC) for walking, and in 24h it underestimates EE by 23% ALE is currently used for a long-term behavioral trends study with the University of Geneva students and faculty.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.9,Computing methodologies:0.3,Applied computing:0.2,Social and professional topics:0.1",Human-centered computing,Human-centered computing is the most relevant as the paper focuses on pervasive health tracking via smartphones. Applied computing is secondary for the wellness application.,"Accessibility:0.1,Collaborative and social computing:0.2,Human computer interaction (HCI):0.5,Interaction design:0.2,Ubiquitous and mobile computing:0.8,Visualization:0.1","Ubiquitous and mobile computing,Human computer interaction (HCI)","Ubiquitous and mobile computing is relevant for the mobile phone-based activity tracking system, while Human computer interaction (HCI) applies to the real-time user monitoring interface. Other categories lack direct relevance to mobile pervasive computing or user interaction.","Empirical studies in ubiquitous and mobile computing:1,Ubiquitous and mobile computing design and evaluation methods:1,Ubiquitous and mobile computing systems and tools:0.7,Ubiquitous and mobile devices:0.5,Empirical studies in HCI:0.2,HCI design and evaluation methods:0.2,Interaction devices:0.3,Interaction paradigms:0.2,Interaction techniques:0.2,Interactive systems and tools:0.4,Ubiquitous and mobile computing theory, concepts and paradigms:0.6","Empirical studies in ubiquitous and mobile computing,Ubiquitous and mobile computing design and evaluation methods",Empirical studies in ubiquitous and mobile computing applies to the real-world evaluation. Ubiquitous and mobile computing design and evaluation methods is relevant for the app's development and testing. Other options are less directly aligned with the paper's focus on mobile activity tracking.
3764,Extending direct manipulation in a text editor,"This paper describes the implementation of a prototype text editor that incorporates conversation-like features through the direct-manipulation modality. In this way, traditional direct-manipulation interaction techniques such as direct reference via pointing can be extended to include techniques more commonly associated with human conversation, such as negotiation of reference. The paper illustrates the use of the prototype with an extended example, and discusses research issues raised by the implementation.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:1.0,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Human-centered computing,Human-centered computing is highly relevant as the paper discusses direct-manipulation interaction techniques in text editors. Other categories like Software engineering are irrelevant as the focus is on user interaction design.,"Human computer interaction (HCI):0.95,Interaction design:0.9,Collaborative and social computing:0.15,Ubiquitous and mobile computing:0.05,Accessibility:0.05,Visualization:0.05","Human computer interaction (HCI),Interaction design",Human computer interaction (HCI): The paper explores direct manipulation in text editing. Interaction design: It introduces conversation-like interaction features. Collaborative aspects are not central to the contribution.,"Empirical studies in HCI:0,Empirical studies in interaction design:0,HCI design and evaluation methods:0,HCI theory, concepts and models:0,Interaction design process and methods:0,Interaction design theory, concepts and paradigms:0,Interaction devices:0,Interaction paradigms:0,Interaction techniques:1,Interactive systems and tools:1,Systems and tools for interaction design:0","Interaction techniques,Interactive systems and tools",Interaction techniques: The paper extends direct-manipulation techniques for text editing. Interactive systems and tools: The prototype implementation qualifies as an interactive tool. Other categories lack direct relevance.
265,"Flexible, active support for collaboration with ConversationBuilder",We overview the ConversationBuilcfer system and its demonstration at INTERCHI 93.,"General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.9,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Human-centered computing,Human-centered computing is highly relevant as the paper presents a system for collaborative conversation support. Other categories are irrelevant given the short abstract focusing on a human-computer interaction tool.,"Accessibility:0.1,Collaborative and social computing:0.8,Human computer interaction (HCI):0.7,Interaction design:0.3,Ubiquitous and mobile computing:0.1,Visualization:0.1","Collaborative and social computing,Human computer interaction (HCI)",Collaborative and social computing is highly relevant for a system designed to support collaboration. Human computer interaction (HCI) is relevant for interaction design principles. Other fields like Ubiquitous computing are not mentioned.,"Collaborative and social computing design and evaluation methods:0,Collaborative and social computing devices:0,Collaborative and social computing systems and tools:1,Collaborative and social computing theory, concepts and paradigms:0,Empirical studies in HCI:0,Empirical studies in collaborative and social computing:0,HCI design and evaluation methods:0,HCI theory, concepts and models:0,Interaction devices:0,Interaction paradigms:0,Interaction techniques:0,Interactive systems and tools:0",Collaborative and social computing systems and tools,Collaborative and social computing systems and tools is relevant because the paper describes the ConversationBuilder system. Other categories like empirical studies or design methods are not discussed in the brief abstract.
3546,AirPincher: a handheld device for recognizing delicate mid-air hand gestures,"We propose AirPincher, a handheld device for recognizing delicate mid-air hand gestures. AirPincher is designed to overcome disadvantages of the two kinds of existing hand gesture-aware techniques such as wearable sensor-based and external vision-based. The wearable sensor-based techniques cause cumbersomeness of wearing sensors every time and the external vision-based techniques incur performance dependence on distance between a user and a remote display. AirPincher allows a user to hold the device in one hand and to generate several delicate mid-air finger gestures. The gestures are captured by several sensors proximately embedded into AirPincher. These features help AirPincher avoid the aforementioned disadvantages of the existing techniques. It allows several delicate finger gestures, for example, rubbing a thumb against a middle finger, swiping with a thumb on an index finger, pinching with a thumb and an index finger, etc. Due to the inherent haptic feedback of these gestures, AirPincher eventually supports the eyes-free interaction. To validate AirPincher's feasibility, we implemented two use cases, i.e., controlling a pointing cursor and moving a virtual 3D object on the remote screen.","General and reference:0.1,Hardware:0.5,Computer systems organization:0.2,Networks:0.1,Software and its engineering:0.3,Theory of computation:0.2,Mathematics of computing:0.2,Information systems:0.2,Security and privacy:0.1,Human-centered computing:1.0,Computing methodologies:0.6,Applied computing:0.3,Social and professional topics:0.1",Human-centered computing,Human-centered computing is highly relevant as the paper focuses on mid-air hand gesture interaction for user interfaces. Other categories like Hardware are less relevant since the focus is on interaction design rather than hardware development.,"Accessibility:0.0,Collaborative and social computing:0.0,Human computer interaction (HCI):0.9,Interaction design:0.7,Ubiquitous and mobile computing:0.9,Visualization:0.0","Human computer interaction (HCI),Ubiquitous and mobile computing",Human computer interaction (HCI) is relevant for the mid-air gesture recognition interface design. Ubiquitous and mobile computing is relevant for the handheld device enabling mobile interaction. Interaction design received a lower score as the paper focuses on interaction methods rather than design principles.,"Empirical studies in HCI:0.0,Empirical studies in ubiquitous and mobile computing:0.0,HCI design and evaluation methods:0.0,HCI theory, concepts and models:0.0,Interaction devices:1.0,Interaction paradigms:0.2,Interaction techniques:1.0,Interactive systems and tools:0.0,Ubiquitous and mobile computing design and evaluation methods:0.0,Ubiquitous and mobile computing systems and tools:0.8,Ubiquitous and mobile computing theory, concepts and paradigms:0.0,Ubiquitous and mobile devices:0.8","Interaction devices,Interaction techniques","Interaction devices is relevant as the paper introduces a novel handheld device for gesture recognition. Interaction techniques is relevant due to the design of mid-air gesture interaction methods. Ubiquitous and mobile computing systems/tools are moderately relevant as the device is handheld, but the primary focus is on interaction design."
5466,"Comparing alice, greenfoot & scratch","This panel will showcase and compare three leading Initial Learning Environments (ILE): Alice, Greenfoot and Scratch.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.85,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Human-centered computing,Human-centered computing: The paper compares educational tools for learning environments. Other categories like Applied computing or Theory of computation are irrelevant to the focus on educational interfaces.,"Education:0.9,Human computer interaction (HCI):0.3,Interaction design:0.2,Accessibility:0.1,Collaborative and social computing:0.1,Visualization:0.1,Ubiquitous and mobile computing:0.1",Education,Education is highly relevant as the focus is on comparing learning environments. Other categories are less relevant as the paper does not focus on interaction design or collaboration.,"Collaborative learning:0,Computer-assisted instruction:0,Computer-managed instruction:0,Digital libraries and archives:0,Distance learning:0,E-learning:1,Interactive learning environments:1,Learning management systems:0","E-learning,Interactive learning environments","E-learning: The paper discusses tools for initial learning environments in education. Interactive learning environments: Alice, Greenfoot, and Scratch are all interactive systems for learning programming. Other categories like 'Collaborative learning' are irrelevant as the focus is on individual learning tools, not collaboration."
204,Recognising “success” and “failure”: evaluating groupware in a commercial context,"This paper reports on the installation and use of two commercial ‘groupware’ systems, a videolink and electronic document exchange, in a major clearing bank in the UK. It suggests some of the complexities involved in evaluating groupware in a business such that recognising and understanding even such gross terms as ‘success’ and ‘failure’ is far from easy. Consequently it argues that a more nuanced appreciation of success and failure needs to be developed and, therefore, some form of ethnographic, ‘illuminative’ evaluation may prove commercially worthwhile.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:1.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Human-centered computing,Human-centered computing: The paper evaluates groupware systems through ethnographic methods in a commercial context. Other categories are irrelevant as the focus is on user experience and organizational impact.,"Accessibility:0.0,Collaborative and social computing:1.0,Human computer interaction (HCI):0.5,Interaction design:0.0,Ubiquitous and mobile computing:0.0,Visualization:0.0",Collaborative and social computing,Collaborative and social computing is relevant for evaluating groupware systems. Human-computer interaction is marginally relevant for interface aspects.,"Collaborative and social computing design and evaluation methods:1.0,Collaborative and social computing devices:0.1,Collaborative and social computing systems and tools:0.2,Collaborative and social computing theory, concepts and paradigms:0.1,Empirical studies in collaborative and social computing:0.8","Collaborative and social computing design and evaluation methods,Empirical studies in collaborative and social computing",Collaborative and social computing design and evaluation methods is relevant for evaluating groupware systems. Empirical studies is relevant for the ethnographic evaluation approach. Other options are less relevant to the evaluation context.
944,"Collaboration Meets Interactive Surfaces (CMIS): Walls, Tables, Mobiles, and Wearables","This workshop proposes to bring together researchers who are interested in improving collaborative experiences through the combination of multiple interaction surfaces with diverse sizes and formats, ranging from large-scale walls, to tables, mobiles, and wearables. The opportunities for innovation exist, but the ITS, CHI, CSCW, and other HCI communities have not yet thoroughly addressed the problem of bringing effective collaboration activities together using multiple interactive surfaces, especially in complex work domains. Of particular interest is the potential synergy that one can obtain by effectively combining different-sized surfaces and sharing information between devices.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.9,Computing methodologies:0.3,Applied computing:0.3,Social and professional topics:0.1",Human-centered computing,"Human-centered computing is relevant as the paper addresses multi-device collaboration through interactive surfaces, a core topic in human-computer interaction. Applied computing and Computing methodologies are less relevant as the focus is on collaborative interface design rather than algorithmic methods or real-world applications.","Accessibility:0.1,Collaborative and social computing:0.9,Human computer interaction (HCI):0.8,Interaction design:0.3,Ubiquitous and mobile computing:0.5,Visualization:0.1","Collaborative and social computing,Human computer interaction (HCI)",Collaborative and social computing is directly addressed in the workshop's focus on multi-surface collaboration. Human computer interaction (HCI) is core to the study of interactive surfaces. Ubiquitous computing is somewhat relevant but secondary. Accessibility and Visualization are not central themes.,"Collaborative and social computing systems and tools:1.0,Collaborative and social computing devices:0.7,Interaction techniques:0.8,Interaction paradigms:0.7,Empirical studies in collaborative and social computing:0.3,HCI design and evaluation methods:0.4,Interaction devices:0.5,Others:0.2","Collaborative and social computing systems and tools,Interaction techniques,Interaction paradigms",Collaborative and social computing systems and tools is highly relevant for designing multi-surface collaborative systems. Interaction techniques and paradigms are relevant for the interaction methods discussed. Other categories like empirical studies or HCI design methods are less central.
3047,Demo: using mobile devices to personalize pervasive displays,"Our work is motivated by a vision of large-scale open pervasive display networks [4]. In contrast to traditional display networks that typically aim to push content at viewers, the vision of open pervasive display networks focuses on displays that show content from a wide range of sources and that reflect both the context of the display and its audience. There is a significant body of work that has explored how content shown on public displays can be tailored for viewers in the vicinity. In [5] the authors proposed a system that detected Bluetooth devices carried by viewers in order to schedule appropriate adverts. More recently Davies et al [3] proposed the use of Bluetooth device names as a means by which users could signal their interests to nearby displays effectively providing a unidirectional control channel between users and pervasive displays. Using this system viewers could request specific content and applications to be shown such as photo steams, navigation aids and results of web searches. However, power and privacy concerns mean that many users no longer leave their Bluetooth devices switched on and alternative approaches to display personalization are required. Indeed, the emergence of smart-phones with built-in local area wireless networks and accurate positioning systems offers the potential for a wide range of new solutions in this space. At HotMobile 2012 we showed a working implementation of Tacita – a system that capitalises on the widespread availability of smart-phones and cloudbased applications to provide an alternative approach to supporting display personalisation.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:1.0,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Human-centered computing,Human-centered computing is directly relevant as the paper focuses on personalizing pervasive displays through user interaction. Other categories like Networks are not central to the user interface personalization framework.,"Accessibility:0.3,Collaborative and social computing:0.4,Human computer interaction (HCI):0.8,Interaction design:0.6,Ubiquitous and mobile computing:1.0,Visualization:0.2","Ubiquitous and mobile computing,Human computer interaction (HCI)",Ubiquitous and mobile computing is highly relevant as the study focuses on smartphone-based personalization of public displays. Human computer interaction (HCI) is relevant due to the user-centered design approach. Accessibility and Visualization are less relevant as the paper does not focus on assistive technologies or visual design.,"Empirical studies in HCI:0.7,Empirical studies in ubiquitous and mobile computing:0.8,HCI design and evaluation methods:0.6,HCI theory, concepts and models:0.5,Interaction devices:0.4,Interaction paradigms:1.0,Interaction techniques:0.6,Interactive systems and tools:0.8,Ubiquitous and mobile computing design and evaluation methods:0.6,Ubiquitous and mobile computing systems and tools:1.0,Ubiquitous and mobile computing theory, concepts and paradigms:0.5,Ubiquitous and mobile devices:0.9","Interaction paradigms,Ubiquitous and mobile computing systems and tools",Interaction paradigms is relevant for the novel Bluetooth-based interaction approach. Ubiquitous and mobile computing systems and tools directly addresses the technical implementation of display personalization. Other HCI categories are less central to the system design focus.
83,3D visualization of WWW semantic content for browsing and query formulation,"Visualization is a promising technique for both enhancing users' perception of structure in the Internet and providing navigation facilities for its large information spaces. This paper describes an application of the Document Explorer to the visualization of WWW content structure. The system provides visualization, browsing, and query formulation mechanisms based on documents» semantic content. These mechanisms complement text and link-based search by supplying a visual search and query formulation environment using semantic associations among documents. The user can view and interact with visual representations of WWW document relations to traverse this derived document space. The relationships among individual keywords in the documents are also represented visually to support query formulation by direct manipulation of content words in the document set. A suite of navigation and orientation tools is provided which focuses on orientation and navigation using the visual representations of document set and term collection.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.2,Networks:0.1,Software and its engineering:0.3,Theory of computation:0.1,Mathematics of computing:0.2,Information systems:0.4,Security and privacy:0.1,Human-centered computing:0.8,Computing methodologies:0.5,Applied computing:0.3,Social and professional topics:0.1",Human-centered computing,Human-centered computing is relevant for semantic visualization and user interaction. Other categories are less relevant as the focus is not on databases or security.,"Accessibility:0.0,Collaborative and social computing:0.1,Human computer interaction (HCI):0.8,Interaction design:0.6,Ubiquitous and mobile computing:0.2,Visualization:0.9","Visualization,Human computer interaction (HCI)",Visualization is central to the paper's semantic content mapping. Human computer interaction is relevant for the user interaction aspects. Interaction design receives a mid score as it's less explicitly focused.,"Empirical studies in HCI:0.3,Empirical studies in visualization:0.4,HCI design and evaluation methods:0.3,HCI theory, concepts and models:0.2,Interaction devices:0.1,Interaction paradigms:0.2,Interaction techniques:0.3,Interactive systems and tools:0.5,Visualization application domains:0.9,Visualization design and evaluation methods:0.7,Visualization systems and tools:0.6,Visualization techniques:0.8,Visualization theory, concepts and paradigms:0.6","Visualization application domains,Visualization techniques","Visualization application domains is relevant for applying 3D visualization to web content. Visualization techniques is central to the semantic content representation. Interactive systems are secondary as the focus is on visualization, not interaction."
1141,Subliminal wiretapping,"Subliminal Wiretapping is a subtly interactive artwork that utilizes random number generation modified through mind-matter effects to supply a continuous stream of words. Frequent, personal connections emerge from participants interpreting the stream as the words appear.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.8,Computing methodologies:0.1,Applied computing:0.2,Social and professional topics:0.1",Human-centered computing,Human-centered computing is relevant as the paper explores interactive art and user experience with mind-matter effects. Other categories like Applied computing (0.2) are marginally relevant but not central to the core contribution of the artwork.,"Accessibility:0.1,Collaborative and social computing:0.2,Human computer interaction (HCI):0.8,Interaction design:0.7,Ubiquitous and mobile computing:0.3,Visualization:0.1","Human computer interaction (HCI),Interaction design",HCI is central to the interactive artwork. Interaction design applies to the mind-matter effects implementation. Other categories are not primary.,"Empirical studies in HCI:0,Empirical studies in interaction design:0,HCI design and evaluation methods:0.5,HCI theory, concepts and models:0,Interaction design process and methods:1,Interaction design theory, concepts and paradigms:0.5,Interaction devices:0,Interaction paradigms:0,Interaction techniques:1,Interactive systems and tools:1,Systems and tools for interaction design:0","Interaction design process and methods,Interaction techniques,Interactive systems and tools",Interaction design process and methods: The artwork involves designing interactive systems. Interaction techniques: The paper describes mind-matter effects as interaction mechanisms. Interactive systems and tools: The work is an interactive art installation. Empirical studies are not central to this creative work.
2769,Supporting social presence through lightweight photo sharing on and off the desktop,"Lightweight photo sharing, particularly via mobile devices, is fast becoming a common communication medium used for maintaining a presence in the lives of friends and family. How should such systems be designed to maximize this social presence while maintaining simplicity? An experimental photo sharing system was developed and tested that, compared to current systems, offers highly simplified, group-centric sharing, automatic and persistent people-centric organization, and tightly integrated desktop and mobile sharing and viewing. In an experimental field study, the photo sharing behaviors of groups of family or friends were studied using their normal photo sharing methods and with the prototype sharing system. Results showed that users found photo sharing easier and more fun, shared more photos, and had an enhanced sense of social presence when sharing with the experimental system. Results are discussed in the context of design principles for the rapidly increasing number of lightweight photo sharing systems.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.85,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Human-centered computing,Human-centered computing is central for social presence in photo sharing systems. Other fields like Software engineering are not the primary focus.,"Accessibility:0.1,Collaborative and social computing:0.9,Human computer interaction (HCI):0.85,Interaction design:0.8,Ubiquitous and mobile computing:0.75,Visualization:0.2","Collaborative and social computing,Human computer interaction (HCI),Ubiquitous and mobile computing",Collaborative and social computing is highly relevant as the paper focuses on social presence through photo sharing. Human computer interaction is relevant for the design of the sharing interface. Ubiquitous and mobile computing is relevant for the mobile-centric implementation. Visualization is less central to the paper's core contribution.,"Collaborative and social computing design and evaluation methods:0.3,Collaborative and social computing devices:0.0,Collaborative and social computing systems and tools:0.8,Collaborative and social computing theory, concepts and paradigms:0.2,Empirical studies in HCI:0.1,Empirical studies in collaborative and social computing:0.9,Empirical studies in ubiquitous and mobile computing:0.3,HCI design and evaluation methods:0.1,HCI theory, concepts and models:0.0,Interaction devices:0.0,Interaction paradigms:0.0,Interaction techniques:0.0,Interactive systems and tools:0.5,Ubiquitous and mobile computing design and evaluation methods:0.2,Ubiquitous and mobile computing systems and tools:0.4,Ubiquitous and mobile computing theory, concepts and paradigms:0.1,Ubiquitous and mobile devices:0.1","Collaborative and social computing systems and tools,Empirical studies in collaborative and social computing",Collaborative and social computing systems and tools is relevant for the photo sharing system design. Empirical studies in collaborative and social computing is relevant for the field study methodology. Other categories like ubiquitous computing are secondary to the social computing focus.
4431,"We Had a Blast!: An Empirical Affirmation of Blended Learning as the Preferred Learning Mode for Adult Learners""","As many important issues pertaining to blended learning within the Sub-Saharan African context remain unexplored, this study implemented a blended learning approach in a graduate level course at a private university in Ghana, with the objective of exploring adult learners' attitudes, experiences and behaviors towards this learning approach, as well as their perceptions towards blended learning in general. Forty-eight graduate students participated in the study as they engaged in a six-week long blended learning course. Qualitative research methods were used to gather data which were analysed using grounded theory coding techniques, descriptive statistics and content analysis. Findings reveal high levels of student engagement and satisfaction with the learning processes, and an overwhelming endorsement of blended learning as a preferred mode of learning. Implications of these findings for further research and practice are discussed within the context of technology adoption and use in the Ghanaian and Sub-Saharan African higher education contexts.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.85,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Human-centered computing,"Human-centered computing: The study investigates adult learners' experiences with blended learning technologies, focusing on human interaction with educational systems.","Human computer interaction (HCI):1.0,Accessibility:0.2,Collaborative and social computing:0.2,Interaction design:0.2,Ubiquitous and mobile computing:0.1,Visualization:0.1",Human computer interaction (HCI),Human computer interaction (HCI) is relevant as the study examines adult learners' interactions with blended learning systems. Other categories lack direct focus on user-system interaction or educational technology.,"Empirical studies in HCI:1.0,HCI design and evaluation methods:1.0,HCI theory, concepts and models:0.0,Interaction devices:0.0,Interaction paradigms:0.0,Interaction techniques:0.0,Interactive systems and tools:0.0","Empirical studies in HCI,HCI design and evaluation methods",Empirical studies are core to the qualitative research methodology. Design/evaluation methods apply to the blended learning implementation. No other categories align with the educational context.
4473,Context dependent syllable acoustic model for continuous Chinese speech recognition,"The choice of basic modeling unit in building acoustic model for a continuous Mandarin speech recognition task is a very important issue [1]. Unlike traditional phoneme or Initial/Finals (IFs) units based acoustic modeling methods, which usually suffer from the limitations of less accuracy in modeling intrasyllable variations and long scale temporal dependencies, in this paper, a practicable syllable based approach is presented. In contrast with IFs, syllable can implicitly model the intrasyllable variations in good accuracy. Also, by carefully choosing context modeling schemes and parameter tying methods, syllable based acoustic model can capture longer temporal variations while keeping the complexity of model well controlled. Meanwhile, considering the data unbalanced problem, multiple sized unit model based approaches are also implemented in this research. The experiment result shows the acoustic model based on the presented syllable based approach is effective in improving the performance of the Chinese continuous speech recognition.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.2,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.9,Computing methodologies:0.3,Applied computing:0.2,Social and professional topics:0.1",Human-centered computing,Human-centered computing is the most relevant as the paper improves speech recognition for human interaction. Computing methodologies is secondary for the algorithmic approach.,"Accessibility:0.1,Collaborative and social computing:0.1,Human computer interaction (HCI):0.9,Interaction design:0.2,Ubiquitous and mobile computing:0.3,Visualization:0.1",Human computer interaction (HCI),"Human computer interaction (HCI) is relevant because the paper presents a syllable-based acoustic model for continuous speech recognition, a core component of speech interface design. Other categories are rejected due to lack of direct relevance to speech recognition systems or user interaction paradigms.","Empirical studies in HCI:0.1,HCI design and evaluation methods:0.1,HCI theory, concepts and models:0.1,Interaction devices:0.1,Interaction paradigms:0.1,Interaction techniques:0.1,Interactive systems and tools:0.1",,"None of the options are relevant. The paper focuses on speech recognition modeling, not HCI design, evaluation, or interaction techniques."
4383,"Bigger is not always better: display size, performance, and task load during peephole map navigation","Dynamic peephole navigation is an increasingly popular technique for navigating large information spaces such as maps. Users can view the map through handheld, spatially aware displays that serve as peepholes and navigate the map by moving these displays in physical space. We conducted a controlled experiment of peephole map navigation with 16 participants to better understand the effect of a peephole's size on users' map navigation behavior, navigation performance, and task load. Simulating different peephole sizes from 4' (smartphone) up to 120' (control condition), we confirmed that larger peepholes significantly improve learning speed, navigation speed, and reduce task load; however, this added benefit diminishes with growing sizes. Our data shows that a relatively small, tablet-sized peephole can serve as a 'sweet spot' between peephole size and both user navigation performance and user task load.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.9,Computing methodologies:0.0,Applied computing:0.5,Social and professional topics:0.0",Human-centered computing,Human-centered computing is highly relevant as the paper studies display size effects on peephole map navigation. Applied computing has moderate relevance but is secondary to the human factors focus.,"Accessibility:0.1,Collaborative and social computing:0.1,Human computer interaction (HCI):0.9,Interaction design:0.8,Ubiquitous and mobile computing:0.6,Visualization:0.4","Human computer interaction (HCI),Interaction design","Human computer interaction (HCI) is central as the study evaluates peephole navigation techniques. Interaction design is relevant for analyzing display size effects on user performance. Ubiquitous and mobile computing is secondary as the focus is on interaction, not mobility per se.","Empirical studies in HCI:1,Empirical studies in interaction design:1,HCI design and evaluation methods:0.5,HCI theory, concepts and models:0,Interaction design process and methods:0.5,Interaction design theory, concepts and paradigms:0,Interaction devices:0,Interaction paradigms:0,Interaction techniques:1,Interactive systems and tools:0,Systems and tools for interaction design:0","Empirical studies in HCI,Interaction techniques",Empirical studies in HCI is core to the controlled experiment conducted. Interaction techniques applies to the peephole navigation method. HCI design and evaluation methods is moderately relevant as the study evaluates user performance.
261,An investigation of search strategies for hypothesis generation using eye movement data,"The hypothesis generation process is divided into two phases: the search phase and the decision phase. In the former phase, many possibilities for the hypotheses are considered; and in the latter phase a focal rule is decided upon. In the current study, we investigated the search phase using eye movement data. We detected two types of search strategy: one strategy was a spread-attention search in which participants spread their attention over a broad area in rule space; and the other was a focused-attention search in which participants concentrate their attention on a narrow area in rule space. Results of our experiment showed that the focused-attention search strategy was more effective for rule discovery than the spread-attention search strategy.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.9,Computing methodologies:0.3,Applied computing:0.1,Social and professional topics:0.1",Human-centered computing,"Human-centered computing is highly relevant as the paper studies human search strategies in hypothesis generation using eye movement data. Other categories are irrelevant since the focus is on human cognitive processes rather than algorithms, hardware, or software systems.","Accessibility:0.1,Collaborative and social computing:0.1,Human computer interaction (HCI):0.9,Interaction design:0.2,Ubiquitous and mobile computing:0.1,Visualization:0.1",Human computer interaction (HCI),Human computer interaction (HCI) is highly relevant as the study investigates human cognitive strategies during hypothesis generation using eye movement data. Other fields like Accessibility or Visualization are not directly addressed in the abstract.,"Empirical studies in HCI:1.0,HCI design and evaluation methods:0.3,HCI theory, concepts and models:0.8,Interaction devices:0.2,Interaction paradigms:0.3,Interaction techniques:0.2,Interactive systems and tools:0.3","Empirical studies in HCI,HCI theory, concepts and models","Empirical studies in HCI: The paper uses eye movement experiments to study strategies. HCI theory, concepts and models: Discusses focused vs. spread attention as theoretical models. Other options like Interaction devices are not central."
3860,Exploring pressure as an alternative to multi-touch based interaction,"Pressure is a useful medium for interaction as it can be used in different contexts such as for navigating through depth in 3-D, for time-series visualizations, and in zoomable interfaces. We propose pressure based input as an alternative to repetitive multi-touch interactions, such as expanding/pinching to zoom. While most user interface controls for zooming or scrolling are bidirectional, pressure is primarily a one-way continuous parameter (from zero to positive). Human ability to control pressure from positive to zero is limited but needs to be resolved to make this medium accessible to various interactive tasks. We first carry out an experiment to measure the effectiveness of various pressure control functions for controlling pressure in both directions (from zero to positive and positive to zero). Based on this preliminary knowledge, we compare the performance of a pressure based zooming system with a multi-touch expand/pinch gesture based zooming system. Our results show that pressure input is an improvement to multi-touch interactions that involve multiple invocations, such as the one presented in this paper.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.8,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Human-centered computing,"Human-centered computing: The paper evaluates pressure-based interaction as an alternative to multi-touch, directly addressing human-computer interaction design. Other categories are irrelevant to the user interface focus.","Accessibility:0.5,Collaborative and social computing:0.3,Human computer interaction (HCI):1.0,Interaction design:0.75,Ubiquitous and mobile computing:0.3,Visualization:0.3","Human computer interaction (HCI),Interaction design",Human computer interaction is core to the pressure-based interaction study. Interaction design is relevant for comparing input methods. Accessibility is secondary to the primary focus on interaction techniques.,"Empirical studies in HCI:0,Empirical studies in interaction design:0,HCI design and evaluation methods:0,HCI theory, concepts and models:0,Interaction design process and methods:0,Interaction design theory, concepts and paradigms:0,Interaction devices:0,Interaction paradigms:0,Interaction techniques:1,Interactive systems and tools:0,Systems and tools for interaction design:0",Interaction techniques,The paper introduces pressure-based interaction as a technique compared to multi-touch. Other interaction categories are not central to the contribution.
2880,Digital Handwriting with a Finger or a Stylus: A Biomechanical Comparison,"In this paper, we present a study concerning the human hand during digital handwriting on a tablet. Two different cases are considered: writing with the finger, and writing with the stylus. We chose an approach based on the biomechanics of the human hand to compare the two different input methods. Performance is evaluated using metrics originally introduced and developed in robotics, such as the manipulability indexes. Analytical results assess that writing with the finger is more suitable for performing large, but not very accurate motions, while writing with the stylus leads to a higher precision and more isotropic motion performance. We then carried out two experiments of digital handwriting to support the approach and contextualize the results.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.2,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.9,Computing methodologies:0.3,Applied computing:0.2,Social and professional topics:0.1",Human-centered computing,"Human-centered computing is highly relevant as the paper studies biomechanics of digital handwriting input methods. Computing methodologies (0.3) receives moderate relevance for the analytical metrics, but the primary focus is on human-computer interaction.","Accessibility:0.2,Collaborative and social computing:0.1,Human computer interaction (HCI):1.0,Interaction design:0.3,Ubiquitous and mobile computing:0.4,Visualization:0.1",Human computer interaction (HCI),Human computer interaction (HCI) is directly relevant as the paper studies digital handwriting input methods and their biomechanical performance. Other categories like Accessibility have limited relevance as the study focuses on performance analysis rather than accessibility design. Interaction design and Ubiquitous computing are secondary as the core contribution is about input method evaluation.,"Empirical studies in HCI:1,HCI design and evaluation methods:1,HCI theory, concepts and models:0.5,Interaction devices:0.3,Interaction paradigms:0,Interaction techniques:1,Interactive systems and tools:0.3","Interaction techniques,HCI design and evaluation methods,Empirical studies in HCI",Interaction techniques is highly relevant as the paper compares finger vs. stylus input methods. HCI design and evaluation methods is relevant due to the empirical evaluation using biomechanical metrics. Empirical studies in HCI is relevant as it presents a quantitative study. Interaction paradigms and interactive systems are less relevant as the focus is on specific input techniques rather than broader paradigms or system design.
4454,A negotiation architecture for fluid documents,"The information presented in a document often consists of primary content as well as supporting material such as explanatory notes, detailed derivations, illustrations, and the like. We introduce a class of user interface techniques for fluid documents that supports the reader’s shift to supporting material while maintaining the context of the primary material. Our approach initially minimizes the intrusion of supporting material by presenting it as a small visual cue near the annotated primary material. When the user expresses interest in the annotation, it expands smoothly to a readable size. At the same time, the primary material makes space for the expanded annotation. The expanded supporting material must be given space to occupy, and it must be made salient with respect to the surrounding primary material. These two aspects, space and salience, are subject to a negotiation between the primary and supporting material. This paper presents the components of our fluid document techniques and describes the negotiation architecture for ensuring that the presentations of both primary and supporting material are honored.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.9,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Human-centered computing,"Human-centered computing: The paper focuses on user interface techniques for document interaction. Other categories: The work is not about software engineering, computing methodologies, or hardware.","Accessibility:0.0,Collaborative and social computing:0.0,Human computer interaction (HCI):1.0,Interaction design:1.0,Ubiquitous and mobile computing:0.0,Visualization:0.0","Human computer interaction (HCI),Interaction design",Human computer interaction is highly relevant as the paper introduces novel UI techniques for fluid documents. Interaction design is also highly relevant due to the focus on designing user interface components. Visualization is less relevant as the work is about interface negotiation rather than data visualization.,"Empirical studies in HCI:0.2,Empirical studies in interaction design:0.2,HCI design and evaluation methods:1.0,HCI theory, concepts and models:0.3,Interaction design process and methods:1.0,Interaction design theory, concepts and paradigms:0.3,Interaction devices:0.1,Interaction paradigms:0.2,Interaction techniques:1.0,Interactive systems and tools:1.0,Systems and tools for interaction design:0.2","HCI design and evaluation methods,Interaction techniques,Interactive systems and tools",HCI design methods are relevant for the interface techniques. Interaction techniques apply to the negotiation architecture. Interactive systems are central to the fluid document implementation. Other categories like Interaction paradigms are less directly relevant.
26,"A visual convergence of print, television, and the internet: charting 40 years of design change in news presentation","Changes in the visual presentation of news media provide insight into the complex, dynamic relationships that exist between print, television, and the internet. This study explores the longitudinal visual development of five major newspapers, seven network and cable news programs, and twelve news websites by examining the progression of structural and graphic design elements that contribute to the trend of ‘scannable’ information presentation. The analysis is broken down by decade, beginning in 1960 and ending in 2002, and the findings indicate that a visual convergence of media has become more pronounced over the decades as the acceleration of information has increased over time. Implications of this study regarding interdisciplinary research are explored and future research avenues are discussed in the conclusion.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.8,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Human-centered computing,Human-centered computing is relevant for the study of visual design in news media. Other categories are rejected as the focus is on human perception and interaction rather than technical computing systems.,"Accessibility:0.0,Collaborative and social computing:0.0,Human computer interaction (HCI):0.75,Interaction design:0.7,Ubiquitous and mobile computing:0.0,Visualization:0.85","Visualization,Human computer interaction (HCI),Interaction design",Visualization is highly relevant as the paper examines visual presentation of information across media. Human computer interaction (HCI) receives a high score as it relates to how humans interact with information. Interaction design is relevant as it involves design of information presentation. Other categories like Accessibility or Collaborative and social computing are less relevant to the paper's focus on visual design.,"Empirical studies in HCI:0.2,Empirical studies in interaction design:0.3,Empirical studies in visualization:1.0,HCI design and evaluation methods:0.4,HCI theory, concepts and models:0.0,Interaction design process and methods:0.5,Interaction design theory, concepts and paradigms:0.0,Interaction devices:0.0,Interaction paradigms:0.0,Interaction techniques:0.0,Interactive systems and tools:0.0,Systems and tools for interaction design:0.0,Visualization application domains:1.0,Visualization design and evaluation methods:0.6,Visualization systems and tools:0.0,Visualization techniques:0.3,Visualization theory, concepts and paradigms:0.0","Empirical studies in visualization,Visualization application domains","Empirical studies in visualization is central to the decade-long design analysis. Visualization application domains is relevant as the study spans print, TV, and web media. Other categories are irrelevant as the focus is on cross-media design evolution rather than specific interaction techniques."
5414,An investigation of the impact of media capabilities and extraversion on social presence and user satisfaction,"A significant body of research examines media use and user satisfaction, and these studies are mostly focused on the choice of a specific media. The purpose of the present study is to investigate the effects of media capabilities and individual characteristics on social presence, and the subsequent impact on user satisfaction. Drawing on Media Synchronicity Theory, we propose a research model that identifies five physical media capabilities as the determinants of social presence (i.e. the degree to which individuals feel connected to others in online communities), and we then assess the effect of social presence on user satisfaction. Our results suggest that (1) certain media capabilities and (2) extraversion have a positive impact on whether individuals feel connected to others in online communities (i.e. computer-mediated communications). In addition, our moderation analysis shows that extraversion affects social presence differently across communication tasks, suggesting that social presence is a result of the dynamic interaction between media capabilities, the individual, and the task. These results should be of interest to organisations that rely upon virtual meetings to accomplish tasks, as well as to media developers who seek media capabilities that promote a feeling of connected communication between individuals in virtual space.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.9,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Human-centered computing,Human-centered computing is highly relevant as the study focuses on social presence and user satisfaction in computer-mediated communication. Other categories like Applied computing or Social and professional topics are less directly related to the core contribution.,"Accessibility:0.1,Collaborative and social computing:0.9,Human computer interaction (HCI):0.9,Interaction design:0.85,Ubiquitous and mobile computing:0.1,Visualization:0.1","Collaborative and social computing,Human computer interaction (HCI), Interaction design","Collaborative and social computing: The study examines social presence in online communities, a core topic in collaborative systems. Human computer interaction (HCI): Focuses on user satisfaction and media capabilities in computer-mediated communication. Interaction design: Analyzes how interface capabilities influence user experience. Other children like visualization are not relevant to the paper's focus on social computing and user interaction theory.","Collaborative and social computing theory, concepts and paradigms:1.0,Empirical studies in collaborative and social computing:0.9,HCI theory, concepts and models:0.7,Empirical studies in HCI:0.6,Collaborative and social computing systems and tools:0.6,Interaction paradigms:0.5,Collaborative and social computing design and evaluation methods:0.5,Interaction design process and methods:0.4,HCI design and evaluation methods:0.4,Interaction techniques:0.3,Interactive systems and tools:0.3,Systems and tools for interaction design:0.3,Interaction design theory, concepts and paradigms:0.2,Empirical studies in interaction design:0.2,Interaction devices:0.2","Collaborative and social computing theory, concepts and paradigms,Empirical studies in collaborative and social computing",Collaborative and social computing theory is highly relevant as the paper examines social presence in online communities. Empirical studies is relevant for the experimental analysis of media effects. HCI theory is secondary relevant but less focused than the primary categories.
4657,Designing the user interface of the computer-based speech training system ARTUR based on early user tests,"This study has been performed in order to evaluate a prototype for the human – computer interface of a computer-based speech training aid named ARTUR. The main feature of the aid is that it can give suggestions on how to improve articulations. Two user groups were involved: three children aged 9 – 14 with extensive experience of speech training with therapists and computers, and three children aged 6, with little or no prior experience of computer-based speech training. All children had general language disorders. The study indicates that the present interface is usable without prior training or instructions, even for the younger children, but that more motivational factors should be introduced. The granularity of the mesh that classifies mispronunciations was satisfactory, but the flexibility and level of detail of the feedback should be developed further.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.1,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.9,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Human-centered computing,"Human-centered computing (0.9): Focuses on user interface design and user testing for a speech training system. Other categories are irrelevant as the paper does not discuss hardware, security, or software engineering.","Human computer interaction (HCI):0.9,Interaction design:0.85,Accessibility:0.3,Collaborative and social computing:0.2,Ubiquitous and mobile computing:0.4,Visualization:0.1","Human computer interaction (HCI),Interaction design","Human computer interaction (HCI) and Interaction design are highly relevant as the paper evaluates and improves the user interface for speech training. Accessibility is less relevant as the focus is on usability for children with language disorders, not accessibility per se.","Empirical studies in HCI:1,Empirical studies in interaction design:0.5,HCI design and evaluation methods:1,HCI theory, concepts and models:0.5,Interaction design process and methods:1,Interaction design theory, concepts and paradigms:0.5,Interaction devices:0,Interaction paradigms:0.5,Interaction techniques:0.5,Interactive systems and tools:0.5,Systems and tools for interaction design:0.5","Empirical studies in HCI,Interaction design process and methods",Empirical studies in HCI are relevant due to the user testing methodology. Interaction design process and methods apply to the interface design and evaluation. Other options like Interaction devices or techniques are less directly addressed.
2328,A design of motion-support robots for human arms using hexahedron rubber actuators,"In the case of mechanical contact between humans and robots including motion-support robots or robotic orthosis, safety is most important. Therefore, in the motion-support robot presented in this paper, hexahedron rubber actuators (HRA) which have light weight, small size and flexibility of motions are used. In this paper, the basic performance of an antagonized HRA unit which is actuated by air pressure is revealed at first. Moreover, effectiveness of a motion-support robot with 4 DOF is demonstrated through experimental results.","General and reference:0.0,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.2,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.8,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Human-centered computing,"Human-centered computing: The paper designs motion-support robots for human arms, emphasizing safety and human interaction. Other categories like Hardware are less relevant as the focus is on human-robot interaction, not physical components.","Accessibility:0.5,Collaborative and social computing:0.1,Human computer interaction (HCI):0.2,Interaction design:0.1,Ubiquitous and mobile computing:0.3,Visualization:0.1",Accessibility,"Accessibility is relevant to safety-focused assistive robotics, though the paper doesn't explicitly mention disability contexts. Other categories like Ubiquitous computing are less relevant as the focus is on mechanical safety.","Accessibility design and evaluation methods:1,Accessibility systems and tools:1,Accessibility technologies:0,Accessibility theory, concepts and paradigms:0,Empirical studies in accessibility:0","Accessibility design and evaluation methods,Accessibility systems and tools","Accessibility design and evaluation methods: The paper presents a motion-support robot design using HRA actuators, focusing on safety and experimental evaluation. Accessibility systems and tools: The system itself (motion-support robot) is the core contribution. Other children are irrelevant as the focus is on system design and safety, not theoretical paradigms."
4821,Illusion of motion induced by tendon electrical stimulation,"Kinesthetic illusion is a well-known phenomenon elicited by vibratory stimulation to muscle tendon, which presumably causes muscle spindle activity. It is a candidate for kinesthetic sense presentation by direct receptors stimulation, which may lead to novel compact or immobile haptic displays. However, kinesthetic illusion requires strong mechanical vibrations, which hinders its practical use. I proposed to use electrical stimulation to muscle tendon, in which Golgi tendon organ resides, to generate the illusion. The experimental results revealed that tendon electrical stimulation elicited a similar illusion of motion to the kinesthetic illusion.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.9,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Human-centered computing,Human-centered computing: The paper focuses on kinesthetic illusions for haptic feedback in computing systems. Other fields like Hardware or Software are less relevant as the primary contribution is in human-computer interaction.,"Accessibility:0.1,Collaborative and social computing:0.1,Human computer interaction (HCI):0.9,Interaction design:0.8,Ubiquitous and mobile computing:0.2,Visualization:0.1","Human computer interaction (HCI),Interaction design",Human computer interaction (HCI) is relevant as the paper explores haptic interaction via kinesthetic illusions. Interaction design is relevant for designing the stimulation method. Other categories lack direct focus on interaction or perception mechanisms.,"Empirical studies in HCI:0.9,Empirical studies in interaction design:0.8,HCI design and evaluation methods:0.7,HCI theory, concepts and models:0.3,Interaction design process and methods:0.4,Interaction design theory, concepts and paradigms:0.3,Interaction devices:1.0,Interaction paradigms:0.2,Interaction techniques:0.5,Interactive systems and tools:0.6,Systems and tools for interaction design:0.4","Interaction devices,Empirical studies in HCI",Interaction devices is directly relevant for the tendon stimulation haptic interface. Empirical studies in HCI applies to the experimental validation. Other categories like interaction techniques are secondary.
2193,Assisting system of visually impaired in touch panel operation using stereo camera,"In this paper, we propose an assisting system of touch panel operation for people with visual disability. In the system, a user specifies the target button on the panel by verbal input. The system detects the button and user's fingertip by analyzing images obtained through stereo camera. Navigation is made by indicating the direction of the fingertip on the panel through headphones with sound. To construct an efficient navigation method, comparisons were made experimentally concerning to indication of the finger motion direction, choice of navigation sound, and indication of the distance. The effectiveness of the proposed method was verified through experiments.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:1.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Human-centered computing,"Human-centered computing: The paper introduces a user-assisting system for visually impaired individuals, focusing on accessibility and interaction design. Other categories like Applied computing are less relevant as the focus is on human-centric interaction rather than technical applications.","Accessibility:0.9,Collaborative and social computing:0.1,Human computer interaction (HCI):0.7,Interaction design:0.2,Ubiquitous and mobile computing:0.3,Visualization:0.1","Accessibility,Human computer interaction (HCI)","Accessibility is highly relevant as the paper focuses on enabling touch panel use for visually impaired users. Human computer interaction is relevant due to the interface design. Other categories are less relevant as the paper is not primarily about social computing, interaction design, or visualization.","Accessibility design and evaluation methods:0.5,Accessibility systems and tools:1,Accessibility technologies:1,Accessibility theory, concepts and paradigms:0,Empirical studies in HCI:0,Empirical studies in accessibility:0.5,HCI design and evaluation methods:0.5,HCI theory, concepts and models:0,Interaction devices:0,Interaction paradigms:0,Interaction techniques:0,Interactive systems and tools:1","Accessibility systems and tools,Accessibility technologies,Interactive systems and tools",The proposed system falls under accessibility systems and tools. Accessibility technologies are directly applied via stereo cameras and sound navigation. Interactive systems and tools describe the interface design. Other categories like empirical studies are secondary.
1609,Medical and Para-Medical Personnel' Perspectives on Home Health Care Technology,User-based research is strongly recommended in design for older adults. The aim of this paper is to focus the attention on the poorly explored role of medical and para-medical personnel’s perspective on home health care technologies using data that have been gained during the “Active Ageing At Home” (AA@H) project. A focus group was organized at the National Institute of Health & Science on Ageing (INRCA) in Italy. Results demonstrate that several challenges deserve a stronger effort by the whole research sector on ageing and technology: (1) a leading role of the participatory design process; (2) the assessment of the added value of health technologies through robust methods; (3) the definition of an unique identity and well established practices among disciplines; (4) the creation of favorable prerequisites and conditions to the technology uptake.,"General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.9,Computing methodologies:0.1,Applied computing:0.3,Social and professional topics:0.1",Human-centered computing,Human-centered computing is relevant as the paper examines human factors in home health technology design. Applied computing is secondary as it's about healthcare applications but not the core focus.,"Accessibility:0.9,Collaborative and social computing:0.3,Human computer interaction (HCI):0.8,Interaction design:0.7,Ubiquitous and mobile computing:0.4,Visualization:0.2","Accessibility,Human computer interaction (HCI)",Accessibility: The paper addresses design challenges for older adults. Human computer interaction (HCI): Focuses on technology adoption by medical personnel. Other categories are less central to the user-centered design focus.,"Accessibility design and evaluation methods:0.3,Accessibility systems and tools:0.2,Accessibility technologies:0.2,Accessibility theory, concepts and paradigms:0.1,Empirical studies in HCI:0.8,Empirical studies in accessibility:0.7,HCI design and evaluation methods:0.4,HCI theory, concepts and models:0.2,Interaction devices:0.1,Interaction paradigms:0.2,Interaction techniques:0.2,Interactive systems and tools:0.3","Empirical studies in accessibility,Empirical studies in HCI",Empirical studies in accessibility and HCI are primary due to the focus on user perspectives and participatory design in healthcare technology. Other categories like Interaction techniques are less central to the paper's core focus.
3573,"Art, Interaction and Engagement","This paper reviews the development of frameworks for thinking and talking about interactive art in the context of my personal practice over the last forty years. It traces a number of paths taken, from an early simple direct notion of interaction through to communication between people through art systems and, more recently, interactive art for long-term engagement. The frameworks consist of an evolving set of concepts, over several dimensions, which are developing together with the practice of interactive art..","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:1.0,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Human-centered computing,Human-centered computing is relevant for interactive art frameworks. Other fields do not address human-art interaction or engagement.,"Accessibility:0.1,Collaborative and social computing:0.1,Human computer interaction (HCI):0.7,Interaction design:0.9,Ubiquitous and mobile computing:0.1,Visualization:0.1","Interaction design,Human computer interaction (HCI)","The paper discusses frameworks for interactive art, making 'Interaction design' primary. 'Human computer interaction (HCI)' is included for the system-based interaction analysis. Other categories like Visualization are not central.","Empirical studies in HCI:0.4,Empirical studies in interaction design:0.5,HCI design and evaluation methods:0.7,HCI theory, concepts and models:1,Interaction design process and methods:0.6,Interaction design theory, concepts and paradigms:1,Interaction devices:0.2,Interaction paradigms:0.5,Interaction techniques:0.4,Interactive systems and tools:1,Systems and tools for interaction design:0.6","HCI theory, concepts and models,Interaction design theory, concepts and paradigms,Interactive systems and tools","HCI theory: The paper develops frameworks for understanding interactive art. Interaction design theory: The work traces evolving interaction paradigms in art. Interactive systems and tools: The focus is on systems enabling long-term engagement. Rejected categories like Interaction devices are irrelevant as the paper discusses conceptual frameworks, not physical devices."
4174,Guest Editorial - Special Issue on Selected Papers From BioCAS 2011,"The eight papers in this special issue were selected from those presented at the 2011 IEEE Biomedical Circuits and Systems Conference (BioCAS 2011), held November 10-12, 2011, in San Diego, CA, USA.","General and reference:0.1,Hardware:0.9,Computer systems organization:0.2,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Hardware,Hardware is relevant because the paper discusses biomedical circuits and systems. Other categories are too broad or unrelated.,"Communication hardware, interfaces and storage:0.2,Electronic design automation:0.1,Emerging technologies:0.1,Hardware test:0.1,Hardware validation:0.1,Integrated circuits:0.8,Power and energy:0.1,Printed circuit boards:0.1,Robustness:0.1,Very large scale integration design:0.7","Integrated circuits,Very large scale integration design",Integrated circuits and VLSI design are directly relevant as the papers focus on biomedical circuits and systems. Other categories like communication hardware are secondary to the core topic.,"3D integrated circuits:0,Analog and mixed-signal circuits:0.5,Application-specific VLSI designs:0.5,Design reuse and communication-based design:0,Design rules:0,Digital switches:0,Economics of chip design and manufacturing:0,Full-custom circuits:0,Interconnect:0,Logic circuits:0,On-chip resource management:0,On-chip sensors:0,Reconfigurable logic and FPGAs:0,Semiconductor memory:0,Standard cell libraries:0,VLSI design manufacturing considerations:0,VLSI packaging:0,VLSI system specification and constraints:0","Application-specific VLSI designs,Analog and mixed-signal circuits",Application-specific VLSI designs is relevant to biomedical circuits. Analog and mixed-signal circuits is relevant for BioCAS conference focus. Other categories are too specific and not mentioned.
3590,A 256-Mcell Phase-Change Memory Chip Operating at $2{+}$ Bit/Cell,A fully integrated 256-Mcell multi-level cell (MLC) phase-change memory (PCM) chip in 90-nm CMOS technology is presented. The on-chip circuitry supports fast MLC operation at 4 bit/cell. A programmable digital controller is used to optimize closed-loop gain and timing of the iterative MLC programming scheme and two power-efficient 8-bit DACs support current-controlled as well as voltage-controlled write pulses. The read-out consists of a low-power auto-range frontend followed by a 6-bit cyclic ADC that converts the nonlinear PCM resistance in a range between 10 kΩ and 10 MΩ . A verilog-A model derived from a full 3-D simulation of the PCM cell was developed to simulate the complete chip. The chip was used to demonstrate operation at 2 bit/cell and programming below 10 μs with Ge 2Sb 2Te 5 (GST) based PCM cells at a raw bit error rate of ~ 2 × 10- 4. Two main roadblocks for MLC PCM are drift and endurance. The accuracy of the analog frontend in combination with the programmable controller enables drift mitigation at the system level and the exploration of new materials for MLC operation at 3+ bit/cell.,"General and reference:0.0,Hardware:1.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Hardware,Hardware is directly relevant because the paper presents a novel phase-change memory chip design with detailed hardware implementation details.,"Integrated circuits:1,Emerging technologies:1,Electronic design automation:0.8,Communication hardware, interfaces and storage:0.3,Hardware test:0.2,Hardware validation:0.2,Power and energy:0.4,Printed circuit boards:0.1,Robustness:0.3,Very large scale integration design:0.6","Integrated circuits,Emerging technologies",Integrated circuits and Emerging technologies are relevant as the paper focuses on phase-change memory chip development. Other categories like VLSI design are secondary to the core contribution.,"3D integrated circuits:0,Analysis and design of emerging devices and systems:0,Biology-related information processing:0,Circuit substrates:0,Digital switches:0,Electromechanical systems:0,Emerging interfaces:0,Emerging optical and photonic technologies:0,Interconnect:0,Logic circuits:0,Memory and dense storage:1,Plasmonics:0,Quantum technologies:0,Reconfigurable logic and FPGAs:0,Reversible logic:0,Semiconductor memory:1,Spintronics and magnetic technologies:0","Memory and dense storage,Semiconductor memory",Memory and dense storage is relevant as the paper discusses MLC PCM memory chips. Semiconductor memory is directly relevant due to the focus on phase-change memory technology. Other categories are irrelevant as they do not address memory design.
647,A VLSI high-performance encoder with priority lookahead,"In this paper we introduce a VLSI priority encoder that uses a novel priority lookahead scheme to reduce the delay for the worst case operation of the circuit, while maintaining a very low transistor count. The encoder's topmost input request has the highest priority; this priority descends linearly. Two design approaches for the priority encoder are presented, one without a priority lookahead scheme and one with a priority lookahead scheme. For an N-bit encoder, the circuit with the priority lookahead scheme requires only 1.094 times the number of transistors of the circuit without the priority lookahead scheme. Having a 32-bit encoder as an example, the circuit with the priority lookahead scheme is 2.59 times faster than the circuit without the priority lookahead. The worst case operation delay is 4.4 ns for this lookahead encoder, using a 1-/spl mu/m scalable CMOS technology. The proposed lookahead scheme can be extended to larger encoders.","General and reference:0.1,Hardware:0.95,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Hardware,Hardware is highly relevant as the paper focuses on VLSI circuit design and optimization of a priority encoder. Other categories like Computer systems organization or Networks are not directly related to the core contribution of hardware design.,"Communication hardware, interfaces and storage:0.2,Electronic design automation:0.1,Emerging technologies:0.1,Hardware test:0.1,Hardware validation:0.1,Integrated circuits:0.7,Power and energy:0.1,Printed circuit boards:0.1,Robustness:0.1,Very large scale integration design:0.9","Very large scale integration design,Integrated circuits",Very large scale integration design is highly relevant as the paper introduces a VLSI encoder with a novel lookahead scheme. Integrated circuits is relevant due to the focus on VLSI circuit design and transistor count analysis. Other options like Communication hardware or Power and energy are irrelevant as the paper focuses on digital logic design rather than communication systems or energy consumption.,"3D integrated circuits:0.0,Analog and mixed-signal circuits:0.0,Application-specific VLSI designs:1.0,Design reuse and communication-based design:0.0,Design rules:0.0,Digital switches:0.0,Economics of chip design and manufacturing:0.0,Full-custom circuits:0.0,Interconnect:0.0,Logic circuits:1.0,On-chip resource management:0.0,On-chip sensors:0.0,Reconfigurable logic and FPGAs:0.0,Semiconductor memory:0.0,Standard cell libraries:0.0,VLSI design manufacturing considerations:0.0,VLSI packaging:0.0,VLSI system specification and constraints:0.0","Application-specific VLSI designs,Logic circuits",Application-specific VLSI designs is relevant because the paper presents a novel VLSI priority encoder design. Logic circuits is relevant as the encoder is a fundamental digital logic component. Other categories like Analog and mixed-signal circuits are irrelevant as the focus is purely digital.
2091,Practical active compensation techniques for ATE power supply response for testing of mixed signal data storage SOCs,"We will demonstrate the effectiveness of power supply active compensation techniques in mixed signal device performance testing. Read channel speed sorting for data storage SOCs is used to illustrate how we minimize the power transient effect in ATE test, where read-channel current draw varies drastically between different mission-modes and power-saving-modes. These active compensation ideas are critical when decoupling improvement alone cannot reduce the transients to acceptable levels. Compared to other publications, we are focusing on minimizing large device functionality-induced transients; instead of peak power consumption with ATPG generated tests.","General and reference:0.1,Hardware:0.9,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Hardware,Hardware is highly relevant as the paper addresses active compensation techniques for power supply in hardware testing. Other categories like Computer systems organization are less relevant as the focus is on hardware-level testing rather than system architecture.,"Communication hardware, interfaces and storage:0.0,Electronic design automation:0.0,Emerging technologies:0.0,Hardware test:0.85,Hardware validation:0.0,Integrated circuits:0.75,Power and energy:0.9,Printed circuit boards:0.0,Robustness:0.0,Very large scale integration design:0.0","Hardware test,Integrated circuits,Power and energy",Hardware test is relevant for the active compensation techniques in testing. Integrated circuits are relevant for the mixed signal SOC testing. Power and energy is relevant for power supply response analysis. Other categories like Robustness or Emerging technologies are not directly related to the paper's core contribution.,"3D integrated circuits:0,Analog, mixed-signal and radio frequency test:1,Board- and system-level test:0,Defect-based test:0,Design for testability:0,Digital switches:0,Energy distribution:0,Energy generation and storage:0,Fault models and test metrics:0,Hardware reliability screening:0,Impact on the environment:0,Interconnect:0,Logic circuits:0,Memory test and repair:0,Power estimation and optimization:1,Reconfigurable logic and FPGAs:0,Semiconductor memory:0,Test-pattern generation and fault simulation:0,Testing with distributed and parallel systems:0,Thermal issues:0","Analog, mixed-signal and radio frequency test,Power estimation and optimization","Analog, mixed-signal and radio frequency test: The paper focuses on testing mixed-signal data storage SOCs. Power estimation and optimization: Active compensation techniques for power supply response are discussed. Other children: No fault models or energy generation topics are addressed."
4526,Clock tree optimization for Electromagnetic Compatibility (EMC),"Electromagnetic Interference (EMI) generated by electronic systems is increasing with operating frequency and shrinking process technologies. The clock distribution network is one of the major causes of on-chip EMI. In this paper, we discuss the EMI problem in clock tree design. Spectrum analysis shows that slew rate of clock signal is the main parameter determining the high-frequency spectral content distribution. This is the first work to consider maximum and minimum buffer slew rates in clock tree synthesis to reduce EMI. In this paper, we propose a dynamic programming algorithm to optimize the clock tree considering both traditional metrics and Electromagnetic Compatibility (EMC). Our experimental results show that slew can be controlled in a feasible range and high-frequency spectrum contents can be reduced without sacrificing the traditional metrics such as power and skew. With the efficient optimization and pruning method, the biggest benchmark is able to complete in four minutes.","General and reference:0.1,Hardware:1.0,Computer systems organization:0.2,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Hardware,Hardware is highly relevant as the paper focuses on clock tree design optimization for EMI reduction in hardware systems. Other fields like Networks or Software are not directly related to the core contribution of hardware-level clock tree optimization.,"Communication hardware, interfaces and storage:0.1,Electronic design automation:0.3,Emerging technologies:0.1,Hardware test:0.1,Hardware validation:0.1,Integrated circuits:1.0,Power and energy:0.75,Printed circuit boards:0.1,Robustness:0.2,Very large scale integration design:1.0","Integrated circuits,Very large scale integration design,Power and energy",Integrated circuits and Very large scale integration design are highly relevant for clock tree synthesis in VLSI. Power and energy is relevant for EMI reduction and energy efficiency. Other categories like Hardware test are less central.,"3D integrated circuits:0.1,Analog and mixed-signal circuits:0.1,Application-specific VLSI designs:0.1,Design reuse and communication-based design:0.1,Design rules:0.1,Digital switches:0.1,Economics of chip design and manufacturing:0.1,Energy distribution:0.1,Energy generation and storage:0.1,Full-custom circuits:0.1,Impact on the environment:0.1,Interconnect:0.1,Logic circuits:0.1,On-chip resource management:0.1,On-chip sensors:0.1,Power estimation and optimization:0.6,Reconfigurable logic and FPGAs:0.1,Semiconductor memory:0.1,Standard cell libraries:0.1,Thermal issues:0.1,VLSI design manufacturing considerations:1.0,VLSI packaging:0.1,VLSI system specification and constraints:0.1","VLSI design manufacturing considerations,Power estimation and optimization",VLSI design manufacturing considerations is highly relevant for clock tree optimization. Power estimation and optimization is relevant as EMI reduction relates to power management. Other categories are less aligned with the paper's focus.
5692,UV-Writing of a Superstructure Waveguide Bragg Grating in a Planar Polymer Substrate,We report on the fabrication of a superstructure Bragg grating in a planar polymer substrate. Based on a twofold illumination process an integrated waveguide and a superstructure Bragg grating are subsequently written into bulk polymethylmethacrylate by UV-induced refractive index modification. The measured reflected spectrum of the superstructure Bragg grating exhibits multiple reflection peaks and is in good agreement with performed standard simulations based on the beam propagation method and coupled mode theory algorithms. By applying a varying tensile load we determine the strain sensitivity to be about 1.10 pm/µε and demonstrate the applicability of the superstructure Bragg grating for strain measurements with redundant sensing signals.,"General and reference:0.1,Hardware:0.85,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Hardware,Hardware is relevant for the physical fabrication of optical components. Other categories are irrelevant as the focus is on hardware implementation.,"Communication hardware, interfaces and storage:0.8,Electronic design automation:0.4,Emerging technologies:0.3,Hardware test:0.2,Hardware validation:0.3,Integrated circuits:0.6,Power and energy:0.5,Printed circuit boards:0.2,Robustness:0.3,Very large scale integration design:0.5","Communication hardware, interfaces and storage","Communication hardware, interfaces and storage: The paper presents a CMOS analog front-end for UWB communication, a direct hardware implementation. Integrated circuits and Very large scale integration design are secondary due to CMOS implementation details. Other categories like Power and energy are less central.",,,"No possible children provided, so all scores are 0."
2011,High-Level Design and Validation of the BlueSPARC Multithreaded Processor,"This paper presents our experiences in using high-level methods to design and validate a 16-way multithreaded microprocessor called BlueSPARC. BlueSPARC is an in-order, high-throughput processor supporting complex features such as privileged-mode operations, memory management, and a nonblocking cache subsystem. Using a high-level design language called Bluespec System Verilog (BSV), our final implementation achieves comparable synthesis quality to a similar commercial microprocessor developed using conventional register transfer level flows, and is capable of running unmodified commercial applications while hosted on a Xilinx XCV2P70 field-programmable gate array (FPGA) at 90 MHz. To validate our implementation, an FPGA-accelerated approach was developed to efficiently check the correct execution of real, nondeterministic multithreaded programs running on the BlueSPARC processor. Together, the high-level language features of BSV along with our validation approach enabled us to achieve a working FPGA-based implementation in less than one man-year.","General and reference:0.0,Hardware:1.0,Computer systems organization:0.2,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.2,Applied computing:0.0,Social and professional topics:0.0",Hardware,Hardware: The paper describes the design and implementation of a multithreaded processor using hardware description languages. Computer systems organization received partial score for architectural aspects but hardware is the primary domain. Computing methodologies was considered for the design methodology but is secondary to the hardware implementation focus.,"Communication hardware, interfaces and storage:0.0,Electronic design automation:1.0,Emerging technologies:0.0,Hardware test:0.0,Hardware validation:0.75,Integrated circuits:0.0,Power and energy:0.0,Printed circuit boards:0.0,Robustness:0.0,Very large scale integration design:0.0","Electronic design automation,Hardware validation",Electronic design automation is highly relevant as the paper discusses high-level design using Bluespec System Verilog. Hardware validation is relevant due to the FPGA-based implementation and testing. Other categories like Integrated circuits are less specific to the design methodology.,"Functional verification:0.7,Hardware description languages and compilation:1.0,High-level and register-transfer level synthesis:1.0,Logic synthesis:0.4,Methodologies for EDA:0.6,Modeling and parameter extraction:0.3,Physical design (EDA):0.2,Physical verification:0.3,Post-manufacture validation and debug:0.5,Timing analysis:0.2","Hardware description languages and compilation,High-level and register-transfer level synthesis",Hardware description languages and compilation is relevant for using Bluespec System Verilog. High-level and register-transfer level synthesis is central to the design methodology. Functional verification is also relevant but secondary to the primary design contributions.
1600,Design and integration of MPU for human monitoring system,"We have been developing a monitoring system to measure human information from human body. In the previous study, we developed a digital integrated circuit that enables acquisition of heart rate from ECG (Electrocardiogram) instead of MPU (Micro Processing Unit). This circuit reduced the power consumption of the MPU from 97 μW to 19.7 μW. In this research, we redesigned an MPU specialized for the human monitoring system to aim at further lowering power consumption and verified it with a test model. The designed MPU has fewer logic elements than the commercially available MPU and consumes 383 nW power consumption when using standard cell with low power consumption. This can be 98.0% lower than the conventional MPU.","General and reference:0.1,Hardware:0.95,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Hardware,Hardware is highly relevant as the paper centers on designing and optimizing a micro-processing unit (MPU) for power consumption reduction in a human monitoring system. Other categories like Software or Applied Computing are secondary to the core hardware focus.,"Integrated circuits:0.9,Power and energy:0.85,Very large scale integration design:0.7,Electronic design automation:0.2,Emerging technologies:0.2,Hardware test:0.2,Hardware validation:0.2,Communication hardware, interfaces and storage:0.1,Printed circuit boards:0.1,Robustness:0.1","Integrated circuits,Power and energy,Very large scale integration design",Integrated circuits is highly relevant for the MPU design. Power and energy is relevant due to the focus on power consumption reduction. Very large scale integration design is relevant for the specialized VLSI design. Other categories like Communication hardware are less relevant as the focus is on low-power VLSI design rather than communication interfaces.,"Application-specific VLSI designs:1,Power estimation and optimization:1,Standard cell libraries:0.6,Logic circuits:0.3,On-chip sensors:0.2,3D integrated circuits:0.1,Analog and mixed-signal circuits:0.1","Application-specific VLSI designs,Power estimation and optimization",Application-specific VLSI is relevant as the paper designs a specialized MPU. Power optimization is relevant due to the focus on reducing energy consumption. Other categories like standard cell libraries are less central to the core contribution.
676,A Spatial Contrast Retina With On-Chip Calibration for Neuromorphic Spike-Based AER Vision Systems,"We present a 32 times 32 pixels contrast retina microchip that provides its output as an address event representation (AER) stream. Spatial contrast is computed as the ratio between pixel photocurrent and a local average between neighboring pixels obtained with a diffuser network. This current-based computation produces an important amount of mismatch between neighboring pixels, because the currents can be as low as a few pico-amperes. Consequently, a compact calibration circuitry has been included to trimm each pixel. Measurements show a reduction in mismatch standard deviation from 57% to 6.6% (indoor light). The paper describes the design of the pixel with its spatial contrast computation and calibration sections. About one third of pixel area is used for a 5-bit calibration circuit. Area of pixel is 58 mum times 56 mum , while its current consumption is about 20 nA at 1-kHz event rate. Extensive experimental results are provided for a prototype fabricated in a standard 0.35-mum CMOS process.","General and reference:0.1,Hardware:0.9,Computer systems organization:0.3,Networks:0.2,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.2,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Hardware,Hardware is highly relevant as the paper focuses on the design and implementation of a 32x32 pixel contrast retina microchip with on-chip calibration circuits in a 0.35-micron CMOS process. Other fields like Networks or Information systems are not mentioned in the core contribution of physical circuit design.,"Integrated circuits:1.0,Very large scale integration design:0.75,Electronic design automation:0.5,Power and energy:0.1,Communication hardware, interfaces and storage:0.1,Hardware test:0.1,Robustness:0.1,Printed circuit boards:0.1,Emerging technologies:0.1,Hardware validation:0.1","Integrated circuits,Very large scale integration design",Integrated circuits is core to the 32x32 pixel contrast retina chip design. VLSI design is relevant for the CMOS process implementation. EDA and power categories are secondary to the core hardware contribution.,"3D integrated circuits:0,Analog and mixed-signal circuits:1,Application-specific VLSI designs:1,Design reuse and communication-based design:0,Design rules:0,Digital switches:0,Economics of chip design and manufacturing:0,Full-custom circuits:0,Interconnect:0,Logic circuits:0,On-chip resource management:0,On-chip sensors:0,Reconfigurable logic and FPGAs:0,Semiconductor memory:0,Standard cell libraries:0,VLSI design manufacturing considerations:0,VLSI packaging:0,VLSI system specification and constraints:0","Analog and mixed-signal circuits,Application-specific VLSI designs",Analog and mixed-signal circuits is relevant for the current-based computation and calibration. Application-specific VLSI designs is relevant for the custom 32x32 pixel chip design. Other children are irrelevant as the paper does not address digital switches or reconfigurable logic.
4949,An analog MOS model for circuit simulation and benchmark test results,"The BSIM plus MOS transistor model is developed for accurate and efficient analysis and simulation of submicron VLSI circuits. A compact parameter set is created to achieve high accuracy and continuity of the drain current and its derivatives in the subthreshold, triode and saturation regions of operation. Experimental results on individual transistors from deep submicron technologies are presented. Benchmark tests are performed to illustrate the salient features of the BSIM plus model.<<ETX>>","General and reference:0.1,Hardware:0.9,Computer systems organization:0.2,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Hardware,"The paper presents a transistor model for VLSI circuit simulation, which falls under hardware design and modeling. Other categories like Computer systems organization or Software are irrelevant as the focus is on physical component modeling.","Communication hardware, interfaces and storage:0.1,Electronic design automation:0.9,Emerging technologies:0.3,Hardware test:0.2,Hardware validation:0.2,Integrated circuits:0.6,Power and energy:0.1,Printed circuit boards:0.1,Robustness:0.1,Very large scale integration design:0.7","Electronic design automation,Very large scale integration design",Electronic design automation is highly relevant as the paper develops a MOS transistor model for circuit simulation. Very large scale integration design is relevant due to submicron VLSI focus. Integrated circuits are a component but not the primary methodological contribution.,"3D integrated circuits:0.1,Analog and mixed-signal circuits:1.0,Application-specific VLSI designs:0.1,Design reuse and communication-based design:0.1,Design rules:0.1,Economics of chip design and manufacturing:0.1,Full-custom circuits:0.1,Hardware description languages and compilation:0.1,High-level and register-transfer level synthesis:0.1,Logic synthesis:0.1,Methodologies for EDA:0.2,Modeling and parameter extraction:0.8,On-chip resource management:0.1,On-chip sensors:0.1,Physical design (EDA):0.1,Standard cell libraries:0.1,Timing analysis:0.1,VLSI design manufacturing considerations:0.1,VLSI packaging:0.1,VLSI system specification and constraints:0.1","Analog and mixed-signal circuits,Modeling and parameter extraction",Analog and mixed-signal circuits is primary for MOS transistor modeling. Modeling and parameter extraction applies to the compact parameter set for accuracy. Other options are irrelevant to the model's purpose.
4567,Congestion estimation during top-down placement,"Congestion is one of the fundamental issues in VLSI physical design. In this paper, we propose two congestion estimation approaches for early placement stages. First, we theoretically analyze the peak congestion value of the design and experimentally validate the estimation approach. Second, we estimate regional congestion in the early top-down placement. This is done by combining the wirelength distribution model and inter-region wire estimation. Both approaches are based on the well known Rent's rule, which is previously used for wirelength estimation. This is the first attempt to predict congestion using Rent's rule. The estimation results are compared with the layout after placement and global routing. Experiments on large industry circuits show that the early congestion estimation based on Rent's rule is a promising approach.","General and reference:0,Hardware:1,Computer systems organization:0,Networks:0,Software and its engineering:0,Theory of computation:0,Mathematics of computing:0,Information systems:0,Security and privacy:0,Human-centered computing:0,Computing methodologies:0,Applied computing:0,Social and professional topics:0",Hardware,Hardware is directly relevant for VLSI congestion estimation in physical design. Other categories like Networks or Software are not mentioned in the core contribution.,"Communication hardware, interfaces and storage:0.1,Electronic design automation:1.0,Emerging technologies:0.2,Hardware test:0.1,Hardware validation:0.1,Integrated circuits:0.8,Power and energy:0.1,Printed circuit boards:0.1,Robustness:0.1,Very large scale integration design:1.0","Electronic design automation,Very large scale integration design",Electronic design automation is relevant because the paper presents congestion estimation techniques for VLSI physical design. Very large scale integration design is relevant as the focus is on VLSI circuit placement optimization. Other categories like Communication hardware or Power and energy are not discussed in the paper.,"3D integrated circuits:0.0,Analog and mixed-signal circuits:0.0,Application-specific VLSI designs:0.1,Design reuse and communication-based design:0.0,Design rules:0.0,Economics of chip design and manufacturing:0.0,Full-custom circuits:0.0,Hardware description languages and compilation:0.0,High-level and register-transfer level synthesis:0.1,Logic synthesis:0.0,Methodologies for EDA:0.2,Modeling and parameter extraction:0.1,On-chip resource management:0.0,On-chip sensors:0.0,Physical design (EDA):1.0,Standard cell libraries:0.0,Timing analysis:0.2,VLSI design manufacturing considerations:0.0,VLSI packaging:0.0,VLSI system specification and constraints:0.1",Physical design (EDA),The paper focuses on congestion estimation in VLSI physical design. Other EDA categories like high-level synthesis are only marginally relevant.
628,High-speed CMOS ring oscillators with low supply sensitivity,A novel circuit topology for CMOS CML ring oscillators that reduces the supply sensitivity is presented. It is shown that this technique causes only a slight reduction in the maximum frequency of the oscillator and maintains the same random jitter generation while greatly reducing the sinusoidal jitter caused by power supply variation. Measurement results from a prototype chip fabricated in 0.18µm CMOS process verify the effectiveness of the proposed technique.,"General and reference:0.1,Hardware:0.9,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Hardware,Hardware is relevant for CMOS oscillator circuit design. Other categories lack hardware implementation focus.,"Communication hardware, interfaces and storage:0.1,Electronic design automation:0.1,Emerging technologies:0.1,Hardware test:0.1,Hardware validation:0.2,Integrated circuits:0.8,Power and energy:0.7,Printed circuit boards:0.1,Robustness:0.1,Very large scale integration design:0.1","Integrated circuits,Power and energy",Integrated circuits is relevant for the CMOS oscillator design. Power and energy is relevant due to the focus on supply sensitivity reduction. Other categories lack direct technical alignment with the circuit design focus.,"3D integrated circuits:0.3,Digital switches:0.2,Energy distribution:0.1,Energy generation and storage:0.1,Impact on the environment:0.1,Interconnect:0.4,Logic circuits:1.0,Power estimation and optimization:1.0,Reconfigurable logic and FPGAs:0.2,Semiconductor memory:0.3,Thermal issues:0.5","Logic circuits,Power estimation and optimization",Logic circuits: The paper presents a CMOS ring oscillator circuit design. Power estimation and optimization: The focus on reducing supply sensitivity and jitter directly relates to power optimization. Other categories like 'Interconnect' are less relevant.
4776,A Two-Stage Resonant Inverter With Control of the Phase Angle and Magnitude of the Output Voltage,"A high-efficiency two-stage resonant inverter with effective control of both the magnitude and phase angle of the output voltage was proposed in this paper for high-frequency ac (HFAC) power-distribution applications, where a number of resonant inverters need to be paralleled. In order to parallel multiple resonant inverters of the same operation frequency, each inverter module needs independent control of the phase angle and magnitude of the output voltage. It is also desirable that the output voltage has very low total harmonics distortion, as well as high efficiency over wide input and load ranges. The proposed resonant inverter consists of two stages. The first stage is a two-switch dc/dc converter with zero-voltage transition, and the second stage is a half-bridge resonant dc/ac inverter with fixed duty ratio. A series-parallel resonant tank is used to achieve high waveform quality of the output voltage. The magnitude of the output voltage is regulated through the duty-ratio control of the first stage with pulsewidth modulation. The phase angle of the output voltage is regulated through a pulse-phase-modulation control of the second stage. The proposed resonant inverter has the advantages of better waveform quality, wide range of input and load variations for soft-switching, and independent control of the phase angle and magnitude of the output voltage, making it an attractive candidate for applications where a number of resonant inverters need to be placed in parallel to the HFAC bus and a number of distributed loads are connected to the HFAC bus. The performance is verified with both simulation and experiments.","General and reference:0.0,Hardware:0.9,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Hardware,Hardware is highly relevant as the paper describes a novel resonant inverter design for power electronics. Other categories like Software and its engineering are less relevant as the focus is on physical hardware design.,"Communication hardware, interfaces and storage:0.2,Electronic design automation:0.3,Emerging technologies:0.1,Hardware test:0.1,Hardware validation:0.1,Integrated circuits:0.2,Power and energy:0.9,Printed circuit boards:0.1,Robustness:0.1,Very large scale integration design:0.2",Power and energy,Power and energy is highly relevant for the resonant inverter in power distribution. Other categories like Integrated circuits are less central.,"Energy distribution:1.0,Energy generation and storage:0.2,Impact on the environment:0.1,Power estimation and optimization:0.3,Thermal issues:0.1","Energy distribution,Power estimation and optimization","Energy distribution is highly relevant as the paper focuses on parallel inverter operation in power distribution systems. Power estimation and optimization are relevant due to the emphasis on efficiency and control. Other options are less relevant as the paper doesn't focus on generation, environmental impact, or thermal management."
1739,Differential zero compensator in delay-ripple reshaped constant on-time control for buck converter with multi-layer ceramic capacitors,"A differential-zero-compensator (DZC) technology in constant on-time control dc-dc buck converter which can be applied for the ceramic capacitors is proposed in this paper. Ceramic capacitors are widely used in industry because its low cost. However, the stability often confronts with the sub-harmonic problem related to the incorrect output capacitor and ESR used. In this paper, the proposed DZC technology remits the limitation without adding extra pins and the complexity of circuit still maintain simple. Furthermore, the proposed self-compensated on-time timer (ONT) offers the reliable on-time period with linear solution. In addition, utilizing ceramic capacitor derives smaller ripple of 8mV at 8A. High efficiency of 92.5 % can be guaranteed due to the small ESR value. Stability criteria analysis and simulation results demonstrate that the stability can be ensured even if the ESR is small than 2mΩ.","General and reference:0.1,Hardware:0.8,Computer systems organization:0.3,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Hardware,Hardware: The paper proposes a differential-zero-compensator design for DC-DC buck converters. Other categories are irrelevant as the focus is on hardware design and stability optimization.,"Communication hardware, interfaces and storage:0.1,Electronic design automation:0.8,Emerging technologies:0.1,Hardware test:0.1,Hardware validation:0.7,Integrated circuits:0.1,Power and energy:0.9,Printed circuit boards:0.1,Robustness:0.1,Very large scale integration design:0.1","Power and energy,Electronic design automation,Hardware validation",Power and energy is relevant for the buck converter design. Electronic design automation applies to the proposed circuit design. Hardware validation is relevant for stability analysis. Other categories like Communication hardware are irrelevant as the focus is on power electronics.,"Energy distribution:0,Energy generation and storage:0,Functional verification:0,Hardware description languages and compilation:0,High-level and register-transfer level synthesis:0,Impact on the environment:0,Logic synthesis:0,Methodologies for EDA:0,Modeling and parameter extraction:0,Physical design (EDA):0,Physical verification:0,Post-manufacture validation and debug:0,Power estimation and optimization:1,Thermal issues:0,Timing analysis:0",Power estimation and optimization,"The paper focuses on optimizing power efficiency in buck converters using ceramic capacitors, directly aligning with power estimation and optimization. Other EDA or verification categories are not relevant."
3715,A real-time 17-scale object detection accelerator with adaptive 2000-stage classification in 65nm CMOS,"This paper presents an object detection accelerator that features many-scale (17), many-object (up to 50), multi-class (e.g., face, traffic sign), and high accuracy (average precision of 0.79/0.65 for AFW/BTSD datasets). Employing 10 gradient/color channels, integral features are extracted, and the results of 2,000 simple classifiers for rigid boosted templates are adaptively combined to make a strong classification. By jointly optimizing the algorithm and the hardware architecture, the prototype chip implemented in 65nm CMOS demonstrates real-time object detection of 13–35 frames per second with low power consumption of 22–160mW at 0.58–1.0V supply.","General and reference:0.0,Hardware:0.8,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Hardware,Hardware is highly relevant as the paper presents a hardware accelerator for object detection. Other categories are not directly relevant.,"Communication hardware, interfaces and storage:0.2,Electronic design automation:0.3,Emerging technologies:0.1,Hardware test:0.2,Hardware validation:0.2,Integrated circuits:0.8,Power and energy:0.75,Printed circuit boards:0.1,Robustness:0.3,Very large scale integration design:0.7","Integrated circuits,Power and energy,Very large scale integration design",Integrated circuits and VLSI design are central to the CMOS-based hardware implementation. Power and energy relevance stems from the low-power consumption focus. Other categories like communication hardware or test are less directly related to the core contribution.,"3D integrated circuits:0,Analog and mixed-signal circuits:0,Application-specific VLSI designs:1,Design reuse and communication-based design:0,Design rules:0,Digital switches:0,Economics of chip design and manufacturing:0,Energy distribution:0,Energy generation and storage:0,Full-custom circuits:0,Impact on the environment:0,Interconnect:0,Logic circuits:0,On-chip resource management:0,On-chip sensors:0,Power estimation and optimization:1,Reconfigurable logic and FPGAs:0,Semiconductor memory:0,Standard cell libraries:0,Thermal issues:0,VLSI design manufacturing considerations:0,VLSI packaging:0,VLSI system specification and constraints:0","Application-specific VLSI designs,Power estimation and optimization",Application-specific VLSI designs is relevant as the paper presents a specialized hardware accelerator. Power estimation and optimization is relevant due to the focus on low power consumption. Other categories are not directly addressed.
3603,New opportunities for optical phase-locked loops in coherent photonics,"New efforts demonstrate that integrated optical phase-locked loops can provide stable and robust phase locking. Coherent receivers now use DSP processing for long haul communications. However, given a low-cost, low-power coherent receiver, uses in shorter links become viable, and other approaches for removing impairments can be explored.","General and reference:0.1,Hardware:1.0,Computer systems organization:0.4,Networks:0.2,Software and its engineering:0.3,Theory of computation:0.1,Mathematics of computing:0.2,Information systems:0.2,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.3,Applied computing:0.3,Social and professional topics:0.1",Hardware,"Hardware: The paper discusses optical phase-locked loops in coherent photonics, which is a hardware component. 'Computer systems organization' is less relevant as the focus is on specific optical hardware rather than system-level design.","Communication hardware, interfaces and storage:1.0,Electronic design automation:0.2,Emerging technologies:0.3,Hardware test:0.05,Hardware validation:0.1,Integrated circuits:0.4,Power and energy:0.3,Printed circuit boards:0.05,Robustness:0.2,Very large scale integration design:0.2","Communication hardware, interfaces and storage",Communication hardware is directly relevant as the paper discusses optical phase-locked loops for coherent communication systems. Other categories like Integrated circuits or Emerging technologies are secondary but not central to the core contribution.,,,"No relevant children provided in the options. The paper focuses on optical phase-locked loops in coherent photonics, which does not align with any of the given categories."
4294,Hardware Implementation of Associative Memories Based on Multiple-Valued Sparse Clustered Networks,"This paper presents algorithms and hardware implementations of associative memories based on multiple-valued sparse clustered networks (MV-SCNs). SCNs are recently-introduced binary-weighted associative memories that significantly improve the storage and retrieval capabilities over the prior state-of-the art. However, deleting or updating the messages stored in binary-weighted connections result in a significant increase in the data retrieval error probability as the binary-weighted connections deleted may be shared for several data patterns. In order to address the problem, the proposed algorithm exploits multiple-valued weighted connections of the network for storing the messages while maintaining the number of computation nodes in a cluster. The use of the multiple-valued weighted connections reduces the probability of deleting the shared connections compared to the binary-weighted connections. As a result, the proposed algorithm lowers the error rate by an order of magnitude for our sample network with 60% deleted contents compared to the conventional algorithm when the same amount of memory is used. For performance comparisons in hardware, the proposed SCNs are designed using Verilog-HDL and synthesized on TSMC 65 nm CMOS technology. The synthesis results show that the proposed MV-SCNs are around 10% smaller than the conventional binary-weighted SCNs as the number of computation nodes in the proposed SCNs is smaller than that of the conventional SCNs with the comparable speed and memory size.","General and reference:0.1,Hardware:0.8,Computer systems organization:0.3,Networks:0.1,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Hardware,Hardware is highly relevant as the paper focuses on Verilog-HDL implementation and CMOS synthesis. Other categories like Computer systems organization are mentioned but not the core focus.,"Communication hardware, interfaces and storage:0.1,Electronic design automation:0.1,Emerging technologies:0.8,Hardware test:0.1,Hardware validation:0.1,Integrated circuits:0.9,Power and energy:0.1,Printed circuit boards:0.1,Robustness:0.1,Very large scale integration design:0.9","Integrated circuits,Emerging technologies,Very large scale integration design",Integrated circuits: Hardware implementation using CMOS technology and Verilog-HDL. Emerging technologies: Multiple-valued sparse clustered networks represent novel hardware design approaches. Very large scale integration design: Synthesis and performance analysis of the proposed architecture. Other categories like 'Power and energy' are not discussed in the context of this work.,"3D integrated circuits:0,Analog and mixed-signal circuits:0,Analysis and design of emerging devices and systems:0,Application-specific VLSI designs:1,Biology-related information processing:0,Circuit substrates:0,Design reuse and communication-based design:0,Design rules:0,Digital switches:0,Economics of chip design and manufacturing:0.1,Electromechanical systems:0,Emerging interfaces:0,Emerging optical and photonic technologies:0,Full-custom circuits:0.5,Interconnect:0,Logic circuits:0.3,Memory and dense storage:0.8,On-chip resource management:0,On-chip sensors:0,Plasmonics:0,Quantum technologies:0,Reconfigurable logic and FPGAs:0,Reversible logic:0,Semiconductor memory:1,Spintronics and magnetic technologies:0,Standard cell libraries:0,VLSI design manufacturing considerations:0.2,VLSI packaging:0,VLSI system specification and constraints:0","Application-specific VLSI designs,Semiconductor memory","Application-specific VLSI designs: The paper presents hardware implementations for associative memories. Semiconductor memory: Focuses on improving memory storage and retrieval. Other children scored low as the focus is not on interconnect, quantum technologies, or spintronics."
389,Mobile Supercomputers for the Next-Generation Cell Phone,AnySP demonstrates that power efficiency can be achieved on a fully programmable processor in the context of a future mobile terminal supporting 4G wireless and high-definition video coding.,"General and reference:0.1,Hardware:0.95,Computer systems organization:0.3,Networks:0.2,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Hardware,Hardware is highly relevant as the paper focuses on power-efficient programmable processor design for mobile terminals. Other categories like Networks or Software are tangential and not central to the core contribution.,"Communication hardware, interfaces and storage:0.25,Electronic design automation:0.25,Emerging technologies:0.25,Hardware test:0.25,Hardware validation:0.25,Integrated circuits:0.25,Power and energy:1.0,Printed circuit boards:0.25,Robustness:0.25,Very large scale integration design:0.25",Power and energy,Power and energy is directly relevant as the paper focuses on power efficiency in mobile processor design. Other hardware categories are less relevant to the core contribution.,"Power estimation and optimization:1,Energy generation and storage:0.2,Energy distribution:0.3,Impact on the environment:0.4,Thermal issues:0.3",Power estimation and optimization,Power estimation and optimization is highly relevant as the paper focuses on power efficiency in mobile processors. Other energy-related categories are not discussed in the abstract.
2081,Fault diagnosis of analog circuits based on machine learning,"We discuss a fault diagnosis scheme for analog integrated circuits. Our approach is based on an assemblage of learning machines that are trained beforehand to guide us through diagnosis decisions. The central learning machine is a defect filter that distinguishes failing devices due to gross defects (hard faults) from failing devices due to excessive parametric deviations (soft faults). Thus, the defect filter is key in developing a unified hard/soft fault diagnosis approach. Two types of diagnosis can be carried out according to the decision of the defect filter: hard faults are diagnosed using a multi-class classifier, whereas soft faults are diagnosed using inverse regression functions. We show how this approach can be used to single out diagnostic scenarios in an RF low noise amplifier (LNA).","General and reference:0.1,Hardware:0.9,Computer systems organization:0.2,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.3,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.5,Applied computing:0.3,Social and professional topics:0.1",Hardware,"Hardware is directly relevant as the paper focuses on fault diagnosis in analog integrated circuits. Computing methodologies receives mid relevance due to ML techniques, but the core contribution is hardware-focused. Applied computing is less relevant as the application domain is circuit diagnosis, not broader computing applications.","Communication hardware, interfaces and storage:0.1,Electronic design automation:0.0,Emerging technologies:0.0,Hardware test:0.8,Hardware validation:0.7,Integrated circuits:0.6,Power and energy:0.0,Printed circuit boards:0.0,Robustness:0.0,Very large scale integration design:0.0","Hardware test,Hardware validation,Integrated circuits",Hardware test applies to fault diagnosis. Hardware validation is relevant for defect detection. Integrated circuits is core for analog circuits. Other fields lack direct connection.,"3D integrated circuits:0.1,Analog, mixed-signal and radio frequency test:0.8,Board- and system-level test:0.3,Defect-based test:0.7,Design for testability:0.2,Digital switches:0.1,Fault models and test metrics:0.4,Functional verification:0.2,Hardware reliability screening:0.1,Interconnect:0.1,Logic circuits:0.1,Memory test and repair:0.1,Physical verification:0.1,Post-manufacture validation and debug:0.2,Reconfigurable logic and FPGAs:0.1,Semiconductor memory:0.1,Test-pattern generation and fault simulation:0.3,Testing with distributed and parallel systems:0.1","Analog, mixed-signal and radio frequency test,Defect-based test","Analog, mixed-signal and radio frequency test: The paper discusses fault diagnosis in analog circuits. Defect-based test: The approach uses a defect filter to distinguish hard/soft faults. Other categories like Board-level test are irrelevant as the focus is on machine learning-based fault diagnosis, not physical testing methods."
5714,A Single Opamp Third-Order Low-Distortion Delta-Sigma Modulator with SAR Quantizer Embedded Passive Adder,"A third-order low-distortion delta-sigma modulator (DSM), whose third-order noise-shaping ability is achieved by just a single opamp, is proposed. Since only one amplifier is required in the whole circuit, the designed DSM is very power efficient. To realize the adder in front of quantizer without employing the huge-power opamp, a capacitive passive adder, which is the digital-to-analog converter (DAC) array of a successive-approximation-type quantizer, is used. In addition, the feedback path timing is extended from a nonoverlapping interval for the conventional low-distortion structure to half of the clock period, so that the strict operation timing issue with regard to quantization and the dynamic element matching (DEM) logic operation can be solved. In the proposed DSM structure, the features of the unity-gain signal transfer function (STF) and finite-impulse-response (FIR) noise transfer function (NTF) are still preserved, and thus advantages such as a relaxed opamp slew rate and reduced output swing are also maintained, as with the conventional low-distortion DSM. Moreover, the memory effect in the proposed DSM is analyzed when employing the opamp sharing for integrators. The proposed third-order DSM with a 4-bit SAR ADC as the quantizer is implemented in a 90-nm CMOS process. The post-layout simulations show a 79.8-dB signal-to-noise and distortion ratio (SNDR) in the 1.875-MHz signal bandwidth (OSR=16). The active area of the circuit is 0.35 mm2 and total power consumption is 2.85 mW, resulting in a figure of merit (FOM) of 95 fJ/conversion-step. key words: delta-sigma modulator, DSM, opamp sharing, relaxed dynamic element matching (DEM) timing","General and reference:0.1,Hardware:1.0,Computer systems organization:0.2,Networks:0.1,Software and its engineering:0.3,Theory of computation:0.2,Mathematics of computing:0.2,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.2,Social and professional topics:0.1",Hardware,Hardware is relevant due to the circuit design and implementation. Other categories are less directly related.,"Communication hardware, interfaces and storage:0.2,Electronic design automation:0.3,Emerging technologies:0.1,Hardware test:0.1,Hardware validation:0.1,Integrated circuits:0.9,Power and energy:0.9,Printed circuit boards:0.1,Robustness:0.3,Very large scale integration design:0.7","Integrated circuits,Power and energy,Very large scale integration design",Integrated circuits is highly relevant as the paper presents a delta-sigma modulator implemented in CMOS. Power and energy is relevant due to the emphasis on power efficiency. VLSI design is appropriate for the 90nm CMOS implementation. Other categories like Communication hardware are less relevant as the focus is on analog circuit design rather than communication protocols.,"3D integrated circuits:0.0,Analog and mixed-signal circuits:1.0,Application-specific VLSI designs:0.8,Design reuse and communication-based design:0.0,Design rules:0.0,Digital switches:0.0,Economics of chip design and manufacturing:0.0,Energy distribution:0.0,Energy generation and storage:0.0,Full-custom circuits:0.0,Impact on the environment:0.0,Interconnect:0.0,Logic circuits:0.0,On-chip resource management:0.0,On-chip sensors:0.0,Power estimation and optimization:0.6,Reconfigurable logic and FPGAs:0.0,Semiconductor memory:0.0,Standard cell libraries:0.0,Thermal issues:0.0,VLSI design manufacturing considerations:0.0,VLSI packaging:0.0,VLSI system specification and constraints:0.0","Analog and mixed-signal circuits,Application-specific VLSI designs",Analog and mixed-signal circuits: The paper describes a delta-sigma modulator using an opamp and SAR quantizer. Application-specific VLSI designs: The modulator is implemented as a specific VLSI application. Other options are less relevant as they do not directly relate to the circuit design.
3469,Integrated chemical sensors based on carbon black and polymer films using a standard CMOS process and post-processing,"We present an integrated chemical sensor array fabricated using a CMOS process followed by post-processing. The sensor presented in this paper incorporates 324 individually addressable sensing nodes. Post processing involves an electroless nickel and gold plating step to fabricate sensing contacts, and the deposition of a carbon black based polymer sensor material. The operation of the integrated sensor is confirmed. This sensor technology will allow the creation of large arrays of chemically diverse sensors.","General and reference:0.1,Hardware:0.9,Computer systems organization:0.2,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Hardware,Hardware is relevant for the CMOS-based chemical sensor design. Other categories like Computer systems organization are not directly addressed.,"Communication hardware, interfaces and storage:0.1,Electronic design automation:0.1,Emerging technologies:0.3,Hardware test:0.1,Hardware validation:0.1,Integrated circuits:0.85,Power and energy:0.1,Printed circuit boards:0.1,Robustness:0.2,Very large scale integration design:0.8","Integrated circuits,Very large scale integration design",Integrated circuits: CMOS-based sensor fabrication. Very large scale integration design: Post-processing steps for sensor material deposition.,"3D integrated circuits:0,Analog and mixed-signal circuits:0,Application-specific VLSI designs:0.2,Design reuse and communication-based design:0,Design rules:0,Digital switches:0,Economics of chip design and manufacturing:0,Full-custom circuits:0,Interconnect:0,Logic circuits:0,On-chip resource management:0,On-chip sensors:1,Reconfigurable logic and FPGAs:0,Semiconductor memory:0,Standard cell libraries:0,VLSI design manufacturing considerations:0.8,VLSI packaging:0,VLSI system specification and constraints:0","On-chip sensors,VLSI design manufacturing considerations",On-chip sensors are directly relevant for the integrated sensor array. VLSI design manufacturing considerations apply to the CMOS fabrication process. Other categories like Application-specific VLSI designs are less relevant.
2763,System-Level Buffer Allocation for Application-Specific Networks-on-Chip Router Design,"In this paper, a novel system-level buffer planning algorithm that can be used to customize the router design in networks-on-chip (NoCs) is presented. More precisely, given the traffic characteristics of the target application and the total budget of the available buffering space, the proposed algorithm automatically assigns the buffer depth for each input channel, in different routers across the chip, such that the overall performance is maximized. This is in deep contrast with the uniform assignment of buffering resources (currently used in NoC design), which can significantly degrade the overall system performance. Indeed, the experimental results show that while the proposed algorithm is very fast, significant performance improvements can be achieved compared to the uniform buffer allocation. For instance, for a complex audio/video application, about 80% savings in buffering resources, can be achieved by smart buffer allocation using the proposed algorithm","General and reference:0.1,Hardware:0.9,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Hardware,"Hardware: The paper presents a system-level buffer allocation algorithm for NoC router design, which is a hardware design optimization. Categories like Networks or Applied computing are secondary to the physical chip design focus.","Communication hardware, interfaces and storage:0.3,Electronic design automation:0.9,Emerging technologies:0.2,Hardware test:0.1,Hardware validation:0.1,Integrated circuits:0.85,Power and energy:0.1,Printed circuit boards:0.1,Robustness:0.1,Very large scale integration design:0.7","Electronic design automation,Integrated circuits,Very large scale integration design",Electronic design automation is highly relevant as the paper presents a system-level buffer allocation algorithm for NoC router design. Integrated circuits are relevant because NoCs are a core component of IC design. Very large scale integration design is relevant as the work focuses on optimizing buffer allocation in VLSI systems. Other categories like communication hardware or power/energy are less central to the paper's core contribution.,"3D integrated circuits:0,Analog and mixed-signal circuits:0,Application-specific VLSI designs:1,Design reuse and communication-based design:0,Design rules:0,Digital switches:0,Economics of chip design and manufacturing:0,Full-custom circuits:0,Hardware description languages and compilation:0,High-level and register-transfer level synthesis:0,Interconnect:0,Logic circuits:0,Logic synthesis:0,Methodologies for EDA:0,Modeling and parameter extraction:0,On-chip resource management:1,On-chip sensors:0,Physical design (EDA):0,Reconfigurable logic and FPGAs:0,Semiconductor memory:0,Standard cell libraries:0,Timing analysis:0,VLSI design manufacturing considerations:0,VLSI packaging:0,VLSI system specification and constraints:0","Application-specific VLSI designs,On-chip resource management",Application-specific VLSI designs and On-chip resource management are directly addressed in the buffer allocation algorithm for NoCs. Other options are irrelevant.
4143,Quality assurance in memory built-in self-test tools,"In the paper, two methods of ensuring high quality of the memory built-in self-test tools are presented. The described ideas illustrate general methods and are applicable to any commercial memory BIST tool. The first solution describes controller emulation in order to validate each step of the real controller's operations. The second approach presents a way to determine the test algorithms' fault coverage by means of the memory fault simulator. The experimental results show functional benefits and effectiveness of the proposed solutions.","General and reference:0.1,Hardware:0.9,Computer systems organization:0.7,Networks:0.2,Software and its engineering:0.3,Theory of computation:0.1,Mathematics of computing:0.2,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.4,Applied computing:0.2,Social and professional topics:0.1",Hardware,Hardware is directly relevant as the paper focuses on memory testing techniques. Computer systems organization is secondary. Other categories like Software are not central to the core contribution.,"Communication hardware, interfaces and storage:0.0,Electronic design automation:0.0,Emerging technologies:0.0,Hardware test:1.0,Hardware validation:1.0,Integrated circuits:0.25,Power and energy:0.0,Printed circuit boards:0.0,Robustness:0.0,Very large scale integration design:0.0","Hardware test,Hardware validation",Hardware test and Hardware validation are directly relevant as the paper focuses on quality assurance methods for memory BIST tools. Other categories like Integrated circuits are less directly related.,"Memory test and repair:1,Test-pattern generation and fault simulation:0.7,Analog, mixed-signal and radio frequency test:0.2,Board- and system-level test:0.2,Defect-based test:0.2,Design for testability:0.2,Fault models and test metrics:0.2,Functional verification:0.2,Hardware reliability screening:0.2,Physical verification:0.2,Post-manufacture validation and debug:0.2","Memory test and repair,Test-pattern generation and fault simulation",Memory test and repair: The paper focuses on BIST tools for memory testing. Test-pattern generation and fault simulation: The work uses fault simulation to evaluate test algorithms. Other categories like 'Analog test' were rejected because the focus is on digital memory testing.
1297,A Modification of Lee's Path Connection Algorithm,"It is shown that a set of diagnostic tests designed for a redundant circuit under the single-fault assumption is not necessarily a valid test set if a fault occurrence is preceded by the occurrence of some ( undetectable) redundant faults. This is an additional reason ( besides economy) for trying to eliminate certain kinds of redundancy from the circuit. However, single-fault analysis may remain valid for some types of redundancy which serve a useful purpose, such as the elimination of logic hazards in two-level circuits.","General and reference:0.1,Hardware:0.9,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Hardware,Hardware is relevant because the paper discusses fault analysis and redundancy in circuit design. Other categories are not central to the core contribution of hardware-level fault tolerance.,"Communication hardware, interfaces and storage:0.25,Electronic design automation:0.25,Emerging technologies:0.25,Hardware test:1.0,Hardware validation:1.0,Integrated circuits:0.5,Power and energy:0.25,Printed circuit boards:0.25,Robustness:0.75,Very large scale integration design:0.25","Hardware test,Hardware validation,Robustness",Hardware test and Hardware validation are relevant as the paper examines fault detection in circuits. Robustness is relevant due to the analysis of redundant faults. Other categories like VLSI design are less relevant as the paper focuses on fault analysis rather than design methodologies.,"Analog, mixed-signal and radio frequency test:0.1,Board- and system-level test:0.1,Defect-based test:0.1,Design for manufacturability:0.1,Design for testability:0.5,Fault models and test metrics:0.7,Fault tolerance:0.1,Functional verification:0.1,Hardware reliability:0.1,Hardware reliability screening:0.1,Memory test and repair:0.1,Physical verification:0.1,Post-manufacture validation and debug:0.1,Safety critical systems:0.1,Test-pattern generation and fault simulation:0.6,Testing with distributed and parallel systems:0.1","Design for testability,Fault models and test metrics,Test-pattern generation and fault simulation",Design for testability is relevant as the paper discusses circuit testing methodologies. Fault models and test metrics and Test-pattern generation and fault simulation are relevant given the focus on fault analysis and test set validation.
2489,An efficient direct mapped instruction cache for application-specific embedded systems,"Caches may consume half of a microprocessor's total power and cache misses incur accessing off-chip memory, which is both time consuming and energy costly. Therefore, minimizing cache power consumption and reducing cache misses are important to reduce total energy consumption of embedded systems. Direct mapped caches consume much less power than that of same sized set associative caches but with a poor hit rate on average. Through experiments, we observe that memory space of direct mapped instruction caches is not used efficiently in most embedded applications. We design an efficient cache - a configurable instruction cache that can be tuned to utilize the cache sets efficiently for a particular application such that cache memory is exploited more efficiently by index remapping. Experiments on 11 benchmarks drawn from Mediabench show that the efficient cache achieves almost the same miss rate as a conventional two-way set associative cache on average and with total memory-access energy savings of 30% compared with a conventional two-way set associative cache.","General and reference:0.0,Hardware:1.0,Computer systems organization:0.5,Networks:0.1,Software and its engineering:0.3,Theory of computation:0.1,Mathematics of computing:0.2,Information systems:0.1,Security and privacy:0.0,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.0",Hardware,Hardware is central to the cache design and power optimization. Computer systems organization is secondary for system-level analysis. Other categories are irrelevant to the hardware-focused contribution.,"Communication hardware, interfaces and storage:0.1,Electronic design automation:0.2,Emerging technologies:0.1,Hardware test:0.1,Hardware validation:0.1,Integrated circuits:0.7,Power and energy:0.9,Printed circuit boards:0.1,Robustness:0.1,Very large scale integration design:0.2","Power and energy,Integrated circuits",Power and energy is highly relevant as the paper focuses on reducing cache power consumption. Integrated circuits is relevant since the cache is a critical component of microprocessor hardware. Other categories like VLSI design are less directly relevant to the specific cache optimization techniques discussed.,"3D integrated circuits:0.0,Digital switches:0.0,Energy distribution:0.0,Energy generation and storage:0.0,Impact on the environment:0.0,Interconnect:0.0,Logic circuits:0.0,Power estimation and optimization:1.0,Reconfigurable logic and FPGAs:0.0,Semiconductor memory:0.9,Thermal issues:0.0","Power estimation and optimization,Semiconductor memory",Power estimation and optimization: The paper focuses on reducing cache power consumption and energy savings in embedded systems. Semiconductor memory: The design of an efficient instruction cache directly relates to memory architecture. Other options are irrelevant as the paper does not discuss thermal issues or reconfigurable logic.
1929,VLSI Design of a Depth Map Estimation Circuit Based on Structured Light Algorithm,"In this paper, depth map estimation circuit design based on structured light is proposed, wherein a projection light source and an image sensor are utilized in combination to achieve a depth map estimation chip, an accurate, low complex circuit is presented to measure the depth value of an object, and it is applied to 3-D television through depth image-based rendering technology to implement a multiview image display system. The chip that integrates image preprocessing, object segmentation, edge detection, and depth map quantization algorithms performs depth map estimation of objects accurately and quickly. Besides, the effect of human visual sensing effect is taken into consideration and the presented chip achieves a depth map quantified according to the nonlinear depth value in compliance with human visual system. Measurement results show that the chip that is verified using a Taiwan Semiconductor Manufacturing Company 0.18-μm 1P6M CMOS process operates at 83.3 MHz with 19 845 gate counts and performs real time depth map estimation.","General and reference:0.1,Hardware:0.9,Computer systems organization:0.2,Networks:0.1,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Hardware,"Hardware is highly relevant as the paper presents a VLSI circuit design for depth map estimation. Computer systems organization (0.2) receives moderate score for system-level aspects, but the core contribution is in hardware design for structured light applications.","Communication hardware, interfaces and storage:0.1,Electronic design automation:0.3,Emerging technologies:0.1,Hardware test:0.1,Hardware validation:0.1,Integrated circuits:0.2,Power and energy:0.1,Printed circuit boards:0.1,Robustness:0.1,Very large scale integration design:0.9",Very large scale integration design,"Very large scale integration design is highly relevant as the paper presents a VLSI design for depth map estimation. Other categories have low relevance as the paper doesn't focus on communication hardware, emerging technologies, or power/energy issues.","3D integrated circuits:0,Analog and mixed-signal circuits:0,Application-specific VLSI designs:1,Design reuse and communication-based design:0,Design rules:0,Economics of chip design and manufacturing:0,Full-custom circuits:0,On-chip resource management:0,On-chip sensors:0,Standard cell libraries:0,VLSI design manufacturing considerations:1,VLSI packaging:0,VLSI system specification and constraints:0","Application-specific VLSI designs,VLSI design manufacturing considerations","Application-specific VLSI designs: The paper presents a specialized depth map estimation chip for 3D TV. VLSI design manufacturing considerations: The chip is fabricated using a 0.18-μm CMOS process, and performance metrics (gate count, frequency) are evaluated. Other options are irrelevant to the application-specific design and manufacturing context."
3883,A 0.20 mm2 3 nW Signal Acquisition IC for Miniature Sensor Nodes in 65 nm CMOS,"Miniature $\text{mm}^3$ -sized sensor nodes have a very tight power budget, in particular, when a long operational lifetime is required, which is the case, e.g., for implantable devices or unobtrusive IoT nodes. This paper presents a fully integrated signal acquisition IC for these emerging applications. It integrates an amplifier with 32 dB gain and 370 Hz bandwidth that includes positive feedback to enhance input impedance and dc offset compensation. The IC includes also a 10 bit 1 kS/s SAR ADC as well as a clock generator and voltage and current biasing circuits. The overall system achieves an input noise of $27\;\upmu \text {V}_{{\text{rms}}}$ , consumes 3 nW from a 0.6 V supply, occupies $0.20\;\text {mm}^2$ in 65 nm CMOS, and has a single-wire data interface. The amplifier achieves an noise-efficiency factor (NEF) of 2.1 and the ADC has a figure-of-merit (FoM) of 1.5 fJ/conversion-step. Measurements confirm reliable operation for supplies from 0.50 to 0.70 V and temperatures in the range of 0–85 °C. As an application example, an ECG recording is successfully performed with the system while a $0.69\; \text{mm}^2$ photodiode array provides its power supply in indoor lighting conditions.","General and reference:0.1,Hardware:0.9,Computer systems organization:0.2,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.3,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Hardware,"Hardware is directly relevant because the paper presents a novel low-power, integrated signal acquisition IC design in CMOS. Other fields like Computer systems organization are less relevant as the focus is on hardware circuit design rather than system architecture.","Communication hardware, interfaces and storage:0.3,Electronic design automation:0.1,Emerging technologies:0.2,Hardware test:0.1,Hardware validation:0.1,Integrated circuits:1.0,Power and energy:0.8,Printed circuit boards:0.1,Robustness:0.2,Very large scale integration design:0.5","Integrated circuits,Power and energy",Integrated circuits is primary as the paper presents a fully integrated signal acquisition IC. Power and energy is relevant due to the 3nW power consumption focus. Other categories like Communication hardware are less central to the IC design itself.,"3D integrated circuits:0.1,Digital switches:0.1,Energy distribution:0.2,Energy generation and storage:0.8,Impact on the environment:0.2,Interconnect:0.1,Logic circuits:0.2,Power estimation and optimization:1.0,Reconfigurable logic and FPGAs:0.1,Semiconductor memory:0.1,Thermal issues:0.2","Power estimation and optimization,Energy generation and storage",Power estimation and optimization: Central to the ultra-low-power IC design. Energy generation and storage: Mentions photodiode-based energy harvesting. Other options like 3D ICs are not discussed.
5131,Digital linearization of direct-conversion spectrum sensing receiver,"Reliable spectrum sensing ability is a key factor in cognitive radios. However, there are many aspects that impact the sensing reliability. One important aspect is impairments in the cognitive radio receiver hardware. Received signals tend to have high dynamic range which drives the receiver to the nonlinear zone. This may cause nonlinear distortion falling to the sensing band and therefore either triggers a false alarm or missed detection. This paper specifically focuses on the digital compensation of sensing receiver LNA nonlinearities which are typically the most significant sources of nonlinearity. The proposed method is able to notably remove nonlinear distortion from the received signal and thus spectrum sensing algorithms become more reliable. With the help of simulations, this is shown not only for a classical energy detector but also for a cyclostationary feature detector.","General and reference:0.0,Hardware:1.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Hardware,"Hardware: The paper addresses digital linearization techniques for a direct-conversion spectrum sensing receiver, a hardware-centric topic.","Communication hardware, interfaces and storage:0.7,Electronic design automation:0.1,Emerging technologies:0.1,Hardware test:0.1,Hardware validation:0.1,Integrated circuits:0.1,Power and energy:0.1,Printed circuit boards:0.1,Robustness:0.75,Very large scale integration design:0.1","Communication hardware, interfaces and storage,Robustness",Communication hardware is relevant because the paper addresses receiver hardware impairments. Robustness is relevant due to the focus on improving reliability through digital linearization. Other categories are not central to hardware reliability or signal processing.,"Design for manufacturability:0,Fault tolerance:0,Hardware reliability:1,Safety critical systems:0",Hardware reliability,Hardware reliability is directly addressed through the focus on digital compensation of receiver nonlinearities to improve spectrum sensing reliability. Other categories are not relevant to the paper's core contribution.
887,14 GSps Four-Bit Noninterleaved Data Converter Pair in 90 nm CMOS With Built-In Eye Diagram Testability,"This paper presents the design and test of a 14 GSps, four-bit data converter pair in 90 nm CMOS suitable for implementing advanced serial links. The data converter pair consists of a noninterleaved flash analog-to-digital converter (ADC) and a noninterleaved current-steering digital-to-analog converter (DAC). Both the converter designs adopt the wave-pipelining technique to increase the available signal settling time. Through detailed analysis, we show that cascading three active feedback preamplifiers to implement the cores of the comparators in the ADC balances the power budget and the design difficulty when we push the sampling rate to the process limit. Current mode logic gates are used to alleviate the power bouncing issue. To address the difficulty and high cost of testing the extremely high-speed converters, the design embeds the simple design-for-testability circuits cooperating with the on-chip resources to provide two cost-effective test modes. The first test mode cascades the ADC and DAC so that they can be tested at the rated speed without the need of a very high speed logic analyzer. The second test mode enables the eye diagram tests by shuffling the digital outputs of ADC as the inputs of the DAC instead of adopting conventional linear feedback shift register. The experimental results show that the cascaded ADC and DAC pair achieves a 31.0 dBc spurious-free dynamic range and a 25.9 dB signal-to-noise-and-distortion ratio with a 1.11 GHz, -1 dBFS stimulus at 14 GSps. The ADC and DAC consume 214 mW and 85 mW from a 1.0-V supply and occupy 0.1575 mm2 and 0.0636 mm2, respectively.","General and reference:0.0,Hardware:1.0,Computer systems organization:0.75,Networks:0.5,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Hardware,Hardware is highly relevant for the design of high-speed data converters. Computer systems organization is somewhat relevant but secondary. Networks is less relevant to the core contribution.,"Communication hardware, interfaces and storage:0.0,Electronic design automation:0.0,Emerging technologies:0.0,Hardware test:0.8,Hardware validation:0.0,Integrated circuits:0.9,Power and energy:0.0,Printed circuit boards:0.0,Robustness:0.0,Very large scale integration design:0.7","Integrated circuits,Hardware test,Very large scale integration design",Integrated circuits: The paper focuses on the design of high-speed data converters. Hardware test: The paper introduces built-in testability features. Very large scale integration design: The design is implemented in VLSI (90 nm CMOS). Other categories like communication hardware or power and energy are not central to the core contribution.,"3D integrated circuits:0,Analog and mixed-signal circuits:1,Analog, mixed-signal and radio frequency test:0,Application-specific VLSI designs:1,Board- and system-level test:0,Defect-based test:0,Design for testability:1,Design reuse and communication-based design:0,Design rules:0,Digital switches:0,Economics of chip design and manufacturing:0,Fault models and test metrics:0,Full-custom circuits:0,Hardware reliability screening:0,Interconnect:0,Logic circuits:0,Memory test and repair:0,On-chip resource management:0,On-chip sensors:0,Reconfigurable logic and FPGAs:0,Semiconductor memory:0,Standard cell libraries:0,Test-pattern generation and fault simulation:0,Testing with distributed and parallel systems:0,VLSI design manufacturing considerations:0,VLSI packaging:0,VLSI system specification and constraints:0","Analog and mixed-signal circuits,Application-specific VLSI designs,Design for testability",Analog and mixed-signal circuits directly address the ADC/DAC design. Application-specific VLSI designs apply to the 90nm CMOS implementation. Design for testability is critical for the embedded testability features. Other options like 3D ICs or logic circuits are irrelevant to the paper's focus on high-speed data converters.
1988,The ATM Layer Chip: An ASIC for B-ISDN Applications,"The authors describe the architecture of an experimental research prototype application specific integrated circuit (ASIC) designed to serve as a generic building block of the future broadband integrated services digital network (B-ISDN). The chip performs common asynchronous transfer mode (ATM) layer functions such as cell assembly and cell disassembly. A new media access control (MAC) protocol developed for a broadband customer premises network is also integrated in the chip. The chip interfaces to the B-ISDN through a synchronous optical network (SONET) synchronous transmission signal-3c (STS-3c) framer chip. The ATM layer chip has been designed using 1.2 mu m CMOS technology with a die area of 5.4*5.4 mm/sup 2/ and approximately 27000 transistors. Experimental results are described. At the user network interface, the chip can be used to implement broadband terminal adaptors and the network termination. At the broadband local exchange, the chip can be used in the implementation of ATM statistical multiplexers, ATM switch port controllers, etc. >","General and reference:0.1,Hardware:0.9,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Hardware,Hardware is highly relevant for the design of an ATM layer ASIC. Other categories like Networks or Software are irrelevant as the focus is on hardware implementation of communication functions.,"Communication hardware, interfaces and storage:0.8,Electronic design automation:0.2,Emerging technologies:0.1,Hardware test:0.1,Hardware validation:0.3,Integrated circuits:0.9,Power and energy:0.1,Printed circuit boards:0.1,Robustness:0.1,Very large scale integration design:0.8","Integrated circuits,Communication hardware, interfaces and storage,Very large scale integration design",Integrated circuits is highly relevant for the ASIC design. Communication hardware applies to the ATM layer chip. VLSI design is central to the 1.2μm CMOS implementation. Other fields like hardware validation are only incidentally related.,"Application-specific VLSI designs:1.0,Digital switches:0.7,VLSI design manufacturing considerations:0.3,3D integrated circuits:0.1,Analog and mixed-signal circuits:0.1,Design reuse and communication-based design:0.1,Design rules:0.1,Full-custom circuits:0.2,Interconnect:0.1,Logic circuits:0.2,On-chip resource management:0.1,On-chip sensors:0.1,Reconfigurable logic and FPGAs:0.2,Semiconductor memory:0.1,Standard cell libraries:0.1,VLSI packaging:0.1,VLSI system specification and constraints:0.1",Application-specific VLSI designs,Application-specific VLSI designs is directly relevant for the ATM layer ASIC. Digital switches is secondary as the chip handles ATM switching functions.
2347,Crosstalk-aware power optimization with multi-bit flip-flops,"Applying multi-bit flip-flops (MBFFs) for clock power reduction in modern nanometer ICs has been becoming a promising lower-power design technique. Many previous works tried to utilize as more MBFFs with larger bit numbers as possible to gain more clock power saving. However, an MBFF with a larger bit number may lead to serious crosstalk due to the close interconnecting wires belonging to different signal nets which are connected to the same MBFF. To address the problem, this paper analyzes, evaluates, and compares the relationship between power consumption and crosstalk when applying MBFFs with different bit numbers. To solve the addressed problem, a novel crosstalk-aware power optimization approach is further proposed to optimize power consumption while satisfying the crosstalk constraint. Experimental results show that the proposed approach is very effective in crosstalk avoidance when applying MBFFs for power optimization. To our best knowledge, this is also the first work in the literature that considers the crosstalk effect for the MBFF application.","General and reference:0.0,Hardware:1.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Hardware,Hardware is relevant as the paper focuses on power optimization techniques for multi-bit flip-flops in IC design. Other categories like Networks or Software are not central to the hardware-centric optimization problem.,"Communication hardware, interfaces and storage:0.1,Electronic design automation:0.2,Emerging technologies:0.1,Hardware test:0.1,Hardware validation:0.1,Integrated circuits:0.8,Power and energy:1.0,Printed circuit boards:0.1,Robustness:0.1,Very large scale integration design:0.3","Power and energy,Integrated circuits",Power and energy is highly relevant for the power optimization focus. Integrated circuits receives a high score due to the VLSI context. Other options like Hardware test are less relevant as the paper addresses power optimization rather than testing techniques.,"3D integrated circuits:0,Digital switches:0,Energy distribution:0,Energy generation and storage:0,Impact on the environment:0,Interconnect:1,Logic circuits:0,Power estimation and optimization:1,Reconfigurable logic and FPGAs:0,Semiconductor memory:0,Thermal issues:0","Power estimation and optimization,Interconnect",Power estimation and optimization: The paper proposes a power optimization approach for MBFFs. Interconnect: The crosstalk issue directly relates to interconnect analysis. Other categories like Digital switches or Semiconductor memory are irrelevant as the focus is on power optimization and crosstalk in interconnects.
3750,Performance Enhancement of CMOS VLSI Circuits by Transistor Reordering,"A method which uses transistor reordering for the performance enhancement of CMOS circuits is presented. The proposed technique achieves significant reduction in propagation delays with little effect on layout area. The technique can be coupled with transistor sizing to achieve unbounded improvement in circuit delay, and it can be used to decrease dynamic power dissipation. In particular, excellent results have been achieved when the method is applied to data path circuits.","General and reference:0.1,Hardware:0.9,Computer systems organization:0.2,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Hardware,Hardware is highly relevant for the CMOS VLSI circuit design and transistor reordering. Other categories like Software or Networks are not central to the core contribution.,"Communication hardware, interfaces and storage:0.1,Electronic design automation:0.2,Emerging technologies:0.2,Hardware test:0.1,Hardware validation:0.3,Integrated circuits:0.8,Power and energy:0.6,Printed circuit boards:0.1,Robustness:0.1,Very large scale integration design:0.7","Very large scale integration design,Integrated circuits,Power and energy",Very large scale integration design (0.7) and Integrated circuits (0.8): The paper presents a VLSI circuit optimization technique. Power and energy (0.6): The method reduces dynamic power dissipation. Other categories are less relevant.,"3D integrated circuits:0.1,Analog and mixed-signal circuits:0.1,Application-specific VLSI designs:1,Design reuse and communication-based design:0.1,Design rules:0.1,Digital switches:0.1,Economics of chip design and manufacturing:0.1,Energy distribution:0.1,Energy generation and storage:0.1,Full-custom circuits:0.1,Impact on the environment:0.1,Interconnect:0.2,Logic circuits:0.1,On-chip resource management:0.1,On-chip sensors:0.3,Power estimation and optimization:0.9,Reconfigurable logic and FPGAs:0.1,Semiconductor memory:0.1,Standard cell libraries:0.1,Thermal issues:0.1,VLSI design manufacturing considerations:0.1,VLSI packaging:0.1,VLSI system specification and constraints:0.1","Application-specific VLSI designs,Power estimation and optimization",Application-specific VLSI designs is relevant for optimizing CMOS circuits. Power estimation and optimization applies to reducing dynamic power. Other categories like 'interconnect' or 'thermal issues' are not central to the core contribution.
5664,A 40 Mbps H.264/AVC CAVLC decoder using a 64-bit multiple-issue video parsing coprocessor,"In this paper, we describe a programmable CAVLC decoder implemented with a video parsing coprocessor. The video parsing coprocessor is a VLIW processor that issues multiple instructions and supports condition-controlled instructions to efficiently program control intensive algorithms and customized instructions for bit operations and table matching. The complexity of the parsing coprocessor is 92 Kgates logic circuits with 7 KB SRAM and its operating frequency is 200 MHz when synthesized with a 130 nm CMOS technology. The CAVLC decoder, when operated at 192 MHz, can decode a bitstream at the rate of 40 Mbps, which corresponds to the level 4.1 of H.264/AVC full HD 1080p.","General and reference:0.2,Hardware:1.0,Computer systems organization:0.5,Networks:0.1,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.2,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.3,Applied computing:0.2,Social and professional topics:0.2",Hardware,Hardware is directly relevant as the paper describes a video parsing coprocessor's design and implementation. Other categories like Software or Theory of computation are less relevant as the focus is on hardware architecture and performance.,"Communication hardware, interfaces and storage:0.2,Electronic design automation:0.1,Emerging technologies:0.1,Hardware test:0.1,Hardware validation:0.1,Integrated circuits:0.9,Power and energy:0.1,Printed circuit boards:0.1,Robustness:0.1,Very large scale integration design:0.8","Integrated circuits,Very large scale integration design","Integrated circuits: The paper describes a specialized hardware component (video parsing coprocessor) for H.264 decoding, which is a core topic in integrated circuits. Very large scale integration design: The design of the VLIW processor and its bit-level operations align with VLSI design principles. Other categories like Communication hardware are less relevant as the focus is on hardware architecture rather than communication interfaces.","3D integrated circuits:0.0,Analog and mixed-signal circuits:0.0,Application-specific VLSI designs:1.0,Design reuse and communication-based design:0.0,Design rules:0.0,Digital switches:0.0,Economics of chip design and manufacturing:0.0,Full-custom circuits:0.0,Interconnect:0.0,Logic circuits:0.0,On-chip resource management:0.0,On-chip sensors:0.0,Reconfigurable logic and FPGAs:1.0,Semiconductor memory:0.0,Standard cell libraries:0.0,VLSI design manufacturing considerations:0.0,VLSI packaging:0.0,VLSI system specification and constraints:0.0","Application-specific VLSI designs,Reconfigurable logic and FPGAs",Application-specific VLSI designs is relevant for the programmable CAVLC decoder implementation. Reconfigurable logic and FPGAs is relevant due to the VLIW processor's multiple-issue architecture. Other categories like 3D integrated circuits or Full-custom circuits are irrelevant as the paper focuses on application-specific and reconfigurable design.
704,1.8 V.0.35 /spl mu/m CMOS wideband programmable gain amplifier,"∗ Group of Electronic Design, Department of Electronic and Communications Engineering, University of Zaragoza, Pedro Cerbuna 12, 50009 Zaragoza, Spain. e-mails: [becalvo,scelma,pemar2,materesa]@unizar.es, tel.: +34 976 761240, fax: +34 976 762143. Abstract − This paper presents a low-voltage high-frequency programmable gain amplifier (PGA) based on a very simple gm-boosted source-degenerated differential pair using a combined polysilicon-MOS resistor network as the degeneration resistance. The PGA is designed in a standard 0.35 μm CMOS process and consumes less than 0.5 mW from a 1.8 V supply. The gain varies linearly in a dB-scale from 0 to 18 dB in 6 dB steps, while maintaining an almost constant bandwidth exceeding 170 MHz. With 0.2 Vp-p differential output the total harmonic distortion levels are below -72 dB at 10 MHz over the whole gain setting range.","General and reference:0.1,Hardware:0.9,Computer systems organization:0.2,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Hardware,Hardware is highly relevant as the paper presents a CMOS-based PGA design with detailed implementation in a 0.35μm process. Other fields are irrelevant as the focus is on integrated circuit design and performance metrics.,"Communication hardware, interfaces and storage:0.3,Electronic design automation:0.1,Emerging technologies:0.1,Hardware test:0.1,Hardware validation:0.1,Integrated circuits:0.9,Power and energy:0.7,Printed circuit boards:0.1,Robustness:0.1,Very large scale integration design:0.1","Integrated circuits,Power and energy",Integrated circuits is relevant for the CMOS PGA design. Power and energy applies to the low-voltage focus. Other fields like Communication hardware are less directly related.,"3D integrated circuits:0.1,Digital switches:0.2,Energy distribution:0.2,Energy generation and storage:0.1,Impact on the environment:0.1,Interconnect:1.0,Logic circuits:0.6,Power estimation and optimization:1.0,Reconfigurable logic and FPGAs:0.3,Semiconductor memory:0.2,Thermal issues:0.3","Interconnect,Power estimation and optimization",Interconnect is relevant due to the focus on amplifier design and circuit structure. Power estimation and optimization align with the low-voltage CMOS design. Other fields like Energy distribution are less directly related.
4122,Enhancement-Mode AlGaN/GaN HEMTs with Low On-Resistance and Low Knee-Voltage,"Based on fluoride-based plasma treatment of the gate region in AlGaN/GaN HEMTs and post-gate rapid thermal annealing (RTA), enhancement mode (E-mode) AlGaN/GaN HEMTs with low on-resistance and low knee-voltage were fabricated. The fabricated E-mode AlGaN/GaN HEMT with 1 μm-long gate exhibits a threshold voltage of 0.9 V, a knee-voltage of 2.2 V, a maximum drain current density of 310 mA/mm, a peak g m of 148 mS/mm, a current gain cutoff frequency f T of 10.1 GHz and a maximum oscillation frequency f max of 34.3 GHz. In addition, the fluoride-based plasma treatment was also found to be effective in lowering the gate leakage current, in both forward and reverse bias. Two orders of magnitude reducation in gate leakage current was observed in the fabricated E-mode HEMTs compared to the conventional D-mode HEMTs without fluoride-based plasma treatment.","General and reference:0.1,Hardware:0.9,Computer systems organization:0.6,Networks:0.2,Software and its engineering:0.3,Theory of computation:0.2,Mathematics of computing:0.3,Information systems:0.2,Security and privacy:0.1,Human-centered computing:0.2,Computing methodologies:0.4,Applied computing:0.3,Social and professional topics:0.1",Hardware,"Hardware: The paper focuses on the design and fabrication of enhancement-mode AlGaN/GaN HEMTs, which are semiconductor devices central to hardware engineering.","Communication hardware, interfaces and storage:0.1,Electronic design automation:0.1,Emerging technologies:0.1,Hardware test:0.1,Hardware validation:0.1,Integrated circuits:0.9,Power and energy:0.7,Printed circuit boards:0.1,Robustness:0.1,Very large scale integration design:0.1","Integrated circuits,Power and energy",Integrated circuits is relevant for the HEMT fabrication and performance analysis. Power and energy is relevant due to the focus on low on-resistance and power efficiency. Other categories like 'Communication hardware' are rejected as the focus is on semiconductor devices rather than communication systems.,"3D integrated circuits:0,Digital switches:1,Energy distribution:0,Energy generation and storage:0,Impact on the environment:0,Interconnect:0.5,Logic circuits:0,Power estimation and optimization:1,Reconfigurable logic and FPGAs:0,Semiconductor memory:0,Thermal issues:0.5,System on a chip:0.5","Digital switches,Power estimation and optimization",Digital switches is highly relevant as HEMTs are fundamental semiconductor switches. Power estimation and optimization is relevant due to the focus on device performance metrics like on-resistance and knee-voltage. Thermal issues is moderately relevant for potential heat management considerations.
2806,Can SAT be used to improve sequential ATPG methods?,"In this work we investigate the integration of SAT methods into a simulation-based sequential ATPG tool, STRATEGATE, with the aim of improving the state-of-the-art in sequential ATPG. We offer a detailed analysis of possible scenarios and algorithms for performing such an integration. Our preliminary investigations show that such hybrid approaches can be very promising.","General and reference:0.0,Hardware:1.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.5,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Hardware,Hardware is relevant because the paper addresses SAT integration in ATPG tools for hardware testing. Theory of computation is partially relevant due to SAT algorithms but not the core focus.,"Communication hardware, interfaces and storage:0.1,Electronic design automation:0.1,Emerging technologies:0.1,Hardware test:0.8,Hardware validation:0.6,Integrated circuits:0.4,Power and energy:0.1,Printed circuit boards:0.1,Robustness:0.2,Very large scale integration design:0.7","Hardware test,Very large scale integration design",Hardware test is highly relevant as the paper focuses on improving ATPG methods. Very large scale integration design is relevant due to the application in circuit testing. Other categories like Integrated circuits are less central.,"3D integrated circuits:0,Analog and mixed-signal circuits:0,Analog, mixed-signal and radio frequency test:0,Application-specific VLSI designs:0,Board- and system-level test:0,Defect-based test:0,Design for testability:0,Design reuse and communication-based design:0,Design rules:0,Economics of chip design and manufacturing:0,Fault models and test metrics:0,Full-custom circuits:0,Hardware reliability screening:0,Memory test and repair:0,On-chip resource management:0,On-chip sensors:0,Standard cell libraries:0,Test-pattern generation and fault simulation:1,Testing with distributed and parallel systems:0,VLSI design manufacturing considerations:0,VLSI packaging:0,VLSI system specification and constraints:0",Test-pattern generation and fault simulation,Test-pattern generation and fault simulation is directly relevant as the paper discusses integrating SAT methods into a sequential ATPG tool to improve test pattern generation. Other options are unrelated to the specific focus on test pattern generation algorithms.
1659,A scalable algorithm for RTL insertion of gated clocks based on ODCs computation,"We propose a new algorithm for automatic clock-gating insertion applicable at the register transfer level (RTL). The basic rationale of our approach is to eliminate redundant computations performed by temporally unobservable blocks through aggressive exploitation of observability don't care (ODC) conditions. ODCs are efficiently detected from an RTL description by focusing only on data-path modules with easily detectable input unobservability conditions. ODCs are then propagated in the form of logic expressions toward the registers by backward traversal and levelization of the design. Finally, the logic expressions are mapped onto hardware to provide control signals to the clock-gating logic at a reduced cost in area and speed. The technique is characterized by fast processing time, high scalability to large designs, and tight user control on clock-gating overhead. Our approach is compatible with standard industrial design flows, and reduces power consumption significantly with a small overhead in delay and area. Experimental results obtained on a set of industrial RTL designs containing several tens of thousands of gates show average power reductions of around 42%. On the same examples, the application of traditional clock-gating leads to average savings reductions close to 29%.","General and reference:0.1,Hardware:0.9,Computer systems organization:0.6,Networks:0.2,Software and its engineering:0.2,Theory of computation:0.2,Mathematics of computing:0.2,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.3,Applied computing:0.2,Social and professional topics:0.1",Hardware,Hardware (0.9): The paper introduces an RTL-level clock-gating algorithm for hardware optimization. Other categories like Systems Organization are secondary to the hardware design focus.,"Communication hardware, interfaces and storage:0.1,Electronic design automation:0.9,Emerging technologies:0.1,Hardware test:0.1,Hardware validation:0.1,Integrated circuits:0.8,Power and energy:0.9,Printed circuit boards:0.1,Robustness:0.1,Very large scale integration design:0.8","Power and energy,Electronic design automation,Very large scale integration design",Power and energy is relevant due to clock-gating for power reduction. Electronic design automation applies to the RTL algorithm. Very large scale integration design fits the VLSI context. Communication hardware is less relevant as the focus is on power optimization rather than communication systems.,"3D integrated circuits:0,Analog and mixed-signal circuits:0,Application-specific VLSI designs:0,Design reuse and communication-based design:0,Design rules:0,Economics of chip design and manufacturing:0,Energy distribution:0,Energy generation and storage:0,Full-custom circuits:0,Hardware description languages and compilation:0,High-level and register-transfer level synthesis:1,Impact on the environment:0,Logic synthesis:0,Methodologies for EDA:0,Modeling and parameter extraction:0,On-chip resource management:0,On-chip sensors:0,Physical design (EDA):0,Power estimation and optimization:1,Standard cell libraries:0,Thermal issues:0,Timing analysis:0,VLSI design manufacturing considerations:0,VLSI packaging:0,VLSI system specification and constraints:0","High-level and register-transfer level synthesis,Power estimation and optimization",High-level and register-transfer level synthesis are relevant for the RTL algorithm. Power estimation and optimization are relevant because the paper focuses on reducing power consumption. Other categories like Analog circuits are not central to the core contribution.
680,Fast-switching analog PLL with finite-impulse response,"This paper describes a method for speeding up the linear settling response of integer-N phase-locked loops. Extending the discrete-time model of the PLL first proposed by Gardner, simple design rules are derived which guarantee accurate frequency settling in few reference cycles. Simulations show that the proposed design technique improves up to six times the settling time of a conventional design. The stability margins and the noise behavior of the proposed system are analyzed.","General and reference:0.1,Hardware:0.8,Computer systems organization:0.3,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Hardware,Hardware is highly relevant as the paper presents a novel analog PLL circuit design for fast frequency settling. Other fields like Software or Networks are not central to the circuit-level analysis and simulation.,"Communication hardware, interfaces and storage:0.9,Electronic design automation:0.1,Emerging technologies:0.1,Hardware test:0.1,Hardware validation:0.1,Integrated circuits:0.8,Power and energy:0.1,Printed circuit boards:0.1,Robustness:0.1,Very large scale integration design:0.7","Communication hardware, interfaces and storage,Integrated circuits",Communication hardware is directly relevant as the paper discusses PLL design for communication systems. Integrated circuits is relevant due to the focus on analog circuit design. Very large scale integration design is somewhat relevant but secondary to the core contribution.,"3D integrated circuits:0.1,Digital switches:0.2,Interconnect:0.1,Logic circuits:0.6,Reconfigurable logic and FPGAs:0.1,Semiconductor memory:0.1",Logic circuits,Logic circuits are the closest fit for PLL design analysis. Other options like FPGAs or 3D ICs are irrelevant to analog PLLs.
3807,Phase analysis of coaxial short and open circuits,"Phase characteristics of coaxial short and open circuits used to calibrate reflectometers are analyzed by using the lossy transmission line theory and mode-matching method based on the cosine/sine-Fourier transforms. Calculated phases of short and open circuits with Type-N, 7 mm and 3.5 mm connectors are compared with ones predicted from the calibration kit parameters provided by the manufacturer.","General and reference:0.1,Hardware:0.95,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Hardware,Hardware is highly relevant because the paper analyzes physical coaxial circuits and transmission line theory. Other categories like Computer systems organization or Networks are not mentioned in the core technical focus.,"Communication hardware, interfaces and storage:1.0,Electronic design automation:0.1,Emerging technologies:0.1,Hardware test:0.8,Hardware validation:0.7,Integrated circuits:0.1,Power and energy:0.1,Printed circuit boards:0.1,Robustness:0.1,Very large scale integration design:0.1","Communication hardware, interfaces and storage,Hardware validation,Hardware test","Communication hardware, interfaces and storage is directly relevant for analyzing coaxial circuits. Hardware validation and Hardware test are relevant for calibration and phase analysis. Other categories like Integrated circuits are less central.","Analog, mixed-signal and radio frequency test:1.0,Board- and system-level test:0.7,Design for testability:0.4,Test-pattern generation and fault simulation:0.2","Analog, mixed-signal and radio frequency test",Analog and RF test is directly relevant due to the phase analysis of coaxial circuits. Board-level test is moderately relevant but not the primary focus of the paper.
2125,Low-power implementation of H.324 audiovisual codec dedicated to mobile computing,"A VLSI implementation of the H.324 audiovisual codec is described. A number of sophisticated low-power architectures have been devised dedicatedly for the mobile use. A set of specific functional units, each corresponding to a process of H.263 video codec, is employed to lighten different performance bottlenecks. A compact DSP core composed of two MAC units is used for both ACELP and MP-MLQ coding schemes of the G.723.1 speech codec. The proposed audiovisual codec core has been implemented by using 0.35 /spl mu/m CMOS 4LM technology, which contains totally 420 K transistors with the dissipation of 224.32 mW from single 3.3 V supply.","General and reference:0.1,Hardware:0.8,Computer systems organization:0.3,Networks:0.1,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.3,Applied computing:0.2,Social and professional topics:0.1",Hardware,Hardware is central to the VLSI codec implementation. Other categories are secondary or unrelated.,"Communication hardware, interfaces and storage:0.2,Electronic design automation:0.3,Emerging technologies:0.1,Hardware test:0.1,Hardware validation:0.1,Integrated circuits:0.85,Power and energy:0.75,Printed circuit boards:0.1,Robustness:0.1,Very large scale integration design:0.9","Integrated circuits,Power and energy,Very large scale integration design",Integrated circuits: Core to VLSI implementation. Power and energy: Focus on low-power architectures. Very large scale integration design: Directly describes the implementation. Communication hardware is secondary to hardware design.,"3D integrated circuits:0.2,Analog and mixed-signal circuits:1,Application-specific VLSI designs:1,Design reuse and communication-based design:0.2,Design rules:0.2,Digital switches:0.2,Economics of chip design and manufacturing:0.2,Energy distribution:0.2,Energy generation and storage:0.2,Full-custom circuits:0.3,Impact on the environment:0.2,Interconnect:0.2,Logic circuits:0.2,On-chip resource management:0.3,On-chip sensors:0.2,Power estimation and optimization:1,Reconfigurable logic and FPGAs:0.2,Semiconductor memory:0.2,Standard cell libraries:0.2,Thermal issues:0.2,VLSI design manufacturing considerations:0.3,VLSI packaging:0.2,VLSI system specification and constraints:0.2","Analog and mixed-signal circuits,Application-specific VLSI designs,Power estimation and optimization",Analog and mixed-signal circuits: The design includes analog sensor interfaces and RF components. Application-specific VLSI designs: The chip is tailored for audiovisual codecs. Power estimation and optimization: The paper emphasizes low-power implementation. Thermal issues is rejected as power optimization is the focus rather than thermal management.
271,"A scaling compatible, synthesis friendly VCO-based delta-sigma ADC design and synthesis methodology","Conventional analog/mixed-signal (AMS) circuits design methodology relying heavily on the use of operational amplifiers (opamps) to process signals in voltage-domain (VD) encounters severe difficulties in advanced nanometer-scale CMOS process. We present a novel scaling compatible, synthesis friendly ring voltage-controlled oscillator (VCO) based time-domain (TD) delta-sigma analog-to-digital converter (ADC) whose performance improves as technology advances. Decomposed into digital gates (e.g. inverters) and a small set of simple customized cells (e.g. resistors), its layout is fully synthesizable by leveraging digital layout synthesis tools. Post-layout simulation results demonstrate the scaling compatibility of the proposed ADC and a drastic boost to design productivity.","General and reference:0.0,Hardware:1.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Hardware,Hardware is highly relevant because the paper presents a novel analog-to-digital converter (ADC) design methodology focused on hardware implementation. Other categories like Computer Systems Organization are less specific to the circuit-level design discussed.,"Communication hardware, interfaces and storage:0.1,Electronic design automation:1.0,Emerging technologies:0.3,Hardware test:0.1,Hardware validation:0.5,Integrated circuits:1.0,Power and energy:0.5,Printed circuit boards:0.1,Robustness:0.1,Very large scale integration design:1.0","Electronic design automation,Integrated circuits,Very large scale integration design",Electronic design automation: The paper introduces a synthesis-friendly methodology for ADC design. Integrated circuits: The ADC is a core IC component. Very large scale integration design: The design leverages digital synthesis tools for layout. Other categories like power and energy are secondary to the core focus on design methodology.,"3D integrated circuits:0,Analog and mixed-signal circuits:1,Application-specific VLSI designs:0,Design reuse and communication-based design:0,Design rules:0,Digital switches:0,Economics of chip design and manufacturing:0,Full-custom circuits:0,Hardware description languages and compilation:0,High-level and register-transfer level synthesis:1,Interconnect:0,Logic circuits:0,Logic synthesis:0,Methodologies for EDA:0,Modeling and parameter extraction:0,On-chip resource management:0,On-chip sensors:0,Physical design (EDA):0,Reconfigurable logic and FPGAs:0,Semiconductor memory:0,Standard cell libraries:0,Timing analysis:0,VLSI design manufacturing considerations:0,VLSI packaging:0,VLSI system specification and constraints:0","Analog and mixed-signal circuits,High-level and register-transfer level synthesis",Analog and mixed-signal circuits is relevant as the paper discusses ADC design. High-level and register-transfer level synthesis is relevant because the paper mentions a synthesis-friendly methodology. Other categories like 3D integrated circuits are unrelated.
1873,Beyond DVFS: A First Look at Performance under a Hardware-Enforced Power Bound,"Dynamic Voltage Frequency Scaling (DVFS) has been the tool of choice for balancing power and performance in high-performance computing (HPC). With the introduction of Intel's Sandy Bridge family of processors, researchers now have a far more attractive option: user-specified, dynamic, hardware-enforced processor power bounds. In this paper we provide a first look at this technology in the HPC environment and detail both the opportunities and potential pitfalls of using this technique to control processor power. As part of this evaluation we measure power and performance for single-processor instances of several of the NAS Parallel Benchmarks. Additionally, we focus on the behavior of a single benchmark, MG, under several different power bounds. We quantify the well-known manufacturing variation in processor power efficiency and show that, in the absence of a power bound, this variation has no correlation to performance. We then show that execution under a power bound translates this variation in efficiency into variation in performance.","General and reference:0.1,Hardware:1.0,Computer systems organization:0.2,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Hardware,"The paper focuses on hardware-enforced power management in HPC, which is a hardware design and optimization topic.","Power and energy:0.95,Communication hardware, interfaces and storage:0.3,Integrated circuits:0.4,Hardware validation:0.25",Power and energy,Power and energy is directly addressed through hardware-enforced power bounds. Other hardware-related categories are not the primary focus.,"Energy distribution:0,Energy generation and storage:0,Impact on the environment:0,Power estimation and optimization:1,Thermal issues:0",Power estimation and optimization,Power estimation and optimization is directly addressed through hardware-enforced power bounds. Other categories like Thermal issues are not explicitly discussed.
4067,A 14-bit 1.0-GS/s dynamic element matching DAC with >80 dB SFDR up to the Nyquist,"A 14-bit 1.0-GS/s current-steering digital-to-analog converter (DAC) was designed in a 65-nm CMOS process. For such current-steering DACs with a high sampling rate, the code-dependent load variations and switching glitches are a main bottleneck which limits the spurious-free dynamic range (SFDR). Dynamic element matching (DEM) has been an effective solution to randomize these glitches for a higher SFDR and also to reduce the matching requirement of the current cells for an area-efficient design which also improves the SFDR with reduced parasitic capacitance. An effective method named TRI-DEMRZ is proposed in this paper, consisting of time-relaxed interleaving, DEM and return-to-zero encoding. We also apply TRI-DEMRZ in synergy with complementary switched current sources (CSCS) to design the DAC for the purpose of a small die size and enhanced SFDR performance. Post-layout simulations show >80 dB SFDR up to the Nyquist. This DAC has a mixed 1.2 V / 2.5 V power supply and an active area of 0.48 mm2.","General and reference:0.1,Hardware:0.9,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Hardware,Hardware is highly relevant as the paper describes a 14-bit 1.0-GS/s DAC design in CMOS. Other categories like Networks or Software engineering are not discussed in the paper.,"Integrated circuits:0.95,Communication hardware, interfaces and storage:0.6,Hardware validation:0.7,Power and energy:0.5",Integrated circuits,"Integrated circuits: The paper designs and validates a 14-bit DAC in 65-nm CMOS, a core topic in integrated circuits. Communication hardware is secondary; the focus is on DAC design, not communication systems.","3D integrated circuits:0.2,Digital switches:1.0,Interconnect:1.0,Logic circuits:0.4,Reconfigurable logic and FPGAs:0.3,Semiconductor memory:0.2","Digital switches,Interconnect",Digital switches is relevant for the current-steering DAC design. Interconnect is relevant for addressing code-dependent load variations. 3D integrated circuits and Semiconductor memory are not discussed.
2980,Demo: Energy Harvesting Using Everyday Objects,"We present an energy harvesting technique that leverages readily available time-varying ambient electric field (EF) as source, and explore the utilization of surrounding everyday objects that are constructed from conductive materials. In this abstract, we describe the general concept and the circuit model of our approach. We showed that by incorporating energy store and release circuitry, we were able to deliver power to a usable level. To thoroughly test the technique, we developed Wireless Logger hardware module and conduct extensive one week-long power harvesting experiment. We show our ambient electric field profiler tool that checks the electrical properties of an environment. This tool is useful to obtain parameters to know whether our technique is feasible for a given environment. We highlight limitations that encourage us to refine our design, explore other grounding strategies, and implement techniques to extract the maximum harvestable energy in the future iterations of this strategy. We believe that the perpetual issue in Internet-of-Things is electrical power, and this research contributes as a simple, lightweight, thus usability-wise, a powerful solution for this issue.","General and reference:0.1,Hardware:0.9,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Hardware,"Hardware: The paper presents a novel energy harvesting technique involving circuit design and hardware implementation (Wireless Logger module). Other categories like 'Applied computing' are less relevant as the focus is on hardware design, not application domains.","Communication hardware, interfaces and storage:0.1,Electronic design automation:0.1,Emerging technologies:0.7,Hardware test:0.1,Hardware validation:0.1,Integrated circuits:0.1,Power and energy:0.9,Printed circuit boards:0.1,Robustness:0.1,Very large scale integration design:0.1","Power and energy,Emerging technologies",Power and energy receives 0.9 as the core topic is energy harvesting. Emerging technologies scores 0.7 for the novel approach. All other fields are irrelevant to the energy harvesting technique.,"Analysis and design of emerging devices and systems:0,Biology-related information processing:0,Circuit substrates:0,Electromechanical systems:0,Emerging interfaces:0,Emerging optical and photonic technologies:0,Energy distribution:0,Energy generation and storage:1,Impact on the environment:0,Memory and dense storage:0,Plasmonics:0,Power estimation and optimization:0,Quantum technologies:0,Reversible logic:0,Spintronics and magnetic technologies:0,Thermal issues:0",Energy generation and storage,"Energy generation and storage: The paper proposes energy harvesting from ambient electric fields, directly aligning with this category. Other categories like Electromechanical systems or Emerging interfaces are irrelevant as the focus is on energy harvesting, not mechanical systems or interface design."
4940,"On the implementation of shifters, multipliers, and dividers in VLSI floating point units","Several options for the implementation of combinatorial shifters, multipliers, and dividers for a VLSI floating point unit are presented and compared. The comparisons are made in the context of a single chip implementation in light of the constraints imposed by currently available MOS technology.","General and reference:0.1,Hardware:0.9,Computer systems organization:0.3,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.2,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Hardware,"Hardware is highly relevant as the paper focuses on the VLSI implementation of arithmetic units (shifters, multipliers, dividers), which is a core topic in hardware design. Computer systems organization is less relevant because the focus is on component-level implementation rather than system architecture.","Communication hardware, interfaces and storage:0.3,Electronic design automation:0.4,Emerging technologies:0.1,Hardware test:0.1,Hardware validation:0.1,Integrated circuits:1.0,Power and energy:0.1,Printed circuit boards:0.1,Robustness:0.1,Very large scale integration design:1.0","Integrated circuits,Very large scale integration design","Integrated circuits: The paper focuses on VLSI implementation of arithmetic units. Very large scale integration design: Directly related to the VLSI floating-point unit design. Other categories (e.g., Communication hardware) are irrelevant as the paper doesn't discuss networking or storage.","3D integrated circuits:0,Analog and mixed-signal circuits:0,Application-specific VLSI designs:1,Design reuse and communication-based design:0,Design rules:0.3,Digital switches:0,Economics of chip design and manufacturing:0.2,Full-custom circuits:0.5,Interconnect:0,Logic circuits:0.3,On-chip resource management:0,On-chip sensors:0,Reconfigurable logic and FPGAs:0,Semiconductor memory:0,Standard cell libraries:0,VLSI design manufacturing considerations:1,VLSI packaging:0,VLSI system specification and constraints:0","Application-specific VLSI designs,VLSI design manufacturing considerations",Application-specific VLSI designs and manufacturing considerations are directly addressed in the implementation analysis. Other VLSI-related categories are less relevant.
3810,Centip3De: a many-core prototype exploring 3D integration and near-threshold computing,"Process scaling has resulted in an exponential increase of the number of transistors available to designers. Meanwhile, global interconnect has not scaled nearly as well, because global wires scale only in one dimension instead of two, resulting in fewer, high-resistance routing tracks. This paper evaluates the use of three-dimensional (3D) integration to reduce global interconnect by adding multiple layers of silicon with vertical connections between them using through-silicon vias (TSVs). Because global interconnect can be millimeters long, and silicon layers tend to be only tens of microns thick in 3D stacked processes, the power and performance gains by using vertical interconnect can be substantial. To address the thermal issues that arise with 3D integration, this paper also evaluates the use of near-threshold computing---operating the system at a supply voltage just above the threshold voltage of the transistors.
 Specifically, we will discuss the design and test of Centip3De, a large-scale 3D-stacked near-threshold chip multiprocessor. Centip3De uses Tezzaron's 3D stacking technology in conjunction with Global Foundries' 130 nm process. The Centip3De design comprises 128 ARM Cortex-M3 cores and 256MB of integrated DRAM. Silicon measurements are presented for a 64-core version of the design.<!-- END_PAGE_1 -->","General and reference:0.1,Hardware:0.95,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Hardware,Hardware is highly relevant due to the focus on 3D chip design and physical architecture. Categories like Software or Networks are not the primary domain of this research.,"Emerging technologies:0.9,Integrated circuits:0.85,Power and energy:0.75,Communication hardware, interfaces and storage:0.1,Electronic design automation:0.3,Hardware test:0.1,Hardware validation:0.15,Printed circuit boards:0.1,Robustness:0.2,Very large scale integration design:0.2","Emerging technologies,Integrated circuits,Power and energy",Emerging technologies is relevant for 3D integration and near-threshold computing. Integrated circuits and Power and energy are relevant for the chip design and power optimization. Other categories are less central to the paper's focus.,"3D integrated circuits:1,Analysis and design of emerging devices and systems:0,Biology-related information processing:0,Circuit substrates:0,Digital switches:0,Electromechanical systems:0,Emerging interfaces:0,Emerging optical and photonic technologies:0,Energy distribution:0,Energy generation and storage:0,Impact on the environment:0,Interconnect:0,Logic circuits:0,Memory and dense storage:0,Plasmonics:0,Power estimation and optimization:0,Quantum technologies:0,Reconfigurable logic and FPGAs:0,Reversible logic:0,Semiconductor memory:0,Spintronics and magnetic technologies:0,Thermal issues:1,Other categories:0","3D integrated circuits,Thermal issues",3D integrated circuits are central to the design of Centip3De. Thermal issues are directly addressed through near-threshold computing. Other categories like Interconnect or Power estimation are secondary to the core contributions.
4863,Design and Analysis of a Delay Sensor Applicable to Process/Environmental Variations and Aging Measurements,"With technology scaling, the deviation between predicted path delay using simulation and actual path delay on silicon increases due to process variation and aging. Hence, on-chip measurement architectures are now widely used due to their higher accuracy and lower cost compared to using external expensive measurement devices. In this paper, a novel path-delay measurement architecture called path-based ring oscillator (Path-RO) which takes into account variations is proposed. Path-RO can perform accurate on-chip path-delay measurement with nearly no impact on functional data path. At the same time, process variations will not affect the measurement accuracy. The accuracy degradation due to aging is also negligible, which enables Path-RO to monitor path delay throughout aging process. This delay sensor is perfectly suitable for fast and accurate speed binning as well. By targeting speed paths, the speed of chip can be binned efficiently even in presence of clock skew. Various simulation results collected by Path-RO inserted into b19 circuit demonstrate its high accuracy and efficiency.","General and reference:0.1,Hardware:1.0,Computer systems organization:0.3,Networks:0.1,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.3,Applied computing:0.2,Social and professional topics:0.1",Hardware,"Hardware: The paper designs an on-chip delay sensor for measuring path delays, directly related to hardware architecture. Other categories like Networks or Software and its engineering are less relevant as the focus is on hardware measurement techniques rather than systems or networking.","Communication hardware, interfaces and storage:0.1,Electronic design automation:0.5,Emerging technologies:0.2,Hardware test:0.95,Hardware validation:0.85,Integrated circuits:0.8,Power and energy:0.3,Printed circuit boards:0.1,Robustness:0.65,Very large scale integration design:0.75","Hardware test,Integrated circuits,Very large scale integration design",Hardware test: Focuses on on-chip path delay measurement. Integrated circuits: Implements the solution on silicon. Very large scale integration design: Discusses VLSI design techniques. Other children like Power and energy are less directly relevant.,"3D integrated circuits:0,Analog and mixed-signal circuits:0.4,Analog, mixed-signal and radio frequency test:0.6,Application-specific VLSI designs:0.3,Board- and system-level test:0.2,Defect-based test:0.2,Design for testability:0.5,Design reuse and communication-based design:0.3,Design rules:0.2,Digital switches:0,Economics of chip design and manufacturing:0.3,Fault models and test metrics:0.4,Full-custom circuits:0.3,Hardware reliability screening:1,Interconnect:0.2,Logic circuits:0.4,Memory test and repair:0.3,On-chip resource management:0.5,On-chip sensors:1,Reconfigurable logic and FPGAs:0.4,Semiconductor memory:0.3,Standard cell libraries:0.2,Test-pattern generation and fault simulation:0.4,Testing with distributed and parallel systems:0.3,VLSI design manufacturing considerations:0.4,VLSI packaging:0.3,VLSI system specification and constraints:0.3","Hardware reliability screening,On-chip sensors",Hardware reliability screening and On-chip sensors are directly relevant as the paper presents a sensor for monitoring hardware aging and reliability. Other options like Analog circuits are less relevant as the focus is on digital delay measurement rather than analog test methods.
464,Power efficient embedded processor IPs through application-specific tag compression in data caches,"In this paper, we present a methodology for power minimization by data cache tag compression. The set of tags being accessed by the major application loops is analyzed statically during compile time and an efficient and optimal compression scheme is proposed Only a very limited number of tag bits are stored in the tag array for cache conflict identification, thus achieving a significant reduction in the number of active bitlines, sense amps, and comparator cells. The underlying hardware support for dynamically compressing the tags consists of a highly cost and power efficient programmable encoder which lies outside the cache access path, thus not affecting the processor cycle time. A detailed VLSI implementation has been performed and a number of experimental results on a set of embedded applications and numerical kernels is reported Energy dissipation decreases of up to 95% can be observed for the tag arrays, while significant energy reductions in the range of 10%-50% are observed when amortized across the overall cache subsystem.","General and reference:0.0,Hardware:1.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Hardware,Hardware: The paper proposes tag compression techniques for embedded processor data caches to reduce power consumption. Other categories like Software and its engineering are not core.,"Communication hardware, interfaces and storage:0.0,Electronic design automation:0.0,Emerging technologies:0.0,Hardware test:0.0,Hardware validation:0.0,Integrated circuits:0.0,Power and energy:1.0,Printed circuit boards:0.0,Robustness:0.0,Very large scale integration design:0.7","Power and energy,Very large scale integration design",Power and energy is directly relevant for the power efficiency focus. Very large scale integration design is relevant for embedded processor IP optimization. Other categories like Emerging Technologies are not explicitly addressed.,"3D integrated circuits:0,Analog and mixed-signal circuits:0,Application-specific VLSI designs:1,Design reuse and communication-based design:0,Design rules:0,Economics of chip design and manufacturing:0,Energy distribution:0,Energy generation and storage:0,Full-custom circuits:0,Impact on the environment:0,On-chip resource management:0,On-chip sensors:0,Power estimation and optimization:1,Standard cell libraries:0,Thermal issues:0,VLSI design manufacturing considerations:0,VLSI packaging:0,VLSI system specification and constraints:0","Application-specific VLSI designs,Power estimation and optimization","Application-specific VLSI designs: The paper focuses on application-specific tag compression in embedded processors, which is a core topic in application-specific VLSI design. Power estimation and optimization: The paper's primary contribution is power minimization through tag compression techniques, directly aligning with power optimization. Other options like 3D integrated circuits or thermal issues are not discussed."
1091,Sleep transistor distribution in row-based MTCMOS designs,"The Multi-Threshold CMOS (MTCMOS) technology has become a popular technique for standby power reduction. This technology utilizes high-Vth sleep transistors to reduce sub threshold leakage currents during the standby mode of CMOS VLSI Circuits. The performance of MTCMOS circuits strongly depends on the size of the sleep transistors and the parasitics on the virtual ground network. Given a placed net list of a row-based MTCMOS design and the number of sleep transistor cells on each standard cell row, this paper introduces an optimal algorithm for linearly placing the allocated sleep transistors on each standard cell row so as to minimize the performance degradation of the MTCMOS circuit, which is in part due to unwanted voltage drops on its virtual ground network. Experimental results show that, compared to existing methods of placing the sleep transistors on cell rows, the proposed technique results in up to 11% reduction in the critical path delay of the circuit.","General and reference:0.1,Hardware:0.9,Computer systems organization:0.3,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Hardware,"Hardware: The paper presents sleep transistor placement optimization for MTCMOS circuits, a hardware design problem. Other categories like computer systems organization have limited relevance compared to direct hardware design focus.","Communication hardware, interfaces and storage:0.1,Electronic design automation:0.8,Emerging technologies:0.1,Hardware test:0.1,Hardware validation:0.1,Integrated circuits:0.8,Power and energy:0.1,Printed circuit boards:0.1,Robustness:0.1,Very large scale integration design:0.8","Electronic design automation,Integrated circuits,Very large scale integration design",Electronic design automation: The paper presents an algorithm for sleep transistor placement in MTCMOS designs. Integrated circuits: The work focuses on power management in VLSI circuits. Very large scale integration design: The algorithm directly addresses VLSI design challenges. Other children are irrelevant as the paper is about circuit design optimization rather than communication hardware or power systems.,"3D integrated circuits:0.0,Analog and mixed-signal circuits:0.0,Application-specific VLSI designs:0.0,Design reuse and communication-based design:0.0,Design rules:0.0,Digital switches:0.0,Economics of chip design and manufacturing:0.0,Full-custom circuits:0.0,Hardware description languages and compilation:0.0,High-level and register-transfer level synthesis:0.0,Interconnect:0.0,Logic circuits:0.0,Logic synthesis:0.0,Methodologies for EDA:0.0,Modeling and parameter extraction:0.0,On-chip resource management:0.0,On-chip sensors:0.0,Physical design (EDA):1.0,Reconfigurable logic and FPGAs:0.0,Semiconductor memory:0.0,Standard cell libraries:0.0,Timing analysis:0.0,VLSI design manufacturing considerations:1.0,VLSI packaging:0.0,VLSI system specification and constraints:0.0","VLSI design manufacturing considerations,Physical design (EDA)",VLSI design manufacturing considerations and Physical design (EDA) are relevant because the paper focuses on optimizing sleep transistor placement in MTCMOS designs to reduce power consumption. Other categories like Timing analysis or Interconnect are not directly addressed.
4784,An Electronically Fine-Tunable Multi-Input–Single-Output Universal Filter,"A new electronically fine-tunable multi-input-single-output (MISO) universal filter is proposed. The filter circuit compared with active-device-based counterparts is very simple and fully integrable. Moreover, a few number of active transistors provide a reduction in power consumption, and the filter enjoys advantages such as no component matching and small size area. Postlayout simulation results using parameters of AMS CMOS 0.35-μm process technology show good agreement with theoretical expectations.","General and reference:0.1,Hardware:0.9,Computer systems organization:0.2,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Hardware,Hardware is the most relevant category as the paper focuses on designing an electronically tunable analog filter circuit. Other categories like Software are not discussed.,"Communication hardware, interfaces and storage:0.0,Electronic design automation:0.0,Emerging technologies:0.0,Hardware test:0.0,Hardware validation:0.0,Integrated circuits:1.0,Power and energy:0.0,Printed circuit boards:0.0,Robustness:0.0,Very large scale integration design:0.3","Integrated circuits,Very large scale integration design",Integrated circuits is highly relevant because the paper proposes an MISO universal filter circuit. Very large scale integration design (0.3) is moderately relevant due to the CMOS 0.35-μm process technology mentioned. Other categories like 'Power and energy' are not the primary focus.,"3D integrated circuits:0.0,Analog and mixed-signal circuits:1.0,Application-specific VLSI designs:0.75,Design reuse and communication-based design:0.0,Design rules:0.0,Digital switches:0.0,Economics of chip design and manufacturing:0.0,Full-custom circuits:0.0,Interconnect:0.5,Logic circuits:0.0,On-chip resource management:0.0,On-chip sensors:0.0,Reconfigurable logic and FPGAs:0.0,Semiconductor memory:0.0,Standard cell libraries:0.0,VLSI design manufacturing considerations:0.5,VLSI packaging:0.0,VLSI system specification and constraints:0.0","Analog and mixed-signal circuits,Application-specific VLSI designs","Analog and mixed-signal circuits: The paper proposes an electronically tunable MISO universal filter implemented in CMOS, a core analog/mixed-signal circuit design. Application-specific VLSI designs: The design is optimized for low power and component matching, aligning with application-specific VLSI principles. Other options like 3D ICs or logic circuits are irrelevant due to no mention of 3D integration or digital logic focus."
1229,New Approaches for Carbon Nanotubes-Based Biosensors and Their Application to Cell Culture Monitoring,"Amperometric biosensors are complex systems and they require a combination of technologies for their development. The aim of the present work is to propose a new approach in order to develop nanostructured biosensors for the real-time detection of multiple metabolites in cell culture flasks. The fabrication of five Au working electrodes onto silicon substrate is achieved with CMOS compatible microtechnology. Each working electrode presents an area of 0.25 mm2, so structuration with carbon nanotubes and specific functionalization are carried out by using spotting technology, originally developed for microarrays and DNA printing. The electrodes are characterized by cyclic voltammetry and compared with commercially available screen-printed electrodes. Measurements are carried out under flow conditions, so a simple fluidic system is developed to guarantee a continuous flow next to the electrodes. The working electrodes are functionalized with different enzymes and calibrated for the real-time detection of glucose, lactate, and glutamate. Finally, some tests are performed on surnatant conditioned medium sampled from neuroblastoma cells (NG-108 cell line) to detect glucose and lactate concentration after 72 hours of cultivation. The developed biosensor for real-time and online detection of multiple metabolites shows very promising results towards circuits and systems for cell culture monitoring.","General and reference:0.1,Hardware:0.9,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Hardware,Hardware: The paper describes the fabrication and characterization of biosensor hardware components. Other categories are irrelevant as the focus is on physical sensor design.,"Communication hardware, interfaces and storage:0.0,Electronic design automation:0.0,Emerging technologies:1.0,Hardware test:0.0,Hardware validation:0.0,Integrated circuits:0.5,Power and energy:0.0,Printed circuit boards:0.0,Robustness:0.0,Very large scale integration design:0.0",Emerging technologies,"Emerging technologies is highly relevant as the paper presents novel carbon nanotube-based biosensor designs. Integrated circuits receives a moderate score due to the CMOS-compatible fabrication, but the core contribution is in biosensor technology rather than IC design.","Analysis and design of emerging devices and systems:0.8,Biology-related information processing:0.9,Circuit substrates:0.2,Electromechanical systems:0.3,Emerging interfaces:0.2,Emerging optical and photonic technologies:0.1,Memory and dense storage:0.1,Plasmonics:0.1,Quantum technologies:0.1,Reversible logic:0.1,Spintronics and magnetic technologies:0.2","Biology-related information processing,Analysis and design of emerging devices and systems","The paper develops biosensors for cell culture monitoring, aligning with biology-related processing and emerging devices. Other fields are not central to the work."
1881,A 1.1V 12μW 86dB DR Sigma-Delta Modulator for Health Monitoring System,"A low-voltage low-power Sigma-Delta modulator for health monitoring system is designed using a standard 0.18μm CMOS technology. To achieve high accuracy with low power consumption in low supply voltage environment, the designed modulator is implemented with a one-bit third-order topology, in which the input-feedforward structure and switched-opamp (SO) technique are combined. Using the proposed design method, the SO meets the system requirements with the minimal power consumption and good stability in a feedback loop. The modulator achieves 86dB dynamic range (DR) over a 300Hz bandwidth with an oversampling ratio (OSR) of 128, while it occupies 0.16mm2 and consumes 12μW under a 1.1V supply.","General and reference:0.1,Hardware:1.0,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Hardware,The paper presents a low-power hardware design for a Sigma-Delta modulator. Other categories like Networks are not relevant to the core contribution.,"Integrated circuits:1.0,Power and energy:0.75,Hardware validation:0.6,Communication hardware, interfaces and storage:0.1,Electronic design automation:0.1,Emerging technologies:0.1,Hardware test:0.1,Printed circuit boards:0.1,Robustness:0.1,Very large scale integration design:0.1","Integrated circuits,Power and energy",Integrated circuits (1.0) - The paper describes a CMOS-based Sigma-Delta modulator. Power and energy (0.75) - The design emphasizes low-power consumption (12μW) and voltage optimization. Other categories like hardware test or communication hardware are less directly addressed.,"3D integrated circuits:0,Digital switches:0,Energy distribution:1,Energy generation and storage:0,Impact on the environment:0,Interconnect:0,Logic circuits:0,Power estimation and optimization:1,Reconfigurable logic and FPGAs:0,Semiconductor memory:0,Thermal issues:0","Energy distribution,Power estimation and optimization",Energy distribution is relevant for low-power design. Power estimation and optimization are core to achieving the modulator's efficiency goals. Other options like thermal issues are less directly relevant.
4031,High frequency CMOS amplifier with improved linearity,"In this paper, a novel amplifier linearisation technique based on the negative impedance compensation is presented. As demonstrated by using Volterra model, the proposed technique is suitable for linearising amplifiers with low open-loop gain, which is appropriate for RF/microwave applications. A single-chip CMOS amplifier has been designed using the proposed method, and the simulation results show that high gain accuracy (improved by 38%) and high linearity (IMD3 improved by 14 dB, OIP3 improved by 11 dB and adjacent channel power ratio (ACPR) improved by 44% for CDMA signal) can be achieved.","General and reference:0.1,Hardware:0.9,Computer systems organization:0.2,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Hardware,Hardware: The paper presents a CMOS amplifier design with improved linearity for RF/microwave applications. Other categories like Networks are secondary as the focus is on hardware component design.,"Communication hardware, interfaces and storage:0.7,Electronic design automation:0.2,Emerging technologies:0.1,Hardware test:0,Hardware validation:0.3,Integrated circuits:0.9,Power and energy:0.2,Printed circuit boards:0,Robustness:0.4,Very large scale integration design:0.6","Integrated circuits,Communication hardware, interfaces and storage",Integrated circuits is directly relevant as the paper discusses CMOS amplifier design. Communication hardware is relevant due to the RF/microwave application. Other categories like VLSI design are secondary.,"3D integrated circuits:0.1,Digital switches:0.1,Interconnect:0.1,Logic circuits:0.1,Reconfigurable logic and FPGAs:0.1,Semiconductor memory:0.1",,"All options scored low as the paper focuses on RF amplifier linearization techniques, which fall outside the listed semiconductor/memory categories."
2162,"Improving the Calibration of Image Sensors Based on IOFBs, Using Differential Gray-Code Space Encoding","This paper presents a fast calibration method to determine the transfer function for spatial correspondences in image transmission devices with Incoherent Optical Fiber Bundles (IOFBs), by performing a scan of the input, using differential patterns generated from a Gray code (Differential Gray-Code Space Encoding, DGSE). The results demonstrate that this technique provides a noticeable reduction in processing time and better quality of the reconstructed image compared to other, previously employed techniques, such as point or fringe scanning, or even other known space encoding techniques.","General and reference:0.0,Hardware:1.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.5,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.25,Applied computing:0.0,Social and professional topics:0.0",Hardware,"Hardware is highly relevant as the paper focuses on image sensor calibration techniques. Other categories like Mathematics of computing receive a lower score due to secondary algorithmic contributions, while Computing methodologies are marginally relevant for encoding techniques.","Communication hardware, interfaces and storage:1.0,Electronic design automation:0.0,Emerging technologies:0.0,Hardware test:0.5,Hardware validation:0.0,Integrated circuits:0.0,Power and energy:0.0,Printed circuit boards:0.0,Robustness:0.0,Very large scale integration design:0.0","Communication hardware, interfaces and storage","Communication hardware, interfaces and storage: The paper presents a calibration method for image sensors with IOFBs. Hardware test: Calibration involves testing but is secondary to the hardware design focus.",,,No children options are provided for classification.
3749,Polymeric Flexible Immunosensor Based on Piezoresistive Micro-Cantilever with PEDOT/PSS Conductive Layer,"In this paper, a fully polymeric micro-cantilever with the surface passivation layer of parylene-C and the strain resistor of poly(3,4-ethylenedioxythiophene)/poly (styrene sulfonate) (PEDOT/PSS) was proposed and demonstrated for immunoassays. By optimizing the design and fabrication of the polymeric micro-cantilever, a square resistance of 220 Ω/□ for PEDOT/PSS conductive layer have been obtained. The experimental spring constant and the deflection sensitivity were measured to be 0.017 N/m and 8.59 × 10−7 nm−1, respectively. The biological sensing performances of polymeric micro-cantilever were investigated by the immunoassay for human immunoglobulin G (IgG). The immunosensor was experimentally demonstrated to have a linear behavior for the detection of IgG within the concentrations of 10~100 ng/mL with a limit of detection (LOD) of 10 ng/mL. The experimental results indicate that the proposed polymeric flexible conductive layer-based sensors are capable of detecting trace biological substances.","General and reference:0.1,Hardware:0.9,Computer systems organization:0.2,Networks:0.1,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.3,Social and professional topics:0.1",Hardware,Hardware is directly relevant for the design of the polymeric micro-cantilever sensor. Applied computing is marginally relevant for medical applications but not the core contribution.,"Communication hardware, interfaces and storage:0.1,Electronic design automation:0.2,Emerging technologies:0.7,Hardware test:0.1,Hardware validation:0.3,Integrated circuits:0.6,Power and energy:0.2,Printed circuit boards:0.1,Robustness:0.1,Very large scale integration design:0.5","Integrated circuits,Emerging technologies,Very large scale integration design",Integrated circuits (0.6) and Very large scale integration design (0.5): The paper describes micro-cantilever fabrication for biosensors. Emerging technologies (0.7): The polymer-based sensor represents novel materials. Other categories are only peripherally relevant.,"3D integrated circuits:0.1,Analog and mixed-signal circuits:0.2,Analysis and design of emerging devices and systems:0.2,Application-specific VLSI designs:0.3,Biology-related information processing:0.8,Circuit substrates:0.1,Design reuse and communication-based design:0.1,Design rules:0.1,Digital switches:0.1,Economics of chip design and manufacturing:0.1,Electromechanical systems:0.2,Emerging interfaces:0.1,Emerging optical and photonic technologies:0.1,Full-custom circuits:0.1,Interconnect:0.1,Logic circuits:0.1,Memory and dense storage:0.1,On-chip resource management:0.1,On-chip sensors:1,Plasmonics:0.1,Quantum technologies:0.1,Reconfigurable logic and FPGAs:0.1,Reversible logic:0.1,Semiconductor memory:0.1,Spintronics and magnetic technologies:0.1,Standard cell libraries:0.1,VLSI design manufacturing considerations:0.1,VLSI packaging:0.1,VLSI system specification and constraints:0.1","On-chip sensors,Biology-related information processing",On-chip sensors is relevant for the micro-cantilever sensor design. Biology-related information processing applies to the immunoassay application. Other options like 'VLSI design' or 'analog circuits' are less directly aligned.
3825,Low temperature bonding for 3D interconnects,"Advanced methods for low temperature bonding are reviewed and discussed in terms of the concept of the surface activation and the future outlook of their development in 3D integration. As one of the methods, a new attempt for room temperature bonding is introduced, which enables to bond inorganic materials such as Si oxides, glasses and sapphire as well as single crystalline piezoelectric materials. The method is based on the concept of the surface activated bonding (SAB) with certain modification including formation of Fe nano-adhesion layer which accompanied by additional Si intermediate layer formed by a special ion beam source. It was found that wafers of such materials can be bonded very strongly without any heat treatment at room temperature.","General and reference:0.1,Hardware:0.9,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Hardware,"Hardware: The paper presents advanced low-temperature bonding techniques for 3D interconnects, a hardware fabrication topic. No other fields address semiconductor manufacturing directly.","Communication hardware, interfaces and storage:0.0,Electronic design automation:0.8,Emerging technologies:0.7,Hardware test:0.0,Hardware validation:0.0,Integrated circuits:0.6,Power and energy:0.0,Printed circuit boards:0.0,Robustness:0.0,Very large scale integration design:0.6","Electronic design automation,Emerging technologies,Very large scale integration design",Electronic design automation is highly relevant as the paper discusses advanced manufacturing techniques for 3D integration. Emerging technologies is relevant due to the focus on novel bonding methods. Very large scale integration design is relevant because of the application to VLSI systems. Other categories like Integrated circuits are relevant but not as central as the top three.,"3D integrated circuits:1,Analog and mixed-signal circuits:0,Analysis and design of emerging devices and systems:0,Application-specific VLSI designs:0,Biology-related information processing:0,Circuit substrates:0,Design reuse and communication-based design:0,Design rules:0,Economics of chip design and manufacturing:0,Electromechanical systems:0,Emerging interfaces:0,Emerging optical and photonic technologies:0,Full-custom circuits:0,Hardware description languages and compilation:0,High-level and register-transfer level synthesis:0,Logic synthesis:0,Memory and dense storage:0,Methodologies for EDA:0,Modeling and parameter extraction:0,On-chip resource management:0,On-chip sensors:0,Physical design (EDA):0,Plasmonics:0,Quantum technologies:0,Reversible logic:0,Spintronics and magnetic technologies:0,Standard cell libraries:0,Timing analysis:0,VLSI design manufacturing considerations:0.5,VLSI packaging:1,VLSI system specification and constraints:0","3D integrated circuits,VLSI packaging","3D integrated circuits is relevant due to the focus on 3D integration and bonding techniques. VLSI packaging is relevant because the paper discusses bonding methods for inorganic materials in VLSI contexts. Other options are irrelevant as the paper does not discuss economics, analog circuits, or other specific design methodologies."
874,Chopper-Stabilized Bidirectional Current Acquisition Circuits for Electrochemical Amperometric Biosensors,"Two low-noise bidirectional current acquisition circuits for interfacing with electrochemical amperometric biosensor arrays are presented. The first design is a switched-capacitor transimpedance amplifier (TIA). The second design is a current conveyer (CC) with regulated-cascode current mirrors. Both circuits employ chopper stabilization to reduce flicker noise. The TIA and the CC were prototyped in 0.13 μm CMOS and consume 3 μW and 4 μW from a 1.2 V supply, respectively. The electrical and electrochemical recording properties of both circuits have been characterized. The current conveyer exhibits superior performance in low-concentration electrochemical catalytic reporter sensing, as less switching noise is injected into the biosensor.","General and reference:0.1,Hardware:0.9,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Hardware,Hardware (0.9): The paper presents low-noise CMOS circuit designs for biosensors. Other categories like Applied computing are not central to the hardware-focused contributions.,"Communication hardware, interfaces and storage:0.1,Electronic design automation:0.2,Emerging technologies:0.1,Hardware test:0.1,Hardware validation:0.1,Integrated circuits:0.9,Power and energy:0.7,Printed circuit boards:0.1,Robustness:0.3,Very large scale integration design:0.6","Integrated circuits,Power and energy",Integrated circuits is highly relevant for the CMOS-based biosensor interface design. Power and energy is relevant for the low-power optimization focus. Other categories like VLSI design have moderate relevance.,"3D integrated circuits:0.1,Digital switches:0.1,Energy distribution:0.1,Energy generation and storage:0.1,Impact on the environment:0.1,Interconnect:0.1,Logic circuits:0.3,Power estimation and optimization:0.6,Reconfigurable logic and FPGAs:0.1,Semiconductor memory:0.1,Thermal issues:0.1","Power estimation and optimization,Logic circuits",Power estimation and optimization is relevant due to the low-power design focus (3μW/4μW consumption). Logic circuits is relevant because the paper presents novel CMOS circuit designs for biosensors. Other categories have lower scores as the paper focuses on specific circuit implementations rather than general power management strategies.
3977,Development of an ac-dc thermal converter at millivolt level operating at cryogenic temperature,"A device for the measurements of ac-dc transfer difference at millivolt level is under development at IEN. In this device a Cr heater, Nb leads and a Nb transition edge thermometer have been deposited by evaporation on a silicon nitride (SiN) membrane. Analysis on the possible effects on the ac-dc transfer difference and first experimental tests show the possibility of high accuracy direct measurements at the level of 1 mV.","General and reference:0.1,Hardware:1.0,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Hardware,Hardware is highly relevant as the paper describes a physical device for cryogenic measurements. Other categories are irrelevant as the paper focuses on hardware design rather than software or networks.,"Communication hardware, interfaces and storage:0.0,Electronic design automation:0.0,Emerging technologies:0.0,Hardware test:0.0,Hardware validation:0.75,Integrated circuits:0.0,Power and energy:1.0,Printed circuit boards:0.0,Robustness:0.0,Very large scale integration design:0.0","Power and energy,Hardware validation",Power and energy is relevant as the paper discusses a thermal converter for electrical measurements. Hardware validation is relevant for the device testing. Other categories like Integrated circuits are not central to the core contribution.,"Energy distribution:0.1,Energy generation and storage:0.2,Functional verification:0.1,Impact on the environment:0.1,Physical verification:0.1,Post-manufacture validation and debug:0.1,Power estimation and optimization:0.1,Thermal issues:1.0",Thermal issues,"Thermal issues is highly relevant as the paper discusses a thermal converter and thermometer for cryogenic temperature measurements. Other categories like energy generation/storage or power optimization are only tangentially related to the device's energy efficiency, not the core thermal measurement focus."
4965,Functional level embedded self testing for Walsh transform based adaptive hardware,"The paper presents an embedded self test circuit for adaptive systems whose exact specification is unknown. In particular, a functional testing mechanism for systems that have an acceptable representation as polynomials of low order is introduced. The testing mechanism is based on linear-checks and is suitable for Walsh transform based architectures. The paper shows that it is possible to define a small set of linear-checks which does not depend on the actual functionality that the hardware has converged to. Moreover, the check-set can be defined even without knowing the number of input variables nor their precision. In addition, the implementation cost of this testing scheme is negligible in respect to the cost of overall system.","General and reference:0.1,Hardware:1.0,Computer systems organization:0.3,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.2,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Hardware,Hardware is highly relevant as the paper introduces a functional testing mechanism for adaptive hardware systems. Other categories like 'Computer systems organization' or 'Software and its engineering' are less relevant since the focus is on hardware testing rather than system architecture or software design.,"Communication hardware, interfaces and storage:0.0,Electronic design automation:0.0,Emerging technologies:0.0,Hardware test:0.9,Hardware validation:0.8,Integrated circuits:0.0,Power and energy:0.0,Printed circuit boards:0.0,Robustness:0.0,Very large scale integration design:0.0","Hardware test,Hardware validation",Hardware test: The paper introduces a functional self-test circuit for adaptive hardware. Hardware validation: The mechanism ensures hardware correctness through linear-checks. Other categories like Integrated circuits are not directly discussed in the core contribution.,"Analog, mixed-signal and radio frequency test:0,Board- and system-level test:0,Defect-based test:0,Design for testability:1,Fault models and test metrics:0,Functional verification:1,Hardware reliability screening:0,Memory test and repair:0,Physical verification:0,Post-manufacture validation and debug:0,Test-pattern generation and fault simulation:0,Testing with distributed and parallel systems:0","Functional verification,Design for testability",Functional verification: The paper introduces a functional testing mechanism for adaptive hardware. Design for testability: The embedded self-test circuit is a DFT solution. Other categories like memory testing are not discussed.
1521,A reconfigurable distributed architecture for clock generation in large many-core SoC,"This paper focuses on clock generation and distribution in large SoC. After a brief analysis of diverse existed approaches, we propose a distributed architecture based on coupled local clock generators. Three prototypes are presented to demonstrate the feasibility of a large globally synchronous SoC with high reliability by using this approach. Moreover, the reconfigurability feature of this architecture provides a platform for exploring topologies with potentially improved performance.","General and reference:0.1,Hardware:0.9,Computer systems organization:0.3,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Hardware,"Hardware: The paper designs a reconfigurable distributed architecture for clock generation in many-core SoCs, focusing on hardware implementation. Other categories are rejected because the work is about physical circuit design, not software or systems organization.","Communication hardware, interfaces and storage:0.4,Electronic design automation:0.3,Emerging technologies:0.2,Hardware test:0.3,Hardware validation:0.8,Integrated circuits:0.7,Power and energy:0.2,Printed circuit boards:0.1,Robustness:0.6,Very large scale integration design:0.9","Very large scale integration design,Hardware validation",Very large scale integration design is relevant for the clock generation architecture in many-core SoCs. Hardware validation is relevant as the paper discusses reliability and reconfigurability. Other categories like Power and energy are not central to the core contribution.,"3D integrated circuits:0,Analog and mixed-signal circuits:0,Application-specific VLSI designs:1,Design reuse and communication-based design:0.5,Design rules:0,Economics of chip design and manufacturing:0,Full-custom circuits:0,Functional verification:0,On-chip resource management:0.5,On-chip sensors:0,Physical verification:0,Post-manufacture validation and debug:0,Standard cell libraries:0,VLSI design manufacturing considerations:0,VLSI packaging:0,VLSI system specification and constraints:0","Application-specific VLSI designs,On-chip resource management,Design reuse and communication-based design","Application-specific VLSI designs: The paper presents a distributed architecture for clock generation in SoCs, a specific VLSI application. On-chip resource management: The reconfigurable architecture suggests managing on-chip resources. Design reuse and communication-based design: Reconfigurability implies potential for design reuse."
4882,10Gb/s 15mW optical receiver with integrated Germanium photodetector and hybrid inductor peaking in 0.13µm SOI CMOS technology,"As data networks scale to meet increasing bandwidth requirements, the shortcomings of copper channels are becoming more apparent. Dispersion, attenuation, and crosstalk are the main impediments. They can be mitigated with equalization, coding, and shielding, but these techniques carry considerable power, complexity, and cable bulk penalties while offering only modest improvements in reach and limited scalability. Optical communication has been recognized as the successor to copper links; however, high cost of optical transceivers—a result of low degree of integration and reliance on exotic materials and technologies—has limited their proliferation outside of the long-haul systems. This paper presents a receiver developed using a silicon photonics technology platform that enables monolithic optoelectronic device integration in a low-cost CMOS process.","General and reference:0.1,Hardware:1.0,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Hardware,Hardware is highly relevant as the paper presents a silicon photonics-based optical receiver design. Other categories do not address hardware component development.,"Communication hardware, interfaces and storage:0.8,Electronic design automation:0.0,Emerging technologies:0.0,Hardware test:0.0,Hardware validation:0.0,Integrated circuits:1.0,Power and energy:0.3,Printed circuit boards:0.0,Robustness:0.0,Very large scale integration design:0.0","Integrated circuits,Communication hardware, interfaces and storage",Integrated circuits is central as the paper describes a CMOS-based optical receiver. Communication hardware is relevant due to the optical communication application. Power and energy is only tangentially related to low-power design.,"3D integrated circuits:0.3,Digital switches:0.1,Interconnect:0.8,Logic circuits:0.2,Reconfigurable logic and FPGAs:0.1,Semiconductor memory:0.1",Interconnect,"Interconnect: The paper focuses on monolithic integration of optoelectronic devices in CMOS, addressing signal transmission challenges. 3D integrated circuits is less relevant as the focus is on planar monolithic integration."
1144,Functional testing of LSI gate arrays,"This is a report on the testing of LSI gate arrays. A large number of combinational and sequential digital networks were tested using the traditional s-a-1, s-a-0 faul t model. These random logic functions were implemented with LSI gate array devices. This report describes the environment within which these devices were tested and the testing results. Also, some problems encountered in testing these devices and solutions to these problems are summarized.","General and reference:0.1,Hardware:0.8,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Hardware,"Hardware is highly relevant as the paper focuses on testing LSI gate arrays, a hardware component. Other categories like Computer systems organization (0.1) are less relevant because the focus is on hardware testing, not system architecture.","Communication hardware, interfaces and storage:0.3,Electronic design automation:0.4,Emerging technologies:0.1,Hardware test:0.9,Hardware validation:0.7,Integrated circuits:0.65,Power and energy:0.2,Printed circuit boards:0.3,Robustness:0.3,Very large scale integration design:0.5","Hardware test,Integrated circuits",Hardware test is directly relevant for fault model analysis. Integrated circuits are relevant for LSI gate array implementation. Other categories like VLSI design are less specific to the testing focus.,"3D integrated circuits:0.1,Analog, mixed-signal and radio frequency test:0.1,Board- and system-level test:0.1,Defect-based test:0.1,Design for testability:0.7,Digital switches:0.1,Fault models and test metrics:0.9,Hardware reliability screening:0.1,Interconnect:0.1,Logic circuits:0.2,Memory test and repair:0.1,Reconfigurable logic and FPGAs:0.1,Semiconductor memory:0.1,Test-pattern generation and fault simulation:0.3,Testing with distributed and parallel systems:0.1","Fault models and test metrics,Design for testability","Fault models and test metrics: The paper explicitly discusses fault models (s-a-1, s-a-0) for testing LSI gate arrays. Design for testability: The work addresses testing challenges and solutions for digital circuits. Other children like Reconfigurable logic are not relevant to the traditional fault models discussed."
926,Microfiber Resonator in Polymer Matrix,We propose a simple technique to form miniature optical circuits using microfibers embedded into a low refractive index matrix. As an example we demonstrate a silica microfiber knot resonator embedded in a fluoroacrylate polymer. Fabrication issues and initial experimental results are reported. We also present simulations aimed at understanding the current limitations to the Q-factor and the role of the embedding polymer refractive index on the Q-factor of future resonators. It is anticipated that using commercially available polymers high Q-factor resonators with radii as small as 100 micrometers can be made by this technique.,"General and reference:0.0,Hardware:1.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Hardware,Hardware is directly relevant as the paper discusses fabrication of optical components. Other categories are irrelevant as the focus is on physical device design.,"Communication hardware, interfaces and storage:0.5,Electronic design automation:0.75,Emerging technologies:0.75,Hardware test:0.25,Hardware validation:0.25,Integrated circuits:1.0,Power and energy:0.0,Printed circuit boards:0.5,Robustness:0.0,Very large scale integration design:0.5","Integrated circuits,Electronic design automation,Emerging technologies",Integrated circuits: The paper discusses optical circuits using microfiber resonators. Electronic design automation: Fabrication and simulation techniques are relevant. Emerging technologies: The method is novel and uses commercial polymers. Other categories like Communication hardware are less relevant as the focus is on circuit design.,"3D integrated circuits:0,Analysis and design of emerging devices and systems:0.5,Biology-related information processing:0,Circuit substrates:0.5,Digital switches:0,Electromechanical systems:0.5,Emerging interfaces:0,Emerging optical and photonic technologies:1,Hardware description languages and compilation:0,High-level and register-transfer level synthesis:0,Interconnect:0,Logic circuits:0,Logic synthesis:0,Memory and dense storage:0,Methodologies for EDA:0,Modeling and parameter extraction:0.5,Physical design (EDA):0,Plasmonics:0,Quantum technologies:0,Reconfigurable logic and FPGAs:0,Reversible logic:0,Semiconductor memory:0,Spintronics and magnetic technologies:0,Timing analysis:0",Emerging optical and photonic technologies,Emerging optical technologies is relevant for microfiber resonator development. Other categories like Circuit substrates are less central to the optical component focus.
2858,"The design of a high speed ASIC unit for the hash function SHA-256 (384, 512)","After recalling the basic algorithms published by NIST for implementing the hash functions SHA-256 (384, 512), a basic circuit characterized by a cascade of full adder arrays is given. Implementation options are discussed and two methods for improving speed are exposed: the delay balancing and the pipelining. An application of the former is first given, obtaining a circuit that reduces the length of the critical path by a full adder array. A pipelined version is then given, obtaining a reduction of two full adder arrays in the critical path. The two methods are afterwards combined and the results obtained through hardware synthesis are exposed, where a comparison between the new circuits is also given.","General and reference:0.1,Hardware:0.9,Computer systems organization:0.2,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.2,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Hardware,Hardware is highly relevant because the paper focuses on ASIC design and hardware optimization techniques for SHA-256. Other options like Security and Privacy are only tangentially related to cryptographic functions.,"Communication hardware, interfaces and storage:0.2,Electronic design automation:0.6,Emerging technologies:0.1,Hardware test:0.1,Hardware validation:0.3,Integrated circuits:0.9,Power and energy:0.4,Printed circuit boards:0.1,Robustness:0.1,Very large scale integration design:0.9","Integrated circuits,Very large scale integration design,Electronic design automation",Integrated circuits (0.9) as the paper is about ASIC design for SHA-256. Very large scale integration design (0.9) because the focus is on VLSI implementation techniques. Electronic design automation (0.6) is relevant for the hardware synthesis methods discussed. Other options like communication hardware or printed circuit boards are not central to the core contribution.,"Application-specific VLSI designs:1.0,High-level and register-transfer level synthesis:0.9,Timing analysis:0.7,Design rules:0.3,Interconnect:0.4,Logic synthesis:0.5,Physical design (EDA):0.4,VLSI design manufacturing considerations:0.2,3D integrated circuits:0.1,Analog and mixed-signal circuits:0.1","Application-specific VLSI designs,High-level and register-transfer level synthesis",Application-specific VLSI designs is highly relevant as the paper focuses on ASIC design for SHA-256. High-level synthesis is relevant for pipelining and delay balancing techniques. Other categories like Timing analysis are secondary as the paper emphasizes architectural optimizations rather than detailed timing constraints.
4593,Approaching Speed-of-light Distortionless Communication for On-chip Interconnect,"We extend the surfliner on-chip distortionless transmission line scheme and provide more details for the implementation issues. Surfliner seeks to approach distortionless transmission by intentionally adding shunt resistors between the signal line and the ground. In theory if we distributively make the shunt conductance G=RC/L, there is no distortion at the receiver end and the signal propagates at the speed of light. We show the feasibility and advantages of this shunt resistor scheme by a real design case of single-ended microstrip line in 0.10mum technology. The simulation results indicate we can achieve near perfect signaling of 10 Gbps data over a 10 mm serial link, yet no pre-emphasis/equalization or other special techniques are needed. Guidelines for determining the optimal value and spacing of the shunt resistors are also provided.","General and reference:0.1,Hardware:1.0,Computer systems organization:0.2,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Hardware,Hardware is relevant because the paper focuses on on-chip interconnect design and physical implementation of distortionless transmission lines. Other categories like Networks or Computer systems organization are less relevant as the core contribution is hardware-level circuit design.,"Communication hardware, interfaces and storage:0.9,Electronic design automation:0.1,Emerging technologies:0.1,Hardware test:0.1,Hardware validation:0.1,Integrated circuits:0.8,Power and energy:0.5,Printed circuit boards:0.1,Robustness:0.1,Very large scale integration design:0.3","Communication hardware, interfaces and storage,Integrated circuits","Communication hardware, interfaces and storage: Directly addresses on-chip communication design. Integrated circuits: Implementation in 0.10µm technology is core. Other options like 'Power and energy' are secondary to the primary focus on distortionless transmission.","3D integrated circuits:0,Digital switches:0,Interconnect:1,Logic circuits:0.2,Reconfigurable logic and FPGAs:0,Semiconductor memory:0",Interconnect,Interconnect: The paper focuses on on-chip interconnect design for distortionless communication. Other options like 3D ICs or Semiconductor memory are not discussed in the context of this transmission line research.
1212,7.5 A 3.3ns-access-time 71.2μW/MHz 1Mb embedded STT-MRAM using physically eliminated read-disturb scheme and normally-off memory architecture,"Nonvolatile memory, spin-transfer torque magnetoresistive RAM (STT-MRAM) is being developed to realize nonvolatile working memory because it provides high-speed accesses, high endurance, and CMOS-logic compatibility. Furthermore, programming current has been reduced drastically by developing the advanced perpendicular STT-MRAM [1]. Several-megabit STT-MRAM with sub-5ns operation is demonstrated in [2]. Advanced perpendicular STT-MRAM achieve ~3× power saving by reducing leakage current in memory cells compared with SRAM for last level cache (LLC) [3]. Such high-speed RAM applications, however, entail several issues: the probability of read disturbance error increases and the active power of STT-MRAM must be decreased for higher access speed. Moreover, the leakage power of peripheral circuits must be decreased, because the high-speed RAM requires high-performance transistors having high leakage current in peripheral circuitry [4], limiting the energy efficiency of STT-MRAM. To resolve these issues, this paper presents STT-MRAM circuit designs: a short read-pulse generator with small overhead using hierarchical bitline for eliminating read disturbance, a charge-optimization scheme to avoid excessive active charging/discharging power, and ultra-fast power gating and power-on adaptive to RAM status for reducing leakage power.","General and reference:0.1,Hardware:1.0,Computer systems organization:0.4,Networks:0.1,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.3,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.3,Applied computing:0.2,Social and professional topics:0.1",Hardware,Hardware is highly relevant as the paper describes STT-MRAM circuit design innovations. Other categories like Networks or Software are not applicable to the memory architecture focus.,"Communication hardware, interfaces and storage:0.1,Electronic design automation:0.4,Emerging technologies:0.6,Hardware test:0.2,Hardware validation:0.3,Integrated circuits:0.9,Power and energy:0.7,Printed circuit boards:0.1,Robustness:0.3,Very large scale integration design:0.6","Integrated circuits,Power and energy",Integrated circuits is highly relevant as the paper describes STT-MRAM design and optimization. Power and energy is relevant due to the focus on leakage power reduction. Emerging technologies is moderately relevant as STT-MRAM is a developing field.,"3D integrated circuits:0.1,Digital switches:0.2,Energy distribution:0.1,Energy generation and storage:0.1,Impact on the environment:0.1,Interconnect:0.1,Logic circuits:0.2,Power estimation and optimization:1.0,Reconfigurable logic and FPGAs:0.1,Semiconductor memory:0.8,Thermal issues:0.1","Semiconductor memory,Power estimation and optimization",Semiconductor memory is highly relevant as the paper focuses on STT-MRAM design. Power estimation and optimization is relevant due to the emphasis on reducing power consumption. Other categories are less relevant as the paper does not discuss thermal issues or 3D circuits.
5944,A novel robust signaling scheme for high-speed low-power communication over long wires,This paper describes a new capacitively coupled driver and a receiver with a new analog equalizer for high-speed low-power communication over long on-chip wires. The proposed signaling scheme improves upon state of the art capacitively driven interconnect based scheme for enhanced robustness and energy efficiency of the signaling scheme. The proposed signaling scheme has been designed in 90nm CMOS process. Simulations indicate that the proposed scheme can transmit and receive data at the rate of 3.22Gbps over a 10mm long channel while consuming only 0.107pJ/bit. This is the lowest reported energy/bit for high-speed on-chip communication over long on-chip wires. Monte Carlo and process corner simulations show that the proposed scheme allows up to 3Gbps of data rate even in the presence of intra-die and inter-die process variations.,"General and reference:0.1,Hardware:0.8,Computer systems organization:0.3,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Hardware,Hardware is highly relevant as the paper presents a novel on-chip signaling scheme designed in CMOS for low-power communication. Computer systems organization is less relevant because the focus is on physical-layer circuit design rather than system-level architecture.,"Communication hardware, interfaces and storage:0.85,Electronic design automation:0.1,Emerging technologies:0.1,Hardware test:0.1,Hardware validation:0.1,Integrated circuits:0.75,Power and energy:0.6,Printed circuit boards:0.1,Robustness:0.5,Very large scale integration design:0.3","Communication hardware, interfaces and storage,Integrated circuits","Communication hardware is directly relevant for the on-chip signaling scheme design. Integrated circuits are relevant due to the 90nm CMOS implementation. Power and energy receive partial relevance due to energy efficiency focus, but lower scores than core hardware design categories.","3D integrated circuits:0.1,Digital switches:0.1,Interconnect:1.0,Logic circuits:0.1,Reconfigurable logic and FPGAs:0.1,Semiconductor memory:0.3",Interconnect,"Interconnect is directly relevant as the paper focuses on on-chip communication over long wires. Semiconductor memory is less relevant as the work is about signaling schemes, not memory storage. Other categories like 3D integrated circuits are not discussed."
3074,Pulse broadening in Photonic Crystal Fibers due to self phase modulation,"Fiber nonlinearities are more enhanced in Photonic Crystal Fibers (PCFs) due to their extremely small Mode Field Diameters (MFDs). One of the outcomes of these nonlinearities is self phase modulation (SPM). SPM makes a Gaussian pulse propagating in a nonlinear, non dispersive medium to undergo frequency chirping without changing its temporal envelope. This generates additional frequency components in the pulse causing spectral broadening. Such a phenomenon can be further explored in designing broad band coherent light sources to be used in wavelength division multiplexing systems. A quick method to estimate the amount of spectral broadening in PCFs is presented.","General and reference:0.1,Hardware:0.8,Computer systems organization:0.1,Networks:0.2,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Hardware,Hardware is highly relevant as the paper discusses photonic crystal fiber design and nonlinear optical effects. Other categories like 'Networks' are less relevant as the focus is on physical fiber properties rather than network communication systems.,"Communication hardware, interfaces and storage:0.9,Electronic design automation:0.1,Emerging technologies:0.6,Hardware test:0.0,Hardware validation:0.0,Integrated circuits:0.7,Power and energy:0.0,Printed circuit boards:0.0,Robustness:0.0,Very large scale integration design:0.1","Communication hardware, interfaces and storage,Integrated circuits,Emerging technologies","Communication hardware, interfaces and storage: Discusses nonlinear effects in photonic crystal fibers for optical communication. Integrated circuits: PCF is a photonic integrated component. Emerging technologies: Addresses advanced optical fiber design. Other categories like VLSI design are less relevant as the focus is on fiber rather than chip-level design.","3D integrated circuits:0.1,Analysis and design of emerging devices and systems:0.8,Biology-related information processing:0.1,Circuit substrates:0.2,Digital switches:0.1,Electromechanical systems:0.1,Emerging interfaces:0.1,Emerging optical and photonic technologies:0.9,Interconnect:0.2,Logic circuits:0.1,Memory and dense storage:0.1,Plasmonics:0.1,Quantum technologies:0.2,Reconfigurable logic and FPGAs:0.1,Reversible logic:0.1,Semiconductor memory:0.1,Spintronics and magnetic technologies:0.1",Emerging optical and photonic technologies,Emerging optical and photonic technologies is highly relevant as the paper focuses on nonlinear effects in photonic crystal fibers. Other options like Circuit substrates are less relevant to the core contribution.
806,Design and application of electronically programmable LSI arrays,"It has been shown that arrays of calculating elements, providing various functions of their input variables, can be used as general-purpose signal processors. Such elements could, in general, operate on binary, analog, or numerical inputs. Networks or arrays composed of elements in each of these domains have advantages in particular applications. The numerical processing element and its use in programmable arrays is the subject of this paper.","General and reference:0.1,Hardware:1.0,Computer systems organization:0.5,Networks:0.2,Software and its engineering:0.3,Theory of computation:0.2,Mathematics of computing:0.2,Information systems:0.2,Security and privacy:0.2,Human-centered computing:0.2,Computing methodologies:0.2,Applied computing:0.2,Social and professional topics:0.2",Hardware,Hardware is highly relevant as the paper discusses the design of programmable LSI arrays. Other categories like Computer Systems Organization are secondary to the hardware-centric contribution.,"Communication hardware, interfaces and storage:0.1,Electronic design automation:0.1,Emerging technologies:0.2,Hardware test:0.1,Hardware validation:0.8,Integrated circuits:0.1,Power and energy:0.1,Printed circuit boards:0.1,Robustness:0.1,Very large scale integration design:0.9","Hardware validation,Very large scale integration design",Hardware validation: The paper discusses verification of programmable hardware arrays. Very large scale integration design: The research focuses on LSI arrays as a specific hardware design approach. Other categories like Communication hardware or Power and energy are less relevant as the paper is focused on general-purpose signal processing arrays.,"3D integrated circuits:0,Analog and mixed-signal circuits:0,Application-specific VLSI designs:1,Design reuse and communication-based design:0,Design rules:0,Economics of chip design and manufacturing:0,Full-custom circuits:0,Functional verification:0,On-chip resource management:0,On-chip sensors:0,Physical verification:0,Post-manufacture validation and debug:0,Standard cell libraries:0,VLSI design manufacturing considerations:0,VLSI packaging:0,VLSI system specification and constraints:1","Application-specific VLSI designs,VLSI system specification and constraints","Application-specific VLSI designs: The paper discusses programmable LSI arrays, a core aspect of application-specific design. VLSI system specification and constraints: The focus on numerical processing elements and their system-level application aligns with this category. Other options are irrelevant as the paper does not address analog circuits, economics, or packaging."
4302,Evaluating the impact of customized instruction set on coarse grained reconfigurable arrays,"Customizing the instruction set for particular applications has become a successful practice in the industry in the design of application specific processors. Following the same principle, this paper evaluates the impact of embedding specialized instructions within the processing elements (PEs) of coarse grained reconfigurable arrays (CGRAs). We systematically extract and select regular clusters of instructions from the data flow graph of applications. We then embed the selected clusters as specialized instructions within some PEs. We argue that these customized units concentrates the execution of recurrently emerging groups of instructions within a fewer number of PEs. That leads to a more efficient usage of architectural resources, such as control and functional units. Our results demonstrate that arrays with specialized FUs within PEs executes more instructions per area unit and reduce power consumption as compared to architectures where the PEs only contain basic operations. Moreover, we preserve the flexibility of the architecture within the targeted set of applications, which is an important aspect for reconfigurable designs. We show that the adoption of custom instructions in CGRAs retains the typical high instruction parallelism of these architectures with a reduced hardware cost.","General and reference:0.1,Hardware:0.95,Computer systems organization:0.3,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Hardware,Hardware is highly relevant as the paper evaluates customized instruction sets in reconfigurable hardware. Other categories are not relevant as the focus is on hardware architecture rather than software or applications.,"Communication hardware, interfaces and storage:0.1,Electronic design automation:0.75,Emerging technologies:0.2,Hardware test:0.1,Hardware validation:0.3,Integrated circuits:0.4,Power and energy:0.3,Printed circuit boards:0.1,Robustness:0.2,Very large scale integration design:1.0",Very large scale integration design,"Very large scale integration design is directly relevant as the paper discusses customized instruction sets in CGRAs, a key aspect of VLSI design. Electronic design automation (0.75) is secondary as the methodology involves design optimization. Other fields like Power and energy (0.3) are less relevant as the primary focus is on architectural efficiency.","3D integrated circuits:0,Analog and mixed-signal circuits:0,Application-specific VLSI designs:1,Design reuse and communication-based design:0,Design rules:0,Economics of chip design and manufacturing:0,Full-custom circuits:0,On-chip resource management:1,On-chip sensors:0,Standard cell libraries:0,VLSI design manufacturing considerations:0,VLSI packaging:0,VLSI system specification and constraints:0","Application-specific VLSI designs,On-chip resource management","Application-specific VLSI designs: The paper evaluates customized instruction sets for CGRAs, a key aspect of application-specific design. On-chip resource management: The study focuses on optimizing resource usage (control and functional units) in PEs. Other categories are irrelevant as the paper does not discuss analog circuits or manufacturing economics."
4374,Resource sharing of pipelined custom hardware extension for energy-efficient application-specific instruction set processor design,"Application-Specific Instruction set Processor (ASIP) has become an increasingly popular platform for embedded systems because of its high performance and flexibility. Energy efficiency is critical for portable and embedded devices, and should be addressed separately from performance consideration. The hardware extension in ASIPs can speed-up program execution, but also incurs area overhead and static energy consumption of the processors. Traditional data path merging techniques reduce circuit overhead by reusing hardware resources for executing multiple custom instructions. However, they introduce structural hazard for custom instructions on extended processors, and hence reduce the performance improvement. In this paper, we introduce a pipelined configurable hardware structure for the hardware extension in ASIPs, so that structural hazards can be remedied. With multiple subgraphs of operations selected for custom hardware realization, we devise a novel operation-to-hardware mapping algorithm based on Integer Linear Programming (ILP) to automatically construct a resource-efficient pipelined configurable hardware extension. We demonstrate that different resource sharing schemes would affect both the hardware overhead and datapath delay of the custom instructions. We analyze the design tradeoffs between resource efficiency and performance improvement, and present the design space exploration results.","General and reference:0.1,Hardware:0.9,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Hardware,Hardware is relevant for ASIP design and resource sharing. Other categories lack connection to hardware architecture optimization.,"Communication hardware, interfaces and storage:0.1,Electronic design automation:0.1,Emerging technologies:0.1,Hardware test:0.1,Hardware validation:0.1,Integrated circuits:0.1,Power and energy:0.9,Printed circuit boards:0.1,Robustness:0.1,Very large scale integration design:0.85","Power and energy,Very large scale integration design",Power and energy is directly relevant as the paper focuses on energy-efficient ASIP design. Very large scale integration design is relevant for the hardware extension techniques. Other options like Integrated circuits are less specific to the core contribution.,"3D integrated circuits:0.1,Analog and mixed-signal circuits:0.1,Application-specific VLSI designs:1,Design reuse and communication-based design:0.3,Design rules:0.1,Economics of chip design and manufacturing:0.2,Energy distribution:0.1,Energy generation and storage:0.1,Full-custom circuits:0.3,Impact on the environment:0.1,On-chip resource management:1,On-chip sensors:0.1,Power estimation and optimization:0.7,Standard cell libraries:0.1,Thermal issues:0.2,VLSI design manufacturing considerations:0.2,VLSI packaging:0.1,VLSI system specification and constraints:0.3","Application-specific VLSI designs,On-chip resource management",Application-specific VLSI designs: The paper focuses on ASIP design with hardware extensions. On-chip resource management: The paper discusses resource sharing and pipelining for energy efficiency. Power estimation and optimization is secondary as the primary focus is on resource management rather than power analysis.
5661,Discriminative Reordering Models for Statistical Machine Translation,"In large size high resolution plasma panels, a smaller discharge gap between the electrodes reduces the operating margin of the panel, i.e., the difference between the maximum and minimum sustain voltage (Vsmax and Vsmin). Rather than preset these values which vary with each panel at the time of fabrication, the invention provides a control system for determining the Vsmax and Vsmin and the optimum operating point between these values for each panel. An associated microprocessor determines the Vsmin for each panel through a testing algorithm, and the Vsmax is provided by either adding a predetermined increment to the Vsmin or by a testing sequence. The operating point is designated as a predetermined increment below Vsmax. The invention operates each time the panel is turned on, thereby compensating for voltage drift or other panel parameter variations. By using a high speed microprocessor, the entire sequence is accomplished in a short time, while exercising the cells through the test sequence eliminates some of the start-up problems sometimes associated with such displays.""","General and reference:0.1,Hardware:0.7,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.2,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.5,Applied computing:0.3,Social and professional topics:0.1",Hardware,Hardware is highly relevant for the plasma panel control system design. Other categories like Computing methodologies are less relevant as the focus is on hardware implementation rather than algorithms.,"Power and energy:1,Hardware validation:0.9,Hardware test:0.8,Communication hardware, interfaces and storage:0.1,Electronic design automation:0.05,Emerging technologies:0.05,Integrated circuits:0.05,Printed circuit boards:0.05,Robustness:0.05,Very large scale integration design:0.05","Power and energy,Hardware validation,Hardware test","Power and energy: Focuses on voltage optimization for panels. Hardware validation: Testing algorithms for panel performance. Hardware test: Describes testing sequences. Other children like Communication hardware, EDA, etc., are not relevant as the paper does not discuss communication or VLSI design.","Analog, mixed-signal and radio frequency test:0.1,Board- and system-level test:0.1,Defect-based test:0.1,Design for testability:0.1,Energy distribution:0.1,Energy generation and storage:0.1,Fault models and test metrics:0.1,Functional verification:0.8,Hardware reliability screening:0.7,Impact on the environment:0.1,Memory test and repair:0.1,Physical verification:0.1,Post-manufacture validation and debug:0.1,Power estimation and optimization:0.1,Test-pattern generation and fault simulation:0.8,Testing with distributed and parallel systems:0.1,Thermal issues:0.1","Functional verification,Test-pattern generation and fault simulation",Functional verification is relevant for the control system's design to determine optimal operating parameters. Test-pattern generation and fault simulation are relevant as the system uses testing algorithms to validate panel performance. Other categories like Power estimation are less directly addressed.
139,Interleaved imaging: an imaging system design inspired by rod-cone vision,"Under low illumination conditions, such as moonlight, there simply are not enough photons present to create a high quality color image with integration times that avoid camera-shake. Consequently, conventional imagers are designed for daylight conditions and modeled on human cone vision. Here, we propose a novel sensor design that parallels the human retina and extends sensor performance to span daylight and moonlight conditions. Specifically, we describe an interleaved imaging architecture comprising two collections of pixels. One set of pixels is monochromatic and high sensitivity; a second, interleaved set of pixels is trichromatic and lower sensitivity. The sensor implementation requires new image processing techniques that allow for graceful transitions between different operating conditions. We describe these techniques and simulate the performance of this sensor under a range of conditions. We show that the proposed system is capable of producing high quality images spanning photopic, mesopic and near scotopic conditions.","General and reference:0.1,Hardware:0.8,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Hardware,"Hardware: The paper presents a novel sensor design for imaging systems that mimics human rod-cone vision. Other fields: The paper doesn't focus on network communication, theory of computation, software engineering, information systems, etc.","Integrated circuits:0.8,Communication hardware, interfaces and storage:0.7,Emerging technologies:0.6,Hardware validation:0.4,Power and energy:0.3","Integrated circuits,Communication hardware, interfaces and storage",Integrated circuits is relevant for the novel sensor design. Communication hardware is relevant for pixel array implementation. Emerging technologies receives moderate relevance for retinal-inspired design. Hardware validation is less relevant as the paper focuses on design rather than testing.,"3D integrated circuits:0.6,Digital switches:0.1,Interconnect:0.7,Logic circuits:0.2,Reconfigurable logic and FPGAs:0.3,Semiconductor memory:0.1","3D integrated circuits,Interconnect",3D integrated circuits: The paper describes a novel sensor design with interleaved pixel arrays. Interconnect: The system requires new image processing techniques for sensor implementation. Other categories like logic circuits are less relevant as the focus is on sensor architecture rather than logic design.
1385,A Sub-1ppm/°C high-order curvature-compensated bandgap reference,"Based on the principle of piecewise compensation, a modified high-order compensation technique with adaptive feedback control scheme for bandgap reference is proposed. This method focuses on forming multiple local extrema of the curve of reference voltage instead of single valley or peak in the entire operating temperature range, which significantly improves the temperature independence. The circuitry is simulated in CSMC 0.6 mum CMOS process, and it has a temperature coefficient in the -55degC to 105degC range of 0.46ppm/degC (average) with power supply current lower than 0.2 muA.","General and reference:0.0,Hardware:1.0,Computer systems organization:0.25,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Hardware,Hardware is highly relevant because the paper presents a novel hardware design for a bandgap reference circuit. Other categories like Computer systems organization are less directly relevant as the focus is on low-level circuit design.,"Communication hardware, interfaces and storage:0.1,Electronic design automation:0.05,Emerging technologies:0.05,Hardware test:0.05,Hardware validation:0.05,Integrated circuits:1.0,Power and energy:0.75,Printed circuit boards:0.1,Robustness:0.15,Very large scale integration design:0.2","Integrated circuits,Power and energy",Integrated circuits is directly relevant to the bandgap reference design. Power and energy is relevant due to the low power supply current. Other categories like Very large scale integration design are less specific to the core contribution.,"Thermal issues:1,Power estimation and optimization:0.7,3D integrated circuits:0.1","Thermal issues,Power estimation and optimization",Thermal issues are primary due to temperature compensation in the circuit. Power estimation is relevant as low power consumption is highlighted. Other categories like 3D circuits are irrelevant to the design.
3175,Ring oscillators for functional and delay test of latches and flip-flops,"This work proposes a novel design strategy to use ring oscillators for functional validation and delay test of storage elements (latches and flip-flops). Besides the logic verification, power consumption and aging effect analysis can also be efficiently performed for the circuits under test. The proposed test solutions are also quite suitable for self-timed and self-checking full verification of latches and flip-flops, as well as for comparing different implementations of such sequential cells. This test approach has been validated at the gate level by using HDL descriptions as well as at the transistor level through electrical simulations.","General and reference:0.1,Hardware:0.9,Computer systems organization:0.2,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Hardware,Hardware is directly relevant as the paper focuses on ring oscillator design for hardware testing. Other categories like Computer systems organization are less relevant as the focus is on hardware-level verification.,"Communication hardware, interfaces and storage:0.1,Electronic design automation:0.1,Emerging technologies:0.1,Hardware test:0.9,Hardware validation:0.9,Integrated circuits:0.1,Power and energy:0.1,Printed circuit boards:0.1,Robustness:0.1,Very large scale integration design:0.1","Hardware test,Hardware validation","Hardware test is relevant because the paper discusses using ring oscillators for testing circuits. Hardware validation is relevant because it involves verifying circuit functionality. Other fields are irrelevant as the paper doesn't focus on communication hardware, emerging technologies, or other areas.","Analog, mixed-signal and radio frequency test:1,Board- and system-level test:0,Defect-based test:0,Design for testability:1,Fault models and test metrics:0,Functional verification:0,Hardware reliability screening:0,Memory test and repair:0,Physical verification:0,Post-manufacture validation and debug:0,Test-pattern generation and fault simulation:0,Testing with distributed and parallel systems:0","Analog, mixed-signal and radio frequency test,Design for testability","Analog, mixed-signal and radio frequency test is relevant because ring oscillators are analog circuits used for testing. Design for testability is relevant as the paper proposes a test methodology for storage elements. Other categories are irrelevant as the focus is on circuit-level testing, not higher-level system testing or fault models."
4273,Summary of Envelope Modulator designs by the University of Oviedo,Envelope Tracking (ET) and Envelope Elimination and Restoration (EER) are techniques which were designed to increase the efficiency of Radio Frequency Power Amplifiers (RF PA) while maintaining a strong linearity of the system. Both techniques employ a special type of DC/DC converter called Envelope Modulator or Envelope Amplifier which has attracted the interest of Spanish power electronics and communications groups since the end of the last decade. This paper summarizes the designs proposed by the Power Supply Systems Group from the University of Oviedo since 2007.,"General and reference:0.1,Hardware:0.9,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Hardware,Hardware is highly relevant as the paper focuses on envelope modulator design for RF power amplifiers. Other categories are irrelevant since the paper discusses hardware design rather than software or networking.,"Communication hardware, interfaces and storage:0.8,Electronic design automation:0.0,Emerging technologies:0.0,Hardware test:0.0,Hardware validation:0.0,Integrated circuits:0.0,Power and energy:0.9,Printed circuit boards:0.0,Robustness:0.0,Very large scale integration design:0.0","Communication hardware, interfaces and storage,Power and energy",Communication hardware is relevant for the design of envelope modulators in RF systems. Power and energy is relevant as the work focuses on power amplifier efficiency. Other categories like VLSI design are not central to the paper.,"Energy distribution:0.3,Energy generation and storage:0.2,Impact on the environment:0.1,Power estimation and optimization:1.0,Thermal issues:0.4",Power estimation and optimization,Power estimation and optimization is highly relevant as the paper focuses on improving RF PA efficiency through envelope modulator designs. Other categories like Energy distribution and Thermal issues are less directly related.
4655,A High-Sensitivity Tunable Two-Beam Fiber-Coupled High-Density Magnetometer with Laser Heating,"Atomic magnetometers (AM) are finding many applications in biomagnetism, national security, industry, and science. Fiber-coupled (FC) designs promise to make them compact and flexible for operation. Most FC designs are based on a single-beam configuration or electrical heating. Here, we demonstrate a two-beam FC AM with laser heating that has 5 fT/Hz1/2 sensitivity at low frequency (50 Hz), which is higher than that of other fiber-coupled magnetometers and can be improved to the sub-femtotesla level. This magnetometer is widely tunable from DC to very high frequencies (as high as 100 MHz; the only issue might be the application of a suitable uniform and stable bias field) with a sensitivity under 10 fT/Hz1/2 and can be used for magneto-encephalography (MEG), magneto-cardiography (MCG), underground communication, ultra-low MRI/NMR, NQR detection, and other applications.","General and reference:0.0,Hardware:0.9,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.2,Social and professional topics:0.0",Hardware,"Hardware (0.9): Describes a physical magnetometer device with fiber-coupled and laser heating components. Applied computing (0.2): Limited relevance due to potential applications in MEG/MCG. Other categories are irrelevant as the paper focuses on hardware design rather than software, networks, or computing methodologies.","Communication hardware, interfaces and storage:0.8,Emerging technologies:0.75,Electronic design automation:0.3,Hardware test:0.2,Hardware validation:0.2,Integrated circuits:0.1,Power and energy:0.1,Printed circuit boards:0.1,Very large scale integration design:0.1","Communication hardware, interfaces and storage,Emerging technologies","Communication hardware, interfaces and storage is relevant due to the focus on fiber-coupled magnetometer design. Emerging technologies is relevant as the laser-heated two-beam configuration represents a novel advancement. Other categories like Integrated circuits and Power and energy are not central to the paper's contributions.","Analysis and design of emerging devices and systems:0.3,Biology-related information processing:0.2,Circuit substrates:0.1,Electromechanical systems:0.1,Emerging interfaces:0.1,Emerging optical and photonic technologies:1.0,Memory and dense storage:0.1,Plasmonics:0.1,Quantum technologies:0.4,Reversible logic:0.1,Spintronics and magnetic technologies:0.2",Emerging optical and photonic technologies,Emerging optical and photonic technologies is highly relevant due to the fiber-coupled magnetometer with laser heating. Quantum technologies is secondary but not as central.
1124,Cheap hardware support for software debugging and profiling,"We wish to determine the effectiveness of some simple hardware for debugging and profiling compiled programs on a conventional processor. The hardware cost is small -- a counter decremented on each instruction that raises an exception when its value becomes zero. With the counter a debugger can provide data watchpoints and reverse execution: a profiler can measure the total instruction cost of a code segment and sample the program counter accurately. Such a counter has been included on a single-board MC68020 workstation, for which system software is currently being written. We will report our progress at the symposium.","General and reference:0.0,Hardware:1.0,Computer systems organization:0.7,Networks:0.0,Software and its engineering:0.3,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.2,Applied computing:0.0,Social and professional topics:0.0",Hardware,"Hardware (1.0): The paper introduces a low-cost hardware counter for debugging and profiling. Computer systems organization (0.7): The counter interacts with system software. Software and its engineering (0.3): Debugging tools are part of software engineering. Others: None focus on algorithms, theory, or application domains.","Communication hardware, interfaces and storage:0,Electronic design automation:0,Emerging technologies:0,Hardware test:1,Hardware validation:1,Integrated circuits:0,Power and energy:0,Printed circuit boards:0,Robustness:0,Very large scale integration design:0","Hardware test,Hardware validation","The paper evaluates hardware counter effectiveness for debugging and profiling, directly addressing hardware testing and validation. Other options like 'Power and energy' are unrelated to the core contribution.","Analog, mixed-signal and radio frequency test:0.1,Board- and system-level test:0.8,Defect-based test:0.2,Design for testability:0.7,Fault models and test metrics:0.2,Functional verification:0.3,Hardware reliability screening:0.1,Memory test and repair:0.2,Physical verification:0.1,Post-manufacture validation and debug:0.4,Test-pattern generation and fault simulation:0.3,Testing with distributed and parallel systems:0.1","Board- and system-level test,Design for testability",Board- and system-level test is relevant for the hardware counter implementation. Design for testability is relevant as the design aims to improve debugging/profiling. Other categories are less directly connected.
5695,A 15 mW 69 dB 2 Gsamples/s CMOS analog front-end for low-band UWB applications,"The paper presents a CMOS analog front-end (AFE) for a short range impulse radio system in low-band ultrawideband (UWB) based on 2 ns width PAM. The wideband amplifier provides 69 dB gain from 30 MHz to 1.2 GHz, realized by a limiting amplifier with an active-inductor load and a cross-coupled g/sub m/ gain-boosting cell in each gain stage. The received signal is digitized at 2 GHz sampling rate by 8 parallel 1-bit ADCs, each of which is composed of transmission-gate type sample-and-hold circuits and a dynamic differential comparator. A timing generator based on a 4-stage ring oscillator in a PLL is employed to provide 8-phase 250 MHz clocks. Realized in 0.18 /spl mu/m CMOS technology, the AFE occupies a 1.0/spl times/1.2 mm/sup 2/ active area with the 3/sup rd/ order loop filter integrated in the same die. This chip also includes an integrated pulse generator for loop-back self-testing purposes. The receiver AFE achieves 200 /spl mu/V sensitivity and consumes 15 mW from a 1.8 V supply.","General and reference:0.1,Hardware:0.9,Computer systems organization:0.2,Networks:0.3,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Hardware,"Hardware: The paper presents a CMOS analog front-end design, a core hardware topic. The UWB context is secondary to the hardware implementation focus.","Communication hardware, interfaces and storage:0.9,Electronic design automation:0.5,Emerging technologies:0.3,Hardware test:0.2,Hardware validation:0.4,Integrated circuits:0.8,Power and energy:0.6,Printed circuit boards:0.2,Robustness:0.3,Very large scale integration design:0.7","Communication hardware, interfaces and storage,Integrated circuits","Communication hardware, interfaces and storage: The paper presents a CMOS analog front-end for UWB communication. Integrated circuits: The AFE is implemented in 0.18µm CMOS technology. Other categories like Power and energy are secondary to the core hardware design focus.","3D integrated circuits:0.2,Digital switches:0.3,Interconnect:0.7,Logic circuits:0.4,Reconfigurable logic and FPGAs:0.1,Semiconductor memory:0.2","Interconnect,Logic circuits",Interconnect: The AFE involves physical design aspects of signal transmission. Logic circuits: The paper describes ADCs and digital components. Other options like 3D integrated circuits are not explicitly discussed.
692,Fast exact and quasi-minimal minimization of highly testable fixed-polarity AND/XOR canonical networks,"The authors introduce fast exact and quasi-minimal algorithms for minimal fixed polarity AND/XOR canonical representation of Boolean functions. The method uses features of arrays of disjoint cubes representations of functions to identify the minimal networks. These features can drastically reduce the search space and provide high quality heuristics for quasi-minimal representations. Experimental results show that these special AND/XOR networks, on the average, have a similar number of terms to Boolean AND/OR networks while there were functions for which AND/XOR circuits were much smaller. The circuits generated are much more testable.<<ETX>>","General and reference:0.1,Hardware:0.9,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Hardware,Hardware is directly relevant as the paper presents novel Boolean function minimization techniques for AND/XOR networks. Other categories are irrelevant since the focus is on logic circuit design rather than software or applications.,"Communication hardware, interfaces and storage:0.2,Electronic design automation:0.3,Emerging technologies:0.2,Hardware test:0.9,Hardware validation:0.2,Integrated circuits:0.2,Power and energy:0.2,Printed circuit boards:0.2,Robustness:0.2,Very large scale integration design:0.7","Hardware test,Very large scale integration design",Hardware test is relevant as the paper focuses on testable AND/XOR networks. Very large scale integration design is relevant due to the emphasis on minimizing circuits for hardware implementation. Other categories are less directly related to the core contribution.,"3D integrated circuits:0.2,Analog and mixed-signal circuits:0.2,Analog, mixed-signal and radio frequency test:0.2,Application-specific VLSI designs:0.2,Board- and system-level test:0.2,Defect-based test:0.2,Design for testability:1,Design reuse and communication-based design:0.2,Design rules:0.2,Economics of chip design and manufacturing:0.2,Fault models and test metrics:0.2,Full-custom circuits:0.2,Hardware reliability screening:0.2,Memory test and repair:0.2,On-chip resource management:0.2,On-chip sensors:0.2,Standard cell libraries:0.2,Test-pattern generation and fault simulation:0.2,Testing with distributed and parallel systems:0.2,VLSI design manufacturing considerations:0.2,VLSI packaging:0.2,VLSI system specification and constraints:0.2",Design for testability,Design for testability: Focuses on optimizing AND/XOR circuits for high testability. Other options are unrelated to the specific testability design methods discussed.
3846,A 6-b 1.6-GS/s ADC With Redundant Cycle One-Tap Embedded DFE in 90-nm CMOS,"ADC-BASED serial link receivers are emerging in order to scale data rates over high attenuation channels. Embedding partial equalization inside the front-end ADC can potentially result in lowering the complexity of back-end DSP and/or decreasing the ADC resolution requirement, which results in a more energy-efficient receiver. This paper presents a 6-b 1.6-GS/s ADC with a novel embedded DFE structure. A redundant cycle technique is proposed for a time-interleaved SAR ADC, which relaxes the DFE feedback critical path delay with low power/area overhead. The 6-b prototype ADC with embedded one-tap DFE is fabricated in an LP 90-nm CMOS process and achieves 4.75-bits peak ENOB and 0.46 pJ/conv.-step FOM at a 1.6-GS/s sampling rate. Enabling the embedded DFE while operating at 1.6 Gb/s over a 46-in FR4 channel with 14-dB loss at Nyquist bandwidth opens a previously closed eye and allows for a 0.2 UI timing margin at a BER=10-9. Total ADC power including front-end T/Hs and reference buffers is 20.1 mW, and the core time-interleaved ADC occupies 0.24 mm 2 area.","General and reference:0.1,Hardware:0.9,Computer systems organization:0.2,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Hardware,Hardware is highly relevant because the paper presents a novel 6-bit ADC design in 90nm CMOS with embedded DFE. Other categories like 'Computer systems organization' are less relevant as the paper focuses on the physical design rather than system-level architecture.,"Communication hardware, interfaces and storage:0.1,Electronic design automation:0.1,Emerging technologies:0.1,Hardware test:0.1,Hardware validation:0.1,Integrated circuits:0.9,Power and energy:0.8,Printed circuit boards:0.1,Robustness:0.1,Very large scale integration design:0.7","Integrated circuits,Power and energy",Integrated circuits is highly relevant as the paper presents a novel ADC design in 90nm CMOS. Power and energy receives high relevance due to the focus on energy efficiency and power consumption analysis. Very large scale integration design is also relevant but with lower score as the paper focuses more on specific ADC architecture rather than general VLSI design methodology.,"3D integrated circuits:0,Digital switches:0,Energy distribution:0,Energy generation and storage:0,Impact on the environment:0,Interconnect:0.5,Logic circuits:0,Power estimation and optimization:0.5,Reconfigurable logic and FPGAs:0,Semiconductor memory:0,Thermal issues:0","Interconnect,Power estimation and optimization","Interconnect is relevant as the paper discusses ADC design in CMOS. Power estimation and optimization is relevant for energy efficiency. Semiconductor memory is less relevant as the focus is on ADC, not memory."
2606,An ultra low-power delta-sigma modulator using charge-transfer amplifier technique,"In this paper, an ultra low-power delta-sigma (DeltaSigma) modulator applying charge-transfer amplifier (CTA) technique for voice-band applications is presented. Both the fully-differential charge-transfer amplifier and integrator are developed for higher dynamic range of the modulator. A 67 dB of the peak SNR within a 4 kHz of bandwidth is reached in the presented modulator under a 2.5 MHz of sampling rate. The prototype circuit has been implemented in a 0.18 mum 1P6M CMOS technology. The chip area excluding PADs is 0.50 times 0.28 mm2. Due to its zero static current of CTA, only dynamic power is consumed in the circuit. The power consumption of the analog part of the presented second-order DeltaSigma modulator is only 3.4 muW. The total power consumption of the whole modulator is 36 muW at a 1.8 V of supply voltage.","General and reference:0.1,Hardware:1.0,Computer systems organization:0.2,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.2,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Hardware,"Hardware is highly relevant as the paper focuses on analog circuit design, power consumption, and CMOS implementation of a delta-sigma modulator. Other categories like Software or Theory of computation are not addressed.","Communication hardware, interfaces and storage:0.2,Electronic design automation:0.2,Emerging technologies:0.1,Hardware test:0.1,Hardware validation:0.1,Integrated circuits:1.0,Power and energy:0.8,Printed circuit boards:0.1,Robustness:0.1,Very large scale integration design:0.3","Integrated circuits,Power and energy",Integrated circuits is highly relevant as the paper presents a CMOS-based analog circuit design. Power and energy is relevant due to the focus on ultra-low-power consumption. Other options like 'Communication hardware' or 'Very large scale integration design' are less directly related to the circuit's specific design and power optimization.,"3D integrated circuits:0,Digital switches:0,Energy distribution:0,Energy generation and storage:0,Impact on the environment:0,Interconnect:0,Logic circuits:0,Power estimation and optimization:1,Reconfigurable logic and FPGAs:0,Semiconductor memory:0,Thermal issues:0",Power estimation and optimization,Power estimation and optimization is highly relevant because the paper focuses on ultra-low-power design with explicit power consumption metrics (3.4μW analog power). Other options like energy distribution or thermal issues are not discussed in the context of this circuit design.
950,A 0.92-THz SiGe Power Radiator Based on a Nonlinear Theory for Harmonic Generation,"We propose a nonlinear device model and a systematic methodology to generate maximum power at any desired harmonic. The proposed power optimization technique is based on the Volterra-Wiener theory of nonlinear systems. By manipulating the device nonlinearity and optimizing the embedding network, optimum conditions for harmonic power generation are provided. Using this theory, a 920–944-GHz frequency quadrupler is designed in a 130-nm SiGe:C process. The circuit achieves the peak output power of −17.3 and −10 dBm of effective isotropic radiated power and consumes 5.7 mW of dc power. To the best of our knowledge, this circuit demonstrates the highest generated power among Si/SiGe-based sources at this frequency range.","General and reference:0.1,Hardware:0.9,Computer systems organization:0.1,Networks:0.3,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Hardware,Hardware is relevant as the paper presents a SiGe circuit design for THz radiation. Networks is partially relevant but secondary to the hardware implementation. Other categories are not core to the circuit design focus.,"Communication hardware, interfaces and storage:0.1,Electronic design automation:0.1,Emerging technologies:0.2,Hardware test:0.1,Hardware validation:0.1,Integrated circuits:0.9,Power and energy:0.8,Printed circuit boards:0.1,Robustness:0.1,Very large scale integration design:0.1","Integrated circuits,Power and energy",Integrated circuits is directly relevant as the paper presents a SiGe-based circuit design. Power and energy is relevant due to the focus on power optimization. Emerging technologies is marginally relevant but not central.,"Semiconductor memory:1.0,Power estimation and optimization:0.8,Reconfigurable logic and FPGAs:0.5,Others:0.3","Semiconductor memory,Power estimation and optimization",Semiconductor memory is highly relevant for the SiGe-based device design. Power optimization is relevant for the circuit's energy efficiency. Reconfigurable logic is less central.
459,Optical dielectric resonator antenna,"In this paper we propose that a dielectric dot on the top of a ground plane could act as an antenna in optical frequency range. We investigate its electromagnetic properties numerically and observe that its behaviors are similar to dielectric resonator antenna in radio frequency. Further we optimize the aspect ratio of optical dielectric resonator antenna based on return loss, study the fundamental resonant frequency for different dielectric constant, and verify the array pattern multiplication for proposed optical antenna. Using the array of optical dielectric resonator antenna, we could achieve high directivity. We believe that optical dielectric resonator antenna could open new possibilities to study the novel phenomenon of light matter interaction. To the best of our knowledge, this is the first report of dielectric resonator antenna in optical frequencies.","General and reference:0.0,Hardware:1.0,Computer systems organization:0.2,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Hardware,"Hardware: The paper designs an optical dielectric resonator antenna, focusing on physical hardware implementation and electromagnetic properties. Computer systems organization is weakly relevant but not central.","Communication hardware, interfaces and storage:1.0,Electronic design automation:0.0,Emerging technologies:0.7,Hardware test:0.0,Hardware validation:0.0,Integrated circuits:0.0,Power and energy:0.0,Printed circuit boards:0.0,Robustness:0.0,Very large scale integration design:0.0","Communication hardware, interfaces and storage,Emerging technologies",Communication hardware is relevant as the paper introduces an optical dielectric resonator antenna for communication. Emerging technologies are relevant due to the novelty of applying dielectric resonator principles to optical frequencies. Other categories like Power and Energy are not central to the antenna's design.,"Analysis and design of emerging devices and systems:0.7,Biology-related information processing:0.2,Circuit substrates:0.3,Electromechanical systems:0.6,Emerging optical and photonic technologies:0.9,Memory and dense storage:0.2,Plasmonics:0.1,Quantum technologies:0.2,Reversible logic:0.1,Spintronics and magnetic technologies:0.3","Emerging optical and photonic technologies,Electromechanical systems",Emerging optical and photonic technologies is highly relevant as the paper introduces an optical frequency antenna. Electromechanical systems is relevant due to the mechanical aspects of the antenna design. Other categories like Circuit substrates or Spintronics are less relevant to the core contribution.
3830,E-BLOW: E-beam lithography overlapping aware stencil planning for MCC system,"Electron beam lithography (EBL) is a promising maskless solution for the technology beyond 14nm logic node. To overcome its throughput limitation, recently the traditional EBL system is extended into MCC system. In this paper, we present E-BLOW, a tool to solve the overlapping aware stencil planning (OSP) problems in MCC system. EBLOW is integrated with several novel speedup techniques, i.e., successive relaxation, dynamic programming and KDTree based clustering, to achieve a good performance in terms of runtime and solution quality. Experimental results show that, compared with previous works, E-BLOW demonstrates better performance for both conventional EBL system and MCC system.","General and reference:0.1,Hardware:0.95,Computer systems organization:0.6,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Hardware,Hardware is highly relevant as the paper focuses on E-beam lithography (a semiconductor fabrication technique) and MCC system optimization. Other categories like Computer systems organization have partial relevance but are secondary to the hardware-centric contributions.,"Communication hardware, interfaces and storage:0.0,Electronic design automation:0.8,Emerging technologies:0.7,Hardware test:0.0,Hardware validation:0.0,Integrated circuits:0.6,Power and energy:0.0,Printed circuit boards:0.0,Robustness:0.0,Very large scale integration design:0.6","Electronic design automation,Emerging technologies,Very large scale integration design",Electronic design automation is highly relevant as the paper discusses E-beam lithography planning. Emerging technologies is relevant due to the focus on novel manufacturing techniques. Very large scale integration design is relevant because of the application to VLSI systems. Other categories like Integrated circuits are relevant but not as central as the top three.,"3D integrated circuits:0,Analog and mixed-signal circuits:0,Analysis and design of emerging devices and systems:0,Application-specific VLSI designs:0,Biology-related information processing:0,Circuit substrates:0,Design reuse and communication-based design:0,Design rules:0,Economics of chip design and manufacturing:0,Electromechanical systems:0,Emerging interfaces:0,Emerging optical and photonic technologies:0,Full-custom circuits:0,Hardware description languages and compilation:0,High-level and register-transfer level synthesis:0,Logic synthesis:0,Memory and dense storage:0,Methodologies for EDA:1,Modeling and parameter extraction:0,On-chip resource management:0,On-chip sensors:0,Physical design (EDA):0,Plasmonics:0,Quantum technologies:0,Reversible logic:0,Spintronics and magnetic technologies:0,Standard cell libraries:0,Timing analysis:0,VLSI design manufacturing considerations:1,VLSI packaging:0,VLSI system specification and constraints:0","Methodologies for EDA,VLSI design manufacturing considerations",Methodologies for EDA is relevant due to the focus on stencil planning algorithms for EBL. VLSI design manufacturing considerations is relevant as the paper addresses EBL in the context of advanced manufacturing. Other options are irrelevant as the paper does not discuss packaging or 3D integration.
4536,On Dual-Rail Control Logic for Enhanced Circuit Robustness,"Ultra low-power design and energy harvesting applications require digital systems to operate under extremely low voltages approaching the point of balance between dynamic and static power consumption which is attained in the sub-threshold operation mode. Delay variations are extremely large in this mode, which calls for the use of asynchronous circuits that are speed-independent or quasi-delay-insensitive. However, even these classes of asynchronous logic become vulnerable because certain timing assumptions commonly accepted under normal operating conditions are no longer valid. In particular, the delay of inverters, often used as the so-called input 'bubbles', can no longer be neglected and they have to be either removed or properly acknowledged to ensure speed-independence. This paper presents an automated approach to synthesis of robust controllers for sub-threshold digital systems based on dual-rail implementation of control logic which eliminates inverters completely. This and other important properties are analysed and compared to the standard single-rail solutions. Dual-rail controllers are shown not to have significant overheads in terms of area and power consumption and are even faster in some cases due to the elimination of inverters from critical paths. The presented automated synthesis techniques are very efficient and can be applied to very large controllers as demonstrated in benchmarks.","General and reference:0.1,Hardware:1.0,Computer systems organization:0.3,Networks:0.1,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.2,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Hardware,Hardware is highly relevant as the paper discusses asynchronous circuit design and dual-rail control logic for robust hardware systems. Other categories like 'Computer systems organization' are less specific to the physical implementation of circuits.,"Communication hardware, interfaces and storage:0.1,Electronic design automation:0.8,Emerging technologies:0.5,Hardware test:0.1,Hardware validation:0.1,Integrated circuits:0.6,Power and energy:0.4,Printed circuit boards:0.1,Robustness:0.9,Very large scale integration design:0.7","Robustness,Electronic design automation,Very large scale integration design","Robustness is central to the paper's focus on sub-threshold circuit reliability. Electronic design automation and Very large scale integration design are relevant as the paper presents automated synthesis techniques for dual-rail controllers. Categories like Power and energy are secondary, as the primary contribution is about robustness and design methodology.","3D integrated circuits:0,Analog and mixed-signal circuits:0,Application-specific VLSI designs:0.2,Design for manufacturability:0,Design reuse and communication-based design:0,Design rules:0,Economics of chip design and manufacturing:0,Fault tolerance:0,Full-custom circuits:0,Hardware description languages and compilation:0,Hardware reliability:1,High-level and register-transfer level synthesis:1,Logic synthesis:0.5,Methodologies for EDA:0,Modeling and parameter extraction:0.2,On-chip resource management:0,On-chip sensors:0,Physical design (EDA):0,Safety critical systems:0,Standard cell libraries:0,Timing analysis:0.5,VLSI design manufacturing considerations:0,VLSI packaging:0,VLSI system specification and constraints:0","Hardware reliability,High-level and register-transfer level synthesis","Hardware reliability is relevant because the paper addresses robustness under sub-threshold operation. High-level synthesis is relevant due to automated controller synthesis techniques. Timing analysis is moderately relevant for delay considerations, but others like VLSI packaging are not."
273,Energy-delay tradeoffs in 32-bit static shifter designs,"This paper compares the energy-delay tradeoff curves of 32-bit static barrel and funnel shifters. The Stanford Circuit Optimization Tool (SCOT) is used to determine best transistor sizes in a 90 nm process. The paper evaluates the effect of multiplexer valency, circuit design, and physical placement. It also quantifies the costs of various shift operations. A funnel shifter using 4- and 8-input static multiplexer stages gives the best energy-delay tradeoff, with a knee at 440 ps (15 FO4 inverter delays) consuming 0.9 pJ per shift.","General and reference:0.1,Hardware:1.0,Computer systems organization:0.7,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.3,Applied computing:0.1,Social and professional topics:0.1",Hardware,"Hardware is highly relevant as the paper focuses on 32-bit static shifter design optimization in a 90nm process. Computer systems organization receives moderate relevance due to circuit design considerations, but Hardware is the primary domain. Other fields like Software or Security are irrelevant.","Communication hardware, interfaces and storage:0.1,Electronic design automation:1.0,Emerging technologies:0.2,Hardware test:0.3,Hardware validation:0.5,Integrated circuits:1.0,Power and energy:1.0,Printed circuit boards:0.1,Robustness:0.1,Very large scale integration design:0.8","Electronic design automation,Integrated circuits,Power and energy",Electronic design automation: The paper uses SCOT for optimization. Integrated circuits: The focus is on shifter design. Power and energy: The core contribution analyzes energy-delay tradeoffs. Other categories like VLSI are secondary to the primary focus on design and power.,"3D integrated circuits:0.1,Digital switches:0.1,Energy distribution:0.1,Energy generation and storage:0.1,Hardware description languages and compilation:0.1,High-level and register-transfer level synthesis:0.2,Impact on the environment:0.1,Interconnect:0.1,Logic circuits:0.6,Logic synthesis:0.3,Methodologies for EDA:0.1,Modeling and parameter extraction:0.1,Physical design (EDA):0.1,Power estimation and optimization:0.9,Reconfigurable logic and FPGAs:0.1,Semiconductor memory:0.1,Thermal issues:0.1,Timing analysis:0.8","Power estimation and optimization,Timing analysis",Power estimation and optimization is central to the energy-delay tradeoff analysis. Timing analysis is directly relevant to the execution time evaluation. Logic circuits is moderately relevant but less critical than the primary domains.
865,On the optimal sub-routing structures of 2-D FPGA greedy routing architectures,"For the FPGA Greedy Routing Architectures (GRAs), the optimal mapping problem of the entire chip can be decomposed into a sequence of three kinds of optimal m-side predetermined 4-way FPGA mapping problems, where m could be 1, 2, or 3. In this paper, we formulate the graph models of such sub-routing problems and investigate their minimum structures. The results give the lower bounds of routing resources in achieving all such kinds of GRAs and the theoretic models developed could be useful to studies on other FPGA routing problems as well.","General and reference:0.1,Hardware:0.9,Computer systems organization:0.2,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.2,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Hardware,"Hardware is highly relevant as the paper focuses on FPGA routing architecture design. Computer systems organization is partially relevant, but the core contribution is in hardware design optimization.","Electronic design automation:0.9,Very large scale integration design:0.8,Communication hardware, interfaces and storage:0.3,Hardware test:0.1,Power and energy:0.2,Robustness:0.1,Hardware validation:0.1,Printed circuit boards:0.1,Emerging technologies:0.4","Electronic design automation,Very large scale integration design","Electronic design automation: The paper formulates graph models for FPGA routing optimization, a core EDA task. Very large scale integration design: The study addresses routing resource allocation in FPGA architectures. Other children (e.g., Communication hardware, Power and energy) are irrelevant as the focus is on design methodology rather than hardware implementation specifics.","3D integrated circuits:0.1,Analog and mixed-signal circuits:0.1,Application-specific VLSI designs:0.5,Design reuse and communication-based design:0.2,Design rules:0.1,Economics of chip design and manufacturing:0.1,Full-custom circuits:0.3,Hardware description languages and compilation:0.2,High-level and register-transfer level synthesis:0.7,Logic synthesis:1.0,Methodologies for EDA:0.4,Modeling and parameter extraction:0.6,On-chip resource management:0.2,On-chip sensors:0.1,Physical design (EDA):0.5,Standard cell libraries:0.1,Timing analysis:0.3,VLSI design manufacturing considerations:0.4,VLSI packaging:0.1,VLSI system specification and constraints:0.3","Logic synthesis,High-level and register-transfer level synthesis",Logic synthesis is directly relevant as the paper analyzes optimal routing structures in FPGAs. High-level synthesis is relevant since the work involves mapping problems that connect to higher-level design. Analog circuits and packaging are irrelevant.
3302,Keynote 1 - The once and future FPGA: The confluence of configurable processing and reconfigurable technology,"We want to consider the unification of hard computing resources with reconfigurable fabrics. This is a perennial topic in reconfigurable computing. A decade ago we had Altera Excalibur, Xilinx Virtex II Pro, and on the small side, Stretch. Then processors started going very soft, with Nios, microBlaze and other soft-cores. Now we see new generations of hardened processors in FPGAs, with hardened ARMs in Altera and Xilinx devices. But there are yet new opportunities to consider if we look at application-specific hardened processors in FPGAs. Specialized signal processing cores built with configurable processor technology can complement the more general-purpose hardened control processors now available. This talk will review some history, paint a picture of what is possible today, and peer a little bit into the future.","General and reference:0.1,Hardware:0.9,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Hardware,Hardware: Focuses on FPGA architecture and reconfigurable computing. Other categories are too theoretical or unrelated.,"Integrated circuits:1.0,Very large scale integration design:0.8,Emerging technologies:0.5,Communication hardware, interfaces and storage:0.0,Electronic design automation:0.0,Hardware test:0.0,Hardware validation:0.0,Power and energy:0.0,Printed circuit boards:0.0,Robustness:0.0","Integrated circuits,Very large scale integration design",Integrated circuits: The talk discusses FPGA integration with hardened processors. Very large scale integration design is relevant for advanced FPGA design trends. Emerging technologies is less specific to the core topic.,"3D integrated circuits:0,Analog and mixed-signal circuits:0,Application-specific VLSI designs:0.7,Design reuse and communication-based design:0,Design rules:0,Digital switches:0,Economics of chip design and manufacturing:0,Full-custom circuits:0,Interconnect:0,Logic circuits:0,On-chip resource management:0,On-chip sensors:0,Reconfigurable logic and FPGAs:1,Semiconductor memory:0,Standard cell libraries:0,VLSI design manufacturing considerations:0,VLSI packaging:0,VLSI system specification and constraints:0","Reconfigurable logic and FPGAs,Application-specific VLSI designs",Reconfigurable logic and FPGAs is directly discussed in the context of FPGA design evolution and hardened processors. Application-specific VLSI designs is relevant due to the mention of specialized signal processing cores. Other categories like 3D circuits or interconnect are unrelated to the paper's focus.
241,Static magnetic memory: its applications to computers and controlling systems,"Memory The static magnetic memory has been developed only very recently, but it has already been adopted in the memory systems of several large scale digital computers , such as the Mark IV of the Compu-the Eniac is also reported to be adding this type of storage system to augment its limited storage capacity. The basic principle of the static magnetic memory is rather simple. It utilizes the hysteretic properties of the magnetic material. Fig. 1 shows a typical characteristic of this material. At the condition of zero magnetizing force, the material may stay at either 1 or O, depending on the previous history of operation. These two stable states of the material make it a very simple binary digit storage element. If a negative polarity magnetiziug","General and reference:0.1,Hardware:0.9,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Hardware,Hardware: The paper discusses static magnetic memory design and its application in early computer systems. Other categories like Computer systems organization are less relevant as the focus is on memory hardware implementation.,"Emerging technologies:1.0,Hardware validation:0.7,Communication hardware, interfaces and storage:0.5,Integrated circuits:0.4,Power and energy:0.3,Printed circuit boards:0.2,Robustness:0.2,Very large scale integration design:0.2,Electronic design automation:0.1,Hardware test:0.1","Emerging technologies,Hardware validation",Emerging technologies: Static magnetic memory is a novel hardware development. Hardware validation: Discusses practical implementation and testing. Other categories like integrated circuits are less specific to the memory design focus.,"Analysis and design of emerging devices and systems:0.6,Biology-related information processing:0.1,Circuit substrates:0.1,Electromechanical systems:0.1,Emerging interfaces:0.1,Emerging optical and photonic technologies:0.1,Functional verification:0.1,Memory and dense storage:1.0,Physical verification:0.1,Plasmonics:0.1,Post-manufacture validation and debug:0.1,Quantum technologies:0.1,Reversible logic:0.1,Spintronics and magnetic technologies:0.8","Memory and dense storage,Spintronics and magnetic technologies",Memory and dense storage is highly relevant as the paper focuses on static magnetic memory systems. Spintronics and magnetic technologies are relevant due to the use of magnetic materials for storage. Other categories are less relevant as the paper does not discuss emerging interfaces or quantum technologies.
941,A family of parallel-prefix modulo 2/sup n/-1 adders,"We reveal the cyclic nature of idempotency in the case of modulo 2/sup n/-1 addition. Then based on this property, we derive for each n, a family of minimum logic depth modulo 2/sup n/-1 adders, which allows several trade-offs between the number of operators, the internal wire length, and the fanout of internal nodes. Performance data, gathered using static CMOS implementations, reveal that the proposed architectures outperform all previously reported ones in terms of area and/or operation speed.","General and reference:0.1,Hardware:0.9,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Hardware,Hardware: The paper presents novel modulo adder architectures with performance improvements. Other categories are irrelevant as the focus is on circuit design and optimization.,"Communication hardware, interfaces and storage:0.1,Electronic design automation:0.1,Emerging technologies:0.1,Hardware test:0.1,Hardware validation:0.1,Integrated circuits:0.9,Power and energy:0.1,Printed circuit boards:0.1,Robustness:0.1,Very large scale integration design:0.9","Integrated circuits,Very large scale integration design",Integrated circuits and Very large scale integration design are directly relevant as the paper focuses on hardware architecture optimization for modulo addition. Other categories are not central to the contribution.,"3D integrated circuits:0,Analog and mixed-signal circuits:0.1,Application-specific VLSI designs:0.3,Design reuse and communication-based design:0.2,Design rules:0.1,Digital switches:0.2,Economics of chip design and manufacturing:0,Full-custom circuits:0.4,Interconnect:0.2,Logic circuits:1,On-chip resource management:0.1,On-chip sensors:0,Reconfigurable logic and FPGAs:0.3,Semiconductor memory:0.1,Standard cell libraries:0.5,VLSI design manufacturing considerations:0.6,VLSI packaging:0.1,VLSI system specification and constraints:0.2","Logic circuits,VLSI design manufacturing considerations",Logic circuits is relevant due to the focus on modulo adder architectures. VLSI design manufacturing considerations fits the performance evaluation in CMOS implementations. Other fields like reconfigurable logic are secondary.
6053,Low-power ASK receiver circuit for wireless communication system,"In this paper, we propose low-power receiver circuits for a wireless communication system using an ASK signal. The circuit structures are is suitable for low supply current. The proposed circuits are designed and simulated by Spectre using 0.8 /spl mu/m CMOS process parameters, and operates with a supply current below 1 /spl mu/A.","General and reference:0.1,Hardware:0.8,Computer systems organization:0.2,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Hardware,Hardware: The paper presents a low-power ASK receiver circuit design and simulation for wireless communication.,"Communication hardware, interfaces and storage:1.0,Electronic design automation:0.3,Emerging technologies:0.2,Hardware test:0.1,Hardware validation:0.1,Integrated circuits:0.6,Power and energy:0.8,Printed circuit boards:0.1,Robustness:0.1,Very large scale integration design:0.3","Communication hardware, interfaces and storage,Power and energy",Communication hardware is highly relevant for ASK receiver design. Power and energy is relevant for low-power circuit implementation. Integrated circuits receives moderate relevance for CMOS design.,"Energy distribution:0.1,Energy generation and storage:0.1,Impact on the environment:0.1,Power estimation and optimization:1.0,Thermal issues:0.2",Power estimation and optimization,Power estimation and optimization is highly relevant as the paper focuses on low-power circuit design. Other categories like Thermal issues are not explicitly addressed.
4189,An Automated Permuting Capacitor Device for Calibration of IVDs,"A new automated permuting capacitor device (APCD) for calibration of inductive voltage dividers (IVDs) with four-terminal-pair definition was designed and tested. The permuting capacitors composed of 11-SMD capacitors were housed in a bulk metal in a symmetrical arrangement. Thus, the APCD is compact and the temperature of the APCD could be controlled within several mKs. Using coaxial microwave switches, the APCD was permuted quickly and automatically using electrical signals during the calibration process. This prevented the APCD from being thermally perturbed by manual operation. Owing to the improved stability of the device, the effects of stray capacitance and voltage coefficient of the APCD were evaluated accurately. The calibration of an IVD using the permuting capacitor method with this APCD in the frequency range 1-2 kHz with accuracy comparable with that of the straddling method is presented in this paper.","General and reference:0.1,Hardware:1.0,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Hardware,Hardware is highly relevant as the paper describes an automated capacitor device for calibration. Other categories like Applied computing are secondary.,"Hardware test:0.9,Hardware validation:0.8,Communication hardware, interfaces and storage:0.4,Electronic design automation:0.2,Emerging technologies:0.2,Integrated circuits:0.2,Power and energy:0.2,Printed circuit boards:0.2,Robustness:0.2,Very large scale integration design:0.2","Hardware test,Hardware validation","Hardware test: The APCD is designed for calibration testing of IVDs. Hardware validation: The device ensures accurate evaluation of stray capacitance effects. Other options are less relevant as the focus is on test hardware, not design or energy.","Analog, mixed-signal and radio frequency test:1,Board- and system-level test:1,Defect-based test:0,Design for testability:0,Fault models and test metrics:0,Functional verification:0,Hardware reliability screening:0,Memory test and repair:0,Physical verification:0,Post-manufacture validation and debug:0,Test-pattern generation and fault simulation:0,Testing with distributed and parallel systems:0","Analog, mixed-signal and radio frequency test,Board- and system-level test","Analog, mixed-signal...: The APCD is an analog device for voltage divider calibration. Board-level test: The paper describes a system-level calibration method. Other categories like fault models or memory testing are not discussed."
3005,"Design of a closed-loop, bi-directional brain-machine-interface integrated on-chip spike sorting","This paper proposed a design of a closed-loop, bi-directional brain-machine-interface (BMI). The proposed chip consists of 16-channel neural acquisition, 8-channel neural stimulation, action potential detection, feature extraction and support vector machine (SVM) for spike sorting. A closed-loop control strategy is utilized to trigger different stimulation pattern according to different detected spike categories. The 16-channel neural signal acquisition is comprised of 16-channel low noise amplifier (LNA) which shares a single Programmable Gain Amplifier (PGA) draws a total current of 218uA under a power supply of 3.3V. Arbitrary combination is realized for the stimulator channel control. The frequency, the output current amplitude, the pulse width and the burst number are all tunable. The first and second derivative extrema method is applied to the detected action potential before spike sorting, realizing a great reduction on the data to be processed in the SVM. A single two-class sorting module is implemented as a results from the tradeoff between area and latency. More than two-class spike sorting is realized by a repeating of the two-class sorting procedure. The design has been fabricated in TSMC 180nm HV COMS process, occupying a silicon area of 5mm × 2mm.","General and reference:0.1,Hardware:0.9,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Hardware,"The paper details the design of a brain-machine interface with on-chip components, a hardware-focused contribution. Other categories like 'Computer systems organization' are not the primary focus.","Communication hardware, interfaces and storage:0.2,Electronic design automation:0.6,Emerging technologies:0.1,Hardware test:0.1,Hardware validation:0.1,Integrated circuits:0.9,Power and energy:0.4,Printed circuit boards:0.1,Robustness:0.1,Very large scale integration design:0.8","Integrated circuits,Very large scale integration design,Electronic design automation",Integrated circuits and VLSI design are central as the paper presents a custom BMI chip with 16-channel neural acquisition. Electronic design automation is relevant for the SVM-based spike sorting implementation. Other categories like power and communication hardware are secondary components but not core contributions.,"3D integrated circuits:0.1,Analog and mixed-signal circuits:0.9,Application-specific VLSI designs:0.8,Design reuse and communication-based design:0.1,Design rules:0.1,Digital switches:0.1,Economics of chip design and manufacturing:0.1,Full-custom circuits:0.3,Hardware description languages and compilation:0.1,High-level and register-transfer level synthesis:0.1,Interconnect:0.1,Logic circuits:0.1,Logic synthesis:0.1,Methodologies for EDA:0.2,Modeling and parameter extraction:0.1,On-chip resource management:0.1,On-chip sensors:0.2,Physical design (EDA):0.1,Reconfigurable logic and FPGAs:0.1,Semiconductor memory:0.1,Standard cell libraries:0.1,Timing analysis:0.1,VLSI design manufacturing considerations:0.1,VLSI packaging:0.1,VLSI system specification and constraints:0.1","Analog and mixed-signal circuits,Application-specific VLSI designs",Analog and mixed-signal circuits is relevant because the paper discusses a BMI chip with analog components like LNA and PGA. Application-specific VLSI designs is relevant because the design is tailored for a closed-loop BMI application. Other categories like Digital switches or Logic synthesis are irrelevant as the focus is on analog and application-specific VLSI design.
4186,An analog decoder for concatenated magnetic recording schemes,"This paper presents an all-analog iterative decoding network for an EPR4 magnetic recording system. A powerful serially concatenated architecture is considered, consisting of a simple outer code, an interleaver with reasonable size and a rate 1 EPR4 channel as inner code. The analog chip design is based on analog 0.18 /spl mu/m CMOS technology. Simulation results for both digital and analog implementations are shown. Practical implementation issues such as considerations of mismatch effects over performance are also discussed.","General and reference:0.1,Hardware:1.0,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Hardware,Hardware is highly relevant as the paper presents an analog decoder design for magnetic recording. Other categories like Computer systems organization are secondary.,"Communication hardware, interfaces and storage:1.0,Integrated circuits:0.8,Electronic design automation:0.5,Emerging technologies:0.3,Hardware test:0.2,Hardware validation:0.2,Power and energy:0.1,Printed circuit boards:0.1,Robustness:0.1,Very large scale integration design:0.1","Communication hardware, interfaces and storage,Integrated circuits",Communication hardware is highly relevant for the analog decoder design. Integrated circuits (0.8) addresses the CMOS implementation. Other categories like VLSI design are less specific to the analog chip focus.,"Semiconductor memory:1,3D integrated circuits:0,Digital switches:0,Interconnect:0,Logic circuits:0,Reconfigurable logic and FPGAs:0",Semiconductor memory,"Semiconductor memory: The paper focuses on magnetic recording systems, a core topic in semiconductor memory research. Other categories are unrelated to analog decoding or storage media."
394,Building and Evaluating COTS Based Optical Interlinks for Nanosatellites,Nanosatellites in low earth orbit are becoming more popular due their proven performance using accessible COTS components. Optical interlinks for such satellites have been suggested before as an alternative to RF links. In this paper we present the steps involved in selecting suitable COTS components for such links and evaluating them as well. Finally we compare the achievable performance of these links to what current low power RF technologies offer.,"General and reference:0.1,Hardware:0.9,Computer systems organization:0.3,Networks:0.2,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Hardware,Hardware is highly relevant as the paper focuses on optical interlink design for nanosatellites. Networks is secondary to the hardware implementation focus.,"Communication hardware, interfaces and storage:1.0,Hardware test:0.5,Hardware validation:0.5,Power and energy:0.2,Printed circuit boards:0.2,Robustness:0.3,Very large scale integration design:0.4,Electronic design automation:0.3,Emerging technologies:0.2,Integrated circuits:0.2","Communication hardware, interfaces and storage","Communication hardware, interfaces and storage is highly relevant as the paper focuses on building and evaluating optical interlinks for nanosatellites. Other categories like hardware test and validation are less central as the paper primarily discusses component selection and performance evaluation rather than testing methodologies.",,,No children provided for classification.
1765,Novel Devices for (Sub)millimeter-Wave Space Applications,"Submillimeter-wave space applications require special components for power generation, detection and multiplication. This article presents recent progress in three various solid-state device technologies for submillimeter-wave space instruments: the InP HEMT for ultra-low noise detection at very low power in the IF amplifier, the heterostructure barrier varactor in the frequency multiplier, and the superconducting hot electron bolometer mixer for quantum-limited heterodyne detection above 700 GHz.","General and reference:0.1,Hardware:0.9,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Hardware,Hardware is highly relevant as the paper discusses submillimeter-wave solid-state device development. Other categories are irrelevant as the focus is on physical components rather than systems or software.,"Communication hardware, interfaces and storage:0.0,Electronic design automation:0.0,Emerging technologies:1.0,Hardware test:0.0,Hardware validation:0.0,Integrated circuits:0.0,Power and energy:0.0,Printed circuit boards:0.0,Robustness:0.0,Very large scale integration design:0.0",Emerging technologies,Emerging technologies is directly relevant because the paper presents novel solid-state device technologies for submillimeter-wave applications. Other categories like Integrated circuits or VLSI design are not explicitly discussed as core contributions.,"Analysis and design of emerging devices and systems:0.9,Biology-related information processing:0.1,Circuit substrates:0.1,Electromechanical systems:0.1,Emerging interfaces:0.1,Emerging optical and photonic technologies:0.3,Memory and dense storage:0.1,Plasmonics:0.1,Quantum technologies:0.1,Reversible logic:0.1,Spintronics and magnetic technologies:0.1",Analysis and design of emerging devices and systems,"The paper presents novel solid-state devices for submillimeter-wave space instruments, which aligns with 'Analysis and design of emerging devices and systems'. 'Emerging optical and photonic technologies' is partially relevant but less precise than the primary category."
3072,A CMOS Contact Imager for Cell Detection in Bio-Sensing Applications,"Many experimental procedures in cell biology rely on the use of biochemical light-emitting reporters to enhance structures or processes of interest in a cell sample. The ability to integrate these widely-used protocols on the surface of a CMOS biosensor array, combined with the capacity to perform cell actuation/stimulation, would provide researchers with a valuable tool to conduct high-throughput, high-density, accurate analyses. To this end, a low noise, high-sensitivity CMOS contact imager is presented for the detection of cell cultures coupled directly to the sensor surface. Design considerations are discussed for the application of neural activity recording of neuronal networks cultured in vitro from dissociated neuron cells. A 128 times 128 CMOS imager implemented in 0.18-mum CMOS technology is presented featuring the implementation of the active reset technique with the use of the active column sensor pixel architecture","General and reference:0.1,Hardware:0.9,Computer systems organization:0.2,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Hardware,Hardware is highly relevant as the paper presents a CMOS contact imager design for bio-sensing applications. Other categories like 'Computer systems organization' are less relevant as the focus is on hardware component design rather than system architecture.,"Communication hardware, interfaces and storage:0.1,Electronic design automation:0.0,Emerging technologies:0.1,Hardware test:0.0,Hardware validation:0.0,Integrated circuits:0.9,Power and energy:0.0,Printed circuit boards:0.0,Robustness:0.0,Very large scale integration design:0.75","Integrated circuits,Very large scale integration design",Integrated circuits: The paper presents a CMOS imager design for bio-sensing. Very large scale integration design: The imager is implemented in 0.18-micron CMOS technology. Communication hardware is less relevant as the focus is on sensor design rather than communication interfaces.,"3D integrated circuits:0.2,Analog and mixed-signal circuits:0.7,Application-specific VLSI designs:0.9,Design reuse and communication-based design:0.3,Design rules:0.2,Digital switches:0.1,Economics of chip design and manufacturing:0.2,Full-custom circuits:0.6,Interconnect:0.3,Logic circuits:0.2,On-chip resource management:0.3,On-chip sensors:0.8,Reconfigurable logic and FPGAs:0.3,Semiconductor memory:0.2,Standard cell libraries:0.2,VLSI design manufacturing considerations:0.4,VLSI packaging:0.3,VLSI system specification and constraints:0.3","Application-specific VLSI designs,On-chip sensors",Application-specific VLSI designs is relevant for the custom CMOS imager design. On-chip sensors is relevant as the paper focuses on biosensor array integration. Other options like Full-custom circuits are less directly relevant to the core contribution.
4244,Custom wide counterflow pipelines for high-performance embedded applications,"Application-specific instruction set processor (ASIP) design is a promising technique to meet the performance and cost goals of high-performance systems. ASIPs are especially valuable for embedded computing applications (e.g., digital cameras, color printers, cellular phones, etc.) where a small increase in performance and decrease in cost can have a large impact on a product's viability. Sutherland, Sproull, and Molnar originally proposed a processor organization called the counterflow pipeline (CFP) as a general-purpose architecture. We observed that the CFP is appropriate for ASIP design due to its simple and regular structure, local control and communication, and high degree of modularity. We describe a new CFP architecture, called the wide counterflow pipeline (WCFP), that extends the original proposal to be better suited for custom embedded instruction-level parallel processors. This presents a novel and practical application of the CFP to automatic and quick turnaround design of ASIPs. We introduce the WCFP architecture and describe several microarchitecture capabilities needed to get good performance from custom WCFPs. We demonstrate that custom WCFPs have performance that is up to four times better than that of ASIPs based on the CFP. Using an analytic cost model, we show that custom WCFPs do not unduly increase the cost of the original counterflow pipeline architecture, yet they retain the simplicity of the CFP. We also compare custom WCFPs to custom VLIW architectures and demonstrate that the WCFP is performance competitive with traditional VLIWs without requiring complicated global interconnection of functional devices.","General and reference:0.0,Hardware:1.0,Computer systems organization:0.75,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Hardware,Hardware is highly relevant for the processor architecture design. Computer systems organization is moderately relevant. Other categories are not applicable.,"Communication hardware, interfaces and storage:0.2,Electronic design automation:1.0,Emerging technologies:0.3,Hardware test:0.1,Hardware validation:0.1,Integrated circuits:0.3,Power and energy:0.2,Printed circuit boards:0.1,Robustness:0.1,Very large scale integration design:0.4","Electronic design automation,Very large scale integration design","Electronic design automation is directly relevant for the WCFP architecture design methodology. Very large scale integration design is relevant for the custom processor implementation. Other categories are rejected as the paper focuses on architecture design rather than testing, power, or PCBs.","3D integrated circuits:0.1,Analog and mixed-signal circuits:0.1,Application-specific VLSI designs:1.0,Design reuse and communication-based design:0.3,Design rules:0.2,Economics of chip design and manufacturing:0.4,Full-custom circuits:0.7,Hardware description languages and compilation:0.3,High-level and register-transfer level synthesis:0.6,Logic synthesis:0.4,Methodologies for EDA:0.5,Modeling and parameter extraction:0.3,On-chip resource management:0.2,On-chip sensors:0.1,Physical design (EDA):0.3,Standard cell libraries:0.2,Timing analysis:0.3,VLSI design manufacturing considerations:0.4,VLSI packaging:0.1,VLSI system specification and constraints:0.3","Application-specific VLSI designs,Full-custom circuits",Application-specific VLSI designs (1.0) is directly relevant as the paper focuses on ASIP design for embedded applications. Full-custom circuits (0.7) is secondary since the WCFP is a custom architecture. Other options like High-level synthesis (0.6) and Timing analysis (0.3) are less central to the paper's contribution on counterflow pipeline design.
97,A Multi-accuracy-Level Approximate Memory Architecture Based on Data Significance Analysis,"Approximate memory is a promising technology for emerging recognition, mining and vision applications. These applications require the processing of large volumes of data to achieve energy-efficiency with negligible accuracy loss. This paper proposes a multi-level approximate memory architecture based on data significance analysis. In this architecture, a memory array is divided into several separated banks with different predefined accuracy levels. A key novelty of this work is the design of a memory controller that distributes data to the memory banks according to the results of data significance analysis. When applied to a DCT (Discrete Cosine Transform) processing module, the proposed approximate memory controller can achieve over 60% power saving with onchip memory model of multiple supply voltage SRAM banks, at the cost of a marginal output PSNR (Peak Signal to Noise Ratio) degradation of 3.34 dB.","General and reference:0.1,Hardware:0.9,Computer systems organization:0.7,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Hardware,Hardware is directly relevant for memory architecture design. Computer systems organization is secondary for memory controller integration. Other categories are not related to approximate memory design.,"Communication hardware, interfaces and storage:0.1,Electronic design automation:0.6,Emerging technologies:0.5,Hardware test:0.2,Hardware validation:0.2,Integrated circuits:0.8,Power and energy:1.0,Printed circuit boards:0.1,Robustness:0.2,Very large scale integration design:0.8","Power and energy,Integrated circuits,Very large scale integration design",Power and energy is highly relevant as the paper explicitly mentions 60% power savings through voltage scaling. Integrated circuits and VLSI design are directly relevant for memory architecture implementation. Other hardware categories like communication hardware or printed circuit boards are unrelated to the core memory controller design.,"3D integrated circuits:0,Analog and mixed-signal circuits:0,Application-specific VLSI designs:1,Design reuse and communication-based design:0,Design rules:0,Digital switches:0,Economics of chip design and manufacturing:0,Energy distribution:0,Energy generation and storage:0,Full-custom circuits:0,Impact on the environment:0,Interconnect:0,Logic circuits:0,On-chip resource management:0,On-chip sensors:0,Power estimation and optimization:1,Reconfigurable logic and FPGAs:0,Semiconductor memory:1,Standard cell libraries:0,Thermal issues:0,VLSI design manufacturing considerations:0,VLSI packaging:0,VLSI system specification and constraints:0","Semiconductor memory,Power estimation and optimization",Semiconductor memory: The paper proposes an approximate memory architecture with SRAM banks. Power estimation and optimization: The design achieves significant power savings through accuracy-level management. Application-specific VLSI designs is relevant but secondary; other options like Energy generation are irrelevant.
4083,Designing and Learning Substitutable Plane Graph Grammars,"Though graph grammars have been widely investigated for 40 years, few learning results exist for them. The main reasons come from complexity issues that are inherent when graphs, and a fortiori graph grammars, are considered. The picture is however different if one considers drawings of graphs, rather than the graphs themselves. E.g., it has recently been proved that the isomorphism and pattern searching problems could be solved in polynomial time for plane graphs, that is, planar embedding of planar graphs. In this paper, we introduce the Plane Graph Grammars (PGG) and detail how they differ to usual graph grammar formalisms while at the same time they share important properties with string context-free grammars. In particular, the parsing of a graph with a given PGG is polynomial for languages with appropriate restrictions. We demonstrate that PGG are well-shaped for learning: we show how recent results on string grammars can be extended to PGG by providing a learning algorithm that identifies in the limit the class of substitutable plane graph languages. Our algorithm runs in polynomial time assuming the same restriction used for polynomial parsing, and the amount of data needed for convergence is comparable to the one required in the case of strings.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.9,Mathematics of computing:0.1,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.1,Applied computing:0.0,Social and professional topics:0.0",Theory of computation,"Theory of computation is highly relevant because the paper introduces Plane Graph Grammars and their learning algorithms, which are theoretical contributions to formal language theory. Computing methodologies is less relevant as the focus is on grammatical formalisms rather than practical algorithms.","Computational complexity and cryptography:0.75,Design and analysis of algorithms:1.0,Formal languages and automata theory:1.0,Logic:0.0,Models of computation:0.0,Randomness, geometry and discrete structures:0.0,Semantics and reasoning:0.0,Theory and algorithms for application domains:0.0","Design and analysis of algorithms,Formal languages and automata theory,Computational complexity and cryptography",Design and analysis of algorithms is directly relevant for the polynomial-time learning algorithm. Formal languages and automata theory applies to the grammar formalism. Computational complexity and cryptography is relevant for the polynomial-time complexity analysis.,"Algebraic complexity theory:0,Algorithm design techniques:1,Approximation algorithms analysis:0,Automata extensions:0,Automata over infinite objects:0,Circuit complexity:0,Communication complexity:0,Complexity classes:0,Complexity theory and logic:0,Concurrent algorithms:0,Cryptographic primitives:0,Cryptographic protocols:0,Data structures design and analysis:0,Distributed algorithms:0,Formalisms:1,Grammars and context-free languages:1,Graph algorithms analysis:1,Interactive proof systems:0,Mathematical optimization:0,Online algorithms:0,Oracles and decision trees:0,Parallel algorithms:0,Parameterized complexity and exact algorithms:0,Problems, reductions and completeness:0,Proof complexity:0,Quantum complexity theory:0,Regular languages:0,Streaming, sublinear and near linear time algorithms:0,Tree languages:0","Algorithm design techniques,Grammars and context-free languages,Graph algorithms analysis",Algorithm design techniques: The paper introduces polynomial-time algorithms for parsing plane graphs. Grammars and context-free languages: PGG are explicitly compared to string context-free grammars. Graph algorithms analysis: Plane graph properties enable polynomial-time solutions. Other categories like 'Cryptography' are irrelevant to grammar learning.
714,Minimizing the Maximum Lateness,Hans Kellerer Universität Graz 10.,"General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.2,Theory of computation:1.0,Mathematics of computing:0.7,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.3,Social and professional topics:0.1",Theory of computation,"Theory of computation: The title suggests scheduling/optimization, a core topic in theoretical computer science. Other categories lack sufficient evidence due to the abstract's brevity.","Design and analysis of algorithms:0.95,Models of computation:0.5,Computational complexity and cryptography:0.3,Formal languages and automata theory:0.2,Logic:0.2,Randomness, geometry and discrete structures:0.3,Semantics and reasoning:0.2,Theory and algorithms for application domains:0.4",Design and analysis of algorithms,"Design and analysis of algorithms is directly relevant as the title references minimizing maximum lateness, a classic scheduling/optimization problem. Other categories lack sufficient context in the minimal abstract.","Algorithm design techniques:0.5,Approximation algorithms analysis:0,Concurrent algorithms:0,Data structures design and analysis:0,Distributed algorithms:0,Graph algorithms analysis:0,Mathematical optimization:1,Online algorithms:0,Parallel algorithms:0,Parameterized complexity and exact algorithms:0,Streaming, sublinear and near linear time algorithms:0",Mathematical optimization,"Mathematical optimization: The title and abstract focus on minimizing maximum lateness, a scheduling problem typically addressed through optimization. Algorithm design techniques could be secondary if specific algorithms are discussed, but the title alone suggests optimization as the primary domain. Other categories like Graph algorithms are not relevant here."
495,Odd Yao-Yao Graphs may Not be Spanners,"It is a long standing open problem whether Yao-Yao graphs $\mathsf{YY}_{k}$ are all spanners. Bauer and Damian \cite{bauer2013infinite} showed that all $\mathsf{YY}_{6k}$ for $k \geq 6$ are spanners. Li and Zhan \cite{li2016almost} generalized their result and proved that all even Yao-Yao graphs $\mathsf{YY}_{2k}$ are spanners (for $k\geq 42$). However, their technique cannot be extended to odd Yao-Yao graphs, and whether they are spanners are still elusive. In this paper, we show that, surprisingly, for any integer $k \geq 1$, there exist odd Yao-Yao graph $\mathsf{YY}_{2k+1}$ instances, which are not spanners.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.8,Mathematics of computing:0.2,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Theory of computation,Theory of computation is highly relevant as the paper addresses graph theory and algorithmic properties. Mathematics of computing is secondary due to the theoretical underpinnings. Other fields like Networks are less relevant as the focus is on abstract graph properties.,"Randomness, geometry and discrete structures:1.0,Design and analysis of algorithms:0.5,Formal languages and automata theory:0.2,Models of computation:0.1,Computational complexity and cryptography:0.1,Logic:0.1,Semantics and reasoning:0.1,Theory and algorithms for application domains:0.1","Randomness, geometry and discrete structures","Randomness, geometry and discrete structures is directly relevant as the paper analyzes Yao-Yao graphs, a geometric data structure. Design and analysis of algorithms is less relevant as the focus is on structural properties rather than algorithmic efficiency.",,,No possible children provided for classification.
4478,Derandomization Beyond Connectivity: Undirected Laplacian Systems in Nearly Logarithmic Space,"We give a deterministic \tilde{O}(\log n)-space algorithm for approximately solving linear systems given by Laplacians of undirected graphs, and consequently also approximating hitting times, commute times, and escape probabilities for undirected graphs. Previously, such systems were known to be solvable by randomized algorithms using O(\log n) space (Doron, Le Gall, and Ta-Shma, 2017) and hence by deterministic algorithms using O(\log^{3/2} n) space (Saks and Zhou, FOCS 1995 and JCSS 1999).Our algorithm combines ideas from time-efficient Laplacian solvers (Spielman and Teng, STOC 04; Peng and Spielman, STOC 14) with ideas used to show that Undirected S-T Connectivity is in deterministic logspace (Reingold, STOC 05 and JACM 08; Rozenman and Vadhan, RANDOM 05).","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.9,Mathematics of computing:0.3,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Theory of computation,Theory of computation is directly relevant for the derandomization of Laplacian solvers in logarithmic space. Other categories do not align with the core theoretical contributions in algorithms and complexity.,"Computational complexity and cryptography:0.7,Design and analysis of algorithms:0.9,Formal languages and automata theory:0.3,Logic:0.1,Models of computation:0.6,Randomness, geometry and discrete structures:0.5,Semantics and reasoning:0.1,Theory and algorithms for application domains:0.3","Design and analysis of algorithms,Computational complexity and cryptography","Design and analysis of algorithms is relevant for the deterministic Laplacian solver algorithm, while Computational complexity and cryptography applies to the space complexity analysis. Other categories are rejected due to minimal connection to theoretical algorithm design.","Algorithm design techniques:1,Graph algorithms analysis:1,Complexity classes:0.7,Algebraic complexity theory:0.3,Approximation algorithms analysis:0.4,Circuit complexity:0.2,Communication complexity:0.2,Complexity theory and logic:0.5,Concurrent algorithms:0.1,Cryptographic primitives:0.1,Cryptographic protocols:0.1,Data structures design and analysis:0.3,Distributed algorithms:0.4,Interactive proof systems:0.1,Mathematical optimization:0.3,Online algorithms:0.2,Oracles and decision trees:0.2,Parallel algorithms:0.3,Parameterized complexity and exact algorithms:0.3,Problems, reductions and completeness:0.4,Proof complexity:0.2,Quantum complexity theory:0.2,Streaming, sublinear and near linear time algorithms:0.5","Algorithm design techniques,Graph algorithms analysis",Algorithm design techniques is relevant for the Laplacian solver approach. Graph algorithms analysis applies to the graph-theoretical nature of the problem. Other options are less directly aligned with the paper's focus on derandomization of linear system solvers.
3271,Weights in stable marriage problems increase manipulation opportunities,"The stable marriage problem is a well-known problem of matching men to women so that no man and woman, who are not married to each other, both prefer each other. Such a problem has a wide variety of practical applications, ranging from matching resident doctors to hospitals, to matching students to schools or more generally to any two-sided market. In the classical stable marriage problem, both men and women express a strict preference order over the members of the other sex, in a qualitative way. Here we consider stable marriage problems with weighted preferences: each man (resp., woman) provides a score for each woman (resp., man). In this context, we consider the manipulability properties of the procedures that return stable marriages. While we know that all procedures are manipulable by modifying the preference lists or by truncating them, here we consider if manipulation can occur also by just modifying the weights while preserving the ordering and avoiding truncation. It turns out that, by adding weights, we indeed increase the possibility of manipulating and this cannot be avoided by any reasonable restriction on the weights.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.9,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Theory of computation,Theory of computation: The paper analyzes algorithmic properties of stable marriage problems with weighted preferences. Other options are irrelevant as the focus is on theoretical algorithms rather than software engineering.,"Computational complexity and cryptography:0.0,Design and analysis of algorithms:0.9,Formal languages and automata theory:0.3,Logic:0.4,Models of computation:0.0,Randomness, geometry and discrete structures:0.5,Semantics and reasoning:0.0,Theory and algorithms for application domains:0.7","Design and analysis of algorithms,Theory and algorithms for application domains",Design and analysis of algorithms is highly relevant as the paper analyzes algorithmic manipulation in stable marriage problems. Theory and algorithms for application domains is also relevant due to the application in two-sided markets. Other categories like Formal languages are less central.,"Algorithm design techniques:0.6,Algorithmic game theory and mechanism design:0.9,Approximation algorithms analysis:0.4,Concurrent algorithms:0.2,Data structures design and analysis:0.5,Database theory:0.1,Distributed algorithms:0.3,Graph algorithms analysis:0.4,Machine learning theory:0.2,Mathematical optimization:0.6,Online algorithms:0.5,Parallel algorithms:0.3,Parameterized complexity and exact algorithms:0.4,Streaming, sublinear and near linear time algorithms:0.3","Algorithmic game theory and mechanism design,Algorithm design techniques",Algorithmic game theory is central as the paper analyzes manipulation in stable marriage problems with weights. Algorithm design techniques is relevant due to the proposed method. Other options like Online algorithms or Mathematical optimization are secondary.
3496,Integrality Gaps for Strong SDP Relaxations of UNIQUE GAMES,"With the work of Khot and Vishnoi (FOCS 2005) as a starting point, we obtain integrality gaps for certain strong SDP relaxations of unique games. Specifically, we exhibit a gap instance for the basic semidefinite program strengthened by all valid linear inequalities on the inner products of up to $\exp(\Omega(\log\log~n)^{1/4})$ vectors. For stronger relaxations obtained from the basic semidefinite program by $R$ rounds of Sherali--Adams lift-and-project, we prove a unique games integrality gap for $R = \Omega(\log\log~n)^{1/4}$.By composing these SDP gaps with UGC-hardness reductions, the above results imply corresponding integrality gaps for every problem for which a UGC-based hardness is known. Consequently, this work implies that including any valid constraints on up to$\exp(\Omega(\log\log~n)^{1/4})$ vectors to natural semidefinite program, does not improve the approximation ratio for any problem in the following classes: constraint satisfaction problems, ordering constraint satisfaction problems and metric labeling problems over constant-size metrics. We obtain similar SDP integrality gaps for balanced separator, building on Devanur et al. (STOC 2006). We also exhibit, for explicit constants $\gamma, \delta ≫ 0$, an n-point negative-type metric which requires distortion $\Omega(\log\log n)^{\gamma}$ to embed into$\ell_1$, although all its subsets of size$\exp(\Omega(\log\log~n)^{\delta})$ embed isometrically into $\ell_1$.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.2,Networks:0.1,Software and its engineering:0.2,Theory of computation:1.0,Mathematics of computing:0.6,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.5,Applied computing:0.2,Social and professional topics:0.1",Theory of computation,"Theory of computation is highly relevant as the paper addresses integrality gaps for the Unique Games problem, a key theoretical challenge in approximation algorithms. Mathematics of computing (0.6) is secondary as the focus is on computational complexity rather than pure mathematics.","Computational complexity and cryptography:1.0,Design and analysis of algorithms:0.7,Formal languages and automata theory:0.1,Logic:0.1,Models of computation:0.1,Randomness, geometry and discrete structures:0.1,Semantics and reasoning:0.1,Theory and algorithms for application domains:0.1",Computational complexity and cryptography,Computational complexity and cryptography is highly relevant for integrality gaps in SDP relaxations of Unique Games. Design and analysis of algorithms is secondary but still relevant. Other fields are not core to the contribution.,"Algebraic complexity theory:0,Circuit complexity:0,Communication complexity:0,Complexity classes:0,Complexity theory and logic:1,Cryptographic primitives:0,Cryptographic protocols:0,Interactive proof systems:0,Oracles and decision trees:0,Problems, reductions and completeness:1,Proof complexity:0,Quantum complexity theory:0","Complexity theory and logic,Problems, reductions and completeness","Complexity theory and logic: The paper analyzes SDP integrality gaps in complexity theory. Problems, reductions and completeness: The results relate to reductions and completeness in constraint satisfaction problems. Other categories are irrelevant as the paper does not discuss cryptographic protocols or quantum theory."
1768,Principal type inference for GADTs,"We present a new method for GADT type inference that improves the precision of previous approaches. In particular, our approach accepts more type-correct programs than previous approaches when they do not employ type annotations. A side benefit of our approach is that it can detect a wide range of runtime errors that are missed by previous approaches. Our method is based on the idea to represent type refinements in pattern-matching branches by choice types, which facilitate a separation of the typing and reconciliation phases and thus support case expressions. This idea is formalized in a type system, which is both sound and a conservative extension of the classical Hindley-Milner system. We present the results of an empirical evaluation that compares our algorithm with previous approaches.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:1.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Theory of computation,Theory of computation is highly relevant as the paper presents a novel type inference method for GADTs. Other categories are not directly related to the core contribution.,"Computational complexity and cryptography:0.0,Design and analysis of algorithms:0.0,Formal languages and automata theory:0.0,Logic:0.0,Models of computation:0.0,Randomness, geometry and discrete structures:0.0,Semantics and reasoning:1.0,Theory and algorithms for application domains:0.0",Semantics and reasoning,Semantics and reasoning is directly relevant because the paper introduces a new type inference method for GADTs in functional programming. Other categories like Design and analysis of algorithms are not central to the core contribution.,"Program constructs:0.4,Program reasoning:0.9,Program semantics:0.8","Program reasoning,Program semantics",Program reasoning: The paper improves type inference for program correctness analysis. Program semantics: The type system is formally defined with semantic properties. Program constructs has lower relevance as the focus is on type inference rather than language construct design.
2495,Online Checkpointing with Improved Worst-Case Guarantees,"In the online checkpointing problem, the task is to continuously maintain a set of k checkpoints that allow rewinding an ongoing computation faster than by a full restart. The only operation allowed is to replace an old checkpoint by the current state. Our aim is checkpoint placement strategies that minimize rewinding cost, i.e., such that at all times T when requested to rewind to some time t â¤ T the number of computation steps that need to be redone to get to t from a checkpoint before t is as few as possible. In particular, we want the closest checkpoint earlier than t to be no farther away from t than qk times the ideal distance T/k + 1, where qk is a small constant. 
 
Improving earlier work showing 1 + 1/k â¤ qk â¤ 2, we show that qk can be chosen asymptotically less than 2. We present algorithms with asymptotic discrepancy qk â¤ 1.59 + o1 valid for all k and qk â¤ ln4 + o1 â¤ 1.39 + o1 valid for k being a power of two. Experiments indicate the uniform bound pk â¤ 1.7 for all k. For small k, we show how to use a linear programming approach to compute good checkpointing algorithms. This gives discrepancies of less than 1.55 for all k < 60. 
 
We prove the first lower bound that is asymptotically more than 1, namely qk â¥ 1.30-o1. We also show that optimal algorithms yielding the infimum discrepancy exist for all k.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.2,Networks:0.1,Software and its engineering:0.2,Theory of computation:0.9,Mathematics of computing:0.3,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.3,Applied computing:0.1,Social and professional topics:0.1",Theory of computation,"Theory of computation: The paper presents theoretical analysis of checkpointing algorithms with worst-case guarantees, which is a core topic in algorithm design and computational complexity. Other fields like Networks or Software are less relevant as the focus is purely theoretical and not on implementation or communication systems.","Computational complexity and cryptography:0.2,Design and analysis of algorithms:0.9,Formal languages and automata theory:0.1,Logic:0.1,Models of computation:0.3,Randomness, geometry and discrete structures:0.2,Semantics and reasoning:0.1,Theory and algorithms for application domains:0.4",Design and analysis of algorithms,Design and analysis of algorithms is highly relevant as the paper presents new checkpointing algorithms with performance guarantees. Other theory categories are less relevant as the focus is on algorithm design rather than formal languages or computational complexity.,"Algorithm design techniques:1.0,Approximation algorithms analysis:0.5,Concurrent algorithms:0.3,Data structures design and analysis:0.4,Distributed algorithms:0.3,Graph algorithms analysis:0.4,Mathematical optimization:0.3,Online algorithms:1.0,Parallel algorithms:0.4,Parameterized complexity and exact algorithms:0.3,Streaming, sublinear and near linear time algorithms:0.2","Algorithm design techniques,Online algorithms",Algorithm design techniques is relevant due to the focus on checkpointing algorithm development. Online algorithms is directly applicable to the problem domain. Other categories are less central to the core contribution.
5280,On redundancy elimination tolerant scheduling rules,"In Ferrucci, Pacini and Sessa (1995) an extended form of resolution, called Reduced SLD resolution (RSLD), is introduced. In essence, an RSLD derivation is an SLD derivation such that redundancy elimination from resolvents is performed after each rewriting step. It is intuitive that redundancy elimination may have positive effects on derivation process. However, undesiderable effects are also possible. In particular, as shown in this paper, program termination as well as completeness of loop checking mechanisms via a given selection rule may be lost. The study of such effects has led us to an analysis of selection rule basic concepts, so that we have found convenient to move the attention from rules of atom selection to rules of atom scheduling. A priority mechanism for atom scheduling is built, where a priority is assigned to each atom in a resolvent, and primary importance is given to the event of arrival of new atoms from the body of the applied clause at rewriting time. This new computational model proves able to address the study of redundancy elimination effects, giving at the same time interesting insights into general properties of selection rules. As a matter of fact, a class of scheduling rules, namely the specialisation independent ones, is defined in the paper by using not trivial semantic arguments. As a quite surprising result, specialisation independent scheduling rules turn out to coincide with a class of rules which have an immediate structural characterisation (named stack-queue rules). Then we prove that such scheduling rules are tolerant to redundancy elimination, in the sense that neither program termination nor completeness of equality loop check is lost passing from SLD to RSLD.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.2,Theory of computation:0.9,Mathematics of computing:0.3,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Theory of computation,Theory of computation is relevant for analyzing redundancy elimination in logic programming and selection rule properties. Other categories are rejected because the paper focuses on theoretical properties of computational models rather than implementation or application domains.,"Computational complexity and cryptography:0.7,Design and analysis of algorithms:0.85,Formal languages and automata theory:0.4,Logic:0.6,Models of computation:0.5,Randomness, geometry and discrete structures:0.3,Semantics and reasoning:0.6,Theory and algorithms for application domains:0.4","Design and analysis of algorithms,Logic",Design and analysis of algorithms is highly relevant as the paper introduces a novel scheduling rule framework for logic programs and analyzes its computational properties. Logic is relevant due to the paper's focus on SLD resolution and redundancy elimination in logic programming. Other fields like Computational complexity and cryptography are less directly connected since the paper's primary focus is not on complexity classes or cryptographic applications.,"Abstraction:0,Algorithm design techniques:0.2,Approximation algorithms analysis:0,Automated reasoning:0.1,Concurrent algorithms:0,Constraint and logic programming:1,Constructive mathematics:0,Data structures design and analysis:0,Description logics:0,Distributed algorithms:0,Equational logic and rewriting:0.3,Finite Model Theory:0,Graph algorithms analysis:0,Higher order logic:0.1,Hoare logic:0,Linear logic:0,Logic and verification:0.2,Mathematical optimization:0,Modal and temporal logics:0,Online algorithms:0,Parallel algorithms:0,Parameterized complexity and exact algorithms:0,Programming logic:1,Proof theory:0.1,Separation logic:0,Streaming, sublinear and near linear time algorithms:0,Type theory:0,Verification by model checking:0","Constraint and logic programming,Programming logic",Constraint and logic programming is directly relevant as the paper discusses RSLD resolution and scheduling in logic programming. Programming logic is relevant due to the analysis of selection rules and their properties. Other options like Algorithm design techniques or Equational logic are only tangentially related to the core focus on logic programming semantics.
737,Memoization of Coroutined Constraints,"Some linguistic constraints cannot be effectively resolved during parsing at the location in which they are most naturally introduced. This paper shows how constraints can be propagated in a memoizing parser (such as a chart parser) in much the same way that variable bindings are, providing a general treatment of constraint coroutining in memoization. Prolog code for a simple application of our technique to Bouma and van Noord's (1994) categorial grammar analysis of Dutch is provided.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.2,Theory of computation:0.8,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Theory of computation,Theory of computation is relevant for constraint propagation in parsing algorithms. Other categories like Software are less relevant as the focus is on theoretical parsing methods rather than software engineering.,"Computational complexity and cryptography:0.1,Design and analysis of algorithms:0.1,Formal languages and automata theory:0.9,Logic:0.1,Models of computation:0.8,Randomness, geometry and discrete structures:0.1,Semantics and reasoning:0.1,Theory and algorithms for application domains:0.1","Formal languages and automata theory,Models of computation",Formal languages and automata theory is highly relevant as the paper discusses constraint resolution in parsing. Models of computation receives moderate relevance due to its connection with memoization techniques.,"Abstract machines:0,Automata extensions:0,Automata over infinite objects:0,Computability:0,Concurrency:0,Formalisms:1,Grammars and context-free languages:1,Interactive computation:0,Probabilistic computation:0,Quantum computation theory:0,Regular languages:0,Streaming models:0,Timed and hybrid models:0,Tree languages:0","Grammars and context-free languages,Formalisms",Grammars and context-free languages are central to constraint propagation in parsing. Formalisms are relevant for the constraint coroutining framework. Concurrency and other formal methods are omitted as they are not the core contribution.
4414,Space Efficient Edge-Fault Tolerant Routing,"Let G be an undirected weighted graph with n vertices and m edges, and k >= 1 be an integer. We preprocess the graph in O^~(mn) time, constructing a data structure of size O^~ k deg{v}+n^{1/k}) words per vertex v in V, which is then used by our routing scheme to ensure successful routing of packets even in the presence of a single edge fault. The scheme adds only O(k) words of information to the message. 
Moreover, the stretch of the routing scheme, i.e., the maximum ratio of the cost of the path along which the packet is routed to the cost of the actual shortest path that avoids the fault, is only O(k^2). 
 
Our results match the best known results for routing schemes that do not consider failures, with only the stretch being larger by a small constant factor of O(k). Moreover, a 1963 girth conjecture of Erdos, known to hold for k=1,2,3 and 5, implies that Omega(n^{1+1/k}) space is required by any routing scheme that has a stretch less than 2k+1. 
Hence our data structures are essentially space efficient. 
The algorithms are extremely simple, easy to implement, and with minor modifications, can be used under a centralized setting to efficiently answer distance queries in the presence of faults. 
 
An important component of our routing scheme that may be of independent interest is an algorithm to compute the shortest cycle passing through each edge. As an intermediate result, we show that computing this in a distributed model that stores at each vertex the shortest path tree rooted at that node requires Theta(mn) message passings in the worst case.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.2,Software and its engineering:0.1,Theory of computation:1.0,Mathematics of computing:0.3,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Theory of computation,"Theory of computation is directly relevant as the paper focuses on graph algorithms, fault-tolerant routing, and theoretical analysis of data structures. Networks is only peripherally relevant as it discusses routing but does not focus on network protocols or systems.","Computational complexity and cryptography:0.1,Design and analysis of algorithms:0.9,Formal languages and automata theory:0.1,Logic:0.1,Models of computation:0.1,Randomness, geometry and discrete structures:0.1,Semantics and reasoning:0.1,Theory and algorithms for application domains:0.1",Design and analysis of algorithms,Design and analysis of algorithms is highly relevant for edge-fault tolerant routing. Other theoretical categories are less central to the algorithmic contributions.,"Graph algorithms analysis:1.0,Data structures design and analysis:0.8,Distributed algorithms:0.6,Other options:0.2","Graph algorithms analysis,Data structures design and analysis",Graph algorithms analysis is fundamental to the routing scheme. Data structures design relates to the space-efficient implementation.
4318,Drawing Binary Tanglegrams: An Experimental Evaluation,"A tanglegram is a pair of trees whose leaf sets are in one-to-one correspondence; matching leaves are connected by inter-tree edges. In applications such as phylogenetics or hierarchical clustering, it is required that the individual trees are drawn crossing-free. A natural optimization problem, denoted tanglegram layout problem, is thus to minimize the number of crossings between inter-tree edges. 
 
The tanglegram layout problem is NP-hard even for complete binary trees, for general binary trees the problem is hard to approximate if the Unique Games Conjecture holds. In this paper we present an extensive experimental comparison of a new and several known heuristics for the general binary case. We measure the performance of the heuristics with a simple integer linear program and a new exact branch-and-bound algorithm. The new heuristic returns the first solution that the branch-and-bound algorithm computes (in quadratic time). Surprisingly, in most cases this simple heuristic is at least as good as the best of the other heuristics.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.3,Theory of computation:0.8,Mathematics of computing:0.4,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Theory of computation,Theory of computation: The paper addresses NP-hard tanglegram layout problems and approximation hardness. 'Mathematics of computing' is secondary as the focus is on computational complexity rather than pure mathematics.,"Computational complexity and cryptography:0.8,Design and analysis of algorithms:0.9,Formal languages and automata theory:0.2,Logic:0.2,Models of computation:0.2,Randomness, geometry and discrete structures:0.2,Semantics and reasoning:0.2,Theory and algorithms for application domains:0.2","Design and analysis of algorithms,Computational complexity and cryptography",Design and analysis of algorithms is relevant for the tanglegram layout heuristics. Computational complexity and cryptography is relevant as the problem is NP-hard. Other categories like formal languages are less relevant as the focus is on graph algorithms.,"Algebraic complexity theory:0.0,Algorithm design techniques:0.0,Approximation algorithms analysis:0.0,Circuit complexity:0.0,Communication complexity:0.0,Complexity classes:0.0,Complexity theory and logic:0.0,Concurrent algorithms:0.0,Cryptographic primitives:0.0,Cryptographic protocols:0.0,Data structures design and analysis:0.0,Distributed algorithms:0.0,Graph algorithms analysis:1.0,Interactive proof systems:0.0,Mathematical optimization:0.8,Online algorithms:0.0,Oracles and decision trees:0.0,Parallel algorithms:0.0,Parameterized complexity and exact algorithms:0.0,Problems, reductions and completeness:0.0,Proof complexity:0.0,Quantum complexity theory:0.0,Streaming, sublinear and near linear time algorithms:0.0","Graph algorithms analysis,Mathematical optimization",Graph algorithms analysis is highly relevant as the paper addresses the NP-hard tanglegram layout problem. Mathematical optimization is secondary due to the focus on minimizing crossings via heuristics. Other categories are irrelevant to the graph drawing problem.
47,Hopfield Models as Nondeterministic Finite-State Machines,"The use of neural networks for integrated linguistic analysis may be profitable. This paper presents the first results of our research on that subject: a Hopfield model for syntactical analysis. We construct a neural network as an implementation of a bounded push-down automaton, which can accept context-free languages with limited center-embedding. The network's behavior can be predicted a priori, so the presented theory can be tested. The operation of the network as an implementation of the acceptor is provably correct. Furthermore we found a solution to the problem of spurious states in Hopfield models: we use them as dynamically constructed representations of sets of states of the implemented acceptor. The so-called neural-network acceptor we propose, is fast but large.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:1.0,Mathematics of computing:0.5,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.5,Applied computing:0.0,Social and professional topics:0.0",Theory of computation,"Theory of computation is relevant because the paper connects Hopfield models to finite-state machines and formal language theory. Mathematics of computing and Computing methodologies receive partial scores for the theoretical analysis and neural network implementation, but the primary contribution is in computational theory.","Formal languages and automata theory:1.0,Design and analysis of algorithms:0.75,Models of computation:0.5,Computational complexity and cryptography:0.0,Logic:0.0,Randomness, geometry and discrete structures:0.0,Semantics and reasoning:0.0,Theory and algorithms for application domains:0.0",Formal languages and automata theory,Formal languages and automata theory is highly relevant as the paper models Hopfield networks as finite-state machines. Design and analysis of algorithms is moderately relevant for the theoretical analysis. Other categories are less relevant as the focus is on automata theory rather than general algorithms or complexity.,"Automata extensions:1,Automata over infinite objects:0,Formalisms:0.5,Grammars and context-free languages:1,Regular languages:0.5,Tree languages:0","Grammars and context-free languages,Automata extensions",Grammars and context-free languages are directly addressed through syntactical analysis. Automata extensions apply to the Hopfield model implementation. Formalisms and regular languages are secondary concepts.
4381,The exact cost of exploring streets with a cab,"A fundamental problem in robotics is to compute a path for a robot from its current location to a given target. In this pap er we consider the problem of a robot equipped with an on-board vision system searching for a target t in an unknown environment. We assume that the robot is located at a point s i a polygon that belongs to the well investigated class of polygons call ed streets. In this paper we present the first exact analysis of the continuous angular bisector (CAB) strategy, which has been considered several times before, and show that it has a competitive ratio of 1:6837 in the worst case.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.9,Mathematics of computing:0.5,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Theory of computation,Theory of computation is highly relevant as the paper presents an exact analysis of a robot navigation algorithm. Mathematics of computing has moderate relevance but is secondary to the theoretical computation focus.,"Computational complexity and cryptography:0.2,Design and analysis of algorithms:0.9,Formal languages and automata theory:0.1,Logic:0.1,Models of computation:0.3,Randomness, geometry and discrete structures:0.6,Semantics and reasoning:0.1,Theory and algorithms for application domains:0.8","Design and analysis of algorithms,Theory and algorithms for application domains","Design and analysis of algorithms is central as the paper analyzes the CAB strategy's competitive ratio. Theory and algorithms for application domains is relevant due to the robotics application. Randomness, geometry, etc., are secondary as the focus is on algorithmic performance.","Algorithm design techniques:1,Algorithmic game theory and mechanism design:0,Approximation algorithms analysis:0.5,Concurrent algorithms:0,Data structures design and analysis:0,Database theory:0,Distributed algorithms:0,Graph algorithms analysis:0,Machine learning theory:0,Mathematical optimization:0,Online algorithms:1,Parallel algorithms:0,Parameterized complexity and exact algorithms:0,Streaming, sublinear and near linear time algorithms:0","Algorithm design techniques,Online algorithms",Algorithm design techniques applies to the CAB strategy development. Online algorithms is relevant as the robot must search in an unknown environment. Approximation algorithms analysis is only moderately relevant as the paper provides exact analysis.
1287,Moore automata for flexible routing and flow control in manufacturing workcells,"The potential of flexible-manufacturing workcells (FMCs) to produce a family of parts in many possible orders of operations and choices of different machines is advantageous. Despite intensive research on the theoretical control of discrete-event systems, however, current techniques can still only be used for the supervisory control of simple cells. In this article, a novel modeling and control synthesis technique is presented for FMCs that allow part-routing flexibility. Our proposed methodology combines extended Moore automata and controlled-automata theories to synthesize supervisors for such FMCs.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.9,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Theory of computation,"Theory of computation: The paper introduces a novel control synthesis technique using extended Moore automata for manufacturing workcells, which is a theoretical contribution to discrete-event systems. Other categories like Networks or Software Engineering are not central to the core contribution.","Computational complexity and cryptography:0.2,Design and analysis of algorithms:0.3,Formal languages and automata theory:1.0,Logic:0.2,Models of computation:0.9,Randomness, geometry and discrete structures:0.3,Semantics and reasoning:0.4,Theory and algorithms for application domains:0.5","Formal languages and automata theory,Models of computation",Formal languages and automata theory is directly relevant due to the use of Moore automata. Models of computation is relevant for the control synthesis methodology. Other categories like Design and analysis of algorithms are less central.,"Abstract machines:1,Automata extensions:1,Automata over infinite objects:0,Computability:0,Concurrency:0,Formalisms:0.3,Grammars and context-free languages:0,Interactive computation:0,Probabilistic computation:0,Quantum computation theory:0,Regular languages:0,Streaming models:0,Timed and hybrid models:0,Tree languages:0","Abstract machines,Automata extensions",Abstract machines: The paper uses Moore automata for control synthesis. Automata extensions: The methodology combines extended Moore automata. Formalisms is only weakly relevant as the focus is on automata application rather than formal system design.
3523,On-Line Approximate String Searching Algorithms: Survey and Experimental Results,"The problem of approximate string searching comprises two classes of problems: string searching with k mismatches and string searching with k differences. In this paper we present a short survey and experimental results for well known sequential approximate string searching algorithms. We consider algorithms based on different approaches including dynamic programming, deterministic finite automata, filtering, counting and bit parallelism. We compare these algorithms in terms of running time against pattern length and for several values of k for four different kinds of text: binary alphabet, alphabet of size 8, English alphabet and DNA alphabet. Finally, we compare the experimental results of the algorithms with their theoretical complexities.","General and reference:0.1,Hardware:0.2,Computer systems organization:0.3,Networks:0.2,Software and its engineering:0.3,Theory of computation:0.9,Mathematics of computing:0.4,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.2,Computing methodologies:0.3,Applied computing:0.2,Social and professional topics:0.1",Theory of computation,"Theory of computation is highly relevant as the paper focuses on algorithmic approaches for approximate string searching, including dynamic programming and bit parallelism. Other categories like Hardware or Networks are irrelevant as the paper does not discuss physical systems or communication protocols.","Computational complexity and cryptography:0.5,Design and analysis of algorithms:1.0,Formal languages and automata theory:0.75,Logic:0.0,Models of computation:0.0,Randomness, geometry and discrete structures:0.0,Semantics and reasoning:0.0,Theory and algorithms for application domains:0.25","Design and analysis of algorithms,Formal languages and automata theory",Design and analysis of algorithms is central to the paper's focus on algorithm performance evaluation. Formal languages and automata theory is relevant for the algorithms using DFAs. Computational complexity is secondary to the experimental focus. Application domains are not the primary contribution.,"Algorithm design techniques:0.8,Approximation algorithms analysis:0.7,Automata extensions:0.1,Automata over infinite objects:0.1,Concurrent algorithms:0.1,Data structures design and analysis:0.2,Distributed algorithms:0.1,Formalisms:0.1,Grammars and context-free languages:0.1,Graph algorithms analysis:0.1,Mathematical optimization:0.1,Online algorithms:0.6,Parallel algorithms:0.1,Parameterized complexity and exact algorithms:0.1,Regular languages:0.1,Streaming, sublinear and near linear time algorithms:0.1,Tree languages:0.1","Algorithm design techniques,Approximation algorithms analysis",Algorithm design techniques is relevant as the paper surveys approximate string searching methods. Approximation algorithms analysis is directly relevant to the experimental evaluation of algorithms with k mismatches/differences. Online algorithms is less central to the core contribution.
2878,"From determinism, non-determinism and alternation to recursion schemes for P, NP and Pspace (Invited Talk)","Our goal is to approach the classes of computational complexity P, NP, and Pspace in a recursion-theoretic manner. Here we emphasize the connection between the structure of the recursion schemes and the underlying models of computation.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.9,Mathematics of computing:0.3,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Theory of computation,"Theory of computation is highly relevant as the paper explores recursion schemes for computational complexity classes P, NP, and Pspace. Mathematics of computing (0.3) receives moderate relevance for the theoretical foundations, but the primary focus is computational models.","Computational complexity and cryptography:1.0,Design and analysis of algorithms:0.7,Formal languages and automata theory:0.4,Logic:0.3,Models of computation:0.6,Randomness, geometry and discrete structures:0.2,Semantics and reasoning:0.3,Theory and algorithms for application domains:0.5","Computational complexity and cryptography,Design and analysis of algorithms","Computational complexity is primary as the paper addresses P, NP, and Pspace. Design and analysis of algorithms is relevant for the recursion schemes approach. Other categories like formal languages are less central.","Algebraic complexity theory:0,Algorithm design techniques:0,Approximation algorithms analysis:0,Circuit complexity:0,Communication complexity:0,Complexity classes:1,Complexity theory and logic:1,Concurrent algorithms:0,Cryptographic primitives:0,Cryptographic protocols:0,Data structures design and analysis:0,Distributed algorithms:0,Graph algorithms analysis:0,Interactive proof systems:0,Mathematical optimization:0,Online algorithms:0,Oracles and decision trees:0,Parallel algorithms:0,Parameterized complexity and exact algorithms:0,Problems, reductions and completeness:0,Proof complexity:0,Quantum complexity theory:0,Streaming, sublinear and near linear time algorithms:0","Complexity classes,Complexity theory and logic","Complexity classes is highly relevant as the paper directly addresses P, NP, and Pspace. Complexity theory and logic is relevant due to the connection between recursion schemes and computational models. Other categories like cryptographic primitives or graph algorithms are unrelated to the core focus on computational complexity theory."
3932,A polynomial-time algorithm for the maximum clique problem,"After more than six decades of its introduction, the maximum clique problem, which is one of the most applicable problems in the graph theory, has still no polynomial-time solution. This paper presents a polynomial-time algorithm for this problem, which detects the maximum clique of a given graph through a recursive approach. This polynomial solution to the clique problem, as an NP-complete problem, causes every problem in NP to have a polynomial solution, which leads to the equality of P and NP complexity classes.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.9,Mathematics of computing:0.6,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Theory of computation,"Theory of computation: The paper proposes a polynomial-time algorithm for an NP-complete problem, directly addressing computational complexity theory. Mathematics of computing is secondary as the focus is on algorithmic complexity rather than mathematical methods.","Computational complexity and cryptography:0.9,Design and analysis of algorithms:0.85,Formal languages and automata theory:0.2,Logic:0.7,Models of computation:0.3,Randomness, geometry and discrete structures:0.1,Semantics and reasoning:0.15,Theory and algorithms for application domains:0.4","Computational complexity and cryptography,Design and analysis of algorithms","Computational complexity and cryptography: The paper claims a polynomial solution to an NP-complete problem, directly impacting complexity theory. Design and analysis of algorithms: Presents a new algorithm for maximum clique. Logic and other categories are less central to the core contribution of complexity class implications.","Algebraic complexity theory:0,Algorithm design techniques:0.5,Approximation algorithms analysis:0,Circuit complexity:0,Communication complexity:0,Complexity classes:1,Complexity theory and logic:0.5,Concurrent algorithms:0,Cryptographic primitives:0,Cryptographic protocols:0,Data structures design and analysis:0,Distributed algorithms:0,Graph algorithms analysis:1,Interactive proof systems:0,Mathematical optimization:0,Online algorithms:0,Oracles and decision trees:0,Parallel algorithms:0,Parameterized complexity and exact algorithms:0,Problems, reductions and completeness:0.5,Proof complexity:0,Quantum complexity theory:0,Streaming, sublinear and near linear time algorithms:0","Complexity classes,Graph algorithms analysis",Complexity classes is directly relevant as the paper claims P=NP via a polynomial-time solution to an NP-complete problem. Graph algorithms analysis is relevant due to the focus on the maximum clique problem. Other categories like Algorithm design techniques are only indirectly related.
5852,Mean Flow Time Minimization in Reentrant Job Shops with a Hub,"This paper considers a reentrant job shop with one hub machine which a job enters K times. Between any two consecutive entries into the hub, the job is processed on other machines. The objective is to minimize the total flow time. Under two key assumptions, the bottleneck assumption and the hereditary order (HO) assumption on the processing times of the entries, it is proved that there is an optimal schedule with the shortest processing time (SPT) job order and a dynamic programming algorithm is derived to find such a schedule. An approximation algorithm based on the shortest path algorithm and the SPT job order is also proposed to solve the problem. The approximation algorithm finds an optimal clustered schedule. In clustered schedules, jobs are scheduled in disjoint clusters; they resemble batch processing and seem to be of practical importance. Worst-case bounds for clustered schedules are proved with the HO assumption relaxed. Two special cases with the restriction that there are only two entries to the hub machine are further analyzed to offer more insight into the structure of optimal solutions.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.9,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Theory of computation,Theory of computation is highly relevant for scheduling algorithms in job shops. Other categories like Networks are not central to the core contribution.,"Computational complexity and cryptography:0.3,Design and analysis of algorithms:0.8,Formal languages and automata theory:0.2,Logic:0.2,Models of computation:0.3,Randomness, geometry and discrete structures:0.2,Semantics and reasoning:0.2,Theory and algorithms for application domains:0.7","Design and analysis of algorithms,Theory and algorithms for application domains","Design and analysis of algorithms: The paper introduces scheduling algorithms (SPT, dynamic programming) for minimizing flow time. Theory and algorithms for application domains: The problem is applied to reentrant job shops, a practical industrial context.","Algorithm design techniques:0.7,Algorithmic game theory and mechanism design:0.1,Approximation algorithms analysis:0.8,Concurrent algorithms:0.1,Data structures design and analysis:0.1,Database theory:0.1,Distributed algorithms:0.1,Graph algorithms analysis:0.1,Machine learning theory:0.1,Mathematical optimization:0.9,Online algorithms:0.1,Parallel algorithms:0.1,Parameterized complexity and exact algorithms:0.1,Streaming, sublinear and near linear time algorithms:0.1","Mathematical optimization,Algorithm design techniques,Approximation algorithms analysis",Mathematical optimization is core to flow time minimization. Algorithm design techniques and approximation algorithms apply to SPT and dynamic programming methods. Other categories like Online algorithms are not addressed.
2413,Full abstraction for nominal Scott domains,"We develop a domain theory within nominal sets and present programming language constructs and results that can be gained from this approach. The development is based on the concept of orbit-finite subset, that is, a subset of a nominal sets that is both finitely supported and contained in finitely many orbits. This concept appears prominently in the recent research programme of Bojanczyk et al. on automata over infinite languages, and our results establish a connection between their work and a characterisation of topological compactness discovered, in a quite different setting, by Winskel and Turner as part of a nominal domain theory for concurrency. We use this connection to derive a notion of Scott domain within nominal sets. The functionals for existential quantification over names and `definite description' over names turn out to be compact in the sense appropriate for nominal Scott domains. Adding them, together with parallel-or, to a programming language for recursively defined higher-order functions with name abstraction and locally scoped names, we prove a full abstraction result for nominal Scott domains analogous to Plotkin's classic result about PCF and conventional Scott domains: two program phrases have the same observable operational behaviour in all contexts if and only if they denote equal elements of the nominal Scott domain model. This is the first full abstraction result we know of for higher-order functions with local names that uses a domain theory based on ordinary extensional functions, rather than using the more intensional approach of game semantics.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:1.0,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Theory of computation,Theory of computation is central to this paper’s domain-theoretic analysis of programming languages. Other fields like Software engineering or Mathematics of computing are tangential to its core theoretical contributions.,"Computational complexity and cryptography:0.1,Design and analysis of algorithms:0.3,Formal languages and automata theory:0.2,Logic:0.95,Models of computation:0.85,Randomness, geometry and discrete structures:0.7,Semantics and reasoning:0.9,Theory and algorithms for application domains:0.1","Logic,Semantics and reasoning,Models of computation",Logic is relevant for formal domain theory. Semantics and reasoning is central to the full abstraction result. Models of computation applies to the domain-theoretic framework. Other fields like Cryptography are not related.,"Abstract machines:0.3,Abstraction:0.3,Automated reasoning:0.3,Computability:0.3,Concurrency:1,Constraint and logic programming:0.3,Constructive mathematics:0.3,Description logics:0.3,Equational logic and rewriting:0.3,Finite Model Theory:0.3,Higher order logic:0.3,Hoare logic:0.3,Interactive computation:0.3,Linear logic:0.3,Logic and verification:0.3,Modal and temporal logics:0.3,Probabilistic computation:0.3,Program constructs:0.3,Program reasoning:0.7,Program semantics:1,Programming logic:0.3,Proof theory:0.3,Quantum computation theory:0.3,Separation logic:0.3,Streaming models:0.3,Timed and hybrid models:0.3,Type theory:0.3,Verification by model checking:0.3","Concurrency,Program semantics",Concurrency is core as the paper connects domain theory with concurrency. Program semantics are key for the Scott domain model. Program reasoning is secondary as the focus is on semantics and abstraction.
849,Complete Types in an Extension of the System AF2,"In this paper, we extend the system AF2 in order to have the subject reduction for the ß?-reduction. We prove that the types with positive quantifiers are complete for models that are stable by weak-head expansion.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:1.0,Mathematics of computing:0.3,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Theory of computation,"Theory of computation is highly relevant because the paper extends a formal system (AF2) and proves theoretical properties about type completeness and stability. Mathematics of computing receives partial relevance due to the mathematical proofs, but the primary focus is on computational theory.","Computational complexity and cryptography:0.1,Design and analysis of algorithms:0.1,Formal languages and automata theory:0.1,Logic:0.9,Models of computation:0.1,Randomness, geometry and discrete structures:0.1,Semantics and reasoning:0.8,Theory and algorithms for application domains:0.1","Logic,Semantics and reasoning",Logic is directly relevant as the paper extends a type system with formal proofs. Semantics and reasoning is relevant due to the focus on type completeness and model analysis. Other categories are not central to the theoretical contributions.,"Type theory:1,Logic and verification:0.8,Proof theory:0.6,Program reasoning:0.4,Verification by model checking:0.2","Type theory,Logic and verification",Type theory: The paper extends a type system and proves completeness. Logic and verification: Involves formal proofs for model stability. Other categories like Program reasoning or Verification by model checking are less central to the core contribution.
2070,Implicational logics in natural deduction systems,"In 1959 M. Dummett [3] introduced the logic LC obtained by adding the axiom ACpqCqp to the formalization of the intuitionistic prepositional calculus having modus ponens and substitution as its rules of inference. Later on R. A. Bull [1] showed, by quite a roundabout way, that the implicational theses of LC were precisely the theses of the implicational calculus obtained by adding the axiom CCCpqrCCCqprr to the system of positive implication. In 1964 Bull [2] gave another proof, this time using results of Birkhoff concerning subdirectly reducible algebras. The aim of this short note is to emphasize that the use of Gentzen's natural deduction systems (see Prawitz [4]) allows us to give a much more direct proof.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.9,Mathematics of computing:0.7,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Theory of computation,Theory of computation is highly relevant for the formal logic and natural deduction system analysis. Mathematics of computing (0.7) is secondary due to the logical structure but not the primary focus.,"Computational complexity and cryptography:0.3,Design and analysis of algorithms:0.4,Formal languages and automata theory:0.5,Logic:1.0,Models of computation:0.5,Randomness, geometry and discrete structures:0.4,Semantics and reasoning:0.8,Theory and algorithms for application domains:0.3","Logic,Semantics and reasoning",Logic: The paper studies logical systems and their properties. Semantics and reasoning: The work involves formal reasoning about logical implications. Other categories like Formal languages are less relevant as the focus is on logical systems rather than language structures.,"Abstraction:0.2,Automated reasoning:0.3,Constraint and logic programming:0.1,Constructive mathematics:0.4,Description logics:0.1,Equational logic and rewriting:0.1,Finite Model Theory:0.1,Higher order logic:0.2,Hoare logic:0.1,Linear logic:0.1,Logic and verification:0.3,Modal and temporal logics:0.8,Program constructs:0.1,Program reasoning:0.2,Program semantics:0.1,Programming logic:0.3,Proof theory:1.0,Separation logic:0.1,Type theory:0.2,Verification by model checking:0.3","Proof theory,Modal and temporal logics",Proof theory is highly relevant as the paper focuses on natural deduction systems and proof techniques. Modal and temporal logics are relevant due to the discussion of implicational theses and logical axioms. Other categories like Automated reasoning or Programming logic are less directly related to the core proof-theoretic contribution.
3303,Contract-Based Verification of Hierarchical Systems of Components,"In this paper, we add to the usual notion of contract a structural part specifying the composition operator used to compose the component and its environment. We provide a framework for compositional verification including a proof rule for dominance between contracts based on apparent circular reasoning. We also briefly describe a consistency condition and a method based on assumption generation to generate or refine contracts.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.8,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Theory of computation,Theory of computation: Discusses formal verification methods for component hierarchies. Other categories lack direct theoretical focus.,"Semantics and reasoning:1.0,Design and analysis of algorithms:0.8,Computational complexity and cryptography:0.0,Formal languages and automata theory:0.0,Logic:0.0,Models of computation:0.0,Randomness, geometry and discrete structures:0.0,Theory and algorithms for application domains:0.0",Semantics and reasoning,Semantics and reasoning: The paper introduces contract-based verification with formal reasoning. Design and analysis of algorithms is secondary as the focus is on verification rather than algorithm design.,"Program constructs:0,Program reasoning:1,Program semantics:0.5","Program reasoning,Program semantics","Program reasoning is the core focus, as the paper introduces a framework for compositional verification and proof rules. Program semantics is relevant for defining contract structures. Program constructs are not discussed as the focus is on verification techniques rather than new constructs."
1665,What lies between Design Intent Coverage and Model Checking?,"Practitioners of formal property verification often work around the capacity limitations of formal verification tools by breaking down properties into smaller properties that can be checked on the sub-modules of the parent module. To support this methodology, we have developed a formal methodology for verifying whether the decomposition is indeed sound and complete, that is, whether verifying the smaller properties on the submodules actually guarantees the original property on the parent module. In practice, however designers do not write properties for all modules and thereby our previous methodology was applicable to selected cases only. In this paper we present new formal methods that allow us to handle RTL blocks in the analysis. We believe that the new approach will significantly widen the scope of the methodology, thereby enabling the validation engineer to handle much larger designs than admitted by existing formal verification tools","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.3,Theory of computation:0.8,Mathematics of computing:0.2,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Theory of computation,Theory of computation is relevant for formal verification methods discussed in the paper. 'Software and its engineering' is partially relevant but secondary as the focus is on formal methodology rather than software systems.,"Computational complexity and cryptography:0.1,Design and analysis of algorithms:0.8,Formal languages and automata theory:0.9,Logic:0.7,Models of computation:0.7,Randomness, geometry and discrete structures:0.3,Semantics and reasoning:0.9,Theory and algorithms for application domains:0.6","Formal languages and automata theory,Semantics and reasoning,Design and analysis of algorithms",Formal languages and automata theory is relevant for property decomposition. Semantics and reasoning applies to formal verification. Design and analysis of algorithms is relevant for the methodology's scalability. Logic and Models of computation are less directly related as the focus is on practical verification rather than theoretical models.,"Program reasoning:1.0,Formalisms:0.8,Algorithm design techniques:0.5,Concurrent algorithms:0.3,Distributed algorithms:0.2,Graph algorithms analysis:0.1,Program constructs:0.1,Regular languages:0.0,Online algorithms:0.0,Parameterized complexity and exact algorithms:0.0,Tree languages:0.0,Automata over infinite objects:0.0,Automata extensions:0.0,Grammars and context-free languages:0.0,Streaming, sublinear and near linear time algorithms:0.0","Program reasoning,Formalisms",Program reasoning is core to formal verification methodologies. Formalisms are relevant for the decomposition framework. Other options like algorithm design techniques are secondary to the paper's focus on formal methods.
1440,Bidirectional Contextual Resolution,"This paper describes a formalism and implementation for the interpretation and generation of sentences containing context-dependent constructs like determiners, pronouns, focus, and ellipsis. A variant of quasi-logical form is used as an under specified meaning representation, related to resolved logical forms via conditional equivalences. These equivalences define the interpretation of contextually dependent constructs with respect to a given context. Higher-order unification and abduction are used in relating expressions to contexts. The conditional equivalences can be used unchanged in both the interpretation and the generation direction.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:1.0,Mathematics of computing:0.3,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Theory of computation,"Theory of computation: The work involves formal methods (higher-order unification, abductive reasoning) and logical frameworks for bidirectional context resolution. Other fields like Networks or Software Engineering are not central to the theoretical contributions.","Computational complexity and cryptography:0.1,Design and analysis of algorithms:0.3,Formal languages and automata theory:0.1,Logic:0.8,Models of computation:0.2,Randomness, geometry and discrete structures:0.1,Semantics and reasoning:0.7,Theory and algorithms for application domains:0.1","Logic,Semantics and reasoning",Logic: Uses quasi-logical forms for context resolution. Semantics and reasoning: Addresses interpretation and generation via abduction. Other categories are not core to the formal semantics and reasoning framework.,"Abstraction:0,Automated reasoning:0,Constraint and logic programming:1,Constructive mathematics:0,Description logics:0,Equational logic and rewriting:0,Finite Model Theory:0,Higher order logic:1,Hoare logic:0,Linear logic:0,Logic and verification:0.5,Modal and temporal logics:0,Program constructs:0,Program reasoning:0,Program semantics:0,Programming logic:0,Proof theory:0,Separation logic:0,Type theory:0,Verification by model checking:0","Constraint and logic programming,Higher order logic","Constraint and logic programming: The paper employs higher-order unification and abduction, central to logic programming. Higher order logic: The formalism uses quasi-logical forms and conditional equivalences in higher-order logic. Logic and verification was rejected as the focus is on generation/interpretation, not formal verification."
1845,"Global Constraint Catalog, Volume II, Time-Series Constraints",First this report presents a restricted set of finite transducers used to synthesise structural time-series constraints described by means of a multi-layered function composition scheme. Second it provides the corresponding synthesised catalogue of structural time-series constraints where each constraint is explicitly described in terms of automata with registers.,"General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.2,Theory of computation:0.9,Mathematics of computing:0.3,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Theory of computation,"Theory of computation: The paper introduces formal methods for synthesizing time-series constraints using automata and transducers, which are core theoretical constructs in computation. Software and its engineering (0.2) is secondary as the focus is on constraint modeling rather than software development.","Computational complexity and cryptography:0.1,Design and analysis of algorithms:0.7,Formal languages and automata theory:0.85,Logic:0.1,Models of computation:0.2,Randomness, geometry and discrete structures:0.1,Semantics and reasoning:0.1,Theory and algorithms for application domains:0.1","Formal languages and automata theory,Design and analysis of algorithms",Formal languages and automata theory is relevant because the paper introduces structural time-series constraints using finite transducers. Design and analysis of algorithms is relevant for the function composition scheme and constraint synthesis. Other theoretical categories are less directly connected to the paper's contributions.,"Algorithm design techniques:0.4,Approximation algorithms analysis:0.2,Automata extensions:0.9,Automata over infinite objects:0.5,Concurrent algorithms:0.1,Data structures design and analysis:0.2,Distributed algorithms:0.1,Formalisms:0.8,Grammars and context-free languages:0.3,Graph algorithms analysis:0.1,Mathematical optimization:0.3,Online algorithms:0.1,Parallel algorithms:0.1,Parameterized complexity and exact algorithms:0.1,Regular languages:0.6,Streaming, sublinear and near linear time algorithms:0.2,Tree languages:0.3","Automata extensions,Formalisms",Automata extensions is primary as the paper uses automata with registers to describe time-series constraints. Formalisms is relevant for the multi-layered function composition scheme. Other categories like regular languages are secondary.
2509,The complexity of fixed-parameter problems: guest column,We describe how to found and develop the theory of fixed-parameter complexity and hardness in a manner both fully analgous to the classical theory of NP-completeness and much simpler than the original development. We present some recent advance made using this approach.,"General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:1.0,Mathematics of computing:0.2,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.1,Applied computing:0.0,Social and professional topics:0.0",Theory of computation,"Theory of computation: The paper presents foundational work on fixed-parameter complexity theory, analogous to NP-completeness. Other categories are irrelevant as the paper focuses purely on theoretical computer science concepts.","Computational complexity and cryptography:1.0,Design and analysis of algorithms:0.5,Formal languages and automata theory:0.1,Logic:0.2,Models of computation:0.3,Randomness, geometry and discrete structures:0.1,Semantics and reasoning:0.1,Theory and algorithms for application domains:0.2",Computational complexity and cryptography,Computational complexity and cryptography is highly relevant as the paper discusses fixed-parameter complexity theory. Other algorithmic categories are less central since the focus is on complexity classification rather than specific algorithm design.,"Algebraic complexity theory:0,Circuit complexity:0,Communication complexity:0,Complexity classes:1,Complexity theory and logic:0.8,Cryptographic primitives:0,Cryptographic protocols:0,Interactive proof systems:0,Oracles and decision trees:0.2,Problems, reductions and completeness:1,Proof complexity:0,Quantum complexity theory:0","Complexity classes,Problems, reductions and completeness","Complexity classes: The paper discusses fixed-parameter complexity, a subfield of complexity classes. Problems, reductions and completeness: The study addresses hardness and completeness within fixed-parameter theory. Categories like Quantum complexity are irrelevant as the focus is on classical complexity theory."
5424,Simulating Dynamic Systems Using Linear Time Calculus Theories,"Abstract Dynamic systems play a central role in fields such as planning, verification, and databases. Fragmented throughout these fields, we find a multitude of languages to formally specify dynamic systems and a multitude of systems to reason on such specifications. Often, such systems are bound to one specific language and one specific inference task. It is troublesome that performing several inference tasks on the same knowledge requires translations of your specification to other languages. In this paper we study whether it is possible to perform a broad set of well-studied inference tasks on one specification. More concretely, we extend IDP3 with several inferences from fields concerned with dynamic specifications.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.9,Mathematics of computing:0.3,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Theory of computation,Theory of computation: The paper extends a system to perform logical inferences on dynamic specifications. Other categories like Software engineering are secondary to the theoretical focus.,"Computational complexity and cryptography:0.0,Design and analysis of algorithms:0.0,Formal languages and automata theory:1.0,Logic:0.75,Models of computation:0.0,Randomness, geometry and discrete structures:0.0,Semantics and reasoning:0.75,Theory and algorithms for application domains:0.0","Formal languages and automata theory,Semantics and reasoning","Formal languages and automata theory: The paper extends a formal system for dynamic specifications, which directly relates to formal languages. Semantics and reasoning: The focus on reasoning about dynamic systems aligns with formal semantics. Other categories (e.g., algorithms, cryptography) are irrelevant as the paper studies formal logic, not algorithmic design or complexity.","Automata extensions:0.3,Automata over infinite objects:0.4,Formalisms:1.0,Grammars and context-free languages:0.2,Program constructs:0.3,Program reasoning:1.0,Program semantics:0.4,Regular languages:0.2,Tree languages:0.3","Formalisms,Program reasoning",Formalisms: The paper studies dynamic system formalisms using linear time calculus. Program reasoning: Extends IDP3 to perform dynamic system reasoning. Other categories are less relevant as the focus is on formal methods rather than automata or grammars.
928,On the capability of finite automata in 2 and 3 dimensional space,"The paper describes two algorithms for threading unknown, finite directed Eulerian mazes. Each of these algorithms is performed by a traveling robot whose control is a finite-state automaton. It is assumed that each vertex has a circular list of its outgoing edges. The items of this list are called exits. Each of the algorithms puts in one of the exits of each vertex a scan pebble. These pebbles can be used by a simple robot as traffic signals, which allow it to traverse an Eulerian cycle of the maze. For a directed graph (maze) G(V, E), the simple algorithm performs O(|V | · |E|) edge traversals, while the advanced algorithm traverses every edge three times. Let dout(v) be the out-degree of vertex v. The algorithms use, at each vertex v, a local memory of size O(log dout(v)). Communicated by S. Khuller: submitted January 2002; revised June 2002 Work by S. Even supported by the Fund for the Promotion of Research at the Technion. S. Bhatt et al., Traversing Eulerian Mazes, JGAA, 6(2) 157–173 (2002) 158","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:1.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Theory of computation,Theory of computation is directly relevant as the paper analyzes finite automata in multi-dimensional spaces. Other categories are irrelevant as the focus is on theoretical algorithms.,"Computational complexity and cryptography:0.1,Design and analysis of algorithms:0.9,Formal languages and automata theory:1.0,Logic:0.1,Models of computation:0.75,Randomness, geometry and discrete structures:0.1,Semantics and reasoning:0.1,Theory and algorithms for application domains:0.1","Formal languages and automata theory,Design and analysis of algorithms","Formal languages and automata theory: Focuses on finite automata for maze threading. Design and analysis of algorithms: Analyzes algorithmic complexity (O(|V|·|E|) and edge traversal counts). Other categories (e.g., logic, semantics) are irrelevant as the paper centers on automata and algorithm design, not abstract reasoning or cryptographic complexity.","Algorithm design techniques:0.5,Approximation algorithms analysis:0.3,Automata extensions:0.9,Automata over infinite objects:0.2,Concurrent algorithms:0.1,Data structures design and analysis:0.2,Distributed algorithms:0.2,Formalisms:0.3,Grammars and context-free languages:0.2,Graph algorithms analysis:0.7,Mathematical optimization:0.4,Online algorithms:0.1,Parallel algorithms:0.2,Parameterized complexity and exact algorithms:0.1,Regular languages:0.3,Streaming, sublinear and near linear time algorithms:0.1,Tree languages:0.2","Automata extensions,Graph algorithms analysis","Automata extensions: The paper focuses on finite automata algorithms for maze traversal. Graph algorithms analysis: The maze is modeled as a graph, and traversal algorithms are analyzed. Other categories like Regular languages are less central as the focus is on automata behavior rather than language theory."
2426,A Characterization of Testable Hypergraph Properties,"We provide a combinatorial characterization of all testable properties of k-graphs (i.e. k-uniform hypergraphs). Here, a k-graph property P is testable if there is a randomized algorithm which makes a bounded number of edge queries and distinguishes with probability 2/3 between k-graphs that satisfy P and those that are far from satisfying P. For the 2-graph case, such a combinatorial characterization was obtained by Alon, Fischer, Newman and Shapira. Our results for the k-graph setting are in contrast to those of Austin and Tao, who showed that for the somewhat stronger concept of local repairability, the testability results for graphs do not extend to the 3-graph setting.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.9,Mathematics of computing:0.6,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Theory of computation,"Theory of computation: The paper provides a combinatorial characterization of testable hypergraph properties, a core topic in computational complexity and algorithm design. Mathematics of computing is secondary due to the focus on theoretical foundations.","Computational complexity and cryptography:0.75,Design and analysis of algorithms:1.0,Formal languages and automata theory:0.2,Logic:0.2,Models of computation:0.3,Randomness, geometry and discrete structures:0.4,Semantics and reasoning:0.2,Theory and algorithms for application domains:0.3","Design and analysis of algorithms,Computational complexity and cryptography",Design and analysis of algorithms is relevant due to the focus on hypergraph property testing algorithms. Computational complexity is relevant as the paper addresses testability concepts. Other categories like Formal languages are irrelevant as the work does not involve language theory.,"Algebraic complexity theory:0.1,Algorithm design techniques:0.6,Approximation algorithms analysis:0.3,Circuit complexity:0.1,Communication complexity:0.1,Complexity classes:0.2,Complexity theory and logic:0.3,Concurrent algorithms:0.1,Cryptographic primitives:0.1,Cryptographic protocols:0.1,Data structures design and analysis:0.4,Distributed algorithms:0.2,Graph algorithms analysis:1.0,Interactive proof systems:0.1,Mathematical optimization:0.3,Online algorithms:0.2,Oracles and decision trees:0.1,Parallel algorithms:0.2,Parameterized complexity and exact algorithms:0.2,Problems, reductions and completeness:0.3,Proof complexity:0.1,Quantum complexity theory:0.1,Streaming, sublinear and near linear time algorithms:1.0","Graph algorithms analysis,Streaming, sublinear and near linear time algorithms",Graph algorithms analysis is core as the paper studies hypergraph property testing. Streaming algorithms is relevant due to the sublinear query complexity focus. Other algorithm categories are less specific to the core contribution.
4899,Effort Games and the Price of Myopia,"We consider Effort Games, a game‐theoretic model of cooperation in open environments, which is a variant of the principal‐agent problem from economic theory. In our multiagent domain, a common project depends on various tasks; carrying out certain subsets of the tasks completes the project successfully, while carrying out other subsets does not. The probability of carrying out a task is higher when the agent in charge of it exerts effort, at a certain cost for that agent. A central authority, called the principal, attempts to incentivize agents to exert effort, but can only reward agents based on the success of the entire project.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.2,Theory of computation:0.8,Mathematics of computing:0.2,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Theory of computation,Theory of computation is highly relevant as the paper presents a game-theoretic model for multiagent systems. Other fields like Applied computing or Software engineering are not central to the theoretical contribution.,"Computational complexity and cryptography:0.1,Design and analysis of algorithms:1.0,Formal languages and automata theory:0.1,Logic:0.1,Models of computation:0.1,Randomness, geometry and discrete structures:0.1,Semantics and reasoning:0.1,Theory and algorithms for application domains:0.8","Design and analysis of algorithms,Theory and algorithms for application domains",Design and analysis of algorithms is highly relevant for the game-theoretic analysis of effort games. Theory and algorithms for application domains is secondary due to economic applications.,"Algorithm design techniques:0,Algorithmic game theory and mechanism design:1,Approximation algorithms analysis:0,Concurrent algorithms:0,Data structures design and analysis:0,Database theory:0,Distributed algorithms:0,Graph algorithms analysis:0,Machine learning theory:0,Mathematical optimization:1,Online algorithms:0,Parallel algorithms:0,Parameterized complexity and exact algorithms:0,Streaming, sublinear and near linear time algorithms:0","Algorithmic game theory and mechanism design,Mathematical optimization",Algorithmic game theory and mechanism design are relevant as the paper introduces a game-theoretic model for effort allocation. Mathematical optimization is relevant due to the focus on optimizing task probabilities. Other options are unrelated to the core theoretical framework.
4918,Model Matching for Asynchronous Sequential Machines with Uncontrollable Inputs,"The problem of model matching for finite-state asynchronous sequential machines is examined. In particular, the considered asynchronous machine may receive uncontrollable external inputs, i.e., of which values the controller cannot change or disable. For realizing model matching with a reference model, the asynchronous machine must have additional reachability to deal with transitions by uncontrollable inputs. Necessary and sufficient conditions for the existence of an appropriate controller are given in terms of a reachability relation between the machine and the model. A characterization of feasible control laws is derived and algorithms for their design are outlined.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:1.0,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Theory of computation,"Theory of computation: The paper examines model matching for asynchronous sequential machines with uncontrollable inputs. Other categories are irrelevant as the focus is on formal verification and control theory, not software, hardware, or applications.","Computational complexity and cryptography:0.1,Design and analysis of algorithms:0.4,Formal languages and automata theory:1.0,Logic:0.2,Models of computation:0.8,Randomness, geometry and discrete structures:0.3,Semantics and reasoning:0.1,Theory and algorithms for application domains:0.3","Formal languages and automata theory,Models of computation",Formal languages and automata theory is central to the asynchronous machine analysis. Models of computation is relevant for the controller design. Other categories like Computational complexity are not directly related to the control-theoretic contributions.,"Abstract machines:0.8,Automata extensions:1.0,Automata over infinite objects:0.3,Computability:0.2,Concurrency:0.4,Formalisms:0.5,Grammars and context-free languages:0.2,Interactive computation:0.3,Probabilistic computation:0.2,Quantum computation theory:0.1,Regular languages:0.2,Streaming models:0.1,Timed and hybrid models:0.3,Tree languages:0.4","Automata extensions,Abstract machines",Automata extensions is directly relevant for the model matching problem in asynchronous machines. Abstract machines is relevant for the theoretical framework. Other categories like Concurrency are less specific to the paper's focus on control theory for automata.
689,Alternating Towers and Piecewise Testable Separators,"Two languages are separable by a piecewise testable language if and only if there exists no infinite tower between them. An infinite tower is an infinite sequence of strings alternati ng between the two languages such that every string is a subsequence (scattered substring) of all the strings that f ollow. For regular languages represented by nondeterministic finite automata, the existence of an infinite tower is decidab le in polynomial time. In this paper, we investigate the complexity of a particular method to compute a piecewise testable separator. We show that it is closely related to the height of maximal finite towers, and provide the upper and lower bounds with respect to the size of the given nondeterministic automata. Specifically, we show that the u pper bound is polynomial with respect to the number of states with the cardinality of the alphabet in the exponen t. Concerning the lower bound, we show that towers of exponential height with respect to the cardinality of the alphabet exist. Since these towers mostly turn out to be sequences of prefixes, we also provide a comparison with towe rs of prefixes.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:1.0,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Theory of computation,Theory of computation is directly relevant for the formal language and automata analysis. Other categories are rejected because the paper focuses on theoretical computer science rather than applied methods.,"Computational complexity and cryptography:0.3,Design and analysis of algorithms:0.7,Formal languages and automata theory:0.9,Logic:0.2,Models of computation:0.4,Randomness, geometry and discrete structures:0.2,Semantics and reasoning:0.3,Theory and algorithms for application domains:0.3","Formal languages and automata theory,Design and analysis of algorithms",Formal languages and automata theory is relevant due to the focus on infinite towers and separators. Design and analysis of algorithms is relevant for the optimization framework. Other categories are less directly related to the theoretical analysis of automata.,"Algorithm design techniques:0.2,Approximation algorithms analysis:0.2,Automata extensions:1,Automata over infinite objects:0.2,Concurrent algorithms:0.2,Data structures design and analysis:0.2,Distributed algorithms:0.2,Formalisms:0.2,Grammars and context-free languages:0.2,Graph algorithms analysis:0.2,Mathematical optimization:0.2,Online algorithms:0.2,Parallel algorithms:0.2,Parameterized complexity and exact algorithms:0.2,Regular languages:1,Streaming, sublinear and near linear time algorithms:0.2,Tree languages:0.2","Automata extensions,Regular languages",Automata extensions: Analyzes towers and separators in automata theory. Regular languages: Core focus on piecewise testable languages. Other options lack direct relevance to the theoretical contributions.
1892,On the complexity of numerical analysis,"We study two quite different approaches to understanding the complexity of fundamental problems in numerical analysis. We show that both hinge on the question of understanding the complexity of the following problem, which we call PosSLP; given a division-free straight-line program producing an integer N, decide whether N > 0. We show that PosSLP lies in the counting hierarchy, and combining our results with work of Tiwari, we show that the Euclidean traveling salesman problem lies in the counting hierarchy - the previous best upper bound for this important problem (in terms of classical complexity classes) being PSPACE","General and reference:0.2,Hardware:0.2,Computer systems organization:0.2,Networks:0.2,Software and its engineering:0.2,Theory of computation:1.0,Mathematics of computing:0.7,Information systems:0.2,Security and privacy:0.2,Human-centered computing:0.2,Computing methodologies:0.3,Applied computing:0.2,Social and professional topics:0.2",Theory of computation,Theory of computation is highly relevant because the paper studies the computational complexity of numerical problems like PosSLP and Euclidean TSP. Mathematics of computing is moderately relevant due to the mathematical analysis. Other categories are irrelevant as the paper does not address applied computing or security.,"Computational complexity and cryptography:0.8,Design and analysis of algorithms:0.6,Formal languages and automata theory:0.2,Logic:0.2,Models of computation:0.2,Randomness, geometry and discrete structures:0.2,Semantics and reasoning:0.2,Theory and algorithms for application domains:0.2","Computational complexity and cryptography,Design and analysis of algorithms",Computational complexity and cryptography is central to the PosSLP complexity analysis. Design and analysis of algorithms applies to numerical analysis algorithms. Other fields lack direct relevance to complexity theory.,"Algebraic complexity theory:0.5,Algorithm design techniques:0.8,Approximation algorithms analysis:0.4,Circuit complexity:0.3,Communication complexity:0.2,Complexity classes:1.0,Complexity theory and logic:0.7,Concurrent algorithms:0.2,Cryptographic primitives:0.1,Cryptographic protocols:0.1,Data structures design and analysis:0.3,Distributed algorithms:0.2,Graph algorithms analysis:0.3,Interactive proof systems:0.1,Mathematical optimization:0.2,Online algorithms:0.2,Oracles and decision trees:0.2,Parallel algorithms:0.2,Parameterized complexity and exact algorithms:0.3,Problems, reductions and completeness:0.6,Proof complexity:0.3,Quantum complexity theory:0.2,Streaming, sublinear and near linear time algorithms:0.2","Complexity classes,Algorithm design techniques",Complexity classes are central to analyzing PosSLP's placement in the counting hierarchy. Algorithm design techniques (0.8) are relevant for the methods used in the analysis.
2869,The power of local self-reductions,Identify a string x over.,"General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.7,Mathematics of computing:0.3,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.4,Applied computing:0.1,Social and professional topics:0.1",Theory of computation,Theory of computation: Likely relates to computational complexity or algorithmic reductions. Other fields are irrelevant due to the incomplete abstract and lack of clear context.,"Computational complexity and cryptography:0.85,Design and analysis of algorithms:0.3,Formal languages and automata theory:0.1,Logic:0.1,Models of computation:0.1,Randomness, geometry and discrete structures:0.3,Semantics and reasoning:0.1,Theory and algorithms for application domains:0.1",Computational complexity and cryptography,Computational complexity and cryptography: The title and abstract suggest complexity-theoretic concepts (local self-reductions). Other categories like algorithms are secondary due to incomplete abstract.,"Algebraic complexity theory:0,Circuit complexity:0,Communication complexity:0,Complexity classes:0,Complexity theory and logic:0,Cryptographic primitives:0,Cryptographic protocols:0,Interactive proof systems:0,Oracles and decision trees:0,Problems, reductions and completeness:1,Proof complexity:0,Quantum complexity theory:0","Problems, reductions and completeness","Problems, reductions and completeness is relevant for self-reduction analysis. No other options align with the incomplete abstract's focus."
5575,An Efficient Method for Constructing a Distributed Depth-First Search Tree,"We present an efficient distributed depth-first search algorithm. The algorithm in the worst case requires (1 + r)(|V | − 1) messages and (1 + r)(|V | − 1) units of time, where 0 ≤ r < 1 and |V | is the number of sites in the network. The value of r depends on the search path and the graph topology. In the best case when r = 0, the algorithm requires only |V | − 1 messages and time units. This new algorithm improves over the best of previous algorithms by using a stack-type structure which enables better dynamic backtracking. The improvement in worst case complexity is only minor. However it is much better in practice, manifested by significantly smaller values of r.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.2,Networks:0.1,Software and its engineering:0.0,Theory of computation:0.9,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Theory of computation,Theory of computation is highly relevant as the paper presents a distributed depth-first search algorithm with theoretical guarantees. Other categories like Computer systems organization (0.2) or Networks (0.1) are not central to the algorithm's design.,"Computational complexity and cryptography:0.0,Design and analysis of algorithms:1.0,Formal languages and automata theory:0.0,Logic:0.0,Models of computation:0.0,Randomness, geometry and discrete structures:0.0,Semantics and reasoning:0.0,Theory and algorithms for application domains:0.0",Design and analysis of algorithms,Design and analysis of algorithms is directly relevant for the distributed depth-first search algorithm's performance analysis. Other categories like cryptography or logic are unrelated to the paper's focus on algorithmic efficiency.,"Algorithm design techniques:0.8,Distributed algorithms:1.0,Graph algorithms analysis:0.9,Approximation algorithms analysis:0.2,Concurrent algorithms:0.3,Data structures design and analysis:0.4,Mathematical optimization:0.3,Online algorithms:0.2,Parallel algorithms:0.3,Parameterized complexity and exact algorithms:0.2,Streaming, sublinear and near linear time algorithms:0.2","Distributed algorithms,Graph algorithms analysis,Algorithm design techniques",Distributed algorithms: Paper presents a distributed DFS algorithm. Graph algorithms analysis: DFS is a graph traversal algorithm. Algorithm design techniques: Discusses stack-based design for backtracking.
1755,Approximation algorithms for the covering Steiner problem,"The covering Steiner problem is a generalization of both the k‐MST and the group Steiner problems: given an edge‐weighted graph, with subsets of vertices called the groups, and a nonnegative integer value (called the requirement) for each group, the problem is to find a minimum‐weight tree spanning at least the required number of vertices of every group. When all requirements are equal to 1, this becomes the group Steiner problem, while if there is only one group which contains all vertices of the graph the problem reduces to k‐MST with k equal to the requirement of this unique group. We discuss two different (but equivalent) linear relaxations of the problem for the case when the given graph is a tree and construct polylogarithmic approximation algorithms based on randomized LP rounding of these relaxations. By using a probabilistic approximation of general metrics by tree metrics due to Bartal, our algorithms also solve the covering Steiner problem on general graphs with a further polylogarithmic worsening in the approximation ratio. © 2002 Wiley Periodicals, Inc. Random Struct. Alg., 20:465–483, 2002","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:1.0,Mathematics of computing:0.5,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Theory of computation,Theory of computation is highly relevant as the paper presents approximation algorithms for a generalization of classical problems. Mathematics of computing receives moderate relevance for the algorithm analysis. Other categories are irrelevant as the focus is on computational complexity and algorithm design.,"Computational complexity and cryptography:0.0,Design and analysis of algorithms:1.0,Formal languages and automata theory:0.0,Logic:0.0,Models of computation:0.0,Randomness, geometry and discrete structures:0.75,Semantics and reasoning:0.0,Theory and algorithms for application domains:0.5",Design and analysis of algorithms,"Design and analysis of algorithms is directly relevant as the paper focuses on approximation algorithms for the covering Steiner problem. Randomness, geometry, and discrete structures receive a moderate score due to the use of LP rounding and probabilistic methods. Theory and algorithms for application domains are partially relevant due to the practical problem context.","Approximation algorithms analysis:1,Algorithm design techniques:1,Graph algorithms analysis:0.8,Concurrent algorithms:0.1,Data structures design and analysis:0.1,Distributed algorithms:0.1,Mathematical optimization:0.1,Online algorithms:0.1,Parallel algorithms:0.1,Parameterized complexity and exact algorithms:0.1,Streaming, sublinear and near linear time algorithms:0.1","Algorithm design techniques,Approximation algorithms analysis",Algorithm design techniques and Approximation algorithms analysis are both highly relevant as the paper develops new approximation algorithms for covering Steiner problems. Graph algorithms analysis is moderately relevant as the problem is graph-based but not the primary focus.
2351,Two improved differential evolution schemes for faster global search,"Differential evolution (DE) is well known as a simple and efficient scheme for global optimization over continuous spaces. In this paper we present two new, improved variants of DE. Performance comparisons of the two proposed methods are provided against (a) the original DE, (b) the canonical particle swarm optimization (PSO), and (c) two PSO-variants. The new DE-variants are shown to be statistically significantly better on a seven-function test bed for the following performance measures: solution quality, time to find the solution, frequency of finding the solution, and scalability.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:1.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Theory of computation,Theory of computation is relevant as the paper presents improved differential evolution schemes for global optimization. Categories like Applied computing are irrelevant as the focus is on theoretical algorithmic improvements rather than specific applications.,"Computational complexity and cryptography:0.0,Design and analysis of algorithms:0.9,Formal languages and automata theory:0.0,Logic:0.0,Models of computation:0.0,Randomness, geometry and discrete structures:0.0,Semantics and reasoning:0.0,Theory and algorithms for application domains:0.0",Design and analysis of algorithms,Design and analysis of algorithms is relevant as the paper presents and evaluates improved differential evolution algorithms. Other categories are not directly related to the algorithmic contributions.,"Algorithm design techniques:1,Approximation algorithms analysis:0,Concurrent algorithms:0,Data structures design and analysis:0,Distributed algorithms:0,Graph algorithms analysis:0,Mathematical optimization:1,Online algorithms:0,Parallel algorithms:0,Parameterized complexity and exact algorithms:0,Streaming, sublinear and near linear time algorithms:0","Algorithm design techniques,Mathematical optimization",Algorithm design techniques is relevant for proposing improved DE variants. Mathematical optimization is relevant as the paper addresses global optimization. Other children are irrelevant due to focus on non-optimization algorithm design.
498,Function level programs as mathematical objects,"Most programs written today are “object-level” programs. That is, programs describe how to combine various “objects” (i.e., numbers, symbols, arrays, etc.) to form other objects until the final “result objects” have been formed. New objects are constructed from existing ones by the application of various object-to-object functions such as + or matrix inversion.
 Conventional, von Neumann programs are object level; “expressions” on the right side of assignment statements are exclusively concerned with building an object that is then to be stored. Lambda calculus based languages, such as LISP and ISWIM [Landin 66], are also, in practice, object level languages, although they have the means to be more.","General and reference:0,Hardware:0,Computer systems organization:0,Networks:0,Software and its engineering:0,Theory of computation:1,Mathematics of computing:0.5,Information systems:0,Security and privacy:0,Human-centered computing:0,Computing methodologies:0,Applied computing:0,Social and professional topics:0",Theory of computation,'Theory of computation' is highly relevant as the paper discusses programming paradigms and theoretical foundations. 'Mathematics of computing' is partially relevant but secondary to the core focus on computation theory.,"Computational complexity and cryptography:0.1,Design and analysis of algorithms:0.3,Formal languages and automata theory:0.2,Logic:0.2,Models of computation:1.0,Randomness, geometry and discrete structures:0.1,Semantics and reasoning:0.2,Theory and algorithms for application domains:0.1",Models of computation,Models of computation is the primary category as the paper analyzes object-level vs function-level programming paradigms and their theoretical foundations. Other categories like 'Design and analysis of algorithms' are less relevant since the focus is on computational models rather than specific algorithm design.,"Abstract machines:0.6,Computability:0.5,Concurrency:0.1,Interactive computation:0.1,Probabilistic computation:0.1,Quantum computation theory:0.1,Streaming models:0.1,Timed and hybrid models:0.1","Abstract machines,Computability","Abstract machines and Computability are relevant as the paper discusses the theoretical structure of programs as mathematical objects, aligning with foundational CS concepts."
2697,Planning under Risk and Knightian Uncertainty,Two noteworthy models of planning in AI are probabilistic planning (based on MDPs and its generalizations) and nondeterministic planning (mainly based on model checking). In this paper we: (1) show that probabilistic and nondeterministic planning are extremes of a rich continuum of problems that deal simultaneously with risk and (Knightian) uncertainty; (2) obtain a unifying model for these problems using imprecise MDPs; (3) derive a simplified Bellman's principle of optimality for our model; and (4) show how to adapt and analyze state-of-art algorithms such as (L)RTDP and LDFS in this unifying setup. We discuss examples and connections to various proposals for planning under (general) uncertainty.,"General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.2,Theory of computation:0.8,Mathematics of computing:0.3,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.5,Applied computing:0.2,Social and professional topics:0.1",Theory of computation,"Theory of computation: The paper unifies probabilistic and nondeterministic planning under imprecise MDPs, a theoretical AI model. Other categories are irrelevant as the focus is on foundational algorithmic models, not applied systems.","Computational complexity and cryptography:0.0,Design and analysis of algorithms:1.0,Formal languages and automata theory:0.0,Logic:0.0,Models of computation:0.0,Randomness, geometry and discrete structures:0.0,Semantics and reasoning:0.0,Theory and algorithms for application domains:1.0","Design and analysis of algorithms,Theory and algorithms for application domains",Design and analysis of algorithms is relevant for the unifying model and algorithm analysis. Theory and algorithms for application domains is relevant for the application in planning under uncertainty. Other fields are not directly relevant.,"Algorithm design techniques:0.9,Algorithmic game theory and mechanism design:0.1,Approximation algorithms analysis:0.2,Concurrent algorithms:0.1,Data structures design and analysis:0.2,Database theory:0.1,Distributed algorithms:0.2,Graph algorithms analysis:0.1,Machine learning theory:0.3,Mathematical optimization:0.5,Online algorithms:0.2,Parallel algorithms:0.1,Parameterized complexity and exact algorithms:0.1,Streaming, sublinear and near linear time algorithms:0.1","Algorithm design techniques,Mathematical optimization",Algorithm design techniques: The paper introduces a unified imprecise MDP framework for planning under risk and uncertainty. Mathematical optimization: The Bellman principle derivation and algorithm adaptation involve optimization concepts.
6029,Probabilistic ABox Reasoning: Preliminary Results,"Most probabilistic extensions of description logics focus on the terminological apparatus. While some allow for expressing probabilistic knowledge about concept assertions, systems which can express probabilistic knowledge about role assertions have received very little attention as yet. We present a system PALC which allows us to express degrees of belief in concept and role assertions for individuals. We introduce syntax and semantics for PALC and we define the corresponding reasoning problem. An independence assumption regarding the assertions for different individuals yields additional constraints on the possible interpretations. This considerably reduces the solution space of the PALC reasoning problem.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.9,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Theory of computation,"Theory of computation: The paper presents PALC, a probabilistic logic system for reasoning, which is a theoretical contribution. Software and its engineering is secondary as the focus is on formal systems.","Computational complexity and cryptography:0.0,Design and analysis of algorithms:0.0,Formal languages and automata theory:0.0,Logic:0.75,Models of computation:0.0,Randomness, geometry and discrete structures:0.0,Semantics and reasoning:1.0,Theory and algorithms for application domains:0.0","Semantics and reasoning,Logic",Semantics and reasoning is highly relevant for probabilistic description logics. Logic is relevant for the formal framework of PALC. Other categories like Computational complexity are less directly related to the paper’s focus on probabilistic reasoning.,"Abstraction:0,Automated reasoning:0.8,Constraint and logic programming:0.3,Constructive mathematics:0,Description logics:1,Equational logic and rewriting:0.1,Finite Model Theory:0.2,Higher order logic:0.1,Hoare logic:0,Linear logic:0,Logic and verification:0.4,Modal and temporal logics:0.2,Program constructs:0.1,Program reasoning:0.3,Program semantics:0.1,Programming logic:0.2,Proof theory:0.1,Separation logic:0.1,Type theory:0.2,Verification by model checking:0.1","Description logics,Automated reasoning",Description logics is highly relevant as the paper introduces a probabilistic extension of description logics (Palc). Automated reasoning is moderately relevant due to the focus on reasoning problems and solution constraints. Other categories like type theory are not directly addressed.
2153,Proof Support for Common Logic,"We present an extension of the Heterogeneous Tool Set HETS that enables proof support for Common Logic. This is achieved via logic translations that relate Common Logic and some of its sublogics to already supported logics and automated theorem proving systems. We thus provide the first full theorem proving support for Common Logic, including the possibility of verifying meta-theoretical relationships between Common Logic theories.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.9,Mathematics of computing:0.2,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Theory of computation,Theory of computation is relevant as the paper focuses on formal logic proof systems and theorem proving. Other categories like Software are not the core contribution.,"Logic:1.0,Semantics and reasoning:0.75,Formal languages and automata theory:0.25,Design and analysis of algorithms:0.25,Computational complexity and cryptography:0.25,Models of computation:0.25,Randomness, geometry and discrete structures:0.25,Theory and algorithms for application domains:0.25","Logic,Semantics and reasoning",Logic is highly relevant as the paper extends theorem proving for Common Logic. Semantics and reasoning is partially relevant for verification of theories. Other options are irrelevant as the paper does not address formal languages or complexity.,"Abstraction:0.0,Automated reasoning:1.0,Constraint and logic programming:0.0,Constructive mathematics:0.0,Description logics:0.3,Equational logic and rewriting:0.0,Finite Model Theory:0.0,Higher order logic:0.2,Hoare logic:0.0,Linear logic:0.0,Logic and verification:0.0,Modal and temporal logics:0.0,Program constructs:0.0,Program reasoning:0.0,Program semantics:0.0,Programming logic:0.0,Proof theory:0.5,Separation logic:0.0,Type theory:0.0,Verification by model checking:0.0","Automated reasoning,Proof theory",Automated reasoning: The paper introduces theorem proving support for Common Logic. Proof theory: The work involves formal verification of meta-theoretical relationships. Other options like description logics are less directly relevant.
2032,An Implicit Recursive Language for the Polynomial Time-space Complexity Classes,"We de ne a language over an algebra of words by means of a version of predicative recursion, and we prove that it represents a resource-free characterization of the computations performed by a register machine in time O(n), for each nite k; starting from this result, and by means of a restricted form of composition, we give a characterization of the computations of a register machine with a polynomial bound simultaneously imposed over time and space complexity.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.95,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Theory of computation,Theory of computation is highly relevant for the resource-free characterization of polynomial time-space complexity classes. Other categories like Mathematics of computing are not central to the theoretical framework.,"Computational complexity and cryptography:0.9,Design and analysis of algorithms:0.75,Formal languages and automata theory:0.3,Logic:0.2,Models of computation:0.4,Randomness, geometry and discrete structures:0.1,Semantics and reasoning:0.2,Theory and algorithms for application domains:0.3","Computational complexity and cryptography,Design and analysis of algorithms",Computational complexity and cryptography directly address polynomial time-space characterization. Design and analysis of algorithms align with resource-bounded computation. Other categories like Logic or Formal languages are less relevant to the core complexity-theoretic focus.,"Algebraic complexity theory:0.2,Algorithm design techniques:0.3,Approximation algorithms analysis:0.1,Circuit complexity:0.1,Communication complexity:0.1,Complexity classes:0.9,Complexity theory and logic:0.6,Concurrent algorithms:0.1,Cryptographic primitives:0.1,Cryptographic protocols:0.1,Data structures design and analysis:0.1,Distributed algorithms:0.1,Graph algorithms analysis:0.1,Interactive proof systems:0.1,Mathematical optimization:0.2,Online algorithms:0.1,Oracles and decision trees:0.1,Parallel algorithms:0.1,Parameterized complexity and exact algorithms:0.1,Problems, reductions and completeness:0.4,Proof complexity:0.2,Quantum complexity theory:0.1,Streaming, sublinear and near linear time algorithms:0.2","Complexity classes,Complexity theory and logic",Complexity classes: The paper provides a resource-free characterization of polynomial time-space complexity. Complexity theory and logic: The logical framework for predicative recursion aligns with theoretical foundations. Other children like algorithm design are less central to the complexity-theoretic contribution.
960,An Algorithm to Construct Greedy Drawings of Triangulations,"We show an algorithm to construct a greedy drawing of every given triangulation. The algorithm relies on two main results. First, we show how to construct greedy drawings of a fairly simple class of graphs, called triangulated binary cactuses. Second, we show that every triangulation can be spanned by a triangulated binary cactus. Further, we discuss how to extend our techniques in order to prove that every triconnected planar graph admits a greedy drawing. Such a result, which proves a conjecture by Papadimitriou and Ratajczak, was independently shown by Leighton and Moitra.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.9,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Theory of computation,Theory of computation is highly relevant as the paper presents theoretical algorithms for graph drawing and connectivity proofs. Other areas like Networks or Software Engineering are not the focus.,"Computational complexity and cryptography:0.2,Design and analysis of algorithms:0.9,Formal languages and automata theory:0.1,Logic:0.1,Models of computation:0.3,Randomness, geometry and discrete structures:0.2,Semantics and reasoning:0.1,Theory and algorithms for application domains:0.2",Design and analysis of algorithms,Design and analysis of algorithms is directly relevant as the paper presents a novel algorithm for greedy graph drawings. Other categories like Computational complexity or Geometry are secondary considerations.,"Algorithm design techniques:1,Approximation algorithms analysis:0,Concurrent algorithms:0,Data structures design and analysis:0,Distributed algorithms:0,Graph algorithms analysis:1,Mathematical optimization:0,Online algorithms:0,Parallel algorithms:0,Parameterized complexity and exact algorithms:0,Streaming, sublinear and near linear time algorithms:0","Algorithm design techniques,Graph algorithms analysis",Algorithm design techniques: The paper introduces a novel algorithm for greedy drawings of triangulations. Graph algorithms analysis: The study focuses on properties of graph structures (triangulated binary cactuses) and their role in algorithmic solutions.
4299,On the Expressiveness of the Ambient Logic,"The Ambient Logic (AL) has been proposed for expressing properties of pro- cess mobility in the calculus of Mobile Ambients (MA), and as a basis for query languages on semistructured data. In this paper, we study the expressiveness of AL. We define formulas for capabilities and for communication in MA. We also derive some formulas that capture finitess of a term, name occurrences and persistence. We study extensions of the calculus involving more complex forms of communications, and we define characteristic formulas for the equivalence induced by the logic on a subcalculus of MA. This subcalculus is defined by imposing an image-finiteness condition on the reducts of a MA process.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.95,Mathematics of computing:0.3,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Theory of computation,Theory of computation is highly relevant as the paper analyzes logical expressiveness in computational models. Other categories are not relevant as the paper focuses on theoretical foundations rather than applications or systems.,"Computational complexity and cryptography:0.1,Design and analysis of algorithms:0.3,Formal languages and automata theory:0.75,Logic:1.0,Models of computation:0.2,Randomness, geometry and discrete structures:0.1,Semantics and reasoning:1.0,Theory and algorithms for application domains:0.2","Logic,Semantics and reasoning",Logic is directly relevant as the paper studies the expressiveness of Ambient Logic. Semantics and reasoning (1.0) is also relevant due to the focus on logical properties and equivalences. Formal languages (0.75) is secondary as the logic is applied to process mobility. Other fields are less relevant.,"Abstraction:0,Automated reasoning:0,Constraint and logic programming:0,Constructive mathematics:0,Description logics:0,Equational logic and rewriting:0,Finite Model Theory:0,Higher order logic:0,Hoare logic:0,Linear logic:0,Logic and verification:1,Modal and temporal logics:1,Program constructs:0,Program reasoning:0,Program semantics:0,Programming logic:0,Proof theory:0,Separation logic:0,Type theory:0,Verification by model checking:0","Modal and temporal logics,Logic and verification",Modal and temporal logics: The Ambient Logic is a modal logic used for reasoning about process mobility. Logic and verification: The paper studies the expressiveness of the logic in verification contexts. Other categories are irrelevant as the paper does not discuss programming constructs or proofs.
1940,Normal Bisimulation for Higher Order Pi-Calculus with Unguarded Choice,"In this paper, we present a normal bisimulation for higher order pi-calculus with unguarded choice and prove the coincidence between such normal bisimulation and context bisimulation for higher order π-calculus with unguarded choice. To achieve this aim, we introduce indexed higher order π-calculus with unguarded choice. Furthermore we present corresponding indexed bisimulations in this calculus, and prove the equivalence between indexed context bisimulation and indexed normal bisimulation. As an application of this result, we prove the equivalence between context bisimulation and normal bisimulation for higher order π-calculus with unguarded choice.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.2,Theory of computation:1.0,Mathematics of computing:0.3,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.3,Applied computing:0.1,Social and professional topics:0.1",Theory of computation,Theory of computation is directly relevant to formal methods in process calculus. Other categories like Software are less aligned with the theoretical focus.,"Computational complexity and cryptography:0.0,Design and analysis of algorithms:0.0,Formal languages and automata theory:0.9,Logic:0.0,Models of computation:0.8,Randomness, geometry and discrete structures:0.0,Semantics and reasoning:0.0,Theory and algorithms for application domains:0.0","Formal languages and automata theory,Models of computation",Formal languages and automata theory is relevant for the higher-order π-calculus formalism. Models of computation is relevant for the bisimulation equivalence analysis. Categories like Design and analysis of algorithms are irrelevant as the focus is on theoretical process equivalences rather than algorithm design.,"Abstract machines:0,Automata extensions:0,Automata over infinite objects:0,Computability:0,Concurrency:1,Formalisms:1,Grammars and context-free languages:0.5,Interactive computation:0,Probabilistic computation:0,Quantum computation theory:0,Regular languages:0,Streaming models:0,Timed and hybrid models:0,Tree languages:0","Concurrency,Formalisms","Concurrency: The work is on pi-calculus, a concurrency model. Formalisms: The paper introduces a new bisimulation formalism. Rejected options like Grammars are less central."
3413,Streaming Complexity of Checking Priority Queues,"This work is in the line of designing efficient checkers for testing the reliability of some massive data structures. Given a sequential access to the insert/extract operations on such a structure, one would like to decide, a posteriori only, if it corresponds to the evolution of a reliable structure. In a context of massive data, one would like to minimize both the amount of reliable memory of the checker and the number of passes on the sequence of operations. Chu, Kannan and McGregor initiated the study of checking priority queues in this setting. They showed that use of timestamps allows to check a priority queue with a single pass and memory space O(N^(1/2)), up to a polylogarithmic factor. Later, Chakrabarti, Cormode, Kondapally and McGregor removed the use of timestamps, and proved that more passes do not help. We show that, even in the presence of timestamps, more passes do not help, solving a previously open problem. On the other hand, we show that a second pass, but in reverse direction, shrinks the memory space to O((log N)^2), extending a phenomenon the first time observed by Magniez, Mathieu and Nayak for checking well-parenthesized expressions.","General and reference:0,Hardware:0,Computer systems organization:0,Networks:0,Software and its engineering:0.3,Theory of computation:1,Mathematics of computing:0.5,Information systems:0,Security and privacy:0,Human-centered computing:0,Computing methodologies:0.2,Applied computing:0,Social and professional topics:0",Theory of computation,Theory of computation is highly relevant for analyzing streaming algorithms and data structure verification. Mathematics of computing is marginally relevant for complexity analysis. Other categories are irrelevant as the paper focuses on theoretical algorithmic analysis.,"Computational complexity and cryptography:0.9,Design and analysis of algorithms:0.75,Formal languages and automata theory:0.2,Logic:0.2,Models of computation:0.7,Randomness, geometry and discrete structures:0.2,Semantics and reasoning:0.2,Theory and algorithms for application domains:0.2","Computational complexity and cryptography,Design and analysis of algorithms","Computational complexity: The paper studies memory/pass tradeoffs in checking priority queues, a core complexity problem. Design and analysis of algorithms: The algorithms for checking operations are analyzed. Other categories like formal languages or models of computation are less directly relevant.","Algebraic complexity theory:0,Algorithm design techniques:0,Approximation algorithms analysis:0,Circuit complexity:0,Communication complexity:0,Complexity classes:0,Complexity theory and logic:0,Concurrent algorithms:0,Cryptographic primitives:0,Cryptographic protocols:0,Data structures design and analysis:0,Distributed algorithms:0,Graph algorithms analysis:0,Interactive proof systems:0,Mathematical optimization:0,Online algorithms:0,Oracles and decision trees:0,Parallel algorithms:0,Parameterized complexity and exact algorithms:0,Problems, reductions and completeness:0,Proof complexity:0,Quantum complexity theory:0,Streaming, sublinear and near linear time algorithms:1","Streaming, sublinear and near linear time algorithms,Data structures design and analysis","Streaming, sublinear and near linear time algorithms is relevant because the paper focuses on memory-efficient algorithms for data structure verification. Data structures design and analysis is relevant because the work involves analyzing priority queue implementations. Other children like Complexity classes or Cryptographic protocols are irrelevant since the paper focuses on algorithmic efficiency rather than computational complexity theory or cryptography."
582,Fault-tolerant wait-free shared objects,"The authors classify object failures into two broad categories: responsive and non-responsive. They require that wait-free objects subject to responsive failures continue to respond (in finite time) to operation invocations. The responses may be incorrect. In contrast, wait-free objects subject to non-responsive failures are exempt from responding to operation invocations. Such objects may 'hang' on the invoking process. They divide responsive failures into three models: R-crash,R-omission, and R-arbitrary. They divide non-responsive failures into crash, omission, and arbitrary. An object subject to crash failure behaves correctly until it fails, and once it fails, it never responds to operation invocations. An object subject to omission failures may fail to respond to the invocations of an arbitrary subset of processes, but continue to respond to the invocations of the remaining processes (forever).<<ETX>>","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.9,Mathematics of computing:0.2,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Theory of computation,"Theory of computation: The paper focuses on fault-tolerant distributed systems and wait-free objects, core topics in distributed computing. Other fields like Software or Hardware are not relevant.","Computational complexity and cryptography:0.0,Design and analysis of algorithms:1.0,Formal languages and automata theory:0.0,Logic:0.0,Models of computation:0.5,Randomness, geometry and discrete structures:0.0,Semantics and reasoning:0.0,Theory and algorithms for application domains:0.5",Design and analysis of algorithms,Design and analysis of algorithms is highly relevant as the paper focuses on fault-tolerant shared object algorithms. Models of computation (0.5) is partially relevant but not the primary focus.,"Algorithm design techniques:1.0,Approximation algorithms analysis:0.0,Concurrent algorithms:1.0,Data structures design and analysis:0.0,Distributed algorithms:0.0,Graph algorithms analysis:0.0,Mathematical optimization:0.0,Online algorithms:0.0,Parallel algorithms:0.0,Parameterized complexity and exact algorithms:0.0,Streaming, sublinear and near linear time algorithms:0.0","Algorithm design techniques,Concurrent algorithms",Algorithm design techniques is relevant for the wait-free object design. Concurrent algorithms is relevant for the focus on concurrent system failures. Other options like Distributed algorithms are not discussed in the paper.
2415,On finite rigid structures,Abstract The main result of this paper is a probabilistic construction of finite rigid structures. It yields a finitely axiomatizable class of finite rigid structures where no formula with counting quantifiers defines a linear order.,"General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:1.0,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Theory of computation,Theory of computation is relevant for finite rigid structures and their logical properties. Other fields like Software engineering or Mathematics of computing are not central to this theoretical work.,"Computational complexity and cryptography:0.1,Design and analysis of algorithms:0.1,Formal languages and automata theory:0.3,Logic:0.95,Models of computation:0.85,Randomness, geometry and discrete structures:0.75,Semantics and reasoning:0.9,Theory and algorithms for application domains:0.1","Logic,Semantics and reasoning,Models of computation",Logic is central to the axiomatic development of finite rigid structures. Semantics and reasoning is relevant for the formal properties discussed. Models of computation applies to the domain-theoretic framework. Other fields like Cryptography are not related.,"Abstract machines:0,Abstraction:0,Automated reasoning:0,Computability:0,Concurrency:0,Constraint and logic programming:0,Constructive mathematics:0.5,Description logics:0,Equational logic and rewriting:0,Finite Model Theory:1,Higher order logic:0,Hoare logic:0,Interactive computation:0,Linear logic:0,Logic and verification:0.5,Modal and temporal logics:0,Probabilistic computation:1,Program constructs:0,Program reasoning:0,Program semantics:0,Programming logic:0,Proof theory:0,Quantum computation theory:0,Separation logic:0,Streaming models:0,Timed and hybrid models:0,Type theory:0,Verification by model checking:0","Finite Model Theory,Probabilistic computation",Finite Model Theory is relevant as the paper constructs finite rigid structures and discusses logical definability. Probabilistic computation is relevant due to the probabilistic construction of structures. Other categories like Constructive mathematics or Logic and verification are less directly related to the paper's focus on finite structures and probabilistic methods.
3304,Continuity analysis of programs,"We present an analysis to automatically determine if a program represents a continuous function, or equivalently, if infinitesimal changes to its inputs can only cause infinitesimal changes to its outputs. The analysis can be used to verify the robustness  of programs whose inputs can have small amounts of error and uncertainty---e.g., embedded controllers processing slightly unreliable sensor data, or handheld devices using slightly stale satellite data.
 Continuity is a fundamental notion in mathematics. However, it is difficult to apply continuity proofs from real analysis to functions that are coded as imperative programs, especially when they use diverse data types and features such as assignments, branches, and loops. We associate data types with metric spaces as opposed to just sets of values, and continuity of typed programs is phrased in terms of these spaces. Our analysis reduces questions about continuity to verification conditions that do not refer to infinitesimal changes and can be discharged using off-the-shelf SMT solvers. Challenges arise in proving continuity of programs with branches and loops, as a small perturbation in the value of a variable often leads to divergent control-flow that can lead to large changes in values of variables. Our proof rules identify appropriate ``synchronization points'' between executions and their perturbed counterparts, and establish that values of certain variables converge back to the original results in spite of temporary divergence.
 We prove our analysis sound with respect to the traditional epsilon-delta definition of continuity. We demonstrate the precision of our analysis by applying it to a range of classic algorithms, including algorithms for array sorting, shortest paths in graphs, minimum spanning trees, and combinatorial optimization. A prototype implementation based on the Z3 SMT-solver is also presented.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.85,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Theory of computation,Theory of computation: Focuses on formal continuity analysis of programs. Other categories lack theoretical rigor.,"Semantics and reasoning:1.0,Design and analysis of algorithms:0.8,Computational complexity and cryptography:0.0,Formal languages and automata theory:0.0,Logic:0.0,Models of computation:0.0,Randomness, geometry and discrete structures:0.0,Theory and algorithms for application domains:0.0",Semantics and reasoning,Semantics and reasoning: The analysis reduces continuity verification to formal reasoning with SMT solvers. Design and analysis of algorithms is secondary as the focus is on the continuity property rather than algorithmic design.,"Program constructs:0,Program reasoning:1,Program semantics:0.7","Program reasoning,Program semantics",Program reasoning is central to verifying program continuity through SMT-based verification. Program semantics is relevant for defining metric spaces and continuity in typed programs. Program constructs are not discussed as the paper focuses on verification methods.
3812,Learning from Solution Paths: An Approach to the Credit Assignment Problem,"In this article we discuss a method for learning useful conditions on the application of operators during heuristic search. Since learning is not attempted until a complete solution path has been found for a problem, credit for correct moves and blame for incorrect moves is easily assigned. We review four learning systems that have incorporated similar techniques to learn in the domains of algebra, symbolic integration, and puzzle-solving. We conclude that the basic approach of learning from solution paths can be applied to any situation in which problems can be solved by sequential search. Finally, we examine some potential difficulties that may arise in more complex domains, and suggest some possible extensions for dealing with them.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.95,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Theory of computation,Theory of computation is highly relevant as the paper discusses credit assignment in heuristic search algorithms. Applied computing and Software categories are not central to the theoretical analysis presented.,"Design and analysis of algorithms:0.95,Models of computation:0.3,Formal languages and automata theory:0.1,Logic:0.1,Computational complexity and cryptography:0.2,Randomness, geometry and discrete structures:0.15,Semantics and reasoning:0.1,Theory and algorithms for application domains:0.4",Design and analysis of algorithms,Design and analysis of algorithms is highly relevant as the paper introduces a novel algorithm for credit assignment in heuristic search. Other theoretical categories are less central to the core contribution.,"Algorithm design techniques:1,Approximation algorithms analysis:0,Concurrent algorithms:0,Data structures design and analysis:0,Distributed algorithms:0,Graph algorithms analysis:0,Mathematical optimization:0,Online algorithms:1,Parallel algorithms:0,Parameterized complexity and exact algorithms:0,Streaming, sublinear and near linear time algorithms:0","Algorithm design techniques,Online algorithms",Algorithm design techniques are central to the credit assignment method. Online algorithms are relevant due to sequential learning during search. Other categories like Distributed algorithms are not directly addressed.
1676,Infinite-duration Bidding Games,"Two-player games on graphs are widely studied in formal methods, as they model the interaction between a system and its environment. The game is played by moving a token throughout a graph to produce an infinite path. There are several common modes to determine how the players move the token through the graph; e.g., in turn-based games the players alternate turns in moving the token. We study the bidding mode of moving the token, which, to the best of our knowledge, has never been studied in infinite-duration games. The following bidding rule was previously defined and called Richman bidding. Both players have separate budgets, which sum up to 1. In each turn, a bidding takes place: Both players submit bids simultaneously, where a bid is legal if it does not exceed the available budget, and the higher bidder pays his bid to the other player and moves the token. The central question studied in bidding games is a necessary and sufficient initial budget for winning the game: a threshold budget in a vertex is a value t ∈ [0, 1] such that if Player 1’s budget exceeds t, he can win the game; and if Player 2’s budget exceeds 1 − t, he can win the game. Threshold budgets were previously shown to exist in every vertex of a reachability game, which have an interesting connection with random-turn games—a sub-class of simple stochastic games in which the player who moves is chosen randomly. We show the existence of threshold budgets for a qualitative class of infinite-duration games, namely parity games, and a quantitative class, namely mean-payoff games. The key component of the proof is a quantitative solution to strongly connected mean-payoff bidding games in which we extend the connection with random-turn games to these games, and construct explicit optimal strategies for both players.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.9,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Theory of computation,Theory of computation is relevant for infinite-duration bidding game analysis. Mathematics of computing has minimal relevance for game theory connections. Other categories like Networks are not central.,"Computational complexity and cryptography:0.2,Design and analysis of algorithms:1.0,Formal languages and automata theory:0.9,Logic:0.3,Models of computation:0.4,Randomness, geometry and discrete structures:0.3,Semantics and reasoning:0.3,Theory and algorithms for application domains:0.5","Design and analysis of algorithms,Formal languages and automata theory",Design and analysis of algorithms is central to the study of bidding game strategies. Formal languages and automata theory are relevant due to the connection with game theory and formal methods. Other categories like computational complexity are secondary.,"Algorithm design techniques:0,Approximation algorithms analysis:0,Automata extensions:0,Automata over infinite objects:0,Concurrent algorithms:0,Data structures design and analysis:0,Distributed algorithms:0,Formalisms:1,Grammars and context-free languages:0,Graph algorithms analysis:1,Mathematical optimization:0,Online algorithms:0,Parallel algorithms:0,Parameterized complexity and exact algorithms:0,Regular languages:0,Streaming, sublinear and near linear time algorithms:0,Tree languages:0","Formalisms,Graph algorithms analysis",Formalisms is relevant for the game-theoretic model of bidding. Graph algorithms analysis is relevant for infinite-duration games on graphs. Other categories like Algorithm design techniques or Online algorithms are not directly addressed.
1333,The One-Way Communication Complexity of Subgroup Membership,"This paper studies the one-way communication complexity of the subgroup membership problem, a classical problem closely related to basic questions in quantum computing. Here Alice receives, as input, a subgroup $H$ of a finite group $G$; Bob receives an element $x \in G$. Alice is permitted to send a single message to Bob, after which he must decide if his input $x$ is an element of $H$. We prove the following upper bounds on the classical communication complexity of this problem in the bounded-error setting: (1) The problem can be solved with $O(\log |G|)$ communication, provided the subgroup $H$ is normal; (2) The problem can be solved with $O(d_{\max} \cdot \log |G|)$ communication, where $d_{\max}$ is the maximum of the dimensions of the irreducible complex representations of $G$; (3) For any prime $p$ not dividing $|G|$, the problem can be solved with $O(d_{\max} \cdot \log p)$ communication, where $d_{\max}$ is the maximum of the dimensions of the irreducible $\F_p$-representations of $G$.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:1.0,Mathematics of computing:0.75,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Theory of computation,"Theory of computation is highly relevant as the paper analyzes one-way communication complexity for subgroup membership, a fundamental theoretical problem. Mathematics of computing is moderately relevant due to algebraic structures. Other categories are irrelevant.","Computational complexity and cryptography:0.9,Design and analysis of algorithms:0.75,Formal languages and automata theory:0.1,Logic:0.1,Models of computation:0.1,Randomness, geometry and discrete structures:0.1,Semantics and reasoning:0.1,Theory and algorithms for application domains:0.2","Computational complexity and cryptography,Design and analysis of algorithms",Computational complexity and cryptography is directly relevant as the paper studies communication complexity of a group-theoretic problem. Design and analysis of algorithms is relevant for the algorithmic bounds presented. Other fields like logic or formal languages are not central to the contribution.,"Algebraic complexity theory:0.4,Algorithm design techniques:0.3,Approximation algorithms analysis:0.2,Circuit complexity:0.2,Communication complexity:1.0,Complexity classes:0.3,Complexity theory and logic:0.4,Concurrent algorithms:0.2,Cryptographic primitives:0.2,Cryptographic protocols:0.2,Data structures design and analysis:0.2,Distributed algorithms:0.3,Graph algorithms analysis:0.2,Interactive proof systems:0.2,Mathematical optimization:0.2,Online algorithms:0.2,Oracles and decision trees:0.2,Parallel algorithms:0.2,Parameterized complexity and exact algorithms:0.2,Problems, reductions and completeness:0.3,Proof complexity:0.2,Quantum complexity theory:0.7,Streaming, sublinear and near linear time algorithms:0.2","Communication complexity,Quantum complexity theory",Communication complexity: The paper directly studies the communication complexity of subgroup membership. Quantum complexity theory: The problem is linked to quantum computing. Other categories like Circuit complexity were rejected because the focus is on one-way communication rather than circuits.
4935,How to reason with OWL in a logic programming system,"Logic programming has always been a major ontology modeling paradigm, and is frequently being used in large research projects and industrial applications, e.g., by means of the F-Logic reasoning engine OntoBroker or the TRIPLE query, inference, and transformation language and system. At the same time, the Web Ontology Language OWL has been recommended by the W3C for modeling ontologies for the Web. Naturally, it is desirable to investigate the interoperability between both paradigms. In this paper, we do so by studying an expressive fragment of OWL DL for which reasoning can be reduced to the evaluation of Horn logic programs. Building on the KAON2 algorithms for transforming OWL DL into disjunctive Datalog, we give a detailed account of how and to what extent OWL DL can be employed in standard logic programming systems. En route, we derive a novel, simplified characterization of the supported fragment of OWL DL","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.2,Theory of computation:0.9,Mathematics of computing:0.4,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.3,Applied computing:0.1,Social and professional topics:0.1",Theory of computation,"Theory of computation is highly relevant as the paper addresses formal reasoning between OWL ontologies and logic programming systems, focusing on theoretical transformations of OWL DL into Horn logic. Computing methodologies is less relevant because the core contribution is theoretical rather than algorithmic or applied.","Computational complexity and cryptography:0.1,Design and analysis of algorithms:0.2,Formal languages and automata theory:0.2,Logic:0.8,Models of computation:0.3,Randomness, geometry and discrete structures:0.1,Semantics and reasoning:0.7,Theory and algorithms for application domains:0.2","Logic,Semantics and reasoning",Logic: The paper focuses on OWL reasoning within logic programming. Semantics and reasoning: The interoperability between ontologies and reasoning systems is central. Other children like algorithms or formal languages are not primary.,"Abstraction:0.4,Automated reasoning:0.6,Constraint and logic programming:1,Constructive mathematics:0,Description logics:1,Equational logic and rewriting:0.3,Finite Model Theory:0,Higher order logic:0.3,Hoare logic:0,Linear logic:0,Logic and verification:0.5,Modal and temporal logics:0.4,Program constructs:0,Program reasoning:0.5,Program semantics:0.3,Programming logic:0.6,Proof theory:0.3,Separation logic:0,Type theory:0.4,Verification by model checking:0.3","Constraint and logic programming,Description logics",Constraint and logic programming is relevant as the paper discusses integrating OWL with logic programming. Description logics is relevant because OWL is based on description logics. Automated reasoning is also relevant but less directly than the selected categories.
5696,Uncertainty in Soft Temporal Constraint Problems:A General Framework and Controllability Algorithms forThe Fuzzy Case,"In real-life temporal scenarios, uncertainty and preferences are often essential and coexisting aspects. We present a formalism where quantitative temporal constraints with both preferences and uncertainty can be defined. We show how three classical notions of controllability (that is, strong, weak, and dynamic), which have been developed for uncertain temporal problems, can be generalized to handle preferences as well. After defining this general framework, we focus on problems where preferences follow the fuzzy approach, and with properties that assure tractability. For such problems, we propose algorithms to check the presence of the controllability properties. In particular, we show that in such a setting dealing simultaneously with preferences and uncertainty does not increase the complexity of controllability testing. We also develop a dynamic execution algorithm, of polynomial complexity, that produces temporal plans under uncertainty that are optimal with respect to fuzzy preferences.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.2,Networks:0.1,Software and its engineering:0.2,Theory of computation:0.9,Mathematics of computing:0.3,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Theory of computation,"Theory of computation: The paper introduces a formal framework for temporal constraint problems with uncertainty, a theoretical contribution. Computing methodologies is secondary as algorithms are proposed but not the primary focus.","Computational complexity and cryptography:0.2,Design and analysis of algorithms:0.9,Formal languages and automata theory:0.1,Logic:0.1,Models of computation:0.8,Randomness, geometry and discrete structures:0.1,Semantics and reasoning:0.1,Theory and algorithms for application domains:0.7","Design and analysis of algorithms,Models of computation,Theory and algorithms for application domains","Design and analysis of algorithms: The paper introduces algorithms for controllability testing in temporal constraint problems. Models of computation: The framework involves computational models for handling uncertainty and preferences. Theory and algorithms for application domains: The application to fuzzy temporal problems is a domain-specific focus. Other categories are irrelevant as the paper does not focus on complexity theory, formal languages, or discrete structures.","Abstract machines:0.1,Algorithm design techniques:0.8,Algorithmic game theory and mechanism design:0.2,Approximation algorithms analysis:0.3,Computability:0.1,Concurrency:0.1,Concurrent algorithms:0.2,Data structures design and analysis:0.3,Database theory:0.1,Distributed algorithms:0.2,Graph algorithms analysis:0.1,Interactive computation:0.1,Machine learning theory:0.1,Mathematical optimization:0.7,Online algorithms:0.2,Parallel algorithms:0.1,Parameterized complexity and exact algorithms:0.1,Probabilistic computation:0.3,Quantum computation theory:0.1,Streaming models:0.1,Streaming, sublinear and near linear time algorithms:0.2,Timed and hybrid models:0.4","Algorithm design techniques,Mathematical optimization",Algorithm design techniques: The paper introduces new algorithms for controllability. Mathematical optimization: The framework involves optimization of fuzzy preferences. Other categories like timed models are secondary.
1577,Randomized binary search trees,"In this paper, we present randomized algorithms over binary search trees such that: (a) the insertion of a set of keys, in any fixed order, into an initially empty tree always produces a random binary search tree; (b) the deletion of any key from a random binary search tree results in a random binary search tree; (c) the random choices made by the algorithms are based upon the sizes of the subtrees of the tree; this implies that we can support accesses by rank without additional storage requirements or modification of the data structures; and (d) the cost of any elementary operation, measured as the number of visited nodes, is the same as the expected cost of its standard deterministic counterpart; hence, all search and update operations have guaranteed expected cost O(log n), but now irrespective of any assumption on the input distribution.","General and reference:0.05,Hardware:0.05,Computer systems organization:0.05,Networks:0.05,Software and its engineering:0.1,Theory of computation:0.8,Mathematics of computing:0.1,Information systems:0.05,Security and privacy:0.05,Human-centered computing:0.05,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.05",Theory of computation,"Theory of computation: Randomized algorithms for data structures. Rejected other categories as the paper focuses on theoretical algorithms, not applications or methodologies.","Computational complexity and cryptography:0.1,Design and analysis of algorithms:0.9,Formal languages and automata theory:0.2,Logic:0.1,Models of computation:0.1,Randomness, geometry and discrete structures:0.7,Semantics and reasoning:0.1,Theory and algorithms for application domains:0.1","Design and analysis of algorithms,Randomness, geometry and discrete structures","Design and analysis of algorithms is central as the paper introduces randomized BST algorithms. Randomness, geometry and discrete structures is relevant due to the probabilistic approach. Other categories like Cryptography or Logic are not addressed.","Algorithm design techniques:1,Approximation algorithms analysis:0.4,Concurrent algorithms:0.3,Data structures design and analysis:1,Distributed algorithms:0.3,Graph algorithms analysis:0.2,Mathematical optimization:0.4,Online algorithms:0.3,Parallel algorithms:0.3,Parameterized complexity and exact algorithms:0.2,Streaming, sublinear and near linear time algorithms:0.2","Algorithm design techniques,Data structures design and analysis",Algorithm design techniques and data structures are central to the randomized BST implementation.
3497,A randomized on–line algorithm for the k–server problem on a line,"The k–server problem is one of the most important and well‐studied problems in the area of on–line computation. Its importance stems from the fact that it models many practical problems like multi‐level memory paging encountered in operating systems, weighted caching used in the management of web caches, head motion planning of multi‐headed disks, and robot motion planning. In this paper, we investigate its randomized version for which Θ(log k)–competitiveness is conjectured and yet hardly any","General and reference:0.1,Hardware:0.2,Computer systems organization:0.3,Networks:0.1,Software and its engineering:0.2,Theory of computation:1.0,Mathematics of computing:0.5,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.4,Applied computing:0.2,Social and professional topics:0.1",Theory of computation,"Theory of computation is highly relevant as the paper presents a randomized online algorithm for the k-server problem, a classic problem in online algorithms and competitive analysis. Mathematics of computing (0.5) is secondary as the focus is on algorithmic design rather than pure mathematics.","Computational complexity and cryptography:0.7,Design and analysis of algorithms:1.0,Formal languages and automata theory:0.1,Logic:0.1,Models of computation:0.1,Randomness, geometry and discrete structures:0.1,Semantics and reasoning:0.1,Theory and algorithms for application domains:0.1",Design and analysis of algorithms,Design and analysis of algorithms is highly relevant for the randomized k-server algorithm. Computational complexity is secondary due to the algorithm's theoretical implications. Other fields are not core to the contribution.,"Algorithm design techniques:0.7,Approximation algorithms analysis:0.3,Concurrent algorithms:0.1,Data structures design and analysis:0.1,Distributed algorithms:0.1,Graph algorithms analysis:0.1,Mathematical optimization:0.3,Online algorithms:1.0,Parallel algorithms:0.2,Parameterized complexity and exact algorithms:0.1,Streaming, sublinear and near linear time algorithms:0.1","Online algorithms,Algorithm design techniques","Online algorithms: The paper directly addresses the randomized k-server problem, a classic online algorithm problem. Algorithm design techniques: The work involves developing and analyzing a novel randomized algorithm for this problem. Other categories like Approximation algorithms or Parallel algorithms are not central to the paper's core contribution."
2537,Information science and technology as applications of the physics of signalling,"Adopting the scientific method a theoretical model is proposed as foundation for information science and technology, extending the existing theory of signaling: a fact f becomes known in a physical system only following the success of a test f, tests performed primarily by human sensors and applied to (physical) phenomena within which further tests may be performed. Tests are phenomena and classify phenomena. A phenomenon occupies both time and space, facts and inferences having physical counterparts which are phenomena of specified classes. Identifiers such as f are conventional, assigned by humans; a fact (f', f'') reports the success of a test of generic class f', the outcome f'' of the reported application classifying the successful test in more detail. Facts then exist only within structures of a form dictated by constraints on the structural design of tests. The model explains why responses of real time systems are not uniquely predictable and why restrictions, on concurrency in performing inferences within them, are needed. Improved methods, based on the model and applicable throughout the software life-cycle, are summarised in the paper. No report of similar work has been found in the literature.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.9,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Theory of computation,Theory of computation is highly relevant as the paper proposes a theoretical model for information science based on signaling and test frameworks. Other categories like Applied computing are irrelevant as the focus is on foundational theory rather than applications.,"Computational complexity and cryptography:0.1,Design and analysis of algorithms:0.2,Formal languages and automata theory:0.1,Logic:0.1,Models of computation:0.8,Randomness, geometry and discrete structures:0.1,Semantics and reasoning:0.8,Theory and algorithms for application domains:0.2","Models of computation,Semantics and reasoning",Models of computation and Semantics and reasoning are central to the paper's theoretical framework on signaling and information. Other categories like Logic or Formal languages are not explicitly discussed.,"Concurrency:0.9,Program semantics:0.7,Interactive computation:0.2,Probabilistic computation:0.1","Concurrency,Program semantics",Concurrency is primary as the paper discusses concurrency in real-time systems. Program semantics is relevant for modeling computational structures. Other options are less relevant as the focus is on theoretical models of computation.
875,Sticky bits and universality of consensus,"In this paper we consider implementation of atomic wait-free objects in the context of a shared-memory multiprocessor. We introduce a new primitive object, the “Sticky-Bit”, and show its universality by proving that any safe implementation of a sequential object can be transformed into a wait-free atomic one using only Sticky Bits and safe registers. The Sticky Bit may be viewed as a memory-oriented version of consensus. In particular, the results of this paper imply “universality of consensus” in the sense that given an algorithm to achieve n-processor consensus, we can transform any safe implementation of a sequential object into a wait-free atomic one using polynomial number of additional safe bits. The presented results also imply that the Read-Modify-Write (RMW) hierarchy “collapsesn . More precisely, we show that although an object that supports a l-bit atomic wait-free RMW is strictly more powerful than safe register and an object that supports 3valued atomic wait-free RMW is strictly more powerful than l-bit RMW, the S-value RMW is universal in the sense that any RMW can be atomically implemented from a 3-value atomic RMW in a wait-free fashion.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.9,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Theory of computation,Theory of computation (0.9): The paper introduces Sticky Bits and proves their universality in consensus and wait-free computation. Other categories like Hardware are not directly relevant to the theoretical contributions.,"Computational complexity and cryptography:0.1,Design and analysis of algorithms:0.8,Formal languages and automata theory:0.1,Logic:0.1,Models of computation:0.9,Randomness, geometry and discrete structures:0.1,Semantics and reasoning:0.1,Theory and algorithms for application domains:0.1","Models of computation,Design and analysis of algorithms",Models of computation is highly relevant for the wait-free consensus analysis. Design and analysis of algorithms is relevant for the transformation techniques. Other categories like computational complexity have limited relevance.,"Abstract machines:0.1,Algorithm design techniques:0.1,Approximation algorithms analysis:0.1,Computability:0.1,Concurrency:1.0,Concurrent algorithms:1.0,Data structures design and analysis:0.1,Distributed algorithms:0.3,Graph algorithms analysis:0.1,Interactive computation:0.1,Mathematical optimization:0.1,Online algorithms:0.1,Parallel algorithms:0.1,Parameterized complexity and exact algorithms:0.1,Probabilistic computation:0.1,Quantum computation theory:0.1,Streaming models:0.1,Streaming, sublinear and near linear time algorithms:0.1,Timed and hybrid models:0.1","Concurrency,Concurrent algorithms",Concurrency and Concurrent algorithms are both highly relevant as the paper introduces a new primitive for wait-free consensus in shared-memory systems. Other categories have lower scores as the paper's focus is on fundamental concurrency theory rather than distributed systems or algorithm design.
1471,The Separation Problem for Binary Decision Diagrams,"The separation problem is central to mathematical programming. It asks how a continuous relaxation of an optimization problem can be strengthened by adding constraints that separate or cut off an infeasible solution. We study an analogous separation problem for a discrete relaxation based on binary decision diagrams (BDDs), which have recently proved useful in optimization and constraint programming. The algorithm modifies a relaxed BDD so as to exclude a single solution or family of solutions specified by a partial assignment. A key issue is the growth of the separating BDD when a sequence of solutions are cut off. We prove lower and upper bounds on the growth rate. We also examine growth empirically in a logicbased Benders method for a home health care scheduling problem. We find that the separating BDD tends to grow only linearly with the number of Benders cuts. Introduction The separation problem is fundamental for mathematical programming methods. It is generally understood as the problem of finding a constraint that “separates” or “cuts off” a given infeasible solution. The separation problem normally arises when a relaxation of the problem is solved to obtain a bound on the optimal value of the problem. When the solution of the relaxation is infeasible in the original problem, the relaxation is augmented with one or more constraints that cut off the solution. The tighter relaxation that results can then be solved to obtain a different solution, perhaps one that is feasible in the original problem or provides a better bound. For example, integer programming (IP) solvers typically solve a continuous relaxation of the problem. When the solution is infeasible, separating cuts in the form of linear inequality constraints may be generated and added to the relaxation. The cuts may be general Gomory cuts or mixed-integer rounding cuts, or they may be special-purpose cuts that exploit the problem structure (see (Marchand et al. 2002) for a survey). Given this, one may ask whether separation can be useful for other kinds of relaxations in an optimization context. One possible relaxation is a discrete relaxation based on binary decision diagrams (BDDs), which have recently been applied to optimization and constraint programming (CP). BDDs have long been used for circuit design, configuration, and related purposes (Akers 1978; Lee 1959; Bryant 1986; Hu 1995), but relaxed BDDs can provide an enhancement to the traditional CP domain store, bounds for branchand-bound methods, and a master problem formulation for Benders decomposition (Andersen et al. 2007; Hadžic et al. 2008; Hoda, van Hoeve, and Hooker 2010; Hadžic and Hooker 2006; 2007; Becker et al. 2005; Behle and Eisenbrand 2007). In this paper, we study the separation problem for BDDs. We present a general separation algorithm and investigate the complexity of separation, both analytically and empirically. The algorithm and analysis can be easily extended to multivalued decision diagrams (MDDs). Separating BDDs For our purposes, a BDD is a directed acyclic graph in which the nodes are partioned into layers, with the root node r in layer 1 and the terminal node t in layer n + 1. Layers 1, . . . , n correspond to binary variables x1, . . . , xn, and each arc leaving a node in layer i is labeled 0 or 1 to indicate a value of xi. Each path from the root to the terminal node therefore corresponds to a possible assignment to x = (x1, . . . , xn). Examples of BDDs appear in Fig. 1, where dashed arcs are 0-arcs and solid arcs are 1-arcs. A BDD is reduced when it is the smallest BDD that represents a given set of assignments to x. It can be shown that there is a unique reduced BDD for a given variable ordering (Bryant 1986). Any 0-1 optimization problem with variables x can be represented by a BDD whose r-t paths correspond to feasible solutions of the problem. A separable objective function ∑ i fi(xi) can be represented by assigning length fi(0) to 0-arcs leaving layer i and fi(1) to 1-arcs. An optimal solution of the problem corresponds to a shortest (or longest) path in the BDD. Nonseparable objective functions can be represented by assigning canonical costs as described in (Hooker 2013).","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:1.0,Mathematics of computing:0.5,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Theory of computation,Theory of computation: The paper studies the separation problem for binary decision diagrams in optimization.,"Computational complexity and cryptography:0.0,Design and analysis of algorithms:0.9,Formal languages and automata theory:0.7,Logic:0.1,Models of computation:0.6,Randomness, geometry and discrete structures:0.0,Semantics and reasoning:0.0,Theory and algorithms for application domains:0.0","Design and analysis of algorithms,Formal languages and automata theory",Design and analysis of algorithms is relevant for BDD separation algorithms. Formal languages and automata theory is relevant for BDD structures. Models of computation is less directly relevant as the focus is on algorithmic modifications rather than computational models.,"Algorithm design techniques:0.4,Approximation algorithms analysis:0.2,Automata extensions:0.1,Automata over infinite objects:0.1,Concurrent algorithms:0.1,Data structures design and analysis:0.3,Distributed algorithms:0.2,Formalisms:0.1,Grammars and context-free languages:0.1,Graph algorithms analysis:0.8,Mathematical optimization:0.7,Online algorithms:0.2,Parallel algorithms:0.1,Parameterized complexity and exact algorithms:0.1,Regular languages:0.1,Streaming, sublinear and near linear time algorithms:0.1,Tree languages:0.1","Graph algorithms analysis,Mathematical optimization",Graph algorithms analysis is relevant due to BDD manipulation and path analysis. Mathematical optimization is central to the separation problem for BDDs. Categories like Data structures or Automata are less directly related to the core algorithmic and optimization contributions.
290,Solving combinatorial exchanges: optimality via a few partial bids,"Auctions have been studied in economics and game theory for a long time as important resource allocation mechanisms in distributed environments. In recent years, their role has grown with the emergence of Internet and electronic commerce, as businesses and corporations leverage the new medium to streamline their procurement process. Many businesses are moving to an auction-based purchase method where they issue a request for quotes for the goods and services needed, and let the suppliers bid for a piece of the business. Driven by these fundamentals, auctions and algorithms related to them have become important and popular research topics in computer science. An exchange generalizes the auction mechanism to the setting with multiple buyers and sellers. Some familiar examples are the exchanges for equities and commodities, transportation, electricity, and the businessto-business exchanges. A combinatorial exchange is an exchange where buyers and sellers can bid on bundles (subsets) of goods. Combinatorial markets are desirable because items often have complementarity, and combinatorial bidding minimizes bidders’ risk of getting stuck with only a partial subset. It also improves the overall economic efficiency.","General and reference:0,Hardware:0,Computer systems organization:0,Networks:0,Software and its engineering:0,Theory of computation:1,Mathematics of computing:0.5,Information systems:0,Security and privacy:0,Human-centered computing:0,Computing methodologies:0.5,Applied computing:0.5,Social and professional topics:0",Theory of computation,"Theory of computation: The paper addresses combinatorial optimization problems in auctions, which are foundational to computational complexity and algorithms. Other categories like Applied computing receive moderate scores for practical applications but are secondary to the algorithmic focus.","Design and analysis of algorithms:0.9,Theory and algorithms for application domains:0.8,Computational complexity and cryptography:0.6,Formal languages and automata theory:0.1,Logic:0.1,Models of computation:0.1,Randomness, geometry and discrete structures:0.1,Semantics and reasoning:0.1","Design and analysis of algorithms,Theory and algorithms for application domains",Design and analysis of algorithms: The paper presents algorithms for combinatorial exchanges. Theory and algorithms for application domains: Focuses on auction applications. Computational complexity is less relevant as the work is algorithmic rather than complexity-theoretic.,"Algorithm design techniques:0.0,Algorithmic game theory and mechanism design:1.0,Approximation algorithms analysis:0.0,Concurrent algorithms:0.0,Data structures design and analysis:0.0,Database theory:0.0,Distributed algorithms:0.0,Graph algorithms analysis:0.0,Machine learning theory:0.0,Mathematical optimization:1.0,Online algorithms:0.0,Parallel algorithms:0.0,Parameterized complexity and exact algorithms:0.0,Streaming, sublinear and near linear time algorithms:0.0","Algorithmic game theory and mechanism design,Mathematical optimization",Algorithmic game theory and mechanism design: Focus on combinatorial exchanges in auctions. Mathematical optimization: Discusses optimization of bid selection. Other algorithmic categories are unrelated to market design.
1233,Negative-Weight Shortest Paths and Unit Capacity Minimum Cost Flow in Õ (m10/7 log W) Time (Extended Abstract),"In this paper, we study a set of combinatorial optimization problems on weighted graphs: the shortest path problem with negative weights, the weighted perfect bipartite matching problem, the unit-capacity minimum-cost maximum flow problem, and the weighted perfect bipartite b-matching problem under the assumption that ||b||1 = O(m). We show that each of these four problems can be solved in O(m10/7 log W) time, where W is the absolute maximum weight of an edge in the graph, providing the first polynomial improvement in their sparse-graph time complexity in over 25 years. At a high level, our algorithms build on the interior-point method-based framework developed by Mądry (FOCS 2013) for solving unit-capacity maximum flow problem. We develop a refined way to analyze this framework, as well as provide new variants of the underlying preconditioning and perturbation techniques. Consequently, we are able to extend the whole interior-point method-based approach to make it applicable in the weighted graph regime.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.9,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Theory of computation,Theory of computation: The paper presents novel algorithms for graph optimization problems. Other categories are irrelevant as the focus is on algorithmic complexity.,"Computational complexity and cryptography:0.2,Design and analysis of algorithms:0.9,Formal languages and automata theory:0.1,Logic:0.1,Models of computation:0.3,Randomness, geometry and discrete structures:0.4,Semantics and reasoning:0.1,Theory and algorithms for application domains:0.7","Design and analysis of algorithms,Theory and algorithms for application domains",Design and analysis of algorithms: Develops optimized algorithms for graph problems. Theory and algorithms for application domains: Addresses practical problems in combinatorial optimization. Other fields are less relevant as the focus is on algorithmic improvements.,"Algorithm design techniques:0.5,Algorithmic game theory and mechanism design:0.1,Approximation algorithms analysis:0.3,Concurrent algorithms:0.1,Data structures design and analysis:0.2,Database theory:0.1,Distributed algorithms:0.1,Graph algorithms analysis:1.0,Machine learning theory:0.1,Mathematical optimization:0.4,Online algorithms:0.1,Parallel algorithms:0.1,Parameterized complexity and exact algorithms:0.1,Streaming, sublinear and near linear time algorithms:0.3",Graph algorithms analysis,"Graph algorithms analysis: The paper presents algorithms for graph problems (shortest paths, flows) with improved time complexity. Mathematical optimization is secondary as the focus is algorithmic design for graphs."
2149,Sample Complexity of Policy Search with Known Dynamics,"We consider methods that try to find a good policy for a Markov decision process by choosing one from a given class. The policy is chosen based on its empirical performance in simulations. We are interested in conditions on the complexity of the policy class that ensure the success of such simulation based policy search methods. We show that under bounds on the amount of computation involved in computing policies, transition dynamics and rewards, uniform convergence of empirical estimates to true value functions occurs. Previously, such results were derived by assuming boundedness of pseudodimension and Lipschitz continuity. These assumptions and ours are both stronger than the usual combinatorial complexity measures. We show, via minimax inequalities, that this is essential: boundedness of pseudodimension or fat-shattering dimension alone is not sufficient.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.9,Mathematics of computing:0.2,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Theory of computation,Theory of computation is relevant as the paper focuses on theoretical analysis of policy search algorithms and sample complexity. Other categories like Mathematics of computing are less central to the core contribution.,"Design and analysis of algorithms:1.0,Computational complexity and cryptography:0.75,Models of computation:0.25,Randomness, geometry and discrete structures:0.25,Semantics and reasoning:0.25,Formal languages and automata theory:0.25,Theory and algorithms for application domains:0.25,Logic:0.25","Design and analysis of algorithms,Computational complexity and cryptography",Design and analysis of algorithms is highly relevant as the paper focuses on algorithmic conditions for policy search. Computational complexity and cryptography are partially relevant due to complexity analysis. Other options are irrelevant as the paper does not address formal languages or logic.,"Algebraic complexity theory:0.2,Algorithm design techniques:1,Approximation algorithms analysis:0.4,Circuit complexity:0.1,Communication complexity:0.2,Complexity classes:0.3,Complexity theory and logic:0.8,Concurrent algorithms:0.2,Cryptographic primitives:0.1,Cryptographic protocols:0.1,Data structures design and analysis:0.3,Distributed algorithms:0.2,Graph algorithms analysis:0.2,Interactive proof systems:0.1,Mathematical optimization:0.4,Online algorithms:0.2,Oracles and decision trees:0.3,Parallel algorithms:0.2,Parameterized complexity and exact algorithms:0.2,Problems, reductions and completeness:0.4,Proof complexity:0.1,Quantum complexity theory:0.1,Streaming, sublinear and near linear time algorithms:0.3","Algorithm design techniques,Complexity theory and logic",Algorithm design techniques: The paper analyzes policy search methods for MDPs. Complexity theory and logic: Discusses sample complexity and computational bounds. 'Approximation algorithms analysis' is less relevant as the focus is on theoretical complexity rather than approximations.
668,Comparison of Decision Diagrams for Multiple-Output Logic Functions,"This paper shows four different methods to evaluate multiple-output logic functions using decision diagrams: SBDD, MTBDD, BDD for characteristic functions (CF), and BDDs for ECFNs (Encoded Characteristic Function for Non-zero outputs). Methods to compute average evaluation time for each type of decision diagrams are presented. By experimental analysis using benchmark functions, the number of nodes and average evaluation time are compared. Evaluation using BDDs for ECFN outperforms those using MTBDDs, BDDs for CF, and SBDDs with respect to the size of BDDs and computation time. The sizes of BDDs for ECFNs are smaller than ones of corresponding MTBDDs (Multi-Terminal BDDs), BDDs for CFs (Characteristic Functions), and SBDDs (Shared BDDs).","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:1.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Theory of computation,"The paper evaluates decision diagram methods for multiple-output logic functions, focusing on theoretical performance analysis and comparison. This aligns with theoretical foundations in logic and data structures.","Computational complexity and cryptography:0.1,Design and analysis of algorithms:0.8,Formal languages and automata theory:0.1,Logic:0.9,Models of computation:0.1,Randomness, geometry and discrete structures:0.1,Semantics and reasoning:0.1,Theory and algorithms for application domains:0.1","Logic,Design and analysis of algorithms",Logic is relevant because the paper compares decision diagram methods for logic functions. Design and analysis of algorithms is relevant for the performance evaluation of these methods. Other categories like Formal languages are less directly related to the paper's focus.,"Abstraction:0.1,Algorithm design techniques:0.2,Approximation algorithms analysis:0.1,Automated reasoning:0.1,Concurrent algorithms:0.1,Constraint and logic programming:0.1,Constructive mathematics:0.1,Data structures design and analysis:1.0,Description logics:0.1,Distributed algorithms:0.1,Equational logic and rewriting:0.1,Finite Model Theory:0.1,Graph algorithms analysis:0.1,Higher order logic:0.1,Hoare logic:0.1,Linear logic:0.1,Logic and verification:0.1,Mathematical optimization:0.1,Modal and temporal logics:0.1,Online algorithms:0.1,Parallel algorithms:0.1,Parameterized complexity and exact algorithms:0.1,Programming logic:0.1,Proof theory:0.1,Separation logic:0.1,Streaming, sublinear and near linear time algorithms:0.1,Type theory:0.1,Verification by model checking:0.1",Data structures design and analysis,Data structures design and analysis is directly relevant to the comparison of decision diagrams for logic functions. Other algorithm and logic categories are less central to the data structure analysis focus.
4513,On colorings of squares of outerplanar graphs,"We investigate the clique number, the chromatic number and the inductiveness (or the degeneracy) of the square G2 of an outerplanar graph G, and bound as a function of the maximum degree Δ of G. Our main result is a tight bound of Δ for the inductiveness of the square of any outerplanar graph G, when Δ ≥ τ. This implies that a greedy algorithm yields an optimal coloring of such square graphs, and leads to an exact linear time algorithm that holds for any Δ. We then derive optimal upper bounds on the three parameters for outerplanar graphs of smaller degree Δ < τ, and in the case of chordal outerplanar graphs, classify exactly which graphs have parameters exceeding the absolute minimum. A co-product of the study is a characterization of all strongly simplicial elimination orderings of an arbitrary power of a tree.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:1.0,Mathematics of computing:0.3,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Theory of computation,Theory of computation: The paper presents theoretical results on graph coloring and inductiveness bounds for outerplanar graphs. Mathematics of computing is secondary as the focus is on computational theory.,"Computational complexity and cryptography:0.1,Design and analysis of algorithms:0.9,Formal languages and automata theory:0.1,Logic:0.1,Models of computation:0.1,Randomness, geometry and discrete structures:0.7,Semantics and reasoning:0.1,Theory and algorithms for application domains:0.1","Design and analysis of algorithms,Randomness, geometry and discrete structures","Design and analysis of algorithms is directly relevant as the paper develops optimal coloring algorithms for graph squares. Randomness, geometry and discrete structures receives moderate relevance due to graph-theoretical analysis of inductiveness and clique numbers. Other categories are irrelevant as the focus is on theoretical algorithm development rather than cryptography or formal methods.","Algorithm design techniques:0.2,Approximation algorithms analysis:0.0,Concurrent algorithms:0.0,Data structures design and analysis:0.0,Distributed algorithms:0.0,Graph algorithms analysis:1.0,Mathematical optimization:0.2,Online algorithms:0.0,Parallel algorithms:0.0,Parameterized complexity and exact algorithms:0.2,Streaming, sublinear and near linear time algorithms:0.0","Graph algorithms analysis,Algorithm design techniques",Graph algorithms analysis: The paper studies coloring problems in graph squares with a focus on outerplanar graphs. Algorithm design techniques: The paper presents greedy algorithms and analyzes their performance for graph coloring.
542,An Inductive Theorem on the Correctness of General Recursive Programs,"We prove a relatively simple inductive theorem (analogous to Floyd and Dijkstra's Invariance Theorem for iterative programs) to verify the correctness of an ample class of non-deterministic general recursive programs. This result is based on Hoare and Jifeng's relational semantics. By considering the structure of its code and specification, we propose regularity conditions on the predicate transformer associated to the fixed-point equation of a general (non deterministic) recursive program to prove it correct by induction on a well founded ordering of a covering of the domain of its corresponding input-output relation. All fixed point solutions associated to a predicate transformer satisfying these regularity conditions coincide when restricted to the domain of its least fixed point solution. We find these conditions non unduly restrictive, since continuous operators defining deterministic programs as their corresponding least fixed-point solutions must fulfill them. We couch deterministic programs (viewed as least solutions of fixed-point equations) in a restriction of Hoare and Jifeng's P programming language of (partial) finitary relations into the greatest solutions of fixed-point equations expressed in terms of “total finitary” relations of an adequate restriction of their D language","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.3,Theory of computation:0.9,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Theory of computation,Theory of computation is relevant for the inductive theorem on program correctness. Software and its engineering is only partially relevant as the paper focuses on theoretical foundations rather than practical software engineering.,"Computational complexity and cryptography:0.1,Design and analysis of algorithms:0.95,Formal languages and automata theory:0.3,Logic:0.7,Models of computation:0.2,Randomness, geometry and discrete structures:0.1,Semantics and reasoning:0.8,Theory and algorithms for application domains:0.3","Semantics and reasoning,Design and analysis of algorithms,Logic",Semantics and reasoning: Focuses on formal verification of recursive programs. Design and analysis of algorithms: Discusses inductive theorem proving. Logic: Uses Hoare-style reasoning for correctness. Other categories like 'Formal languages' are secondary to the formal methods focus.,"Abstraction:0.1,Algorithm design techniques:0.2,Approximation algorithms analysis:0.1,Automated reasoning:0.2,Concurrent algorithms:0.1,Constraint and logic programming:0.1,Constructive mathematics:0.1,Data structures design and analysis:0.1,Description logics:0.1,Distributed algorithms:0.1,Equational logic and rewriting:0.1,Finite Model Theory:0.1,Graph algorithms analysis:0.1,Higher order logic:0.1,Hoare logic:1.0,Linear logic:0.1,Logic and verification:0.2,Mathematical optimization:0.1,Modal and temporal logics:0.1,Online algorithms:0.1,Parallel algorithms:0.1,Parameterized complexity and exact algorithms:0.1,Program constructs:0.1,Program reasoning:0.8,Program semantics:0.2,Programming logic:0.2,Proof theory:0.2,Separation logic:0.1,Streaming, sublinear and near linear time algorithms:0.1,Type theory:0.1,Verification by model checking:0.1","Hoare logic,Program reasoning",Hoare logic: The paper builds on Hoare and Jifeng's relational semantics framework. Program reasoning: The work focuses on verifying correctness of recursive programs through inductive reasoning. Other logic/verification categories are less central.
3619,The Intersection of Finite State Automata and Definite Clause Grammars,"Bernard Lang defines parsing as the calculation of the intersection of a FSA (the input) and a CFG. Viewing the input for parsing as a FSA rather than as a string combines well with some approaches in speech understanding systems, in which parsing takes a word lattice as input (rather than a word string). Furthermore, certain techniques for robust parsing can be modelled as finite state transducers.In this paper we investigate how we can generalize this approach for unification grammars. In particular we will concentrate on how we might the calculation of the intersection of a FSA and a DCG. It is shown that existing parsing algorithms can be easily extended for FSA inputs. However, we also show that the termination properties change drastically: we show that it is undecidable whether the intersection of a FSA and a DCG is empty (even if the DCG is off-line parsable).Furthermore we discuss approaches to cope with the problem.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.9,Mathematics of computing:0.1,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.5,Applied computing:0.0,Social and professional topics:0.0",Theory of computation,"Theory of computation: The paper investigates formal language theory (FSA intersection with DCGs) and parsing algorithms, which are core to computational theory. Computing methodologies gets partial relevance for parsing techniques. Other categories like Networks or Security are irrelevant to the formal language analysis.","Artificial intelligence:0.0,Arts and humanities:0.0,Computer forensics:0.0,Computer graphics:0.0,Computers in other domains:0.0,Concurrent computing methodologies:0.0,Continuous mathematics:0.0,Discrete mathematics:0.0,Distributed computing methodologies:0.0,Document management and text processing:0.0,Education:0.0,Electronic commerce:0.0,Enterprise computing:0.0,Information theory:0.0,Law, social and behavioral sciences:0.0,Life and medical sciences:0.0,Machine learning:1.0,Mathematical analysis:0.0,Mathematical software:0.0,Modeling and simulation:0.0,Operations research:0.0,Parallel computing methodologies:0.0,Physical sciences and engineering:0.0,Probability and statistics:0.0,Symbolic and algebraic manipulation:0.0",Machine learning,Machine learning is highly relevant as the paper presents probabilistic models for data cleaning. Other categories like Probability and statistics are irrelevant as the focus is on model application rather than statistical theory.,"Cross-validation:0,Learning paradigms:0,Learning settings:0,Machine learning algorithms:0,Machine learning approaches:0",,"The paper discusses formal parsing algorithms and complexity analysis, which are unrelated to machine learning categories."
5178,Lower Bounds for Testing Digraph Connectivity with One-Pass Streaming Algorithms,"In this note, we show that three graph properties—strong connectivity, acyclicity, and reachability from a vertex <inline-formula><tex-math notation=LaTeX>$s$</tex-math><alternatives> <inline-graphic xlink:href=miglervondollen-ieq1-2854761.gif""/></alternatives></inline-formula> to all vertices—each require a working memory of <inline-formula><tex-math notation=""LaTeX"">$\Omega (\epsilon m)$ </tex-math><alternatives><inline-graphic xlink:href=""miglervondollen-ieq2-2854761.gif""/></alternatives></inline-formula> on a graph with <inline-formula><tex-math notation=""LaTeX"">$m$</tex-math><alternatives> <inline-graphic xlink:href=""miglervondollen-ieq3-2854761.gif""/></alternatives></inline-formula> edges to be determined correctly with probability greater than <inline-formula><tex-math notation=""LaTeX"">$(1+\epsilon)/2$</tex-math> <alternatives><inline-graphic xlink:href=""miglervondollen-ieq4-2854761.gif""/></alternatives></inline-formula>.""""""","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:1.0,Mathematics of computing:0.5,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.2,Applied computing:0.0,Social and professional topics:0.0",Theory of computation,"Theory of computation: The paper establishes lower bounds for one-pass streaming algorithms on graph properties, a core theoretical computer science problem. Mathematics of computing (0.5) is secondary due to the use of probabilistic analysis.","Computational complexity and cryptography:0.3,Design and analysis of algorithms:0.95,Formal languages and automata theory:0.0,Logic:0.0,Models of computation:0.2,Randomness, geometry and discrete structures:0.0,Semantics and reasoning:0.0,Theory and algorithms for application domains:0.0",Design and analysis of algorithms,"Design and analysis of algorithms is highly relevant as the paper presents theoretical analysis of streaming algorithms for graph properties. Computational complexity receives moderate score as the paper discusses lower bounds, but the primary focus is on algorithm analysis.","Algorithm design techniques:0.4,Approximation algorithms analysis:0.3,Concurrent algorithms:0.1,Data structures design and analysis:0.2,Distributed algorithms:0.1,Graph algorithms analysis:1.0,Mathematical optimization:0.3,Online algorithms:0.2,Parallel algorithms:0.1,Parameterized complexity and exact algorithms:0.1,Streaming, sublinear and near linear time algorithms:1.0","Graph algorithms analysis,Streaming, sublinear and near linear time algorithms","Graph algorithms analysis is relevant as the paper focuses on graph properties. Streaming, sublinear... is relevant due to the one-pass streaming algorithm context. Other categories are not central to the paper's focus."
5364,Two-variable first order logic with modular predicates over words,"We consider first order formulae over the signature consisting of the symbols of the alphabet, the symbol < (interpreted as a linear order) and the set MOD of modular numerical predicates. We study the expressive power of FO^2[<,MOD], the two-variable first order logic over this signature, interpreted over finite words. We give an algebraic characterization of the corresponding regular languages in terms of their syntactic morphisms and we also give simple unambiguous regular expressions for them. It follows that one can decide whether a given regular language is captured by FO^2[<,MOD]. Our proofs rely on a combination of arguments from semigroup theory (stamps), model theory (Ehrenfeucht-Fraisse games) and combinatorics.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.9,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Theory of computation,Theory of computation is relevant for the formal logic and algebraic characterization of two-variable first-order logic. Other categories lack connection to formal language theory.,"Computational complexity and cryptography:0.1,Design and analysis of algorithms:0.3,Formal languages and automata theory:1.0,Logic:0.6,Models of computation:0.4,Randomness, geometry and discrete structures:0.3,Semantics and reasoning:0.7,Theory and algorithms for application domains:0.2","Formal languages and automata theory,Semantics and reasoning",Formal languages and automata theory: The paper characterizes regular languages using two-variable logic over words. Semantics and reasoning: The study develops truth-value semantics for augmented logic. Other categories like 'Logic' are less specific than 'Formal languages'.,"Automata extensions:0.2,Automata over infinite objects:0.1,Formalisms:0.3,Grammars and context-free languages:0.1,Program constructs:0.1,Program reasoning:0.1,Program semantics:0.1,Regular languages:0.8,Tree languages:0.1",Regular languages,"Regular languages is highly relevant as the paper provides an algebraic characterization of FO²[<,MOD] over finite words. Other categories like automata extensions are less central to the paper's formal language analysis."
2715,The Structure and Complexity of Minimal NFA's over a Unary Alphabet,"Many difficult open problems in theoretical computer science center around nondeterminism. We study the fundamental problem of converting a given deterministic finite automaton (DFA) to a minimal nondeterministic finite automaton (NFA). Despite extensive work on finite automata, this fundamental problem has remained open. Recently, in [Ji91] we studied this problem and showed that this (and related) problems are computationally hard. Here we study the restriction of this problem to the case when the input DFA is over a one-letter alphabet. Even in this restricted case the problem is computationally hard even though our evidence of hardness is different from (and is weaker than) the standard ones such as NP-hardness. Let A → B denote the problem of converting a finite automaton of type A to a minimal finite automaton of type B. Our main result is that DFA → NFA, when the input is a unary cyclic DFA (a DFA whose graph is a simple cycle), is in NP but not in P unless NP ⊑ DTIME(nO(log n)). Our work was also motivated by the problem of finding structurally simple ‘normal forms’ of NFA's over a unary alphabet. We present some normal forms for minimal NFA's over a unary alphabet and present an application to lower bounds on the size complexity of an NFA. In fact, the normal form result is used in a nontrivial manner to show the NP membership result stated above.","General and reference:0,Hardware:0,Computer systems organization:0,Networks:0,Software and its engineering:0,Theory of computation:1,Mathematics of computing:0.2,Information systems:0,Security and privacy:0,Human-centered computing:0,Computing methodologies:0,Applied computing:0,Social and professional topics:0",Theory of computation,"Theory of computation is highly relevant as the paper addresses automata theory and computational complexity of converting DFAs to minimal NFAs. Mathematics of computing is marginally relevant due to the use of mathematical analysis, but the primary domain is theoretical computer science.","Computational complexity and cryptography:0.9,Design and analysis of algorithms:0.1,Formal languages and automata theory:0.8,Logic:0.1,Models of computation:0.1,Randomness, geometry and discrete structures:0.1,Semantics and reasoning:0.1,Theory and algorithms for application domains:0.1","Computational complexity and cryptography,Formal languages and automata theory",Computational complexity and cryptography is relevant because the paper analyzes NP membership and hardness of DFA→NFA conversion. Formal languages and automata theory is relevant as the work focuses on finite automata. Other categories like Logic or Models of computation are irrelevant as the focus is on complexity and automata theory.,"Algebraic complexity theory:0.1,Automata extensions:0.1,Automata over infinite objects:0.1,Circuit complexity:0.1,Communication complexity:0.1,Complexity classes:1.0,Complexity theory and logic:0.2,Cryptographic primitives:0.1,Cryptographic protocols:0.1,Formalisms:0.1,Grammars and context-free languages:0.1,Interactive proof systems:0.1,Oracles and decision trees:0.1,Problems, reductions and completeness:0.3,Proof complexity:0.1,Quantum complexity theory:0.1,Regular languages:0.2,Tree languages:0.1",Complexity classes,"Complexity classes is highly relevant as the paper analyzes the computational complexity of DFA-to-NFA conversion. Problems, reductions and completeness is secondary due to the NP-hardness discussion."
2590,Controlled temporal non-determinism for reasoning with a machine of finite speed,"Reactive models such as that used by Fran [l] allow one to describe a system that changes with time, by describing the state of an object after an event in terms of the situation before the event. This approach has advantages over the action based approach taken by imperative languages and the monadic approach 121. Notably, the state of any behaviour is defined in only one place, and one does not need to worry about when things should be updated. Systems such as Fran [l] allow one to specify the state that the system should be in at an exact point in absolute time, however current machines are only capable of processing information at a finite speed, and so may be unable to set behaviours to the states specified at the times specified. We will usually need to have a lag between the time at which something is specified to take on a value, and the time at which the state actually changes. What is more, the length of this lag will generally be non-deterministic. This lag needs to be dealt with in some way. The convention has been for no objects to be allowed to lag with respect to others, causing everything to always lag by the same amount. This greatly restricts what one can do with the system. One cannot have behaviours that are allowed to update at different rates, or that update as quickly as possible. One also cannot have practical distributed systems, as the synchronisation overhead would be unworkable. In order to make the reactive approach as expressive as the imperative approach, and thus practically applicable to areas beyond those of realtime processing and animations, we need to introduce a degree of temporal non-determinism into our model.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.9,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Theory of computation,"Theory of computation is relevant as the paper discusses reactive programming models, temporal non-determinism, and formal reasoning about system behavior. Other categories like Software engineering are not the primary focus.","Computational complexity and cryptography:0.1,Design and analysis of algorithms:0.2,Formal languages and automata theory:0.3,Logic:0.3,Models of computation:0.9,Randomness, geometry and discrete structures:0.3,Semantics and reasoning:0.8,Theory and algorithms for application domains:0.3","Models of computation,Semantics and reasoning",Models of computation is relevant because the paper discusses reactive models and temporal behavior in computational systems. Semantics and reasoning is relevant due to the introduction of a new model with specific semantics for handling temporal non-determinism. Other options are not directly related to the paper's core contribution on computational models and semantics.,"Abstract machines:0.3,Computability:0.2,Concurrency:1.0,Interactive computation:0.4,Probabilistic computation:0.5,Program constructs:0.3,Program reasoning:0.7,Program semantics:0.6,Quantum computation theory:0.1,Streaming models:0.2,Timed and hybrid models:1.0","Concurrency,Timed and hybrid models",Concurrency: The paper addresses managing multiple processes with temporal lag in reactive systems. Timed and hybrid models: The focus on time-based state transitions and handling delays aligns with timed models. Other options like Abstract machines or Computability are less relevant as the core is about concurrency and timing.
3821,Information dissipation in noiseless lossy in-network function computation,"We consider the problem of distributed lossy linear function computation in a tree network. We examine two cases: (i) data aggregation (only one sink node computes) and (ii) consensus (all nodes compute the same function). By quantifying the information dissipation in distributed computing, we obtain fundamental limits on network computation rate as a function of incremental distortions (and hence incremental information dissipation) along the edges of the network, and not just the overall distortions used classically. Combining this observation with an inequality on the dominance of mean-square measures over relative-entropy measures, we obtain lower bounds on the rate-distortion function that are tighter than classical cut-set bounds by a difference which can be arbitrarily large in both data aggregation and consensus.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.3,Software and its engineering:0.1,Theory of computation:0.7,Mathematics of computing:0.2,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.3,Applied computing:0.1,Social and professional topics:0.1",Theory of computation,"Theory of computation is highly relevant as the paper presents theoretical analysis of information dissipation in distributed computing. Networks receives moderate relevance due to the network context, but the primary contribution is theoretical computation analysis.","Computational complexity and cryptography:0.2,Design and analysis of algorithms:0.8,Formal languages and automata theory:0.1,Logic:0.1,Models of computation:0.3,Randomness, geometry and discrete structures:0.2,Semantics and reasoning:0.1,Theory and algorithms for application domains:0.7","Design and analysis of algorithms,Theory and algorithms for application domains",Design and analysis of algorithms: The paper develops distributed computation models with distortion analysis. Theory and algorithms for application domains: Applies these to networked systems. Other categories are not core to the work.,"Algorithm design techniques:0.8,Algorithmic game theory and mechanism design:0.1,Approximation algorithms analysis:0.3,Concurrent algorithms:0.2,Data structures design and analysis:0.1,Database theory:0.1,Distributed algorithms:1.0,Graph algorithms analysis:0.2,Machine learning theory:0.1,Mathematical optimization:0.3,Online algorithms:0.1,Parallel algorithms:0.2,Parameterized complexity and exact algorithms:0.1,Streaming, sublinear and near linear time algorithms:0.1","Distributed algorithms,Algorithm design techniques",Distributed algorithms are central to the paper's study of network function computation. Algorithm design techniques are relevant due to the information dissipation framework developed.
2500,Smoothed Analysis of Belief Propagation for Minimum-Cost Flow and Matching,"Belief propagation (BP) is a message-passing heuristic for statistical inference in graphical models such as Bayesian networks and Markov random fields. BP is used to compute marginal distributions or maximum likelihood assignments and has applications in many areas, including machine learning, image processing, and computer vision. However, the theoretical understanding of the performance of BP is unsatisfactory. Recently, BP has been applied to combinatorial optimization problems. It has been proved that BP can be used to compute maximum-weight matchings and minimum-cost flows for instances with a unique optimum. The number of iterations needed for this is pseudo-polynomial and hence BP is not efficient in general. We study belief propagation in the framework of smoothed analysis and prove that with high probability the number of iterations needed to compute maximum-weight matchings and minimum-cost flows is bounded by a polynomial if the weights/costs of the edges are randomly perturbed. To prove our upper bounds, we use an isolation lemma by Beier and Vocking (SIAM J. Comput., 2006) for matching and generalize an isolation lemma for min-cost flow by Gamarnik, Shah, and Wei (Oper. Res., 2012). We also prove almost matching lower tail bounds for the number of iterations that BP needs to converge.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.2,Networks:0.1,Software and its engineering:0.2,Theory of computation:0.9,Mathematics of computing:0.3,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.3,Applied computing:0.1,Social and professional topics:0.1",Theory of computation,"Theory of computation: The paper provides smoothed analysis of belief propagation algorithms for combinatorial optimization, a theoretical contribution to algorithmic performance analysis. Computing methodologies is less relevant as the focus is on theoretical guarantees rather than practical implementation techniques.","Computational complexity and cryptography:0.4,Design and analysis of algorithms:1.0,Formal languages and automata theory:0.3,Logic:0.2,Models of computation:0.5,Randomness, geometry and discrete structures:0.7,Semantics and reasoning:0.3,Theory and algorithms for application domains:0.6",Design and analysis of algorithms,Design and analysis of algorithms is highly relevant as the paper analyzes belief propagation algorithms for optimization problems. Other categories are less relevant as the focus is on algorithmic analysis rather than complexity theory or formal languages.,"Algorithm design techniques:0.8,Approximation algorithms analysis:0.4,Concurrent algorithms:0.2,Data structures design and analysis:0.3,Distributed algorithms:0.2,Graph algorithms analysis:1.0,Mathematical optimization:0.5,Online algorithms:0.3,Parallel algorithms:0.4,Parameterized complexity and exact algorithms:0.3,Streaming, sublinear and near linear time algorithms:0.2","Graph algorithms analysis,Algorithm design techniques",Graph algorithms analysis is directly relevant to the flow and matching problems studied. Algorithm design techniques is appropriate for the smoothed analysis methodology. Other categories are less central to the core contribution.
5284,On Role Logic,"We present role logic, a notation for describing properties of relational structures in shape analysis, databases, and knowledge bases. We construct role logic using the ideas of de Bruijn’s notation for lambda calculus, an encoding of first-order logic in lambda calculus, and a simple rule for implicit arguments of unary and binary predicates. The unrestricted version of role logic has the expressive power of first-order logic with transitive closure. Using a syntactic restriction on role logic formulas, we identify a natural fragment RL 2 of role logic. We show that the RL 2 fragment has the same expressive power as two-variable logic with counting C 2 , and is therefore decidable. We present a translation of an imperative language into the decidable fragment RL 2 , which allows compositional verification of programs that manipulate relational structures. In addition, we show how RL 2 encodes boolean shape anal","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.2,Theory of computation:0.9,Mathematics of computing:0.3,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Theory of computation,"Theory of computation is relevant for role logic, decidability, and formal verification of programs. Other categories are rejected because the paper focuses on theoretical logic and computational properties rather than implementation or application domains.","Logic:0.9,Formal languages and automata theory:0.4,Models of computation:0.5,Semantics and reasoning:0.6",Logic,Logic is highly relevant as the paper presents a new logic notation (role logic) and analyzes its decidability properties. Other fields are less directly connected since the core contribution is a novel logical framework.,"Abstraction:0.1,Automated reasoning:0.3,Constraint and logic programming:0.1,Constructive mathematics:0,Description logics:1,Equational logic and rewriting:0.2,Finite Model Theory:0.3,Higher order logic:0.1,Hoare logic:0,Linear logic:0,Logic and verification:0.2,Modal and temporal logics:0,Programming logic:0.3,Proof theory:0.1,Separation logic:0,Type theory:0.2,Verification by model checking:0.1","Description logics,Programming logic","Description logics is directly relevant as the paper introduces role logic, which is compared to two-variable logic with counting. Programming logic is relevant due to the application in compositional program verification. Other options like Automated reasoning are secondary."
3186,Covering Points of a Digraph with Point-Disjoint Paths and Its Application to Code Optimization,"A point-disjoint path cover of a directed graph is a collection of point-disjoint paths (some paths possibly having zero length) which covers all the points. A path cover which minimizes the number of paths corresponds to an optimal sequence of the steps of a computer program for efficient coding and documentation. The minimization problem for the general directed graph is hard in the sense of being NP-complete. In the case of cycle-free digraphs, however, the problem is polynomial, for it is shown that it can be reduced to the maximum-matching problem. A heuristic given here for finding a near optimal path cover for the general case is based upon applying the maximum-matching algorithm to the subgraphs of an interval decomposition.","General and reference:0.2,Hardware:0.1,Computer systems organization:0.1,Networks:0.3,Software and its engineering:0.2,Theory of computation:1.0,Mathematics of computing:0.6,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Theory of computation,Theory of computation is highly relevant as the paper focuses on graph algorithms and complexity theory for path cover problems. Mathematics of computing receives a moderate score for its mathematical formulation. Other categories are irrelevant as the work is primarily theoretical.,"Computational complexity and cryptography:0.1,Design and analysis of algorithms:0.9,Formal languages and automata theory:0.2,Logic:0.1,Models of computation:0.1,Randomness, geometry and discrete structures:0.3,Semantics and reasoning:0.1,Theory and algorithms for application domains:0.8","Design and analysis of algorithms,Theory and algorithms for application domains",Design and analysis of algorithms is central to the path cover problem formulation. Theory and algorithms for application domains applies to the code optimization application. Other fields like Computational complexity are only tangentially related.,"Algorithm design techniques:0.7,Algorithmic game theory and mechanism design:0,Approximation algorithms analysis:0.6,Concurrent algorithms:0.2,Data structures design and analysis:0.4,Database theory:0,Distributed algorithms:0.3,Graph algorithms analysis:0.9,Machine learning theory:0.1,Mathematical optimization:0.8,Online algorithms:0.3,Parallel algorithms:0.2,Parameterized complexity and exact algorithms:0.5,Streaming, sublinear and near linear time algorithms:0.4","Graph algorithms analysis,Mathematical optimization",Graph algorithms analysis is core to the path cover problem in digraphs. Mathematical optimization is relevant for the NP-complete minimization problem. Algorithm design techniques and approximation are secondary but applicable.
5594,Narrow proofs may be spacious: separating space and width in resolution,"The width of a resolution proof is the maximal number of literals in any clause of the proof. The space of a proof is the maximal number of clauses kept in memory simultaneously if the proof is only allowed to infer new clauses from clauses currently in memory. Both of these measures have previously been studied and related to the resolution refutation size of unsatisfiable CNF formulas. Also, the refutation space of a formula has been proven to be at least as large as the refutation width, but it has been open whether space can be separated from width or the two measures coincide asymptotically. We prove that there is a family of k-CNF formulas for which the refutation width in resolution is constant but the refutation space is non-constant, thus solving a problem mentioned in several previous papers.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.9,Mathematics of computing:0.4,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Theory of computation,"Theory of computation: The paper addresses computational complexity in resolution proofs, a core topic in theoretical computer science. Other categories like 'Mathematics of computing' are secondary to the logical and computational focus.","Computational complexity and cryptography:0.8,Design and analysis of algorithms:0.5,Formal languages and automata theory:0.0,Logic:1.0,Models of computation:0.0,Randomness, geometry and discrete structures:0.0,Semantics and reasoning:0.0,Theory and algorithms for application domains:0.0","Logic,Computational complexity and cryptography",Logic is highly relevant as the paper focuses on resolution proofs and logical reasoning. Computational complexity and cryptography are relevant for the complexity analysis of proof systems. Design and analysis of algorithms is less relevant as the paper is more theoretical than algorithmic.,"Abstraction:0.0,Algebraic complexity theory:0.0,Automated reasoning:0.9,Circuit complexity:0.0,Communication complexity:0.0,Complexity classes:0.1,Complexity theory and logic:0.8,Constraint and logic programming:0.0,Constructive mathematics:0.1,Cryptographic primitives:0.0,Cryptographic protocols:0.0,Description logics:0.0,Equational logic and rewriting:0.0,Finite Model Theory:0.0,Higher order logic:0.0,Hoare logic:0.0,Interactive proof systems:0.0,Linear logic:0.0,Logic and verification:0.7,Modal and temporal logics:0.0,Oracles and decision trees:0.0,Problems, reductions and completeness:0.0,Programming logic:0.0,Proof complexity:1.0,Proof theory:0.6,Quantum complexity theory:0.0,Separation logic:0.0,Type theory:0.0,Verification by model checking:0.0",Proof complexity,"Proof complexity is directly relevant as the paper analyzes resolution proof space and width. Automated reasoning and Complexity theory are secondary as they relate to logical frameworks, but the core contribution is in proof complexity."
4573,Linear Diagrams for Syllogisms (with Relatitonals),"A system for diagramming syllogisms is developed here. Unlike Venn, and other planar diagrams, these diagrams are linear. This allows one to diagram inferences which exceed the virtual four term limit on nonlinear systems. It also can be extended (by the use of vectors) to inferences involving all kinds of relational expressions.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.8,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Theory of computation,Theory of computation is relevant because the paper presents a novel logical diagramming system for syllogisms and relational expressions. Other categories are irrelevant as the focus is on formal logical methods rather than software engineering or applied computing.,"Computational complexity and cryptography:0.1,Design and analysis of algorithms:0.3,Formal languages and automata theory:0.2,Logic:1.0,Models of computation:0.3,Randomness, geometry and discrete structures:0.2,Semantics and reasoning:0.8,Theory and algorithms for application domains:0.4","Logic,Semantics and reasoning",Logic is relevant because the paper presents a logical diagramming system for syllogisms. Semantics and reasoning is relevant as the diagrams support logical inference. Other categories like Design and analysis of algorithms are not central to the contribution.,"Abstraction:0.1,Automated reasoning:1,Constraint and logic programming:0.1,Constructive mathematics:0.1,Description logics:0.1,Equational logic and rewriting:0.1,Finite Model Theory:0.1,Higher order logic:0.1,Hoare logic:0.1,Linear logic:0.1,Logic and verification:1,Modal and temporal logics:0.2,Program constructs:0.1,Program reasoning:0.1,Program semantics:0.1,Programming logic:0.2,Proof theory:0.5,Separation logic:0.1,Type theory:0.1,Verification by model checking:0.1","Automated reasoning,Logic and verification,Proof theory",Automated reasoning: The paper develops a system for diagramming syllogisms. Logic and verification: Involves logical reasoning systems. Proof theory: The diagrams relate to logical proofs. Other options like Hoare logic are not directly relevant.
2395,The ASPIC+ framework for structured argumentation: a tutorial,"This article gives a tutorial introduction to the ASPIC+ framework for structured argumentation. The philosophical and conceptual underpinnings of ASPIC+ are discussed, the main definitions are illustrated with examples and several ways are discussed to instantiate the framework and to reconstruct other approaches as special cases of the framework. The ASPIC+ framework is based on two ideas: the first is that conflicts between arguments are often resolved with explicit preferences, and the second is that arguments are built with two kinds of inference rules: strict, or deductive rules, whose premises guarantee their conclusion, and defeasible rules, whose premises only create a presumption in favour of their conclusion. Accordingly, arguments can in ASPIC+ be attacked in three ways: on their uncertain premises, or on their defeasible inferences, or on the conclusions of their defeasible inferences. ASPIC+ is not a system but a framework for specifying systems. A main objective of the study of the ASPIC+ f...","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.85,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Theory of computation,Theory of computation is relevant for the argumentation framework analysis and inference rule structures. Other categories like 'Software and its engineering' are less relevant as the focus is on theoretical framework analysis rather than implementation.,"Computational complexity and cryptography:0.1,Design and analysis of algorithms:0.3,Formal languages and automata theory:0.1,Logic:1.0,Models of computation:0.2,Randomness, geometry and discrete structures:0.1,Semantics and reasoning:1.0,Theory and algorithms for application domains:0.1","Logic,Semantics and reasoning","Logic: The paper introduces ASPIC+, a structured argumentation framework grounded in logical principles. Semantics and reasoning: The framework explicitly addresses argument structure, inference rules, and reasoning mechanisms for conflict resolution. Other categories (e.g., Design and analysis of algorithms) were rejected because the paper focuses on conceptual foundations rather than algorithmic implementation.","Abstraction:0.0,Automated reasoning:1.0,Constraint and logic programming:0.0,Constructive mathematics:0.0,Description logics:0.0,Equational logic and rewriting:0.0,Finite Model Theory:0.0,Higher order logic:0.0,Hoare logic:0.0,Linear logic:0.0,Logic and verification:1.0,Modal and temporal logics:0.0,Program constructs:0.0,Program reasoning:0.0,Program semantics:0.0,Programming logic:0.0,Proof theory:0.0,Separation logic:0.0,Type theory:0.0,Verification by model checking:0.0","Automated reasoning,Logic and verification",Automated reasoning is relevant due to the use of strict and defeasible rules in argumentation. Logic and verification applies to the formal framework's structure.
2990,The Complexity of Games on Higher Order Pushdown Automata,We prove an n-EXPTIME lower bound for the problem of deciding the winner in a reachability game on Higher Order Pushdown Automata (HPDA) of level n. This bound matches the known upper bound for parity games on HPDA. As a consequence the mu-calculus model checking over graphs given by n-HPDA is n-EXPTIME complete.,"General and reference:0.1,Hardware:0.1,Computer systems organization:0.2,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.9,Mathematics of computing:0.7,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Theory of computation,Theory of computation: The paper proves complexity bounds for games on higher-order pushdown automata. 'Mathematics of computing' was downgraded because the focus is on computational complexity rather than mathematical proofs per se.,"Computational complexity and cryptography:1.0,Design and analysis of algorithms:1.0,Formal languages and automata theory:1.0,Logic:0.2,Models of computation:0.3,Randomness, geometry and discrete structures:0.2,Semantics and reasoning:0.2,Theory and algorithms for application domains:0.2","Computational complexity and cryptography,Formal languages and automata theory",Computational complexity and cryptography is relevant due to the n-EXPTIME lower bound analysis. Formal languages and automata theory is relevant for the HPDA focus. Other categories like Design and analysis of algorithms are secondary as the paper focuses on complexity bounds rather than algorithm design.,"Algebraic complexity theory:0,Automata extensions:1,Automata over infinite objects:0,Circuit complexity:0,Communication complexity:0,Complexity classes:1,Complexity theory and logic:0.5,Cryptographic primitives:0,Cryptographic protocols:0,Formalisms:0,Grammars and context-free languages:0,Interactive proof systems:0,Oracles and decision trees:0,Problems, reductions and completeness:1,Proof complexity:0,Quantum complexity theory:0,Regular languages:0,Tree languages:0","Automata extensions,Complexity classes,Problems, reductions and completeness","Automata extensions is relevant as the paper studies Higher Order Pushdown Automata (HPDA). Complexity classes is relevant due to the n-EXPTIME completeness result. Problems, reductions and completeness is relevant because the paper establishes completeness for model checking. Other categories like Circuit complexity or Quantum complexity theory are not discussed."
2531,Separations in Communication Complexity Using Cheat Sheets and Information Complexity,"While exponential separations are known between quantum and randomized communication complexity for partial functions (Raz, STOC 1999), the best known separation between these measures for a total function is quadratic, witnessed by the disjointness function. We give the first super-quadratic separation between quantum and randomized communication complexity for a total function, giving an example exhibiting a power 2.5 gap. We further present a 1.5 power separation between exact quantum and randomized communication complexity, improving on the previous ≈ 1.15 separation by Ambainis (STOC 2013). Finally, we present a nearly optimal quadratic separation between randomized communication complexity and the logarithm of the partition number, improving upon the previous best power 1.5 separation due to Goos, Jayram, Pitassi, and Watson. Our results are the communication analogues of separations in query complexity proved using the recent cheat sheet framework of Aaronson, Ben-David, and Kothari (STOC 2016). Our main technical results are randomized communication and information complexity lower bounds for a family of functions, called lookup functions, that generalize and port the cheat sheet framework to communication complexity.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.9,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Theory of computation,"Theory of computation is highly relevant as the paper introduces new separations in quantum and randomized communication complexity, a core area in theoretical computer science. Other categories like Hardware or Software and its engineering are irrelevant as the paper focuses on theoretical models rather than systems or implementation.","Computational complexity and cryptography:0.8,Design and analysis of algorithms:0.8,Formal languages and automata theory:0.1,Logic:0.1,Models of computation:0.7,Randomness, geometry and discrete structures:0.1,Semantics and reasoning:0.1,Theory and algorithms for application domains:0.2","Computational complexity and cryptography,Design and analysis of algorithms,Models of computation",Computational complexity and cryptography: Discusses quantum vs randomized communication complexity. Design and analysis of algorithms: Presents new lower bounds for communication complexity. Models of computation: Analyzes communication complexity frameworks. Other children: Theoretical application domains are not central as the focus is on foundational complexity separations.,"Abstract machines:0.0,Algebraic complexity theory:0.0,Algorithm design techniques:0.0,Approximation algorithms analysis:0.0,Circuit complexity:0.0,Communication complexity:1.0,Complexity classes:0.0,Complexity theory and logic:0.0,Computability:0.0,Concurrency:0.0,Concurrent algorithms:0.0,Cryptographic primitives:0.0,Cryptographic protocols:0.0,Data structures design and analysis:0.0,Distributed algorithms:0.0,Graph algorithms analysis:0.0,Interactive computation:0.0,Interactive proof systems:0.0,Mathematical optimization:0.0,Online algorithms:0.0,Oracles and decision trees:0.0,Parallel algorithms:0.0,Parameterized complexity and exact algorithms:0.0,Probabilistic computation:0.0,Problems, reductions and completeness:0.0,Proof complexity:0.0,Quantum complexity theory:1.0,Quantum computation theory:0.0,Streaming models:0.0,Streaming, sublinear and near linear time algorithms:0.0,Timed and hybrid models:0.0","Communication complexity,Quantum complexity theory",Communication complexity: The paper directly studies separations in communication complexity. Quantum complexity theory: Compares quantum vs. randomized communication. Other options like Circuit complexity are irrelevant.
5048,"Profile trees for Büchi word automata, with application to determinization","The determinization of Buchi automata is a celebrated problem, with applications in synthesis, probabilistic verification, and multi-agent systems. Since the 1960s, there has been a steady progress of constructions: by McNaughton, Safra, Piterman, Schewe, and others. Despite the proliferation of solutions, they are all essentially ad-hoc constructions, with little theory behind them other than proofs of correctness. Since Safra, all optimal constructions employ trees as states of the deterministic automaton, and transitions between states are defined operationally over these trees. The operational nature of these constructions complicates understanding, implementing, and reasoning about them, and should be contrasted with complementation, where a solid theory in terms of automata run dags underlies modern constructions.In 2010, we described a profile-based approach to Buchi complementation, where a profile is simply the history of visits to accepting states. We developed a structural theory of profiles and used it to describe a complementation construction that is deterministic in the limit. Here we extend the theory of profiles to prove that every run dag contains a profile tree with at most a finite number of infinite branches. We then show that this property provides a theoretical grounding for a new determinization construction where macrostates are doubly preordered sets of states. In contrast to extant determinization constructions, transitions in the new construction are described declaratively rather than operationally.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.1,Software and its engineering:0.0,Theory of computation:0.95,Mathematics of computing:0.2,Information systems:0.1,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.1,Applied computing:0.05,Social and professional topics:0.0",Theory of computation,"Theory of computation: The paper presents theoretical advances in automata theory, specifically for Büchi automata determinization. Other categories are rejected because the paper focuses on foundational computational models rather than practical implementations or applications.","Computational complexity and cryptography:0.1,Design and analysis of algorithms:0.8,Formal languages and automata theory:0.9,Logic:0.1,Models of computation:0.6,Randomness, geometry and discrete structures:0.1,Semantics and reasoning:0.1,Theory and algorithms for application domains:0.1","Formal languages and automata theory,Design and analysis of algorithms",Formal languages and automata theory is directly relevant for Büchi automata determinization. Design and analysis of algorithms is relevant for the profile tree construction. Models of computation (0.6) is secondary for the theoretical framework.,"Algorithm design techniques:0.2,Approximation algorithms analysis:0.1,Automata extensions:0.8,Automata over infinite objects:1.0,Concurrent algorithms:0.1,Data structures design and analysis:0.1,Distributed algorithms:0.1,Formalisms:0.3,Grammars and context-free languages:0.1,Graph algorithms analysis:0.4,Mathematical optimization:0.1,Online algorithms:0.1,Parallel algorithms:0.1,Parameterized complexity and exact algorithms:0.1,Regular languages:0.1,Streaming, sublinear and near linear time algorithms:0.1,Tree languages:0.3","Automata over infinite objects,Automata extensions","Automata over infinite objects: The paper directly addresses Büchi automata, a classic model for infinite sequences. Automata extensions: The work introduces novel constructions (profile trees) that extend traditional automata frameworks. Other categories like 'Graph algorithms analysis' or 'Tree languages' are tangentially related but not central to the paper's core contribution."
2922,Open problems of Paul Erdös in graph theory,"The main treasure that Paul Erdős has left us is his collection of problems, most of which are still open today. These problems are seeds that Paul sowed and watered by giving numerous talks at meetings big and small, near and far. In the past, his problems have spawned many areas in graph theory and beyond (e.g., in number theory, probability, geometry, algorithms and complexity theory). Solutions or partial solutions to Erdős problems usually lead to further questions, often in new directions. These problems provide inspiration and serve as a common focus for all graph theorists. Through the problems, the legacy of Paul Erdős continues (particularly if solving one of these problems results in creating three new problems, for example.) There is a huge literature of almost 1500 papers written by Erdős and his (more than 460) collaborators. Paul wrote many problem papers, some of which appeared in various (really hard-to-find) proceedings. Here is an attempt to collect and organize these problems in the area of graph theory. The list here is by no means complete or exhaustive. Our goal is to state the problems, locate the sources, and provide the references related to these problems. We will include the earliest and latest known references without covering the entire history of the problems because of space limitations (The most up-to-date list of Erdős' papers can be found in [65]; an electronic file is maintained by Jerry Grossman at grossman@oakland.edu.) There are many survey papers on the impact of Paul's work, e.g., see those in the books: A Tribute to Paul Erdős [84], Combinatorics, Paul Erdős is Eighty, Volumes 1 and 2 [83], and The Mathematics of Paul Erdős, Volumes I and II [81]. To honestly follow the unique style of Paul Erdős, we will mention the fact that Erdős often offered monetary awards for solutions to a number of his favorite problems. In November 1996, a committee of Erdős' friends decided no more such awards will be given in Erdős' name. However, the author, with the help of Ron Graham, will honor future claims on the problems in this paper, provided the requirements previously set by Paul are satisfied (e.g., proofs have been verified and published in recognized journals). Throughout this paper, the constants c, c′, c1, c2, · · · and extremal functions f(n), f(n, k), f(n, k, r, t), g(n), · · · are used extensively, although within the context of each problem, the","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:1.0,Mathematics of computing:0.75,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Theory of computation,Theory of computation is highly relevant as the paper focuses on open graph theory problems central to theoretical computer science. Mathematics of computing is moderately relevant due to mathematical foundations but less central than TCS.,"Computational complexity and cryptography:0.3,Design and analysis of algorithms:1.0,Formal languages and automata theory:0.1,Logic:0.2,Models of computation:0.3,Randomness, geometry and discrete structures:0.9,Semantics and reasoning:0.1,Theory and algorithms for application domains:0.2","Design and analysis of algorithms,Randomness, geometry and discrete structures","Design and analysis of algorithms receives 1.0 as the paper discusses open algorithmic problems in graph theory. Randomness, geometry and discrete structures receives 0.9 since Erdős problems often intersect discrete mathematics. Other categories are irrelevant as the paper focuses on foundational graph theory problems rather than complexity, cryptography, or formal languages.","Algorithm design techniques:0.7,Approximation algorithms analysis:0.0,Concurrent algorithms:0.0,Data structures design and analysis:0.0,Distributed algorithms:0.0,Graph algorithms analysis:1.0,Mathematical optimization:0.7,Online algorithms:0.0,Parallel algorithms:0.0,Parameterized complexity and exact algorithms:0.0,Streaming, sublinear and near linear time algorithms:0.0","Graph algorithms analysis,Algorithm design techniques",Graph algorithms analysis: Focuses on open graph theory problems. Algorithm design techniques: Erdős problems influence algorithmic design. Mathematical optimization is secondary.
3926,Combining approaches for solving satisfiability problems with qualitative preferences,"The ability to effectively reason in the presence of qualitative preferences on literals or formulas is a central issue in Artificial Intelligence. In the last few years, two procedures have been presented in order to reason with propositional satisfiability SAT problems in the presence of additional, partially ordered qualitative preferences on literals or formulas: the first requires a modification of the branching heuristic of the SAT solver in order to guarantee that the first solution is optimal, while the second computes a sequence of solutions, each guaranteed to be better than the previous one. The two approaches have their own advantages and disadvantages and when compared on specific classes of instances --each having an empty partial order --the second seems to have superior performance.In this paper we show that the above two approaches for reasoning with qualitative preferences can be combined yielding a new effective procedure. In particular, in the new procedure we modify the branching heuristic --as in the first approach --by possibly changing the polarity of the returned literal, and then we continue the search --as in the second approach --looking for better solutions. We extended the experimental analysis conducted in previous papers by considering a wide variety of problems, having both an empty and a non-empty partial order: the results show that the new procedure performs better than the two previous approaches on average, and especially on the “hard” problems. As a preliminary result, we show that the framework of qualitative preferences on literals is more general and expressive than the framework on quantitative preferences.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.9,Mathematics of computing:0.6,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.7,Applied computing:0.0,Social and professional topics:0.0",Theory of computation,Theory of computation is relevant due to the algorithmic approach for SAT problems with preferences. Other categories do not focus on theoretical algorithm design.,"Computational complexity and cryptography:0.85,Design and analysis of algorithms:0.9,Formal languages and automata theory:0.2,Logic:0.8,Models of computation:0.3,Randomness, geometry and discrete structures:0.1,Semantics and reasoning:0.15,Theory and algorithms for application domains:0.4","Design and analysis of algorithms,Computational complexity and cryptography,Logic","Design and analysis of algorithms: The paper presents a novel algorithmic approach combining two SAT solving methods. Computational complexity and cryptography: SAT is a canonical NP-complete problem, and the paper's implications for P vs NP are significant. Logic: The work involves reasoning with qualitative preferences in logical frameworks. Other categories like Formal languages or Models of computation are irrelevant as the paper focuses on algorithmic techniques rather than language theory or computational models.","Abstraction:0.3,Algebraic complexity theory:0.1,Algorithm design techniques:1.0,Approximation algorithms analysis:0.4,Automated reasoning:1.0,Circuit complexity:0.2,Communication complexity:0.1,Complexity classes:0.1,Complexity theory and logic:0.3,Concurrent algorithms:0.4,Constraint and logic programming:0.5,Constructive mathematics:0.1,Cryptographic primitives:0.1,Cryptographic protocols:0.1,Data structures design and analysis:0.2,Description logics:0.1,Distributed algorithms:0.3,Equational logic and rewriting:0.2,Finite Model Theory:0.1,Graph algorithms analysis:0.2,Higher order logic:0.1,Hoare logic:0.1,Interactive proof systems:0.1,Linear logic:0.1,Logic and verification:1.0,Mathematical optimization:0.4,Modal and temporal logics:0.2,Online algorithms:0.1,Oracles and decision trees:0.1,Parallel algorithms:0.2,Parameterized complexity and exact algorithms:0.1,Problems, reductions and completeness:0.3,Programming logic:0.2,Proof complexity:0.1,Proof theory:0.1,Quantum complexity theory:0.1,Separation logic:0.1,Streaming, sublinear and near linear time algorithms:0.1,Type theory:0.1,Verification by model checking:0.2","Algorithm design techniques,Automated reasoning,Logic and verification",Algorithm design techniques is relevant as the paper combines two SAT-solving approaches. Automated reasoning is relevant due to the focus on reasoning with preferences. Logic and verification is relevant because the paper addresses formal verification of SAT problems with preferences. Complexity theory is less relevant as the focus is on algorithms rather than theoretical complexity.
1888,Depreciation of public goods in spatial public goods games,"In real situations, the value of public goods will be reduced or even lost because of external factors or for intrinsic reasons. In this work, we investigate the evolution of cooperation by considering the effect of depreciation of public goods in spatial public goods games on a square lattice. It is assumed that each individual gains full advantage if the number of the cooperators nc within a group centered on that individual equals or exceeds the critical mass (CM). Otherwise, there is depreciation of the public goods, which is realized by rescaling the multiplication factor r to (nc/CM)r. It is shown that the emergence of cooperation is remarkably promoted for CM > 1 even at small values of r, and a global cooperative level is achieved at an intermediate value of CM = 4 at a small r. We further study the effect of depreciation of public goods on different topologies of a regular lattice, and find that the system always reaches global cooperation at a moderate value of CM = G − 1 regardless of whether or not there exist overlapping triangle structures on the regular lattice, where G is the group size of the associated regular lattice.","General and reference:0.2,Hardware:0.2,Computer systems organization:0.2,Networks:0.2,Software and its engineering:0.2,Theory of computation:1.0,Mathematics of computing:0.6,Information systems:0.2,Security and privacy:0.2,Human-centered computing:0.2,Computing methodologies:0.4,Applied computing:0.3,Social and professional topics:0.2",Theory of computation,"Theory of computation is highly relevant because the paper analyzes game-theoretic models of cooperation in spatial public goods games, which falls under computational theory and algorithms. Mathematics of computing is moderately relevant due to the mathematical modeling. Other categories are irrelevant as the paper does not address applied computing, security, or hardware.","Computational complexity and cryptography:0.2,Design and analysis of algorithms:0.2,Formal languages and automata theory:0.2,Logic:0.2,Models of computation:0.8,Randomness, geometry and discrete structures:0.8,Semantics and reasoning:0.2,Theory and algorithms for application domains:0.2","Models of computation,Randomness, geometry and discrete structures","Models of computation is relevant as the paper analyzes game-theoretic models of cooperation. Randomness, geometry and discrete structures applies to spatial lattice structures and depreciation modeling. Other fields lack direct connection to evolutionary game theory analysis.","Abstract machines:0.1,Computability:0.2,Concurrency:0.3,Interactive computation:0.4,Probabilistic computation:0.1,Quantum computation theory:0.1,Streaming models:0.2,Timed and hybrid models:0.1",,"None of the categories align with the spatial public goods game analysis. The paper focuses on game theory and cooperation, which are not covered in these computational models."
5431,An optimal distributed depth-first-search algorithm,This paper presents a new distributed depth-first-search algorithm with communication and time complexities of O(|V|). The algorithm is shown to use 2|V|-2 messages and 2|V|-2 units of time and is shown to be optimal in time and message.,"General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.9,Mathematics of computing:0.2,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Theory of computation,Theory of computation is relevant because the paper presents an optimal distributed algorithm for depth-first search with complexity analysis. Other categories like Networks or Software and its engineering are less relevant as the focus is on theoretical algorithm design.,"Computational complexity and cryptography:0.2,Design and analysis of algorithms:1.0,Formal languages and automata theory:0.1,Logic:0.1,Models of computation:0.3,Randomness, geometry and discrete structures:0.2,Semantics and reasoning:0.1,Theory and algorithms for application domains:0.1",Design and analysis of algorithms,Design and analysis of algorithms is highly relevant as the paper presents a novel distributed DFS algorithm with proven time and message complexity. Other options like computational complexity and cryptography are less relevant since the focus is on algorithm design rather than cryptographic applications.,"Algorithm design techniques:0.3,Approximation algorithms analysis:0.1,Concurrent algorithms:0.1,Data structures design and analysis:0.1,Distributed algorithms:1.0,Graph algorithms analysis:1.0,Mathematical optimization:0.2,Online algorithms:0.1,Parallel algorithms:0.1,Parameterized complexity and exact algorithms:0.1,Streaming, sublinear and near linear time algorithms:0.1","Distributed algorithms,Graph algorithms analysis",Distributed algorithms: The paper presents an optimal distributed depth-first-search algorithm with explicit analysis of communication and time complexities. Graph algorithms analysis: The algorithm is a graph traversal technique (DFS) with formal performance bounds.
2996,A polynomial-time approximation scheme for planar multiway cut,"Given an undirected graph with edge lengths and a subset of nodes (called the terminals), the multiway cut (also called the multi-terminal cut) problem asks for a subset of edges, with minimum total length, whose removal disconnects each terminal from all others. The problem generalizes minimum s-t cut, but is NP-hard for planar graphs and APX-hard for general graphs [11]. In this paper, we present a PTAS for multiway cut on planar graphs.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.9,Mathematics of computing:0.3,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.3,Applied computing:0.1,Social and professional topics:0.1",Theory of computation,"Theory of computation: The paper presents a polynomial-time approximation scheme for a planar graph problem, which is a core topic in algorithmic complexity and design. Mathematics of computing and Computing methodologies were rejected because the focus is on theoretical algorithmic results rather than mathematical analysis or computational techniques for specific applications.","Computational complexity and cryptography:0.7,Design and analysis of algorithms:1.0,Formal languages and automata theory:0.1,Logic:0.1,Models of computation:0.1,Randomness, geometry and discrete structures:0.1,Semantics and reasoning:0.1,Theory and algorithms for application domains:0.1","Design and analysis of algorithms,Computational complexity and cryptography",Design and analysis of algorithms is relevant for the PTAS algorithm development. Computational complexity and cryptography is relevant for the complexity analysis of the multiway cut problem. Other categories are less directly related to the core contribution.,"Algebraic complexity theory:0,Algorithm design techniques:1,Approximation algorithms analysis:1,Circuit complexity:0,Communication complexity:0,Complexity classes:0,Complexity theory and logic:0,Concurrent algorithms:0,Cryptographic primitives:0,Cryptographic protocols:0,Data structures design and analysis:0,Distributed algorithms:0,Graph algorithms analysis:1,Interactive proof systems:0,Mathematical optimization:0,Online algorithms:0,Oracles and decision trees:0,Parallel algorithms:0,Parameterized complexity and exact algorithms:0,Problems, reductions and completeness:0,Proof complexity:0,Quantum complexity theory:0,Streaming, sublinear and near linear time algorithms:0","Algorithm design techniques,Approximation algorithms analysis,Graph algorithms analysis",Algorithm design techniques is relevant for the PTAS design. Approximation algorithms analysis is relevant for the PTAS result. Graph algorithms analysis is relevant as it addresses a graph problem. Other categories like Quantum complexity theory are not discussed.
3273,Efficient On-Line Proofs of Equalities and Inequalities of Formulas,An algorithm is presented for proving equivalence and inequivalence of instances of formulas involving constant terms. It is based on the construction of an equality data base in the form of a grammar. The algorithm differes from other approaches to the problem by being an on-line algorithm. Equality between two formulas can be proved in time proportional to the number of constant and function symbols appearing within them. An algorithm is also given for updating the equality data base. It has a worse case running time which is proportional to the square of the number of different formulas previously encountered.,"General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:1.0,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Theory of computation,Theory of computation is relevant due to the algorithmic focus on equality proofs and data structures. Other categories like Mathematics of computing are tangential but not core.,"Computational complexity and cryptography:0.0,Design and analysis of algorithms:0.9,Formal languages and automata theory:0.3,Logic:0.0,Models of computation:0.4,Randomness, geometry and discrete structures:0.6,Semantics and reasoning:0.0,Theory and algorithms for application domains:0.0","Design and analysis of algorithms,Randomness, geometry and discrete structures","Design and analysis of algorithms is highly relevant as the paper presents an on-line algorithm for formula equivalence. Randomness, geometry and discrete structures is also relevant due to the geometric nature of the problem. Other categories like Formal languages are less directly connected.","Algorithm design techniques:0.8,Approximation algorithms analysis:0.4,Concurrent algorithms:0.2,Data structures design and analysis:0.7,Distributed algorithms:0.3,Graph algorithms analysis:0.5,Mathematical optimization:0.3,Online algorithms:0.9,Parallel algorithms:0.3,Parameterized complexity and exact algorithms:0.4,Streaming, sublinear and near linear time algorithms:0.6","Online algorithms,Algorithm design techniques",Online algorithms is central as the paper presents an on-line proof method. Algorithm design techniques is relevant for the novel approach. Other options like Data structures are secondary.
4995,Arc‐Disjoint Paths in Decomposable Digraphs,"We prove that the weak k‐linkage problem is polynomial for every fixed k for totally Φ‐decomposable digraphs, under appropriate hypothesis on Φ. We then apply this and recent results by Fradkin and Seymour (on the weak k‐linkage problem for digraphs of bounded independence number or bounded cut‐width) to get polynomial algorithms for some classes of digraphs like quasi‐transitive digraphs, extended semicomplete digraphs, locally semicomplete digraphs (all of which contain the class of semicomplete digraphs as a subclass) and directed cographs.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:1.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Theory of computation,"Theory of computation: The paper presents polynomial-time algorithms for graph-theoretic problems in directed graphs, a core topic in computational theory. Other categories like Mathematics of computing are irrelevant as the focus is on algorithmic complexity.","Computational complexity and cryptography:0.5,Design and analysis of algorithms:1.0,Formal languages and automata theory:0.0,Logic:0.0,Models of computation:0.0,Randomness, geometry and discrete structures:0.8,Semantics and reasoning:0.0,Theory and algorithms for application domains:0.0","Design and analysis of algorithms,Randomness, geometry and discrete structures","The paper presents polynomial-time algorithms for graph problems (Design and analysis of algorithms) and focuses on digraph structures (Randomness, geometry and discrete structures). Computational complexity is tangentially relevant but not the core focus.","Algorithm design techniques:0.9,Approximation algorithms analysis:0.2,Concurrent algorithms:0.1,Data structures design and analysis:0.1,Distributed algorithms:0.4,Graph algorithms analysis:1,Mathematical optimization:0.3,Online algorithms:0.1,Parallel algorithms:0.2,Parameterized complexity and exact algorithms:0.5,Streaming, sublinear and near linear time algorithms:0.1","Graph algorithms analysis,Algorithm design techniques",Graph algorithms analysis is central to the study of arc-disjoint paths in digraphs. Algorithm design techniques is relevant for the methods used to solve the problems.
2072,Quantifier Structure in Search-Based Procedures for QBFs,"The best currently available solvers for quantified Boolean formulas (QBFs) process their input in prenex form, i.e., all the quantifiers have to appear in the prefix of the formula separated from the purely propositional part representing the matrix. However, in many QBFs derived from applications, the propositional part is intertwined with the quantifier structure. To tackle this problem, the standard approach is to convert such QBFs in prenex form, thereby losing structural information about the prefix. In the case of search-based solvers, the prenex-form conversion introduces additional constraints on the branching heuristic and reduces the benefits of the learning mechanisms. In this paper, we show that conversion to prenex form is not necessary: current search-based solvers can be naturally extended in order to handle nonprenex QBFs and to exploit the original quantifier structure. We highlight the two mentioned drawbacks of the conversion in prenex form with a simple example, and we show that our ideas can also be useful for solving QBFs in prenex form. To validate our claims, we implemented our ideas in the state-of-the-art search-based solver QuBE and conducted an extensive experimental analysis. The results show that very substantial speedups can be obtained","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.9,Mathematics of computing:0.7,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Theory of computation,Theory of computation is highly relevant for the QBF solver design and quantifier structure analysis. Mathematics of computing (0.7) is secondary due to the logical and algorithmic aspects but not the primary focus.,"Computational complexity and cryptography:0.4,Design and analysis of algorithms:1.0,Formal languages and automata theory:0.5,Logic:0.9,Models of computation:0.5,Randomness, geometry and discrete structures:0.6,Semantics and reasoning:0.4,Theory and algorithms for application domains:0.7","Design and analysis of algorithms,Logic,Theory and algorithms for application domains",Design and analysis of algorithms: The paper introduces new algorithms for handling nonprenex QBFs. Logic: The work focuses on quantified Boolean formulas and their logical properties. Theory and algorithms for application domains: The research improves practical QBF solving techniques. Other categories like Formal languages are less relevant.,"Abstraction:0.2,Algorithm design techniques:1.0,Algorithmic game theory and mechanism design:0.1,Approximation algorithms analysis:0.3,Automated reasoning:0.4,Concurrent algorithms:0.1,Constraint and logic programming:0.2,Constructive mathematics:0.1,Data structures design and analysis:0.2,Database theory:0.1,Description logics:0.1,Distributed algorithms:0.1,Equational logic and rewriting:0.1,Finite Model Theory:0.1,Graph algorithms analysis:0.1,Higher order logic:0.2,Hoare logic:0.1,Linear logic:0.1,Logic and verification:0.3,Machine learning theory:0.1,Mathematical optimization:0.2,Modal and temporal logics:0.2,Online algorithms:0.1,Parallel algorithms:0.1,Parameterized complexity and exact algorithms:0.1,Programming logic:1.0,Proof theory:0.6,Separation logic:0.1,Streaming, sublinear and near linear time algorithms:0.1,Type theory:0.2,Verification by model checking:0.2","Algorithm design techniques,Programming logic",Algorithm design techniques are central as the paper introduces a new method for QBF solving. Programming logic is relevant due to the focus on quantifier structures in logical formulas. Categories like Proof theory or Modal logics are secondary as the core contribution is algorithmic rather than purely logical.
1760,Semiglobal robust output regulation with generalized immersion,"The semiglobal robust output regulation problem is solved in this paper for a class of nonlinear systems that do not satisfy the standard conditions for the existence of a linear internal model, but admit a so-called generalized immersion. It is shown how the obstacle given by the presence of the exosystem dynamics in the generalized immersion mapping can be overcome by resorting to a recently developed framework for time-varying internal model design.""","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.9,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Theory of computation,"Theory of computation is highly relevant as the paper addresses control theory for nonlinear systems. Other categories are irrelevant as the paper doesn't focus on computer systems, networks, or software engineering.","Computational complexity and cryptography:0.0,Design and analysis of algorithms:0.75,Formal languages and automata theory:0.0,Logic:0.0,Models of computation:0.0,Randomness, geometry and discrete structures:0.0,Semantics and reasoning:0.0,Theory and algorithms for application domains:1.0","Design and analysis of algorithms,Theory and algorithms for application domains",Design and analysis of algorithms is relevant due to the focus on control system algorithms. Theory and algorithms for application domains is directly relevant as the paper addresses a specific control problem.,"Algorithm design techniques:0.9,Algorithmic game theory and mechanism design:0.1,Approximation algorithms analysis:0.2,Concurrent algorithms:0.4,Data structures design and analysis:0.1,Database theory:0.1,Distributed algorithms:0.2,Graph algorithms analysis:0.1,Machine learning theory:0.1,Mathematical optimization:0.5,Online algorithms:0.1,Parallel algorithms:0.3,Parameterized complexity and exact algorithms:0.1,Streaming, sublinear and near linear time algorithms:0.1",Algorithm design techniques,"The paper proposes a novel algorithmic framework for nonlinear control systems using generalized immersion, which directly maps to 'Algorithm design techniques'. 'Mathematical optimization' is relevant but secondary as the focus is on the algorithm structure rather than optimization theory. Other categories are unrelated to the core contribution."
5282,Transforming floundering into success,"Abstract We show how logic programs with “delays” can be transformed to programs without delays in a way that preserves information concerning floundering (also known as deadlock). This allows a declarative (model-theoretic), bottom-up or goal-independent approach to be used for analysis and debugging of properties related to floundering. We rely on some previously introduced restrictions on delay primitives and a key observation which allows properties such as groundness to be analysed by approximating the (ground) success set.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.2,Theory of computation:0.8,Mathematics of computing:0.3,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Theory of computation,Theory of computation is relevant for analyzing logic program transformations and floundering properties. Other categories are rejected because the paper focuses on theoretical semantics and analysis rather than implementation or application domains.,"Design and analysis of algorithms:0.6,Logic:0.85,Semantics and reasoning:0.7,Models of computation:0.5,Formal languages and automata theory:0.3","Logic,Semantics and reasoning",Logic is highly relevant as the paper focuses on transforming logic programs with delays. Semantics and reasoning is relevant due to the model-theoretic analysis of floundering. Design and analysis of algorithms is less central as the paper emphasizes theoretical analysis rather than algorithm design.,"Abstraction:0,Automated reasoning:0.1,Constraint and logic programming:1,Constructive mathematics:0,Description logics:0,Equational logic and rewriting:0.2,Finite Model Theory:0,Higher order logic:0,Hoare logic:0,Linear logic:0,Logic and verification:0.2,Modal and temporal logics:0,Program constructs:0.1,Program reasoning:0.3,Program semantics:0.2,Programming logic:0.4,Proof theory:0.1,Separation logic:0,Type theory:0,Verification by model checking:0","Constraint and logic programming,Program reasoning",Constraint and logic programming is directly relevant as the paper focuses on logic program transformations and floundering analysis. Program reasoning is relevant due to the study of program properties like groundness and success sets. Other options like Programming logic are secondary but still related.
135,Minimum-Layer Upward Drawings of Trees,"An upward drawing of a rooted tree T is a planar straight-line drawing of T where the vertices of T are placed on a set of horizontal lines, called layers, such that for each vertex u of T, no child of u is placed on a layer vertically above the layer on which u has been placed. In this paper we give a linear-time algorithm to obtain an upward drawing of a given rooted tree T on the minimum number of layers. Moreover, if the given tree T is not rooted, we can select a vertex r of T in linear time such that an upward drawing of T rooted at r would require the minimum number of layers among all the upward drawings of T with any of its vertices as the root. We also extend our results on a rooted tree to give an algorithm for an upward drawing of a rooted ordered tree. To the best of our knowledge, there is no previous algorithm for obtaining an upward drawing of a tree on the minimum number of layers.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.9,Mathematics of computing:0.2,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Theory of computation,"Theory of computation: The paper presents algorithms for drawing trees on the minimum number of layers, which is a graph drawing/visualization problem in computational geometry and graph algorithms. Other fields: The paper does not focus on hardware design, network communication, software engineering, information systems, etc.","Computational complexity and cryptography:0.1,Design and analysis of algorithms:1.0,Formal languages and automata theory:0.1,Logic:0.1,Models of computation:0.1,Randomness, geometry and discrete structures:0.1,Semantics and reasoning:0.1,Theory and algorithms for application domains:0.1",Design and analysis of algorithms,Design and analysis of algorithms is highly relevant for the linear-time algorithm presented. Other categories are irrelevant as the focus is purely on the algorithmic solution for graph drawing.,"Algorithm design techniques:1.0,Approximation algorithms analysis:0.3,Concurrent algorithms:0.1,Data structures design and analysis:0.2,Distributed algorithms:0.1,Graph algorithms analysis:1.0,Mathematical optimization:0.4,Online algorithms:0.1,Parallel algorithms:0.2,Parameterized complexity and exact algorithms:0.1,Streaming, sublinear and near linear time algorithms:0.1","Algorithm design techniques,Graph algorithms analysis","Algorithm design techniques (linear-time tree drawing algorithm), Graph algorithms analysis (upward tree drawing optimization)."
2629,A domain equation for refinement of partial systems,"A reactive system can be specified by a labelled transition system, which indicates static structure, along with temporal-logic formulas, which assert dynamic behaviour. But refining the former while preserving the latter can be difficult, because: (i) Labelled transition systems are ‘total’ – characterised up to bisimulation – meaning that no new transition structure can appear in a refinement. (ii) Alternatively, a refinement criterion not based on bisimulation might generate a refined transition system that violates the temporal properties. In response, Larsen and Thomson proposed modal transition systems, which are ‘partial’, and defined a refinement criterion that preserved formulas in Hennessy–Milner logic. We show that modal transition systems are, up to a saturation condition, exactly the mixed transition systems of Dams that meet a mix condition, and we extend such systems to non-flat state sets. We then solve a domain equation over the mixed powerdomain whose solution is a bifinite domain that is universal for all saturated modal transition systems and is itself fully abstract when considered as a modal transition system. We demonstrate that many frameworks of partial systems can be translated into the domain: partial Kripke structures, partial bisimulation structures, Kripke modal transition systems, and pointer-shape-analysis graphs.","General and reference:0,Hardware:0,Computer systems organization:0.1,Networks:0,Software and its engineering:0.2,Theory of computation:1,Mathematics of computing:0.3,Information systems:0,Security and privacy:0,Human-centered computing:0,Computing methodologies:0.2,Applied computing:0,Social and professional topics:0",Theory of computation,Theory of computation is highly relevant as the paper introduces domain equations and formal methods for system refinement. Software and its engineering is less relevant as the focus is on theoretical models rather than practical software design.,"Computational complexity and cryptography:0.2,Design and analysis of algorithms:0.2,Formal languages and automata theory:0.85,Logic:0.75,Models of computation:0.2,Randomness, geometry and discrete structures:0.2,Semantics and reasoning:0.8,Theory and algorithms for application domains:0.2","Formal languages and automata theory,Semantics and reasoning,Logic","Formal languages and automata theory: The paper discusses formal methods for system refinement using transition systems. Semantics and reasoning: The paper focuses on semantics of transition systems and refinement criteria. Logic: The work involves modal logic and preservation of formulas. Other categories are less relevant as the paper focuses on formal methods rather than algorithms, complexity, or discrete structures.","Abstraction:0.3,Automata extensions:0.1,Automata over infinite objects:0.1,Automated reasoning:0.2,Constraint and logic programming:0.1,Constructive mathematics:0.1,Description logics:0.1,Equational logic and rewriting:0.2,Finite Model Theory:0.1,Formalisms:0.1,Grammars and context-free languages:0.1,Higher order logic:0.1,Hoare logic:0.1,Linear logic:0.1,Logic and verification:0.4,Modal and temporal logics:0.8,Program constructs:0.1,Program reasoning:0.3,Program semantics:0.5,Programming logic:0.4,Proof theory:0.1,Regular languages:0.1,Separation logic:0.1,Tree languages:0.1,Type theory:0.1,Verification by model checking:0.2","Modal and temporal logics,Program semantics",Modal and temporal logics: The paper addresses refinement of systems using modal logics. Program semantics: The research develops a domain equation for system semantics. Other categories like 'Logic and verification' are broader but less specific than the paper's focus on modal logics and semantic structures.
1403,An Importance Sampling Algorithm for Models with Weak Couplings,"We propose an importance sampling algorithm to estimate the partition function of the Ising model and the q-state Potts model. The proposal (auxiliary) distribution is defined on a spanning tree of the Forney factor graph representing the model, and computations are done on the remaining edges. In contrast, in an analogous importance sampling algorithm in the dual Forney factor graph, computations are done on a spanning tree, and the proposal distribution is defined on the remaining edges.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:1.0,Mathematics of computing:0.75,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Theory of computation,Theory of computation: The paper introduces an importance sampling algorithm for probabilistic models. Mathematics of computing: The Ising/Potts models have mathematical foundations but the core contribution is algorithmic.,"Computational complexity and cryptography:0.0,Design and analysis of algorithms:0.9,Formal languages and automata theory:0.0,Logic:0.0,Models of computation:0.0,Randomness, geometry and discrete structures:0.9,Semantics and reasoning:0.0,Theory and algorithms for application domains:0.0","Design and analysis of algorithms,Randomness, geometry and discrete structures","Design and analysis of algorithms is relevant due to the paper's focus on importance sampling algorithms for statistical models. Randomness, geometry and discrete structures receives a high score because the paper discusses Forney factor graphs and spanning trees, which relate to geometric and discrete structures.","Algorithm design techniques:0.8,Approximation algorithms analysis:0.3,Concurrent algorithms:0.1,Data structures design and analysis:0.2,Distributed algorithms:0.1,Graph algorithms analysis:0.4,Mathematical optimization:0.3,Online algorithms:0.5,Parallel algorithms:0.2,Parameterized complexity and exact algorithms:0.1,Streaming, sublinear and near linear time algorithms:0.1","Algorithm design techniques,Graph algorithms analysis",Algorithm design techniques is primary as the paper introduces a novel sampling method. Graph algorithms analysis is secondary due to the Forney factor graph context. Other options like approximation algorithms are less relevant.
138,A Theory of Distributed Systems,"The theory θ presented here is the smallest theory in the temporal logic TLB [10] that all distributed systems, according to our definition of a distributed system, must satisfy. θ is an instance of the classical modal logic S4.2. The central theorems of θ are stated here without proof. Proofs will appear in [10]. Logics like TLA [14] and TLRCS [18] are used for specifying computer programs and reasoning about their behaviors. Their usefulness for large distributed systems has yet to be proven. Systems with multi-thousand node networks exhibit inherently asynchronous concurrency. Logics like TLA and TLRCS only provide a sequential (interleaved) concurrent execution model. Hence TLA and the future-tense part of TLRCS should be instances of the modal logic S4.3.1 [11, p. 179], but we are unaware if this has been proven. Not having a fully asynchronous concurrent execution model makes proofs about distributed systems within these logics suspect. Like quantum systems, distributed systems have pairs of observables that are not simultaneously measurable which leads to inherent uncertainty in their behaviors. θ has an asynchronous concurrent execution model and accounts for this inherent uncertainty. Unlike S4.2, TLB has a second primitive modal operator called the “everywhere sometime” operator that mixes space and time. The reduction formulas of θ allow us to reduce distributed correctness proofs to finitely many program proofs running on single computers. For distributed systems with the peer process architecture, θ allow us to reduce distributed correctness proofs to a single program proof running on a single computer.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.9,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Theory of computation,"Theory of computation: The paper presents a theoretical framework for distributed systems using temporal logic. Other fields: The paper doesn't focus on hardware design, network communication, software engineering, information systems, etc.","Logic:0.9,Models of computation:0.8,Formal languages and automata theory:0.5,Semantics and reasoning:0.7,Design and analysis of algorithms:0.4","Logic,Models of computation",Logic is relevant because the paper develops a temporal logic theory for distributed systems. Models of computation is relevant for its asynchronous concurrency model. Semantics and reasoning receives moderate relevance for logical reasoning aspects. Design and analysis of algorithms is less relevant as the focus is on theoretical foundations rather than specific algorithms.,"Abstract machines:0.1,Abstraction:0.2,Automated reasoning:0.3,Computability:0.1,Concurrency:1.0,Constraint and logic programming:0.1,Constructive mathematics:0.1,Description logics:0.1,Equational logic and rewriting:0.1,Finite Model Theory:0.1,Higher order logic:0.1,Hoare logic:0.2,Interactive computation:0.1,Linear logic:0.1,Logic and verification:0.8,Modal and temporal logics:0.9,Probabilistic computation:0.1,Programming logic:0.2,Proof theory:0.2,Quantum computation theory:0.1,Separation logic:0.1,Streaming models:0.1,Timed and hybrid models:0.1,Type theory:0.1,Verification by model checking:0.2","Concurrency,Modal and temporal logics","Concurrency: The theory directly addresses distributed system concurrency. Modal and temporal logics: The paper presents θ as an instance of S4.2, a modal logic. Logic and verification is moderately relevant but less direct than the other two."
1445,Concept Dissimilarity Based on Tree Edit Distances and Morphological Dilations,"A number of similarity measures for comparing description logic concepts have been proposed. Criteria have been developed to evaluate a measure's fitness for an application. These criteria include on the one hand those that ensure compatibility with the semantics, such as equivalence soundness, and on the other hand the properties of a metric, such as the triangle inequality. In this work we present two classes of dissimilarity measures that are at the same time equivalence sound and satisfy the triangle inequality: a simple dissimilarity measure, based on description trees for the lightweight description logic ℇL; and an instantiation of a general framework, presented in our previous work, using dilation operators from mathematical morphology, and which exploits the link between Hausdorff distance and dilations using balls of the ground distance as structuring elements.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:1.0,Mathematics of computing:0.3,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Theory of computation,"Theory of computation: The work develops formal concept dissimilarity measures for description logic, a theoretical foundations problem. Mathematics of Computing is secondary to the computational logic framework.","Computational complexity and cryptography:0.3,Design and analysis of algorithms:0.8,Formal languages and automata theory:0.4,Logic:0.5,Models of computation:0.3,Randomness, geometry and discrete structures:0.4,Semantics and reasoning:1.0,Theory and algorithms for application domains:0.6","Semantics and reasoning,Design and analysis of algorithms",Semantics and reasoning is relevant due to the focus on concept dissimilarity measures in description logic. Design and analysis of algorithms is relevant as the paper discusses algorithmic properties like equivalence soundness and metric properties.,"Algorithm design techniques:1,Approximation algorithms analysis:0,Concurrent algorithms:0,Data structures design and analysis:0,Distributed algorithms:0,Graph algorithms analysis:1,Mathematical optimization:0,Online algorithms:0,Parallel algorithms:0,Parameterized complexity and exact algorithms:0,Program constructs:0,Program reasoning:0,Program semantics:0,Streaming, sublinear and near linear time algorithms:0","Algorithm design techniques,Graph algorithms analysis","Algorithm design techniques: The paper introduces dissimilarity measures using tree edit distances and morphological dilations. Graph algorithms analysis: Tree edit distances are a classic graph algorithm. Other categories were rejected as the focus is on algorithm design for concept dissimilarity, not optimization or complexity."
4809,A linear-time algorithm for testing the inscribability of trivalent polyhedra,"We present an algorithm for testing the inscribability of a trivalent polyhedron, or, equivalently, testing the circumscribability of a simplicial polyhedron. Our algorithm runs in linear time, using only low-precision integer arithmetic. The algorithm is based on a purely combinatorial characterization of inscribable trivalent polyhedra.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.9,Mathematics of computing:0.3,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.3,Applied computing:0.1,Social and professional topics:0.1",Theory of computation,"Theory of computation is highly relevant as the paper presents a theoretical algorithm for geometric inscribability. Computing methodologies is moderately relevant for the algorithmic approach, but the focus is on theoretical foundations. Other categories are irrelevant.","Computational complexity and cryptography:0.1,Design and analysis of algorithms:0.9,Formal languages and automata theory:0.1,Logic:0.1,Models of computation:0.6,Randomness, geometry and discrete structures:0.7,Semantics and reasoning:0.1,Theory and algorithms for application domains:0.2","Design and analysis of algorithms,Randomness, geometry and discrete structures","Design and analysis of algorithms: The paper presents a linear-time algorithm for inscribability testing. Randomness, geometry and discrete structures: The algorithm deals with geometric structures (polyhedra). Models of computation is moderately relevant as it's about algorithmic models for geometric problems.","Algorithm design techniques:1,Graph algorithms analysis:1,Streaming, sublinear and near linear time algorithms:0.8","Algorithm design techniques,Graph algorithms analysis","Algorithm design techniques: The paper introduces a novel combinatorial characterization leading to a linear-time algorithm. Graph algorithms analysis: The algorithm is tested on trivalent polyhedra, which are graph-like structures. Streaming algorithms are less directly relevant as the focus is on combinatorial properties rather than data stream processing."
5352,Relativized hyperequivalence of logic programs for modular programming,"Abstract A recent framework of relativized hyperequivalence of programs offers a unifying generalization of strong and uniform equivalence. It seems to be especially well suited for applications in program optimization and modular programming due to its flexibility that allows us to restrict, independently of each other, the head and body alphabets in context programs. We study relativized hyperequivalence for the three semantics of logic programs given by stable, supported, and supported minimal models. For each semantics, we identify four types of contexts, depending on whether the head and body alphabets are given directly or as the complement of a given set. Hyperequivalence relative to contexts where the head and body alphabets are specified directly has been studied before. In this paper, we establish the complexity of deciding relativized hyperequivalence with respect to the three other types of context programs.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.3,Theory of computation:0.8,Mathematics of computing:0.2,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Theory of computation,Theory of computation is highly relevant for the logic program equivalence analysis. Other categories are rejected as the paper focuses on theoretical foundations of logic programming rather than practical systems or applications.,"Computational complexity and cryptography:0.6,Design and analysis of algorithms:0.2,Formal languages and automata theory:0.3,Logic:1.0,Models of computation:0.2,Randomness, geometry and discrete structures:0.1,Semantics and reasoning:0.8,Theory and algorithms for application domains:0.4","Logic,Semantics and reasoning",Logic: The paper studies relativized hyperequivalence for logic program semantics. Semantics and reasoning: It analyzes program equivalence under different context constraints. Other categories are rejected as the paper focuses on logical frameworks rather than computational complexity or algorithms.,"Abstraction:0.0,Automated reasoning:0.1,Constraint and logic programming:1.0,Constructive mathematics:0.0,Description logics:0.0,Equational logic and rewriting:0.0,Finite Model Theory:0.0,Higher order logic:0.0,Hoare logic:0.0,Linear logic:0.0,Logic and verification:0.2,Modal and temporal logics:0.0,Program constructs:0.0,Program reasoning:1.0,Program semantics:0.3,Programming logic:0.8,Proof theory:0.0,Separation logic:0.0,Type theory:0.0,Verification by model checking:0.0","Constraint and logic programming,Program reasoning,Programming logic",Constraint and logic programming is central to the study of relativized hyperequivalence in logic programs. Program reasoning is relevant as the paper analyzes program equivalence and semantics. Programming logic is relevant due to the focus on logic-based program analysis. Other categories like Hoare logic or Type theory are irrelevant as the paper does not discuss program verification or type systems.
3862,Multiple Keyword Pattern Matching using Position Encoded Pattern Lattices,"Formal concept analysis is used as the basis for two new multiple keyword string pattern matching algorithms. The algorithms ad- dressed are built upon a so-called position encoded pattern lattice (PEPL). The algorithms presented are in conceptual form only; no experimental results are given. The first algorithm to be presented is easily understood and relies directly on the PEPL for matching. Its worst case complexity depends on both the length of the longest keyword, and the length of the search text. Subsequently a finite-automaton-like structure, called a PEPL automaton, is defined which is derived from the PEPL, and which forms the basis for a second more efficient algorithm. In this case, worst case behaviour depends only on the length of the input stream. The second algorithm's worst case performance is the same as the match- ing phase of the well-known (advanced) Aho-Corasick multiple-keyword pattern matching algorithm—widely regarded as the multiple keyword pattern matching algorithm of choice in contexts such as network in- trusion detection. The first algorithm's performance is comparable to that of the matching phase of the lesser-known failure-function version of Aho-Corasick.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.8,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Theory of computation,"Theory of computation: The paper develops pattern matching algorithms with complexity analysis, which is a core theoretical computer science contribution. Other categories are irrelevant to the algorithmic focus.","Computational complexity and cryptography:0.3,Design and analysis of algorithms:1.0,Formal languages and automata theory:0.75,Logic:0.2,Models of computation:0.4,Randomness, geometry and discrete structures:0.3,Semantics and reasoning:0.2,Theory and algorithms for application domains:0.3","Design and analysis of algorithms,Formal languages and automata theory",Design and analysis of algorithms is central to the PEPL-based pattern matching. Formal languages and automata theory is relevant for the PEPL automaton structure. Other fields are not primary to the algorithmic contribution.,"Algorithm design techniques:1,Approximation algorithms analysis:0,Automata extensions:1,Automata over infinite objects:0,Concurrent algorithms:0,Data structures design and analysis:0,Distributed algorithms:0,Formalisms:0,Grammars and context-free languages:0,Graph algorithms analysis:0,Mathematical optimization:0,Online algorithms:0,Parallel algorithms:0,Parameterized complexity and exact algorithms:0,Regular languages:0,Streaming, sublinear and near linear time algorithms:0,Tree languages:0","Algorithm design techniques,Automata extensions",The paper introduces new algorithms (design techniques) and automata-based structures (automata extensions). Other algorithm categories are not central.
4685,Reassembling Trees for the Traveling Salesman,"Many recent approximation algorithms for different variants of the traveling salesman problem (asymmetric TSP, graph TSP, $s$-$t$-path TSP) exploit the well-known fact that a solution of the natural linear programming relaxation can be written as convex combination of spanning trees. The main argument then is that randomly sampling a tree from such a distribution and then completing the tree to a tour at minimum cost yields a better approximation guarantee than simply taking a minimum cost spanning tree (as in Christofides' algorithm). We argue that an additional step can help: reassembling the spanning trees before sampling. Exchanging two edges in a pair of spanning trees can improve their properties under certain conditions. We demonstrate the usefulness for the metric $s$-$t$-path TSP by devising a deterministic polynomial-time algorithm that improves on Sebo's previously best approximation ratio of $\frac{8}{5}$.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.95,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Theory of computation,"Theory of computation: The paper develops approximation algorithms for TSP variants, a foundational problem in theoretical computer science. 'Mathematics of computing' is rejected as the focus is on algorithm design, not pure mathematics.","Computational complexity and cryptography:0.1,Design and analysis of algorithms:1.0,Formal languages and automata theory:0.1,Logic:0.1,Models of computation:0.1,Randomness, geometry and discrete structures:0.3,Semantics and reasoning:0.1,Theory and algorithms for application domains:0.2",Design and analysis of algorithms,Design and analysis of algorithms is directly relevant as the paper proposes a novel algorithm for TSP approximation. Other categories like randomness or application domains are only tangentially related to the core contribution.,"Algorithm design techniques:0.2,Approximation algorithms analysis:0.9,Concurrent algorithms:0.1,Data structures design and analysis:0.1,Distributed algorithms:0.1,Graph algorithms analysis:0.8,Mathematical optimization:0.3,Online algorithms:0.1,Parallel algorithms:0.1,Parameterized complexity and exact algorithms:0.1,Streaming, sublinear and near linear time algorithms:0.1","Approximation algorithms analysis,Graph algorithms analysis",Approximation algorithms analysis is directly relevant for the TSP approximation ratio improvements. Graph algorithms analysis applies to the tree-based TSP solutions. Other options like Algorithm design techniques are secondary as the focus is on specific algorithmic analysis rather than general design principles.
3270,New Collection Announcement: Focused Retrieval Over the Web,"Focused retrieval (a.k.a., passage retrieval) is important at its own right and as an intermediate step in question answering systems. We present a new Web-based collection for focused retrieval. The document corpus is the Category A of the ClueWeb12 collection. Forty-nine queries from the educational domain were created. The $100$ documents most highly ranked for each query by a highly effective learning-to-rank method were judged for relevance using crowdsourcing. All sentences in the relevant documents were judged for relevance.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.9,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Information systems,Information systems: The paper presents a web-based dataset for focused retrieval with crowdsourced annotations. Other options are irrelevant as the paper focuses on information retrieval rather than security or software engineering.,"Data management systems:0.2,Information retrieval:0.9,Information storage systems:0.0,Information systems applications:0.3,World Wide Web:0.7","Information retrieval,World Wide Web",Information retrieval is the core domain of the paper's focus on passage-level Web search. World Wide Web is relevant due to the Web-based collection. Data management systems is less central as the focus is on retrieval rather than storage/management.,"Document representation:0.3,Evaluation of retrieval results:0.8,Information retrieval query processing:0.7,Online advertising:0.1,Retrieval models and ranking:0.5,Retrieval tasks and goals:0.9,Search engine architectures and scalability:0.4,Specialized information retrieval:0.6,Users and interactive retrieval:0.3,Web applications:0.4,Web data description languages:0.2,Web interfaces:0.2,Web mining:0.5,Web searching and information discovery:0.7,Web services:0.3","Evaluation of retrieval results,Retrieval tasks and goals",Evaluation of retrieval results is highly relevant due to crowdsourced relevance judgments. Retrieval tasks and goals is central as the paper introduces a new dataset for focused retrieval. Other children like Web mining or Web searching are less directly relevant to the core contribution.
830,Querying for video events by semantic signatures from few examples,"We aim to query web video for complex events using only a handful of video query examples, where the standard approach learns a ranker from hundreds of examples. We consider a semantic signature representation, consisting of off-the-shelf concept detectors, to capture the variance in semantic appearance of events. Since it is unknown what similarity metric and query fusion to use in such an event retrieval setting, we perform three experiments on unconstrained web videos from the TRECVID event detection task. It reveals that: retrieval with semantic signatures using normalized correlation as similarity metric outperforms a low-level bag-of-words alternative, multiple queries are best combined using late fusion with an average operator, and event retrieval is preferred over event classification when less than eight positive video examples are available.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.2,Information systems:0.8,Security and privacy:0.1,Human-centered computing:0.2,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Information systems,Information systems is relevant for video event querying and semantic signatures. Other categories are rejected as the paper focuses on information retrieval rather than algorithms or human-computer interaction.,"Information retrieval:1.0,Data management systems:0.5,Information storage systems:0.0,Information systems applications:0.0,World Wide Web:0.0",Information retrieval,Information retrieval is directly addressed in the event query system. Data management systems is less relevant as the focus is on retrieval rather than storage.,"Document representation:0.2,Evaluation of retrieval results:0.8,Information retrieval query processing:1.0,Retrieval models and ranking:1.0,Retrieval tasks and goals:0.5,Search engine architectures and scalability:0.3,Specialized information retrieval:1.0,Users and interactive retrieval:0.4","Information retrieval query processing,Retrieval models and ranking,Specialized information retrieval",Information retrieval query processing: The paper focuses on querying video events using semantic signatures. Retrieval models and ranking: It evaluates normalized correlation as a similarity metric. Specialized information retrieval: The work is tailored to video event retrieval. Other categories like Document representation are less relevant as the focus is on querying rather than representation.
4625,Portals for collaborative research communities: two distinguished case studies,"Case study research excels at bringing us to an understanding of a complex issue or object and can extend experience or add strength to what is already known through previous research. The research work summarized by this paper discusses two different case studies in the field of portals for collaborative research communities, in particular VectorBase and BGA‐Space. VectorBase at its core is a scientific database that focuses on search, data mining and offers multiple integrated bioinformatics tools for analyzing and browsing genomic and related data. BGA‐Space focuses on capturing semantics from scientists during processing of scientific experiments as well as preserving the full life cycle of scientific data to enable their reuse. The two case studies involve heavy research and the application of theories, concepts, and knowledge commonly discussed in the targeted field. Copyright © 2010 John Wiley & Sons, Ltd.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.3,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.9,Security and privacy:0.2,Human-centered computing:0.1,Computing methodologies:0.3,Applied computing:0.1,Social and professional topics:0.1",Information systems,Information systems: Focuses on data management and semantic preservation in scientific portals. Other categories like 'Software and its engineering' or 'Computing methodologies' are secondary to the primary database and information system focus.,"Data management systems:0.8,Information retrieval:0.7,Information storage systems:0.6,Information systems applications:0.5,World Wide Web:0.2","Data management systems,Information retrieval",Data management systems is relevant for the scientific database design in VectorBase. Information retrieval is relevant for the data mining and search capabilities discussed. World Wide Web is less relevant as the focus is on data systems rather than web technologies.,"Data structures:0,Database administration:0.9,Database design and models:0.8,Database management system engines:0.7,Document representation:0,Evaluation of retrieval results:0,Information integration:0.6,Information retrieval query processing:0,Middleware for databases:0,Query languages:0,Retrieval models and ranking:0,Retrieval tasks and goals:0,Search engine architectures and scalability:0,Specialized information retrieval:0,Users and interactive retrieval:0","Database administration,Database design and models",Database administration is relevant as the paper discusses scientific database management systems. Database design and models is relevant due to the focus on data organization and integration. Information integration is secondary but still relevant for data lifecycle management.
3572,Efficient position-independent iconic search using an R-theta index,"An iconic image database is a collection of symbolic images where each image is a collection of labeled point features called icons. A method is presented to support fast position-independent similarity search in an iconic database for symbolic images where the similarity condition involves finding icon pairs that satisfy a specific spatial relationship. This is achieved by introducing an index data structure based on r-θ space, which corresponds to the Cartesian product of separation (i.e., inter-icon distance) and (some representation of) relative spatial orientation. In this space, each pairing of two icons is represented by a single point, and all pairs with the same separation and relative orientation (regardless of absolute position) map to the same point. Similarly, all icon pairs with the same separation but different relative orientations map to points on a line parallel to the θ axis, while all pairs with different separations but the same relative orientation map to points on a line parallel to the r axis. Using such an index, database search for icon pairs with a given spatial relationship or range is accomplished by examining the subarea of the index space into which desired pairs would map. This r-θ index space can be organized using well-known spatial database techniques, such as quadtrees or R-trees. Although the size of such an index grows only linearly with respect to the number of images in the collection, it grows quadratically with the average number of icons in an image. A scheme is described to reduce the size of the index by pruning away a subset of the pairs, at the cost of incurring additional work when searching the database. This pruning is governed by a parameter φ, whose variation provides a continuous range of trade-offs between index size and search time.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:1.0,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Information systems,Information systems is relevant for the database indexing technique. Other fields do not address spatial data structures or query optimization.,"Data management systems:0.8,Information retrieval:0.9,Information storage systems:0.1,Information systems applications:0.1,World Wide Web:0.1","Information retrieval,Data management systems","The paper introduces an r-θ index for symbolic image search, aligning with 'Information retrieval'. The index structure relates to 'Data management systems'. Other categories like Web are not discussed.","Data structures:1,Database administration:0.2,Database design and models:0.8,Database management system engines:0.3,Document representation:0.1,Evaluation of retrieval results:0.4,Information integration:0.2,Information retrieval query processing:0.7,Middleware for databases:0.1,Query languages:0.3,Retrieval models and ranking:0.6,Retrieval tasks and goals:0.4,Search engine architectures and scalability:0.5,Specialized information retrieval:0.6,Users and interactive retrieval:0.2","Data structures,Database design and models,Information retrieval query processing",Data structures: The r-θ index is a novel data structure for spatial queries. Database design and models: The paper discusses how to organize symbolic image databases. Information retrieval query processing: The focus is on similarity search with spatial constraints. Rejected categories like Query languages are irrelevant as the paper introduces a new indexing method rather than query language design.
3423,SkylineSearch: semantic ranking and result visualization for pubmed,"Life sciences researchers perform scientific literature search as part of their daily activities. Many such searches are executed against PubMed, a central repository of life sciences articles, and often return hundreds, or even thousands, of results, pointing to the need for data exploration tools. In this demonstration we present SkylineSearch, a semantic ranking and result visualization system designed specifically for PubMed, and available to the scientific community at skyline.cs.columbia.edu. Our system leverages semantic annotations of articles with terms from the MeSH controlled vocabulary, and presents results as a two-dimensional skyline, plotting relevance against publication date. We demonstrate that SkylineSearch supports a richer data exploration experience than does the search functionality of PubMed, allowing users to find relevant references more easily. We also show that SkylineSearch executes queries and presents results in interactive time.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:1.0,Security and privacy:0.0,Human-centered computing:0.3,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Information systems,"The paper presents a semantic search and visualization system for PubMed, aligning with Information systems. Human-centered computing is secondary due to user interface aspects.","Information retrieval:0.9,Data management systems:0.75,Information systems applications:0.8,Information storage systems:0.3,World Wide Web:0.2","Information retrieval,Information systems applications,Data management systems",Information retrieval is core to the semantic search system. Information systems applications relates to its PubMed deployment. Data management systems connects to the semantic annotations. Storage systems and web are less emphasized.,"Collaborative and social computing systems and tools:0.3,Computational advertising:0.1,Computing platforms:0.2,Data mining:0.3,Data structures:0.2,Database administration:0.2,Database design and models:0.3,Database management system engines:0.2,Decision support systems:0.4,Digital libraries and archives:0.5,Document representation:0.3,Enterprise information systems:0.3,Evaluation of retrieval results:0.4,Information integration:0.3,Information retrieval query processing:0.5,Middleware for databases:0.2,Mobile information processing systems:0.2,Multimedia information systems:0.1,Process control systems:0.1,Query languages:0.3,Retrieval models and ranking:0.6,Retrieval tasks and goals:0.5,Search engine architectures and scalability:0.3,Spatial-temporal systems:0.1,Specialized information retrieval:0.7,Users and interactive retrieval:0.6","Specialized information retrieval,Retrieval models and ranking",Specialized information retrieval is highly relevant for the PubMed-specific search system. Retrieval models and ranking is relevant for the semantic ranking approach. Digital libraries and archives is moderately relevant for the PubMed context. Other categories are rejected as the focus is on semantic ranking rather than general information integration or query processing.
1289,Data mining from web search queries: A comparison of google trends and baidu index,"Numerous studies have explored the possibility of uncovering information from web search queries but few have examined the factors that affect web query data sources. We conducted a study that investigated this issue by comparing Google Trends and Baidu Index. Data from these two services are based on queries entered by users into Google and Baidu, two of the largest search engines in the world. We first compared the features and functions of the two services based on documents and extensive testing. We then carried out an empirical study that collected query volume data from the two sources. We found that data from both sources could be used to predict the quality of Chinese universities and companies. Despite the differences between the two services in terms of technology, such as differing methods of language processing, the search volume data from the two were highly correlated and combining the two data sources did not improve the predictive power of the data. However, there was a major difference between the two in terms of data availability. Baidu Index was able to provide more search volume data than Google Trends did. Our analysis showed that the disadvantage of Google Trends in this regard was due to Google's smaller user base in China. The implication of this finding goes beyond China. Google's user bases in many countries are smaller than that in China, so the search volume data related to those countries could result in the same issue as that related to China.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.9,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Information systems,"Information systems: The paper analyzes data mining from web search queries to predict real-world phenomena (e.g., university/company quality). This aligns with information systems research focused on data analysis and knowledge extraction. Other categories like Networks or Computing Methodologies are not the primary domain.","Data management systems:0.4,Information retrieval:1.0,Information storage systems:0.3,Information systems applications:0.6,World Wide Web:0.8","Information retrieval,World Wide Web",Information retrieval is highly relevant for analyzing search query data. World Wide Web is relevant for the context of search engines. Data management systems is less central.,"Document representation:0,Evaluation of retrieval results:0,Information retrieval query processing:0.3,Online advertising:0,Retrieval models and ranking:0,Retrieval tasks and goals:0,Search engine architectures and scalability:0.5,Specialized information retrieval:0,Users and interactive retrieval:0,Web applications:0,Web data description languages:0,Web interfaces:0,Web mining:1,Web searching and information discovery:1,Web services:0","Web mining,Web searching and information discovery",Web mining: The paper analyzes search query data for predictive modeling. Web searching and information discovery: The study compares search engine data sources. Information retrieval query processing is only weakly relevant.
2808,Optimized Encodings for Consistent Query Answering via ASP from Different Perspectives,"A data integration system provides transparent access to different data sources by suitably combining their data, and providing the user with a unified view of them, called global schema. However, source data are generally not under the control of the data integration process, thus integrated data may violate global integrity constraints even in presence of locally-consistent data sources. In this scenario, it may be anyway interesting to retrieve as much consistent information as possible. The process of answering user queries under global constraint viola- tion is called consistent query answering (CQA). Several notions of CQA have been proposed, e.g. depending on whether the information in the database is as- sumed to be sound or complete. This paper provides a contribution in this setting; it uniforms solutions coming from different perspectives under a common core and provides some optimizations designed to identify and isolate the inefficient part of the computation of consistent answers. The effectiveness of the approach is evidenced by experimental results reported in the paper.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:1.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.5,Applied computing:0.0,Social and professional topics:0.0",Information systems,Information systems is relevant because the paper addresses data integration and query answering. Computing methodologies is secondary due to ASP techniques but not the core focus.,"Data management systems:0.9,Information retrieval:0.3,Information storage systems:0.4,Information systems applications:0.5,World Wide Web:0.2",Data management systems,Data management systems is highly relevant as the paper focuses on consistent query answering in data integration. Other categories like Information systems applications are less central to the core contribution.,"Data structures:0,Database administration:0,Database design and models:0.5,Database management system engines:0,Information integration:1,Middleware for databases:0,Query languages:0.8","Information integration,Query languages",Information integration is directly relevant as the paper focuses on consistent query answering in data integration systems. Query languages is secondary as ASP is used for encoding queries. Database design is less relevant as the focus is on query answering under constraints.
5700,A performance evaluation of a new bitmap-based XML processing approach over RDBMS,"This paper presents a comprehensive performance analysis of PACD; a novel bitmap-based XML processing approach introduced earlier to resolve several performance issues identified in existing XML database technology. The study evaluated three performance aspects of XML database techniques including query processing, XML updates and scalability. Each of these aspects has been tested using various measures and compared with some representative alternative approaches. Despite its narrow domain for the order-access queries and its high cost in terms of the number IO-read operations, PACD almost performed well in terms of query processing, resource consumption during XML updates and has shown acceptable scalability over a variety of XML database categories.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.2,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.9,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Information systems,"Information systems: The paper evaluates XML processing techniques in databases, a core data management topic. Computing methodologies is secondary as the focus is on database performance rather than general methodologies.","Data management systems:0.8,Information retrieval:0.7,Information storage systems:0.5,Information systems applications:0.3,World Wide Web:0.1","Data management systems,Information retrieval",Data management systems: The paper evaluates XML processing over RDBMS. Information retrieval: Discusses query processing and scalability in XML databases. Other categories are less relevant as the focus is not on storage systems or web-specific applications.,"Data structures:0.4,Database administration:0.1,Database design and models:0.7,Database management system engines:0.3,Document representation:0.2,Evaluation of retrieval results:0.2,Information integration:0.1,Information retrieval query processing:0.6,Middleware for databases:0.1,Query languages:0.6,Retrieval models and ranking:0.2,Retrieval tasks and goals:0.1,Search engine architectures and scalability:0.3,Specialized information retrieval:0.2,Users and interactive retrieval:0.1","Database design and models,Information retrieval query processing",Database design and models: The paper introduces a new XML processing approach. Information retrieval query processing: Evaluates query performance. Other categories like query languages are secondary.
3017,Change-a-LOD: Does the Schema on the Linked Data Cloud Change or Not?,"Recent work analyzing changes on the Linked Open Data (LOD) cloud on fine-grained weekly snapshots shows that vocabularies published on the cloud are highly static. While this result is quite expected, there is another kind of schematic information that can be observed on the LOD cloud: the use of the vocabularies in the cloud. With use, we mean the combinations of sets of properties and sets of types to describe the resources in a specific domain. Current literature does not tackle this question sufficiently. In order to gain insight into how the use of vocabularies on the LOD cloud changes over time, we present illustrating examples and a formalization of the research question. Subsequently, we present early results of experiments applied on weekly snapshots that show that the use of vocabularies indeed changes quite a lot over time.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:1.0,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.2,Social and professional topics:0.1",Information systems,Information systems is highly relevant as the paper studies schema evolution in the LOD cloud. Applied computing is secondary due to practical implications for data management.,"Data management systems:1.0,Information retrieval:0.7,Information storage systems:0.4,Information systems applications:0.5,World Wide Web:0.9","Data management systems,World Wide Web",Data management systems are highly relevant for analyzing vocabulary usage evolution in LOD. World Wide Web is relevant for the LOD cloud context. Information retrieval is less central as the focus is on schema dynamics rather than search algorithms.,"Data structures:0.5,Database administration:0.3,Database design and models:0.4,Database management system engines:0.2,Information integration:0.6,Middleware for databases:0.3,Online advertising:0.1,Query languages:0.3,Web applications:0.4,Web data description languages:1.0,Web interfaces:0.5,Web mining:1.0,Web searching and information discovery:0.6,Web services:0.4","Web data description languages,Web mining",Web data description languages: The paper analyzes vocabulary usage in Linked Open Data schemas. Web mining: The study involves temporal analysis of data patterns. Other categories like 'Database design' are less relevant as the focus is on schema evolution rather than database structure.
3602,A Comparison of WordNet and Roget’s Taxonomy for Measuring Semantic Similarity,"This paper presents the results of using Roget’s International Thesaurus as the taxonomy in a semantic similarity measurement task. Four similarity metrics were taken from the literature and applied to Roget’s. The experimental evaluation suggests that the traditional edge counting approach does surprisingly well (a correlation of r=0.88 with a benchmark set of human similarity judgements, with an upper bound of r=0.90 for human subjects performing the same task.)","General and reference:0.1,Hardware:0.3,Computer systems organization:0.4,Networks:0.2,Software and its engineering:0.2,Theory of computation:0.2,Mathematics of computing:0.3,Information systems:1.0,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.5,Applied computing:0.3,Social and professional topics:0.1",Information systems,"Information systems: The paper compares taxonomies (WordNet, Roget’s Thesaurus) for semantic similarity, which relates to information organization and retrieval. 'Mathematics of computing' and 'Computing methodologies' are less central as the focus is on information representation rather than algorithms or mathematical theory.","Data management systems:0.05,Information retrieval:1.0,Information storage systems:0.1,Information systems applications:0.2,World Wide Web:0.05",Information retrieval,Information retrieval is core as the paper evaluates semantic similarity metrics using thesaurus-based taxonomies. Other options are not central to the paper's methodology or applications.,"Document representation:0.2,Evaluation of retrieval results:1.0,Information retrieval query processing:0.4,Retrieval models and ranking:1.0,Retrieval tasks and goals:0.6,Search engine architectures and scalability:0.2,Specialized information retrieval:0.3,Users and interactive retrieval:0.2","Evaluation of retrieval results,Retrieval models and ranking","Evaluation of retrieval results is relevant due to the comparison of Roget’s taxonomy against human similarity judgments. Retrieval models and ranking is relevant because the paper applies similarity metrics to a thesaurus-based taxonomy. Other categories like Document representation are less relevant as the focus is on taxonomy structure, not document encoding."
4303,Towards Linked Data based Enterprise Information Integration,"Data integration in large enterprises is a crucial but at the same time costly, long lasting and challenging problem. In the last decade, the prevalent data integration approaches were primarily based on XML, Web Services and Service Oriented Architectures (SOA). We argue that classic SOA architectures may be well-suited for transaction processing, however more efficient technologies can be employed for enterprise data integration. In particular, the use of the Linked Data paradigm appears to be a very promising approach. In this article we explore challenges large enterprises are still facing with regard to data integration. We discuss Linked Data approaches in these areas and present some examples of successful applications of the Linked Data principles in that context.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.95,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Information systems,Information systems is highly relevant as the paper discusses enterprise data integration using Linked Data. Other categories are not relevant as the focus is on data management rather than networking or algorithms.,"Data management systems:0.75,Information retrieval:0.2,Information storage systems:0.1,Information systems applications:0.3,World Wide Web:1.0",World Wide Web,"World Wide Web is directly relevant as the paper explores Linked Data principles for enterprise integration, a web-centric paradigm. Data management systems (0.75) is secondary as the focus is on integration rather than database systems. Other fields like Information retrieval (0.2) are less relevant.","Online advertising:0,Web applications:0,Web data description languages:1,Web interfaces:0,Web mining:0,Web searching and information discovery:0,Web services:1","Web data description languages,Web services","Web data description languages: The paper discusses Linked Data, which relies on semantic web standards (e.g., RDF). Web services: The abstract mentions SOA and Web Services as prior approaches. Other categories are irrelevant as the paper does not focus on applications, mining, or interfaces."
476,A structured indexing model based on noun phrases,"Most of the indexing models are based on simple independent words, also known as key words. This approach does not take account of the context as well as the relations between the words. Therefore, the precision of system is limited. In this article, we present a structured indexing model based on noun phrases to increase the precision of an Information Retrieval System (IRS). In this model, we used a grammatical parser to extract and structure a noun phrase in determining the various roles of the words of a noun phrase and their syntactic relations. We represent the set of the index terms of query in the form of Bayesian networks which enables us to calculate the matching function between a query and a document. We carried out experiments to test this model. That the positive results obtained encourages us to continue in this direction. I. INTRODUCTION OST of the indexing models use simple index terms (simple words) with the assumption that they are independent. In a similar way, the Vector Space Model represents the documents by vectors of independent index terms. This assumption simplifies the representation of the index terms and decreases the complexity of the phase of interrogation (matching function). However, the precision of the system is not satisfactory. A promising research direction consists in using more complex index terms, like nouns phrases, with the hope to increase the precision of the system. Bruza et al. (3) (4) used index terms called index expression which are noun phrases. Bruza's method is based on the prepositions of the noun phrase to break it up into sub index expression"". An ""index expression"" is represented in the shape of a lattice which is used as a basis for the phase of interrogation. The phase of decomposition of an ""index expression"" in this method is based only on the prepositions""""""","General and reference:0.1,Hardware:0.2,Computer systems organization:0.3,Networks:0.1,Software and its engineering:0.3,Theory of computation:0.2,Mathematics of computing:0.3,Information systems:0.8,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.4,Applied computing:0.2,Social and professional topics:0.1",Information systems,Information systems is relevant for the noun-phrase-based indexing model in IRS. Other categories like Computing methodologies are less directly aligned with the information retrieval focus.,"Data management systems:0.1,Information retrieval:0.9,Information storage systems:0.1,Information systems applications:0.1,World Wide Web:0.1",Information retrieval,"Information retrieval: The paper presents a structured indexing model using noun phrases to improve retrieval precision. Other categories are irrelevant as the focus is on information retrieval systems, not data storage or web technologies.","Document representation:1.0,Evaluation of retrieval results:0.4,Information retrieval query processing:0.5,Retrieval models and ranking:1.0,Retrieval tasks and goals:0.3,Search engine architectures and scalability:0.2,Specialized information retrieval:0.3,Users and interactive retrieval:0.2","Document representation,Retrieval models and ranking",Document representation is central to the noun phrase-based indexing model. Retrieval models and ranking is relevant for the Bayesian network matching function. Other categories like Query processing are secondary aspects.
1557,A broader approach to personalization,"Personalization generally refers to making a Web site more responsive to the unique and individual needs of each user. We propose a broader approach to personalization that provides for interoperability and automation by using the recent data exchange, meta data and privacy standards from the World Wide Web Consortium (W3C), namely, Extensible Markup Language (XML) [12], Resource Description Framework (RDF) [9, 10] and Platform for Privacy Preferences (P3P) [8]. First we briefly discuss these standards. Extensible Markup Language (XML). XML has gained a great momentum and is emerging as the standard for data exchange on the Internet. XML data is self describing through content oriented tags and this enables a computer to understand the meaning of data and hence enhances the ability of remote applications to interpret and operate on documents fetched over the Internet. One of XML's strengths is its extensibility. Anyone can invent new tags for particular subject areas and they define what they mean in document type definitions (DTDs). But if every business uses its own XML definition for describing its data, it is not possible to achieve interoperability. In other words, a tagged document is not very useful without some agreement among inter-operating applications so as to what the tags mean and it is common DTDs which provide for this. A DTD specifies the structure of an XML document by specifying the names of its elements, sub-elements and attributes. XML-Query Language (XML-QL). The need to query XML documents to extract data is well addressed in the literature and one of the available languages is XML-QL [4]. XML-QL has a WHERE-CONSTRUCT clause, like the SELECT-WHERE of SQL, that can express queries, which extract pieces of data from XML documents, as well as transformations, which, for example, can map XML data between DTD's and can integrate XML data from different sources. Although XML-QL shares some functionalities with XML's style sheet mechanism, it supports more data-intensive operations, such as joins and aggregates, and has better support for constructing new XML data, which is required by transformations. There is a need to use recursive functions in certain queries and XML-QL has been extended in this respect in [3]. Resource Description Framework (RDF). RDF is a foundation for processing metadata for providing interoperability between applications that exchange machine understandable information and currently is a recommendation by the World Wide Web Consortium (W3C). RDF imposes a syntax and structural constraints in describing …","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.8,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Information systems,"Information systems: The paper discusses XML, RDF, and P3P standards for data exchange and personalization, which are core to information systems. Other categories like Software engineering or Applied computing are less directly relevant.","Data management systems:0.1,Information retrieval:0.1,Information storage systems:0.1,Information systems applications:0.1,World Wide Web:0.9",World Wide Web,"World Wide Web: The paper discusses personalization using W3C web standards (XML, RDF, P3P). Other options are irrelevant as the focus is on web-based personalization, not information systems or data management.","Online advertising:0.2,Web applications:0.8,Web data description languages:1.0,Web interfaces:0.3,Web mining:0.4,Web searching and information discovery:0.3,Web services:0.7","Web data description languages,Web applications",Web data description languages is highly relevant for XML/RDF usage in personalization. Web applications is relevant for implementing personalized web systems. Web services is moderately relevant for interoperability but not the core focus.
272,On the Utility of WordNet for Ontology Alignment: Is it Really Worth it?,"Many ontology alignment algorithms augment syntactic matching with the use of WordNet (WN) in order to improve their performance. The advantage of using WN in alignment seems apparent. However, we strike a more cautionary note. We analyze the utility of WN in the context of the reduction in precision and increase in execution time that its use entails. For this analysis, we particularly focus on real-world ontologies. We report distinct trends in the performance of WN-based alignment in comparison with alignment that uses syntactic matching only. We analyze the trends and their implications, and provide useful insights on the types of ontology pair for which WN-based alignment may potentially be worthwhile and those types where it may not be.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:1.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.3,Applied computing:0.0,Social and professional topics:0.0",Information systems,"Information systems is highly relevant because the paper evaluates ontology alignment methods, a core topic in knowledge representation and semantic systems. Computing methodologies is marginally relevant for the algorithmic discussion but not the primary focus.","Data management systems:1.0,Information retrieval:1.0,Information storage systems:0.2,Information systems applications:0.3,World Wide Web:0.1","Data management systems,Information retrieval","Data management systems: Ontology alignment is a core data management task. Information retrieval: The paper evaluates alignment performance, which impacts IR systems. Other categories like storage systems are not central to the core contribution.","Data structures:0.1,Database administration:0.1,Database design and models:0.2,Database management system engines:0.1,Document representation:0.1,Evaluation of retrieval results:0.8,Information integration:0.9,Information retrieval query processing:0.3,Middleware for databases:0.1,Query languages:0.1,Retrieval models and ranking:0.3,Retrieval tasks and goals:0.2,Search engine architectures and scalability:0.1,Specialized information retrieval:0.1,Users and interactive retrieval:0.1","Information integration,Evaluation of retrieval results",Information integration is directly relevant as the paper evaluates WN's role in ontology alignment. Evaluation of retrieval results is relevant due to the analysis of precision and execution time tradeoffs. Other options like Database design or Query languages are too specific to database systems and not central to ontology alignment analysis.
862,Extract Reliable Relations from Wikipedia Texts for Practical Ontology Construction,"A feature based relation classification approach is presented in this paper. We aimed to exact relation candidates from Wikipedia texts. A probabilistic and a semantic relatedness features are employed with other linguistic information for the purpose. The experiments show that, relation classification using the proposed relatedness features with surface information like word and part-of-speech tags is competitive with or even outperforms the one of using deep syntactic information. Meanwhile, an approach is proposed to distinguish reliable relation candidates from others, so that these reliable results can be accepted for knowledge building without human verification. The experiments show that, with the relation classification approach presented in this paper, more than 40% of the classification results are reliable, which means, at least 40% of the human and time costs can be saved in practice.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.9,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.3,Applied computing:0.1,Social and professional topics:0.1",Information systems,"Information systems is highly relevant as the paper addresses ontology construction from Wikipedia data. Computing methodologies is partially relevant due to NLP techniques, but the core focus is knowledge representation.","Data management systems:0.8,Information retrieval:0.9,Information storage systems:0.3,Information systems applications:0.5,World Wide Web:0.4","Information retrieval,Data management systems",Information retrieval is highly relevant for the relation classification approach in Wikipedia. Data management systems is relevant for ontology construction applications. Information storage systems is less central to the core contribution in relation extraction.,"Data structures:0.1,Database administration:0.1,Database design and models:0.1,Database management system engines:0.1,Document representation:1.0,Evaluation of retrieval results:0.4,Information integration:0.6,Information retrieval query processing:0.8,Middleware for databases:0.1,Query languages:0.1,Retrieval models and ranking:0.5,Retrieval tasks and goals:0.3,Search engine architectures and scalability:0.2,Specialized information retrieval:0.4,Users and interactive retrieval:0.2","Document representation,Information integration",Document representation is directly relevant as the paper focuses on text-based relation extraction for ontologies. Information integration is relevant because the reliable relations extracted contribute to knowledge base construction. Other categories like Database administration or Query languages are irrelevant as the paper focuses on text processing rather than database systems.
2535,Context modeling for ranking and tagging bursty features in text streams,"Bursty features in text streams are very useful in many text mining applications. Most existing studies detect bursty features based purely on term frequency changes without taking into account the semantic contexts of terms, and as a result the detected bursty features may not always be interesting or easy to interpret. In this paper we propose to model the contexts of bursty features using a language modeling approach. We then propose a novel topic diversity-based metric using the context models to find newsworthy bursty features. We also propose to use the context models to automatically assign meaningful tags to bursty features. Using a large corpus of a stream of news articles, we quantitatively show that the proposed context language models for bursty features can effectively help rank bursty features based on their newsworthiness and to assign meaningful tags to annotate bursty features.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.9,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Information systems,Information systems is highly relevant as the paper addresses text mining and context modeling for bursty features in text streams. Other categories like Theory of computation are irrelevant as the focus is on data analysis and information retrieval.,"Data management systems:0.1,Information retrieval:0.9,Information storage systems:0.2,Information systems applications:0.6,World Wide Web:0.2","Information retrieval,Information systems applications",Information retrieval is core to the paper's focus on ranking bursty features. Information systems applications are relevant due to real-world text stream analysis. Other categories like Data management systems or World Wide Web are less directly addressed.,"Collaborative and social computing systems and tools:0.5,Computational advertising:0.1,Data mining:0.9,Retrieval models and ranking:0.7","Data mining,Retrieval models and ranking",Data mining is primary as the paper focuses on extracting patterns from text streams. Retrieval models and ranking is relevant for ranking bursty features. Collaborative and social computing systems and tools is less relevant as the focus is on data mining rather than collaborative systems.
4177,Management and Conflation of Multiple Representations within an Open Federation Platform,"Building up spatial data infrastructures involves the task of dealing with heterogeneous data sources which often bear inconsistencies and contra- dictions, respectively. One main reason for those inconsistencies emerges from the fact that one and the same real world phenomenon is often stored in multi- ple representations within different databases. It is the special goal of this paper to describe how the problems arising from multiple representations can be dealt with in spatial data infrastructures, especially focusing on the concepts that have been developed within the Nexus project of the University of Stuttgart that is implementing an open, federated infrastructure for context-aware applica- tions. A main part of this contribution consists of explaining the efforts which have been conducted in order to solve the conflicts that occur between multiple representations within conflation or merging processes to achieve consolidated views on the underlying data for the applications.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.9,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Information systems,Information systems is relevant due to the focus on spatial data management and conflation. Other categories are less specific.,"Data management systems:0.8,Information retrieval:0.1,Information storage systems:0.2,Information systems applications:0.7,World Wide Web:0.1","Data management systems,Information systems applications",Data management systems is directly relevant as the paper discusses conflation of multiple spatial data representations. Information systems applications is relevant for the implementation context in spatial data infrastructures. Other categories like information retrieval are not mentioned.,"Collaborative and social computing systems and tools:0.2,Computational advertising:0.1,Computing platforms:0.3,Data mining:0.1,Data structures:0.1,Database administration:0.2,Database design and models:0.2,Database management system engines:0.2,Decision support systems:0.1,Digital libraries and archives:0.1,Enterprise information systems:0.1,Information integration:1.0,Middleware for databases:0.3,Mobile information processing systems:0.1,Multimedia information systems:0.1,Process control systems:0.1,Query languages:0.2,Spatial-temporal systems:1.0","Information integration,Spatial-temporal systems",Information integration: The paper addresses conflict resolution between multiple representations in spatial data infrastructures. Spatial-temporal systems: The focus on spatial data infrastructures and conflation processes directly aligns with this category. Other categories like Database administration or Data mining are too general and not central to the paper's contribution.
3075,Enabling Information Interoperability in the Future Internet,"The Future Internet is currently seen as an opportunity to improve the network infrastructure addressing service-oriented, social trends and economic commitments. Future Internet and next generation communications systems challenges mainly demand end user requirements, personalized provisioning, service-oriented performance, service-awareness networking, information interoperability and data models integration. This paper focuses on information interoperability and cross domain information managing Internet systems. Research activity and results about semantic enrichment tasks for management information contained in both enterprise and networking information and data models respectively is described. Ontologies are used to support reusable, common and manageable service and network information for service composition and management operations in the inference plane in Future internet systems. An introductory application scenario on current service agnostic Internet is depicted.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.9,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Information systems,Information systems is highly relevant as the paper focuses on semantic data integration and ontologies for information interoperability. Other categories like 'Networks' are less relevant as the focus is on data management rather than network infrastructure.,"Data management systems:1.0,Information retrieval:0.5,Information storage systems:0.3,Information systems applications:0.8,World Wide Web:0.4","Data management systems,Information systems applications",Data management systems is relevant due to semantic enrichment of data models and ontologies for interoperability. Information systems applications is relevant as the paper discusses service composition and management in Future Internet systems. Information retrieval and World Wide Web are less central as the focus is on system integration rather than retrieval mechanics or web infrastructure.,"Collaborative and social computing systems and tools:0.2,Computational advertising:0.1,Computing platforms:0.1,Data mining:0.2,Data structures:0.1,Database administration:0.2,Database design and models:0.2,Database management system engines:0.1,Decision support systems:0.2,Digital libraries and archives:0.1,Enterprise information systems:0.2,Information integration:0.9,Middleware for databases:0.2,Mobile information processing systems:0.1,Multimedia information systems:0.1,Process control systems:0.1,Query languages:0.1,Spatial-temporal systems:0.1",Information integration,Information integration is highly relevant as the paper focuses on semantic enrichment for information interoperability. Other options like Enterprise information systems are less directly relevant to the core contribution.
2524,Biological Data Warehousing System for Identifying Transcriptional Regulatory Sites From Gene Expressions of Microarray Data,"Identification of transcriptional regulatory sites plays an important role in the investigation of gene regulation. For this propose, we designed and implemented a data warehouse to integrate multiple heterogeneous biological data sources with data types such as text-file, XML, image, MySQL database model, and Oracle database model. The utility of the biological data warehouse in predicting transcriptional regulatory sites of coregulated genes was explored using a synexpression group derived from a microarray study. Both of the binding sites of known transcription factors and predicted over-represented (OR) oligonucleotides were demonstrated for the gene group. The potential biological roles of both known nucleotides and one OR nucleotide were demonstrated using bioassays. Therefore, the results from the wet-lab experiments reinforce the power and utility of the data warehouse as an approach to the genome-wide search for important transcription regulatory elements that are the key to many complex biological systems","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:1.0,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Information systems,"Information systems: The paper presents a biological data warehouse for integrating and analyzing gene expression data, which is central to data management and information systems. Other fields like Applied computing are secondary to the data-centric focus.","Data management systems:1.0,Information retrieval:0.75,Information storage systems:0.5,Information systems applications:0.3,World Wide Web:0.1",Data management systems,Data management systems is highly relevant as the paper presents a biological data warehouse integrating heterogeneous data sources. Information retrieval is secondary as the focus is on data integration rather than query-based retrieval.,"Data structures:0.3,Database administration:0.2,Database design and models:1.0,Database management system engines:0.5,Information integration:0.8,Middleware for databases:0.4,Query languages:0.2","Database design and models,Information integration",Database design and models is central for the data warehouse implementation. Information integration is relevant for combining heterogeneous data sources. Other options are less directly addressed.
5181,AVATecH - automated annotation through audio and video analysis,"In different fields of the humanities annotations of multimodal resources are a necessary component of the research workflow. Examples include linguistics, psychology, anthropology, etc. However, creation of those annotations is a very laborious task, which can take 50 to 100 times the length of the annotated media, or more. This can be significantly improved by applying innovative audio and video processing algorithms, which analyze the recordings and provide automated annotations. This is the aim of the AVATecH project, which is a collaboration of the Max Planck Institute for Psycholinguistics (MPI) and the Fraunhofer institutes HHI and IAIS. In this paper we present a set of results of automated annotation together with an evaluation of their quality.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.2,Information systems:1.0,Security and privacy:0.0,Human-centered computing:0.4,Computing methodologies:0.5,Applied computing:0.3,Social and professional topics:0.0",Information systems,Information systems: The paper focuses on automated annotation of multimodal resources for humanities research. Human-centered computing (0.4) is secondary due to the end-user application context.,"Data management systems:0.1,Information retrieval:0.4,Information storage systems:0.05,Information systems applications:0.6,World Wide Web:0.0","Information systems applications,Information retrieval",Information systems applications: The paper focuses on automated annotation systems for humanities research. Information retrieval: Audio/video analysis for annotations relates to content retrieval. Data management: Only tangentially relevant as storage is not the focus. Web: No connection to web-specific technologies.,"Collaborative and social computing systems and tools:0.3,Computational advertising:0.1,Computing platforms:0.2,Data mining:1.0,Decision support systems:0.4,Digital libraries and archives:0.2,Document representation:0.1,Enterprise information systems:0.1,Evaluation of retrieval results:0.3,Information retrieval query processing:0.2,Mobile information processing systems:0.2,Multimedia information systems:1.0,Process control systems:0.1,Retrieval models and ranking:0.1,Retrieval tasks and goals:0.1,Search engine architectures and scalability:0.2,Spatial-temporal systems:0.1,Specialized information retrieval:0.2,Users and interactive retrieval:0.1","Data mining,Multimedia information systems",Data mining is relevant for automated annotation techniques. Multimedia information systems is relevant due to audio/video analysis. Other categories are less directly related to the paper's focus.
1208,Using RDF(S) to provide multiple views into a single ontology,"This paper deals with RDF (Resource Description Framework). The main point is to present a general model describing when and how to exploit RDF technology. It is suggested that RDF(S) functions best as a means to provide mechanisms for expressing contextual and case-specific information. In other words, RDF(S) is suitable for providing different views into a single extensive ontology, rather than specifying the actual ontology. The ontology behind the case-specific RDF(S) is li kely to be expressed using some other mechanism than RDF(S).""","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.8,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Information systems,Information systems is relevant because the paper discusses RDF(S) for expressing contextual information. Other categories like 'Software and its engineering' are less directly relevant as the focus is on data representation rather than software construction.,"Data management systems:0.9,Information retrieval:0.3,Information storage systems:0.4,Information systems applications:0.2,World Wide Web:0.3",Data management systems,"Data management systems is highly relevant as the paper discusses RDF(S) for expressing contextual information in ontologies, which is central to semantic data modeling. Other categories like information retrieval or web systems are less relevant as the focus is on data representation rather than querying or web applications.","Data structures:0.2,Database administration:0.1,Database design and models:1.0,Database management system engines:0.3,Information integration:0.8,Middleware for databases:0.1,Query languages:0.1","Database design and models,Information integration","Database design and models is highly relevant as RDF(S) is a data modeling framework. Information integration is relevant since the paper discusses multiple views of an ontology. Other categories are less relevant as the paper does not focus on administration, engines, or query languages."
4889,Timestamping Entities using Contextual Information,"Wikipedia is the result of collaborative effort aiming to represent human knowledge and to make it accessible to the public. Many Wikipedia articles however lack key metadata information. For example, relatively large number of people described in Wikipedia have no information on their birth and death dates. We propose in this paper to estimate entity's lifetimes using link structure in Wikipedia focusing on person entities. Our approach is based on propagating temporal information over links between Wikipedia articles.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.9,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Information systems,"Information systems is highly relevant because the paper focuses on retrieving and analyzing temporal metadata from Wikipedia, a large-scale database. Other categories are irrelevant as the paper does not address hardware, algorithms, or security.","Data management systems:0.3,Information retrieval:1.0,Information storage systems:0.0,Information systems applications:0.0,World Wide Web:0.75","Information retrieval,World Wide Web","Information retrieval: The core contribution is estimating entity lifetimes via link structure analysis. World Wide Web: Wikipedia's link structure is the primary data source. Data management systems is less relevant as the focus is on retrieval, not storage.","Document representation:0.3,Evaluation of retrieval results:0.4,Information retrieval query processing:0.2,Online advertising:0.1,Retrieval models and ranking:0.3,Retrieval tasks and goals:0.4,Search engine architectures and scalability:0.2,Specialized information retrieval:1.0,Users and interactive retrieval:0.5,Web applications:0.1,Web data description languages:0.1,Web interfaces:0.1,Web mining:1.0,Web searching and information discovery:0.3,Web services:0.1","Web mining,Specialized information retrieval","Web mining: The paper focuses on propagating temporal information through Wikipedia's link structure for entity lifetime estimation. Specialized information retrieval: The task is a niche retrieval problem (entity timestamping). Other categories (e.g., Web searching) are less directly relevant."
1143,Exp-DB: Fast Development of Information Systems for Experiment Tracking,"Bioinformatics research groups require information systems keeping track of experiments and their results. However, current solutions are often ad-hoc, difficult to maintain, extend, or use in different context. This paper presents Exp-DB, an infrastructure for the fast development of information systems for experiment tracking. Exp-DB provides a basic, extensible database design, components for accessing the data, and a web-based interface. It allows for fast implementation of an initial system even by non IT-experts which can then be extended step-by-step.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.8,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Information systems,"Information systems is highly relevant as the paper introduces Exp-DB, a database infrastructure for experiment tracking. Other categories like Software and its engineering (0.2) are less relevant because the focus is on database design for bioinformatics, not software development practices.","Data management systems:0.9,Information retrieval:0.2,Information storage systems:0.3,Information systems applications:0.85,World Wide Web:0.65","Data management systems,Information systems applications",Data management systems is relevant due to the extensible database design. Information systems applications is relevant for the experiment tracking system. World Wide Web is partially relevant for the interface but not the core contribution. Information retrieval/storage are not central to the paper's focus.,"Collaborative and social computing systems and tools:0.2,Computational advertising:0.1,Computing platforms:0.1,Data mining:0.1,Data structures:0.1,Database administration:0.1,Database design and models:0.9,Database management system engines:0.4,Decision support systems:0.2,Digital libraries and archives:0.1,Enterprise information systems:0.7,Information integration:0.3,Middleware for databases:0.2,Mobile information processing systems:0.1,Multimedia information systems:0.1,Process control systems:0.1,Query languages:0.2,Spatial-temporal systems:0.1","Database design and models,Enterprise information systems","Database design and models: The paper presents Exp-DB as an infrastructure with an extensible database design for experiment tracking. Enterprise information systems: The system is designed for information management in research environments, aligning with enterprise information systems. Other children like Data mining or Mobile information processing systems are not directly relevant to the core database design focus."
4891,Medical Case-based Retrieval by Leveraging Medical Ontology and Physician Feedback: UIUC-IBM at ImageCLEF 2010,"This paper reports the experiment results of the UIUC-IBM team in participating in the medical case retrieval task of ImageCLEF 2010. We experimented with multiple methods to leverage medical ontology and user (physician) feedback; both have worked very well, achieving the best retrieval performance among all the submissions.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.9,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Information systems,Information systems is relevant because the paper focuses on medical case retrieval using ontologies and user feedback. Other categories are irrelevant as the focus is on information retrieval techniques.,"Data management systems:0.3,Information retrieval:1.0,Information storage systems:0.0,Information systems applications:0.0,World Wide Web:0.0",Information retrieval,"Information retrieval: The paper's core is medical case retrieval using ontology and feedback. Data management systems is less relevant as the focus is on retrieval methods, not storage infrastructure.","Document representation:0.2,Evaluation of retrieval results:0.5,Information retrieval query processing:0.3,Retrieval models and ranking:0.4,Retrieval tasks and goals:0.6,Search engine architectures and scalability:0.2,Specialized information retrieval:1.0,Users and interactive retrieval:1.0","Specialized information retrieval,Users and interactive retrieval","Specialized information retrieval: The focus is on medical case retrieval using ontologies. Users and interactive retrieval: Physician feedback is explicitly leveraged. Other categories (e.g., Document representation) are secondary."
5047,Collaborative Support for Community Data Sharing,"The semantic Web aims to create a web of data where contents can be easily discovered and integrated using metadata. Many ontologies have been proposed over the years in different domains, thus producing a semantic heterogeneity that is difficult to manage. Various automated ontology mapping techniques and tools have been developed to facilitate the bridging and integration of distributed data repositories. Nevertheless, such tools are still in need of human supervision to ensure accuracy. The spread of Web 2.0 approaches demonstrate the possibility and the added value of using collaborative techniques for improving data sharing and consensus reaching. In this paper, we describe our prototype for collaborative ontology mapping and data sharing. The possibility to exploit ontology alignments for querying data is a key capability for data sharing in a networked ontology environment.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:1.0,Security and privacy:0.0,Human-centered computing:0.3,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Information systems,Information systems is highly relevant for ontology mapping and data sharing in semantic web contexts. Human-centered computing is marginally relevant due to collaborative aspects but not the core focus.,"Data management systems:0.9,Information retrieval:0.4,Information storage systems:0.3,Information systems applications:0.7,World Wide Web:0.8","Data management systems,World Wide Web",Data management systems is relevant for ontology-based data sharing. World Wide Web is relevant for semantic web applications. Information systems applications (0.7) applies to the community data sharing context.,"Data structures:0.0,Database administration:0.0,Database design and models:0.0,Database management system engines:0.0,Information integration:1.0,Middleware for databases:0.0,Online advertising:0.0,Query languages:0.0,Web applications:0.8,Web data description languages:0.0,Web interfaces:0.0,Web mining:0.0,Web searching and information discovery:0.0,Web services:0.0","Information integration,Web applications",Information integration: The paper focuses on collaborative ontology mapping for semantic data integration. Web applications: The system is implemented as a web-based tool for data sharing. Other fields like Database design are irrelevant as the focus is on semantic integration rather than database structure.
5825,Toward a complexity theory of information systems development,"Purpose – Existing literature acknowledges information systems development (ISD) to be a complex activity. This complexity is magnified by the continuous changes in user requirements due to changing organizational needs in changing external competitive environments. Research findings show that, if this increasing complexity is not managed appropriately, information systems fail. The paper thus aims to portray the sources of complexity related to ISD and to suggest the use of complexity theory as a frame of reference, analyzing its implications on information system design and development to deal with the emergent nature of IS.Design/methodology/approach – Conceptual analysis and review of relevant literature.Findings – This article provides a conceptual model explaining how top‐down “official” and bottom‐up “emergent” co‐evolutionary adaptations of information systems design with changing user requirements will result in more effective system design and operation. At the heart of this model are seven firs...","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.9,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Information systems,"Information systems is relevant because the paper focuses on the complexity of information systems development and applies complexity theory to this domain. Other categories are less relevant as the focus is not on hardware, software engineering, or security.","Data management systems:0.4,Information retrieval:0.2,Information storage systems:0.3,Information systems applications:1.0,World Wide Web:0.1",Information systems applications,Information systems applications is directly relevant as the paper discusses complexity theory application to information systems development. Other categories like Data management or Information retrieval are less relevant as the focus is on theoretical framework for ISD rather than specific data systems or search technologies.,"Collaborative and social computing systems and tools:0.3,Computational advertising:0,Computing platforms:0.5,Data mining:0.3,Decision support systems:1,Digital libraries and archives:0.3,Enterprise information systems:1,Mobile information processing systems:0.3,Multimedia information systems:0.3,Process control systems:0,Spatial-temporal systems:0","Enterprise information systems,Decision support systems",Enterprise information systems: Focuses on organizational ISD and complexity. Decision support systems: Complexity management in ISD supports decision-making processes. Categories like Mobile systems are not directly relevant.
4246,A Method for Evaluating Full-text Search Queries in Native XML Databases,In this paper we consider the problem of efficiently producing results for full-text keyword search queries over XML documents. We describe full-text search query semantics and propose a method for efficient evaluation of keyword search queries with these semantics suitable for native XML databases. Method uses inverted file index which may be efficiently updated when a part of some XML document is updated.,"General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:1.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Information systems,Information systems is highly relevant for XML database query evaluation. Other categories are not applicable.,"Data management systems:0.6,Information retrieval:1.0,Information storage systems:0.3,Information systems applications:0.4,World Wide Web:0.2","Information retrieval,Data management systems",Information retrieval is central for the keyword search query evaluation method. Data management systems is relevant for XML database context. Other categories are rejected as the paper focuses on search rather than web technologies or storage systems.,"Data structures:0.5,Database administration:0.3,Database design and models:0.6,Database management system engines:0.4,Document representation:0.7,Evaluation of retrieval results:0.3,Information integration:0.2,Information retrieval query processing:1.0,Middleware for databases:0.3,Query languages:0.5,Retrieval models and ranking:0.6,Retrieval tasks and goals:0.4,Search engine architectures and scalability:0.3,Specialized information retrieval:0.4,Users and interactive retrieval:0.2","Information retrieval query processing,Document representation",Information retrieval query processing (1.0) is directly relevant as the paper focuses on full-text search query evaluation. Document representation (0.7) is secondary since XML document structure is central to the approach. Other options like Database design (0.6) and Retrieval models (0.6) are less central to the paper's contribution on XML search methods.
5805,WWG: a wide-area infrastructure to support groups,"Group learning at Internet scale is becoming more frequent in university courses. This complex process requires support by distributed computing learning support infrastructures.This paper describes the design of WWG (World-Wide Groups): a distributed and decentralized infrastructure with the aim of supporting distributed group learning and team work, centered on the distribution of events, so that every participant can be notified and thus be aware of the actions, changes, progress of the groups he belongs to.The design issues, requirements and the resulting architecture are presented. WWG is based on a multi-component architecture where metainformation agents are responsible for helping the events to reach the members of the group; the repository agents are responsible for the storage of group information; and user agents are responsible for the representation of users (sources and sinks of events). In this paper we tried to show that, applying events transformation policies, WWG is scalable at group level.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.3,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:1.0,Security and privacy:0.1,Human-centered computing:0.2,Computing methodologies:0.2,Applied computing:0.2,Social and professional topics:0.1",Information systems,Information systems is highly relevant as the paper describes a distributed infrastructure (WWG) for managing group learning events. Other fields like Networks (0.3) are secondary as the focus is on information management rather than network protocols.,"Data management systems:0.3,Information retrieval:0.1,Information storage systems:0.2,Information systems applications:1.0,World Wide Web:0.4",Information systems applications,"Information systems applications: The paper describes a system (WWG) for distributed group learning, which is an application-focused design. World Wide Web is tangential unless the system explicitly leverages web technologies, which is not detailed here.","Collaborative and social computing systems and tools:1,Computational advertising:0,Computing platforms:0,Data mining:0,Decision support systems:0,Digital libraries and archives:0,Enterprise information systems:0,Mobile information processing systems:0,Multimedia information systems:0,Process control systems:0,Spatial-temporal systems:0",Collaborative and social computing systems and tools,Collaborative and social computing systems and tools is directly relevant as the paper describes a decentralized infrastructure for distributed group learning.
1878,NSSRF: global network similarity search with subgraph signatures and its applications,"Motivation: The exponential growth of biological network database has increasingly rendered the global network similarity search (NSS) computationally intensive. Given a query network and a network database, it aims to find out the top similar networks in the database against the query network based on a topological similarity measure of interest. With the advent of big network data, the existing search methods may become unsuitable since some of them could render queries unsuccessful by returning empty answers or arbitrary query restrictions. Therefore, the design of NSS algorithm remains challenging under the dilemma between accuracy and efficiency. Results: We propose a global NSS method based on regression, denotated as NSSRF, which boosts the search speed without any significant sacrifice in practical performance. As motivated from the nature, subgraph signatures are heavily involved. Two phases are proposed in NSSRF: offline model building phase and similarity query phase. In the offline model building phase, the subgraph signatures and cosine similarity scores are used for efficient random forest regression (RFR) model training. In the similarity query phase, the trained regression model is queried to return similar networks. We have extensively validated NSSRF on biological pathways and molecular structures; NSSRF demonstrates competitive performance over the state‐of‐the‐arts. Remarkably, NSSRF works especially well for large networks, which indicates that the proposed approach can be promising in the era of big data. Case studies have proven the efficiencies and uniqueness of NSSRF which could be missed by the existing state‐of‐the‐arts. Availability and Implementation: The source code of two versions of NSSRF are freely available for downloading at https://github.com/zhangjiaobxy/nssrfBinary and https://github.com/zhangjiaobxy/nssrfPackage. Contact: kc.w@cityu.edu.hk Supplementary information: Supplementary data are available at Bioinformatics online.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:1.0,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Information systems,"The paper introduces a network similarity search method for biological data, a core topic in information systems and bioinformatics.","Data management systems:0.9,Information retrieval:0.85,World Wide Web:0.3","Data management systems,Information retrieval",Data management systems handle network databases. Information retrieval applies to similarity search. Web-related aspects are not central.,"Data structures:0,Database administration:0,Database design and models:0,Database management system engines:0,Document representation:0,Evaluation of retrieval results:0,Information integration:0,Information retrieval query processing:1,Middleware for databases:0,Query languages:0,Retrieval models and ranking:1,Retrieval tasks and goals:0,Search engine architectures and scalability:1,Specialized information retrieval:1,Users and interactive retrieval:0","Information retrieval query processing,Search engine architectures and scalability,Specialized information retrieval","Information retrieval query processing is directly relevant as the paper introduces a method for network similarity search. Search engine architectures and scalability are relevant due to the focus on efficiency for large datasets. Specialized information retrieval is relevant because the work targets biological networks, a domain-specific application."
1214,Full-coverage Identification of English Light Verb Constructions,"The identification of light verb constructions (LVC) is an important task for several applications. Previous studies focused on some limited set of light verb constructions. Here, we address the full coverage of LVCs. We investigate the performance of different candidate extraction methods on two English full-coverage LVC annotated corpora, where we found that less severe candidate extraction methods should be applied. Then we follow a machine learning approach that makes use of an extended and rich feature set to select LVCs among extracted candidates.","General and reference:0.1,Hardware:0.2,Computer systems organization:0.2,Networks:0.1,Software and its engineering:0.3,Theory of computation:0.1,Mathematics of computing:0.2,Information systems:1.0,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.4,Applied computing:0.3,Social and professional topics:0.1",Information systems,Information systems is highly relevant as the paper addresses natural language processing for LVC identification. Other categories like Mathematics are less central to the data processing focus.,"Data management systems:0.2,Information retrieval:0.3,Information storage systems:0.1,Information systems applications:0.6,World Wide Web:0.1",Information systems applications,"Information systems applications: The paper applies machine learning to linguistic data processing for NLP tasks, aligning with system-level applications. Other categories are rejected as the focus is not on data management, retrieval, storage, or web technologies.","Collaborative and social computing systems and tools:0,Computational advertising:0,Computing platforms:0,Data mining:1,Decision support systems:0.5,Digital libraries and archives:0,Enterprise information systems:0,Mobile information processing systems:0,Multimedia information systems:0,Process control systems:0,Spatial-temporal systems:0","Data mining,Decision support systems",Data mining is highly relevant as the paper uses machine learning for LVC identification. Decision support systems is moderately relevant due to the use of feature sets for decision-making. Other options are irrelevant as the paper focuses on natural language processing techniques.
1054,Optimal Data Set Selection: An Application to Grapheme-to-Phoneme Conversion,"In this paper we introduce the task of unlabeled, optimal, data set selection. Given a large pool of unlabeled examples, our goal is to select a small subset to label, which will yield a high performance supervised model over the entire data set. Our first proposed method, based on the rank-revealing QR matrix factorization, selects a subset of words which span the entire word-space effectively. For our second method, we develop the concept of feature coverage which we optimize with a greedy algorithm. We apply these methods to the task of grapheme-to-phoneme prediction. Experiments over a data-set of 8 languages show that in all scenarios, our selection methods are effective at yielding a small, but optimal set of labelled examples. When fed into a state-of-the-art supervised model for grapheme-to-phoneme prediction, our methods yield average error reductions of 20% over randomly selected examples.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.85,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Information systems,Information systems is highly relevant as the paper analyzes XML usage patterns in industry information modeling. Other categories like Software and its engineering are less relevant since the focus is on data modeling rather than software development.,"Data management systems:0.9,Information retrieval:0.3,Information storage systems:0.2,Information systems applications:0.4,World Wide Web:0.1",Data management systems,"Data management systems is highly relevant as the paper addresses dataset selection for efficient model training, a core aspect of data management. Information retrieval is less relevant as the focus is on data selection for labeling rather than retrieving information from a corpus.","Data structures:0.4,Database administration:0.3,Database design and models:1.0,Database management system engines:0.5,Information integration:0.7,Middleware for databases:0.4,Query languages:0.5",Database design and models,"Database design and models: The paper introduces methods for optimal data set selection, which relates to structuring and managing data for training models. Other categories like Information integration are secondary but still relevant to data structuring."
619,Entity Disambiguation with Freebase,"Entity disambiguation with a knowledge base becomes increasingly popular in the NLP community. In this paper, we employ Freebase as the knowledge base, which contains significantly more entities than Wikipedia and others. While huge in size, Freebase lacks context for most entities, such as the descriptive text and hyperlinks in Wikipedia, which are useful for disambiguation. Instead, we leverage two features of Freebase, namely the naturally disambiguated mention phrases (aka aliases) and the rich taxonomy, to perform disambiguation in an iterative manner. Specifically, we explore both generative and discriminative models for each iteration. Experiments on 2, 430, 707 English sentences and 33, 743 Freebase entities show the effectiveness of the two features, where 90% accuracy can be reached without any labeled data. We also show that discriminative models with proposed split training strategy is robust against over fitting problem, and constantly outperforms the generative ones.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.75,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Information systems,Information systems is relevant for knowledge base-based entity disambiguation. Other categories like Computing methodologies are less directly relevant to the core contribution of knowledge management systems for entity resolution.,"Data management systems:0.0,Information retrieval:1.0,Information storage systems:0.8,Information systems applications:0.0,World Wide Web:0.0","Information retrieval,Information storage systems","Information retrieval: Entity disambiguation is a core task in IR. Information storage systems: Freebase is a knowledge base (a type of storage system). Other categories (e.g., Data management systems) are irrelevant as the focus is on disambiguation algorithms, not data management.","Document representation:0.4,Evaluation of retrieval results:0.1,Information retrieval query processing:1.0,Information storage technologies:0.2,Record storage systems:0.1,Retrieval models and ranking:0.7,Retrieval tasks and goals:0.3,Search engine architectures and scalability:0.1,Specialized information retrieval:0.2,Storage architectures:0.1,Storage management:0.1,Storage replication:0.1,Users and interactive retrieval:0.1","Information retrieval query processing,Retrieval models and ranking",Information retrieval query processing is central to entity disambiguation. Retrieval models and ranking are relevant for iterative disambiguation techniques. Other categories like Storage technologies are not directly related to the NLP methodology.
2847,Cross-Language Information Retrieval (CLIR) Track Overview,"Recherche d'informations dans le cas ou les documents sont ecrits dans une langue differente de celle employee dans la requete: resultats obtenus par 13 groupes de travail employant des methodes differentes. Documents ecrits en anglais, francais et allemand. Langue de la requete: anglais, francais, allemand, espagnol et neerlandais","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:1.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.25,Applied computing:0.0,Social and professional topics:0.0",Information systems,Information systems: The paper focuses on cross-language information retrieval systems. Other categories: Computing methodologies has limited relevance to the evaluation framework.,"Data management systems:0.2,Information retrieval:1.0,Information storage systems:0.2,Information systems applications:0.3,World Wide Web:0.2",Information retrieval,Information retrieval is the primary focus of the paper on cross-language information retrieval (CLIR). Other options are less relevant as the study centers on retrieval mechanisms rather than data management or web-specific applications.,"Document representation:0,Evaluation of retrieval results:0,Information retrieval query processing:0,Retrieval models and ranking:1,Retrieval tasks and goals:0,Search engine architectures and scalability:0,Specialized information retrieval:1,Users and interactive retrieval:0","Retrieval models and ranking,Specialized information retrieval",Retrieval models and ranking is relevant because the paper addresses cross-language retrieval challenges through different methods. Specialized information retrieval is relevant as the focus is on multilingual document retrieval. Other categories like query processing or user interaction are less central to the study's methodology.
402,Department of energy strategic roadmap for Earth system science data integration,"The U.S. Department of Energy (DOE) Office of Biological and Environmental Research (BER) Climate and Environmental Sciences Division (CESD) produces a diversity of data, information, software, and model codes across its research and informatics programs and facilities. This information includes raw and reduced observational and instrumentation data, model codes, model-generated results, and integrated data products. Currently, most of these data and information are prepared and shared for program specific activities, corresponding to CESD organization research. A major challenge facing BER CESD is how best to inventory, integrate, and deliver these vast and diverse resources for the purpose of accelerating Earth system science research. This paper provides a concept for a CESD Integrated Data Ecosystem and an initial roadmap for its implementation to address this integration challenge in the “Big Data” domain.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.7,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Information systems,Information systems: The paper addresses data integration challenges in Earth system science. Other categories are irrelevant as the focus is on data management and integration strategies rather than hardware or algorithms.,"Data management systems:1.0,Information retrieval:0.2,Information storage systems:0.2,Information systems applications:1.0,World Wide Web:0.3","Data management systems,Information systems applications",Data management systems is relevant for the focus on data integration. Information systems applications is relevant for applying data systems to scientific research. Other categories are rejected as they do not align with the paper's focus on data ecosystem and integration.,"Collaborative and social computing systems and tools:0.0,Computational advertising:0.0,Computing platforms:0.0,Data mining:0.0,Data structures:0.0,Database administration:0.2,Database design and models:0.8,Database management system engines:0.0,Decision support systems:0.0,Digital libraries and archives:0.0,Enterprise information systems:0.0,Information integration:1.0,Middleware for databases:0.0,Mobile information processing systems:0.0,Multimedia information systems:0.0,Process control systems:0.0,Query languages:0.0,Spatial-temporal systems:0.0","Information integration,Database design and models",Information integration is highly relevant as the paper proposes an integrated data ecosystem for Earth system science. Database design and models is moderately relevant due to discussions about data structures and management. Other categories like Data mining and Spatial-temporal systems are irrelevant as the focus is on system architecture rather than data analysis techniques.
1775,Synchronization options for data warehouse designs,"Data warehouse use has increased significantly in recent years and now plays a fundamental role in many organizations' decision-support processes. A framework that uses parameter sets to define the most suitable synchronization option for a given transaction processing environment helps decrease the update time between the transactional and analytical systems and also reduces the hardware resources required to keep an acceptable data update. The frequency of a data warehouse loading process defines the points of update between the transaction systems and the warehouse with its analytical applications. Normally, data warehouses rely on static updates, with batch loading processes occurring at daily, weekly, monthly, or other periodic intervals. However, today's business needs require an analytical environment that provides (i) continuous data integration with shorter periods for capturing and loading from operational sources, (ii) An active decision engine that can make recommendations, and (iii) high availability. Synchronizing a data warehouse in real time with transactional systems thus requires reducing the interval between update points. To achieve this dynamic option, the analytical database system must immediately reflect updates on transactional data.","General and reference:0.1,Hardware:0.2,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.3,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:1.0,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.2,Social and professional topics:0.1",Information systems,"Information systems is highly relevant because the paper focuses on data warehouse design, synchronization strategies, and analytical systems, which are core to data management and information systems. Other categories like Software and Applied computing are secondary as the paper emphasizes data integration over software engineering or application-specific contexts.","Data management systems:0.9,Information retrieval:0.1,Information storage systems:0.1,Information systems applications:0.3,World Wide Web:0.1",Data management systems,"Data management systems is highly relevant as the paper discusses synchronization techniques for data warehouses. Information systems applications receives a moderate score for business context, but the core contribution is in data management architecture. Other categories are not central to the paper.","Data structures:0.0,Database administration:1.0,Database design and models:0.8,Database management system engines:0.7,Information integration:1.0,Middleware for databases:0.0,Query languages:0.0","Database administration,Information integration",Database administration is central to managing synchronization processes. Information integration is key for real-time data warehouse updates. Database design and engines (0.7-0.8) are secondary as the paper focuses on synchronization strategies rather than design specifics.
5947,iMAP: discovering complex semantic matches between database schemas,"Creating semantic matches between disparate data sources is fundamental to numerous data sharing efforts. Manually creating matches is extremely tedious and error-prone. Hence many recent works have focused on automating the matching process. To date, however, virtually all of these works deal only with one-to-one (1-1) matches, such as address = location. They do not consider the important class of more complex matches, such as address = concat (city, state) and room-pric = room-rate* (1 + tax-rate).We describe the iMAP system which semi-automatically discovers both 1-1 and complex matches. iMAP reformulates schema matching as a search in an often very large or infinite match space. To search effectively, it employs a set of searchers, each discovering specific types of complex matches. To further improve matching accuracy, iMAP exploits a variety of domain knowledge, including past complex matches, domain integrity constraints, and overlap data. Finally, iMAP introduces a novel feature that generates explanation of predicted matches, to provide insights into the matching process and suggest actions to converge on correct matches quickly. We apply iMAP to several real-world domains to match relational tables, and show that it discovers both 1-1 and complex matches with high accuracy.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.8,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Information systems,"Information systems is highly relevant as the paper addresses schema matching for data integration, a core database problem. Other categories like Software and its engineering are less relevant since the focus is on data semantics rather than software development.","Data management systems:0.85,Information retrieval:0.1,Information storage systems:0.1,Information systems applications:0.3,World Wide Web:0.1",Data management systems,Data management systems is directly relevant for schema matching in databases. Other categories like information retrieval are less applicable since the focus is on data integration rather than information search.,"Data structures:0.1,Database administration:0.1,Database design and models:0.1,Database management system engines:0.8,Information integration:1.0,Middleware for databases:0.1,Query languages:0.1","Information integration,Database management system engines",Information integration is directly relevant for schema matching. Database management system engines is relevant as the system is part of DBMS functionality. Query languages are not central to the core contribution.
2182,Multilingual person name recognition and transliteration,"Nous presentons ici un outil de reperage des noms de personnes, a partir d’articles de la presse internationale, capable de reconnaitre les differentes variantes d’un meme nom. L’originalite de notre approche vient de l’identification des variantes de noms a travers les langues et systemes d’ecriture, grec, cyrillique et arabe compris. Etant donne notre contexte multilingue, nous utilisons une representation interne standard de chaque nom ainsi qu’une meme mesure de similarite (au lieu d’adopter l’approche bilingue habituelle de la translitteration). Ce module fait partie d’un outil plus general qui analyse en moyenne 15.000 articles de journaux chaque jour, afin de regrouper les documents similaires, aussi bien dans une meme langue que dans des langues differentes.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.9,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Information systems,Information systems is relevant because the paper focuses on multilingual person name recognition and transliteration across languages/scripts for document clustering. Other categories like Software Engineering or Applied Computing are not central to the core contribution of information retrieval and multilingual data processing.,"Data management systems:0.1,Information retrieval:0.9,Information storage systems:0.2,Information systems applications:0.8,World Wide Web:0.1","Information retrieval,Information systems applications",Information retrieval is central to cross-lingual name recognition. Information systems applications is relevant for implementing the multilingual grouping system. Data management systems is less directly related to the core problem of name variants.,"Collaborative and social computing systems and tools:0.2,Computational advertising:0.1,Computing platforms:0.1,Data mining:0.9,Decision support systems:0.1,Digital libraries and archives:0.3,Document representation:0.8,Enterprise information systems:0.1,Evaluation of retrieval results:0.1,Information retrieval query processing:0.9,Mobile information processing systems:0.1,Multimedia information systems:0.1,Process control systems:0.1,Retrieval models and ranking:0.3,Retrieval tasks and goals:0.2,Search engine architectures and scalability:0.1,Spatial-temporal systems:0.1,Specialized information retrieval:0.4,Users and interactive retrieval:0.1","Information retrieval query processing,Document representation",Information retrieval query processing is highly relevant as the paper focuses on multilingual name variant detection in information retrieval. Document representation is relevant due to the internal standard representation of names. Other categories like Data mining (0.9) are less central than the core IR focus.
1145,Quality of Service and Optimization in Data Integration Systems,"Durch die anhaltende Globalisierung der Weltwirtschaft, ist die Verarbeitung von verteilten Datenbestanden mittlerweile unabdingbar [LKK 97]. Anfrageverarbeitung auf diesen Daten analog zu relationalen Datenbanken ist hierbei eine Schlusselfertigkeit, allerdings in einem Internet-weit skalierten Anwendungsszenario auch schwer zu realisieren. Das Datenintegrationssystem ObjectGlobe soll Anfrageverarbeitung in dieser Art ermoglichen. Dessen Basisarchitektur und das integrierte Dienstgutemanagement (Quality of Service, QoS) wurden in dieser Dissertation erarbeitet und hier kurz zusammengefasst. Die Dissertation [Bra02] selber kann unter folgender URL bezogen werden: http://elib.ub.uni-passau.de/opus/volltexte/2002/27. 1 Die Bedeutung von Anfrageverarbeitung im Internet Die fortschreitende Globalisierung der Weltwirtschaft zieht auch die Forderung nach angepassten globalen Datenverarbeitungsmoglichkeiten im Internet nach sich. Anfrageverarbeitung ist diesbezuglich eine der Kernaufgaben und spielt in Anwendungen, wie z.B. im Bereich der maschinellen Entscheidungsunterstutzung (decision support) eine tragende Rolle. Das Internet bietet hierzu den Zugriff auf eine riesige Anzahl von Datenquellen, wie z.B. Hotelubersichten, Bahnund Flugplane, Aktienkurse und wissenschaftliche Messdaten, wie Satellitenbilder und Genom Datenbanken. Datenintegrationsbzw. Mediatorsysteme wurden in diesem Zusammenhang entwickelt, um auf diese Daten analog zu herkommlichen Datenbanksystemen zugreifen zu konnen. Zuerst waren dies geschlossene Systeme mit einem festgelegten Satz von unterstutzten Datenquellen und einer zentralen Verarbeitung der Mediator-spezifischen Aufgaben. In letzter Zeit wurde verstarkt die Entwicklung verteilter und flexibler Datenintegrationssysteme vorangetrieben, wobei Amos II [JKR99] oder das hier behandelte ObjectGlobe System [BKK 01] zwei Vertreter sind. Innerhalb einer ObjectGlobe Foderation partizipieren drei Arten von Providern: Datenprovider (data provider) stellen ihre Daten zur Verfugung, Funktionenprovider (function provider) steuern Anfrageoperatoren und Funktionen bei und Rechenzeitprovider (cycle provider) konnen zur Verarbeitung von Teilen einer Anfrage herangezogen werden. In ObjectGlobe konnen die Dienste dieser drei Providerarten auf nahezu orthogonale Weise miteinander verknupft werden. Einschrankungen in der Orthogonalitat konnen bestehen nur aufgrund von Sicherheits-, Datenschutzund Kapazitatsaspekten. Auf diese Weise stellt das ObjectGlobe System einen offenen und verteilten Servicemarkt fur Anfrageverarbeitung zur Verfugung. data prov. R1 data prov. R2 scale","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.8,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Information systems,"Information systems is highly relevant for the QoS and optimization in data integration systems. Other categories like Computing methodologies (0.1) are less relevant as the focus is on system architecture, not algorithmic methods.","Data management systems:0.85,Information retrieval:0.3,Information storage systems:0.35,Information systems applications:0.8,World Wide Web:0.6","Data management systems,Information systems applications",Data management systems is relevant for distributed data integration. Information systems applications is relevant for the ObjectGlobe system. World Wide Web is partially relevant but not the core focus.,"Collaborative and social computing systems and tools:0.1,Computational advertising:0.1,Computing platforms:0.1,Data mining:0.1,Data structures:0.1,Database administration:0.1,Database design and models:0.2,Database management system engines:0.2,Decision support systems:0.5,Digital libraries and archives:0.1,Enterprise information systems:0.3,Information integration:0.9,Middleware for databases:0.2,Mobile information processing systems:0.1,Multimedia information systems:0.1,Process control systems:0.1,Query languages:0.8,Spatial-temporal systems:0.1","Information integration,Query languages,Decision support systems",Information integration: The paper focuses on ObjectGlobe as a data integration system for distributed data sources. Query languages: The work discusses query processing in distributed environments. Decision support systems: The paper mentions decision support as a key application area. Other children like Data mining are not central to the integration and query processing focus.
142,NTCIR-5 CLIR Experiments at Oki,"We participated in SLIR, BLIR(PLIR) and MLIR subtasks at the NTCIR-4 CLIR task. Our IR system can handle queries and documents in Chinese, English and Japanese. The system utilizes multiple language resources (bilingual dictionaries, parallel corpora and machine translation systems) for query translation. We adopted the pivot language approach for C-J and J-C search using English as a pivot language. We submitted formal runs for 12 subtasks, and confirmed that the combination of language resources makes performance of BLIR high and that the pivot language approach is promising.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.9,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Information systems,"Information systems: The paper describes experiments in cross-language information retrieval. Other fields: The paper doesn't focus on hardware design, network communication, theory of computation, etc.","Information retrieval:0.9,Data management systems:0.7,World Wide Web:0.3,Information storage systems:0.5,Information systems applications:0.4",Information retrieval,Information retrieval is relevant for the cross-language information retrieval experiments. Data management systems receives moderate relevance for multi-language document handling. World Wide Web is irrelevant as the focus is on IR systems rather than web technologies.,"Document representation:0.4,Evaluation of retrieval results:0.3,Information retrieval query processing:0.7,Retrieval models and ranking:0.4,Retrieval tasks and goals:0.5,Search engine architectures and scalability:0.6,Specialized information retrieval:0.7,Users and interactive retrieval:0.2","Information retrieval query processing,Search engine architectures and scalability,Specialized information retrieval",Information retrieval query processing: The system uses pivot languages and multiple resources for query translation. Search engine architectures and scalability: The system handles multilingual queries and documents. Specialized information retrieval: Focuses on cross-language information retrieval. User interaction is not a primary focus.
1442,View topics: automatically generated characteristic view for content-based 3D object retrieval,"Characteristic view is an effective way to represent a 3D object through a set of distinct projections from different view aspects. In this paper, we proposed techniques for automatic characteristic views generations by clustering views of the object from multiple view aspect. By considering the resulting clusters as View Topics that describe a set of portraits of the object, the object can be represented by a set of view topics that can be applied to 3D object retrieval with similarity measures based on the Vector Space Model and the Language Model as well as advanced techniques such as RBF Kernel method. Our experiments have demonstrated that our method is not only invariant with respect to rotation and scaling, but also invariant with respect to the object reflection, and achieve an overall better performance than existing methods.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:1.0,Security and privacy:0.1,Human-centered computing:0.3,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Information systems,"Information systems: The paper focuses on 3D object retrieval using view clustering, a core information retrieval task. Human-centered computing is secondary to the system-level method.","Data management systems:0.1,Information retrieval:0.9,Information storage systems:0.1,Information systems applications:0.2,World Wide Web:0.1",Information retrieval,Information retrieval: Directly addresses content-based 3D object retrieval. Other categories are not central to the retrieval methodology or clustering techniques.,"Document representation:1,Evaluation of retrieval results:0,Information retrieval query processing:0.5,Retrieval models and ranking:1,Retrieval tasks and goals:0,Search engine architectures and scalability:0,Specialized information retrieval:0,Users and interactive retrieval:0","Document representation,Retrieval models and ranking","Document representation: The paper introduces view topics as a document representation for 3D objects. Retrieval models and ranking: Similarity measures like VSM and RBF Kernel are used for ranking. Query processing was rejected as the focus is on representation, not query handling."
5722,Integration of Data Semantics in Heterogeneous Database Federations,"INTRODUCTION Modern information systems are often distributed in nature; data and services are spread over different component systems wishing to cooperate in an integrated setting. Information integration is a very complex problem and is relevant in several fields, such as data reengineering, data warehousing, Web information systems , e-commerce, scientific databases, and B2B applications. Information systems involving integration of cooperating component systems are called federated information systems; if the component systems are all databases then we speak of a federated database sys-In this article, we will address the situation where the component systems are so-called legacy systems; i.e., systems that are given beforehand and which are to interoperate in an integrated single framework in which the legacy systems are to maintain as much as possible their respective autonomy. A huge challenge is to build federated databases that respect so-called global transaction safety; i.e., global transactions should preserve constraints on the global level of the federation. In a federated database (or FDB, for short) one has different component databases wishing to cooperate in an integrated setting. The component systems are often legacy systems: They have been developed in isolation before development of the actual federated system (they remain to be treated as autonomous entities). Legacy systems were typically designed to support local requirements; i.e., with local data and constraints, and not taking into account any future cooperation with other systems. Different legacy systems may also harbour different underlying data models, subject to different query transaction processing methods (flat files, network, hierarchical, relational, object-oriented, etc.). Getting a collection of autonomous legacy systems to cooperate in a single federated system is known as the interoperability problem. The general term mediation (Wiederhold, 1995) was founded to address the problem of interoperability. A federated database (FDB) can be seen as a special kind of mediation, where the mediator acts as a DBMS-like interface to the FDB application. A mediator is a global service to link local data sources and local application programs. It provides integrated information while letting the component systems of the federation remain intact. Typical mediator tasks include: • accessing and retrieving relevant data from multiple heterogeneous sources, • transforming retrieved data so that they can be integrated, • integrating the homogenized data. The mediator provides a database-like interface to applications. This interface gives the application the impression of a homogeneous, monolithic database. In reality, however, queries and transactions issued against this interface …","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.9,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.3,Social and professional topics:0.0",Information systems,"Information systems is directly relevant for database federation and mediation techniques. Applied computing is secondary due to real-world application contexts like e-commerce, but the core contribution is database system integration.","Data management systems:0.9,Information retrieval:0.3,Information storage systems:0.5,Information systems applications:0.7,World Wide Web:0.1","Data management systems,Information systems applications",Data management systems is directly relevant to the federated database design focus. Information systems applications is relevant to the real-world use cases mentioned. Information storage systems is tangentially related to data storage aspects. World Wide Web is not core to the database federation problem.,"Collaborative and social computing systems and tools:0,Computational advertising:0,Computing platforms:0,Data mining:0,Data structures:0,Database administration:0,Database design and models:1,Database management system engines:0,Decision support systems:0,Digital libraries and archives:0,Enterprise information systems:0,Information integration:1,Middleware for databases:0.5,Mobile information processing systems:0,Multimedia information systems:0,Process control systems:0,Query languages:0","Database design and models,Information integration",Database design and models: The paper addresses federated database systems and their design for legacy integration. Information integration: The focus is on mediating heterogeneous data sources. 'Middleware for databases' received a moderate score due to the mention of mediators but is secondary to the core integration and design contributions.
3696,Semantometrics: Towards fulltext-based research evaluation,"Over the recent years, there has been a growing interest in developing new research evaluation methods that could go beyond the traditional citation-based metrics. This interest is motivated on one side by the wider availability or even emergence of new information evidencing research performance, such as article downloads, views and Twitter mentions, and on the other side by the continued frustrations and problems surrounding the application of purely citation-based metrics to evaluate research performance in practice. Semantometrics are a new class of research evaluation metrics which build on the premise that full-text is needed to assess the value of a publication. This paper reports on the analysis carried out with the aim to investigate the properties of the semantometric contribution measure [1], which uses semantic similarity of publications to estimate research contribution, and provides a comparative study of the contribution measure with traditional bibliometric measures based on citation counting.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.2,Information systems:0.8,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.2,Social and professional topics:0.1",Information systems,"Information systems is highly relevant as the paper discusses research evaluation metrics based on full-text analysis, a core information systems application. Applied computing and computing methodologies receive moderate scores for algorithmic evaluation, but the primary domain is information systems.","Data management systems:0.3,Information retrieval:0.8,Information storage systems:0.2,Information systems applications:0.9,World Wide Web:0.1","Information systems applications,Information retrieval",Information systems applications and Information retrieval are directly relevant for the paper's focus on semantic evaluation metrics and full-text analysis. Data management systems is less relevant as the paper emphasizes evaluation rather than data storage.,"Collaborative and social computing systems and tools:0,Computational advertising:0,Computing platforms:0,Data mining:1,Decision support systems:0,Digital libraries and archives:1,Document representation:0,Enterprise information systems:0,Evaluation of retrieval results:0,Information retrieval query processing:0,Mobile information processing systems:0,Multimedia information systems:0,Process control systems:0,Retrieval models and ranking:0,Retrieval tasks and goals:0,Search engine architectures and scalability:0,Spatial-temporal systems:0,Specialized information retrieval:0,Users and interactive retrieval:0","Data mining,Digital libraries and archives",Data mining: The paper evaluates semantometrics using semantic similarity. Digital libraries and archives: The focus is on full-text-based research evaluation. Other children are unrelated to the evaluation context.
3730,E3: Keyphrase based News Event Exploration Engine,"This paper presents a novel system E3 for extracting keyphrases from news content for the purpose of offering the news audience a broad overview of news events, with especially high content volume. Given an input query, E3 extracts keyphrases and enrich them by tagging, ranking and finding role for frequently associated keyphrases. Also, E3 finds the novelty and activeness of keyphrases using news publication date, to identify the most interesting and informative keyphrases.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.9,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Information systems,"Information systems: The paper presents a system for keyphrase extraction and enrichment from news content, aligning with information retrieval and management. Other categories like Software engineering are secondary to the information-focused contribution.","Data management systems:0.05,Information retrieval:0.9,Information storage systems:0.05,Information systems applications:0.05,World Wide Web:0.7","Information retrieval,World Wide Web",Information retrieval is highly relevant for the keyphrase extraction and ranking system. World Wide Web is relevant as the system processes news content from web sources. Data management systems are less relevant as the focus is on retrieval rather than storage.,"Document representation:0.3,Evaluation of retrieval results:0.1,Information retrieval query processing:0.9,Online advertising:0.0,Retrieval models and ranking:0.7,Retrieval tasks and goals:0.4,Search engine architectures and scalability:0.2,Specialized information retrieval:0.6,Users and interactive retrieval:0.1,Web applications:0.1,Web data description languages:0.1,Web interfaces:0.1,Web mining:0.8,Web searching and information discovery:0.5,Web services:0.1","Information retrieval query processing,Web mining",Information retrieval query processing is relevant as the system extracts and processes keyphrases from news. Web mining is relevant because it involves analyzing news content (a form of web data). Other categories like 'Document representation' or 'Retrieval models and ranking' are secondary to the core focus on query processing and mining techniques.
673,BAH: A Bitmap Index Compression Algorithm for Fast Data Retrieval,"Efficient retrieval of traffic archival data is a must-have technique to detect network attacks, such as APT(advanced persistent threat) attack. In order to take insight from Internet traffic, the bitmap index is increasingly used for efficiently querying over large datasets. However, a raw bitmap index leads to high space consumption and overhead on loading indexes. Various bitmap index compression algorithms are proposed to save storage while improving query efficiency. This paper proposes a new bitmap index compression algorithm called BAH (Byte Aligned Hybrid compression coding). An acceleration algorithm using SIMD is designed to increase the efficiency of AND operation over multiple compressed bitmaps. In all, BAH has a better compression ratio and faster intersection querying speed compared with several previous works such as WAH, PLWAH, COMPAX, Roaring etc. The theoretical analysis shows that the space required by BAH is no larger than 1.6 times the information entropy of the bitmap with density larger than 0.2%. In the experiments, BAH saves about 65% space and 60% space compared with WAH on two datasets. The experiments also demonstrate the query efficiency of BAH with the application in Internet Traffic and Web pages.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:1.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Information systems,"The paper focuses on bitmap index compression for efficient data retrieval in traffic analysis, which is a core database and information systems problem.","Data management systems:1.0,Information retrieval:0.75,Information storage systems:0.1,World Wide Web:0.1,Information systems applications:0.1","Data management systems,Information retrieval",Data management systems is central to bitmap index compression. Information retrieval is relevant for efficient querying in large datasets. Other categories like Web applications are secondary to the core contribution.,"Data structures:1,Database administration:0,Database design and models:0,Database management system engines:1,Document representation:0,Evaluation of retrieval results:0,Information integration:0,Information retrieval query processing:0,Middleware for databases:0,Query languages:0,Retrieval models and ranking:0,Retrieval tasks and goals:0,Search engine architectures and scalability:0,Specialized information retrieval:0,Users and interactive retrieval:0","Data structures,Database management system engines","Data structures is relevant for the bitmap index design, and Database management system engines is relevant for the compression algorithm's integration into database systems. Other children are irrelevant as the paper focuses on storage optimization rather than query processing or user interaction."
300,MYSTIQ: a system for finding more answers by using probabilities,"MystiQ is a system that uses probabilistic query semantics [3] to find answers in large numbers of data sources of less than perfect quality. There are many reasons why the data originating from many different sources may be of poor quality, and therefore difficult to query: the same data item may have different representation in different sources; the schema alignments needed by a query system are imperfect and noisy; different sources may contain contradictory information, and, in particular, their combined data may violate some global integrity constraints; fuzzy matches between objects from different sources may return false positives or negatives. Even in such environment, users some-times want to ask complex, structurally rich queries, using query constructs typically found in SQL queries: joins, subqueries, existential/universal quantifiers, aggregate and group-by queries: for example scientists may use such queries to query multiple scientific data sources, or a law enforcement agency may use it in order to find rare associations from multiple data sources. If standard query semantics were applied to such queries, all but the most trivial queries will return an empty answer.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:1.0,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Information systems,Information systems is highly relevant as the paper discusses probabilistic query systems for data sources. Other categories like Software Engineering or Computing Methodologies are less directly related to the core contribution.,"Data management systems:0.9,Information retrieval:0.6,Information storage systems:0.4,Information systems applications:0.5,World Wide Web:0.3",Data management systems,Data management systems is highly relevant as the paper introduces a novel probabilistic query system for handling imperfect data sources. Information retrieval is secondary as the focus is on query semantics rather than retrieval algorithms.,"Data structures:0.0,Database administration:0.0,Database design and models:0.2,Database management system engines:0.7,Information integration:1.0,Middleware for databases:0.0,Query languages:1.0","Information integration,Query languages","Information integration: The system addresses querying across heterogeneous data sources with imperfections, a core information integration challenge. Query languages: The paper introduces probabilistic query semantics, directly relevant to query language design. Database management system engines is secondary as the focus is on query execution rather than engine architecture."
3588,Summarizing Multilingual Spoken Negotiation Dialogues,"We present the multilingual summarization functionality for VERB-MOBIL, a speech translation system. We reuse resources of the system to create a summary. After content extraction, we interpret the results in the dialog context. A summary generator provides the input to generation. A first evaluation indicates the feasibility of the approach.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:1.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Information systems,"Information systems is directly relevant because the paper presents dialogue summarization techniques for speech translation systems, a key NLP application in information processing.","Information systems applications:1,Data management systems:0.5,Information storage systems:0.3,Information retrieval:0.4,World Wide Web:0.2",Information systems applications,Information systems applications is relevant as the paper presents a multilingual summarization system. Other categories are less directly related to the application of information systems.,"Collaborative and social computing systems and tools:0,Computational advertising:0,Computing platforms:0,Data mining:1,Decision support systems:0,Digital libraries and archives:0,Enterprise information systems:0,Mobile information processing systems:0,Multimedia information systems:1,Process control systems:0,Spatial-temporal systems:0","Data mining,Multimedia information systems",Data mining is relevant due to content extraction for summarization. Multimedia information systems applies as the paper deals with spoken dialogue processing. Other categories are irrelevant as the focus is not on social systems or enterprise tools.
5235,A spatial keyword evaluation framework for network-based spatial queries,"An increasing number of spatial keyword query techniques that return qualified objects based on a comprehensive consideration of spatial and keyword constraints have been presented in the literature. Due to the complexity of the solutions to spatial keyword queries, systems that can effectively demonstrate the mechanisms will attract interest from the spatial database research community. However, very limited visualization systems have been developed for illustrating spatial keyword query evaluation. In this demonstration, we present a system that visualizes advanced solutions to efficiently answer the Spatial Keyword k Nearest Neighbor (SKkNN) query. With the two-level data management method and the friendly user interface implemented by the Standard Widget Toolkit (SWT) and Open Graphics Library (OpenGL), our system is able to not only interact with users in diverse manners, visualize datasets, and display the SKkNN query evaluation process, but it also helps users better understand the solutions in a more intuitive way.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.3,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.9,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Information systems,"Information systems is highly relevant because the paper focuses on spatial keyword queries, visualization, and database systems. Other fields like Software and its engineering or Computing methodologies are not central to the core contribution of spatial database visualization.","Data management systems:0.9,Information retrieval:0.8,Information systems applications:0.6,World Wide Web:0.3,Information storage systems:0.4","Data management systems,Information retrieval","Data management systems: The paper focuses on visualizing spatial keyword query evaluation, a key aspect of data management. Information retrieval: The work involves spatial keyword queries, which are central to information retrieval. Other categories (e.g., World Wide Web) are less relevant as the focus is on spatial data systems rather than web-based applications.","Data structures:0.1,Database administration:0.3,Database design and models:0.2,Database management system engines:0.2,Document representation:0.1,Evaluation of retrieval results:0.1,Information integration:0.7,Information retrieval query processing:0.6,Middleware for databases:0.1,Query languages:0.2,Retrieval models and ranking:0.1,Retrieval tasks and goals:0.3,Search engine architectures and scalability:0.1,Specialized information retrieval:0.1,Users and interactive retrieval:0.5","Information integration,Information retrieval query processing",Information integration is relevant for the spatial keyword query framework. Information retrieval query processing is relevant for the SKkNN query visualization. Other categories like query languages are less central than the system's core integration and query visualization focus.
5447,Triggered Real-Time Databases with Consistency Constraints,"Abstract Real-time database systems incorporate the notion of a deadline into the database system model. USU- ally, deadlines are associated with transactions, and the system attempts to execute a given set of trans- actions so as to both meet the deadlines and ensure the database consistency. This paper presents an al- ternative model of real-time database processing in which deadlines are associated with consistency con- straints rather than directly with transactions. This model leads to a predicate-baaed approach to transac- tion management that allows greater concurrency and more flexibility in modeling real-world systems. 1 Introduction Real-time database systems (RTDBs) incorporate tim- ing considerations into a database system. Not only must the transactions execute correctly, but also, they must complete execution within some time limit called a deadline. Systems that incorporate strict deadlines are called","General and reference:0.05,Hardware:0.05,Computer systems organization:0.1,Networks:0.05,Software and its engineering:0.05,Theory of computation:0.05,Mathematics of computing:0.05,Information systems:0.25,Security and privacy:0.05,Human-centered computing:0.05,Computing methodologies:0.05,Applied computing:0.05,Social and professional topics:0.05",Information systems,"The paper focuses on real-time database systems and consistency constraints, which align with information systems. Other categories like computer systems organization are not central to the database-focused contribution.","Data management systems:0.9,Information retrieval:0.1,Information storage systems:0.2,Information systems applications:0.1,World Wide Web:0.1",Data management systems,Data management systems is highly relevant as the paper introduces a novel real-time database model with consistency constraints. Other fields like Information retrieval or Web are only tangentially related.,"Data structures:0.0,Database administration:0.2,Database design and models:0.8,Database management system engines:1.0,Information integration:0.0,Middleware for databases:0.0,Query languages:0.0","Database management system engines,Database design and models","Database management system engines: The paper introduces a new model for real-time database processing, directly impacting engine design. Database design and models: The predicate-based transaction management suggests a novel design model. Other fields are not central to the paper's contribution."
3418,On Ontology Alignment Experiments,"Ontology Alignment is a process for finding related entities of different ontologies. This paper discusses the results of our research in this area. One of them is a formulation for a new structural measure which extends famous related works. In this measure with a special attention to the transitive properties, it is tried to increase recall with less harm on precision. Second contribution is a new method for compound measure creation without any need to the mapping extraction phase. Effectiveness of these ideas is discussed and quantitative evaluations are explained in this paper.","General and reference:0,Hardware:0,Computer systems organization:0,Networks:0,Software and its engineering:0.3,Theory of computation:0,Mathematics of computing:0.2,Information systems:1,Security and privacy:0.1,Human-centered computing:0,Computing methodologies:0.5,Applied computing:0.4,Social and professional topics:0",Information systems,Information systems is highly relevant for ontology alignment in semantic data management. Computing methodologies is marginally relevant for algorithm design. Other categories are irrelevant as the focus is on knowledge representation techniques.,"Data management systems:0.8,Information retrieval:0.9,Information storage systems:0.2,Information systems applications:0.7,World Wide Web:0.3","Information retrieval,Data management systems,Information systems applications",Information retrieval is highly relevant as the paper focuses on ontology alignment for finding related entities. Data management systems relates to the structural measures discussed. Information systems applications is relevant for applying these techniques. Information storage systems and World Wide Web are less directly addressed in the abstract.,"Collaborative and social computing systems and tools:0.2,Computational advertising:0.1,Computing platforms:0.1,Data mining:0.4,Data structures:0.2,Database administration:0.3,Database design and models:0.3,Database management system engines:0.2,Decision support systems:0.3,Digital libraries and archives:0.3,Document representation:0.2,Enterprise information systems:0.3,Evaluation of retrieval results:0.4,Information integration:1,Information retrieval query processing:0.8,Middleware for databases:0.2,Mobile information processing systems:0.2,Multimedia information systems:0.1,Process control systems:0.1,Query languages:0.3,Retrieval models and ranking:0.5,Retrieval tasks and goals:0.3,Search engine architectures and scalability:0.2,Spatial-temporal systems:0.1,Specialized information retrieval:0.5,Users and interactive retrieval:0.4","Information integration,Information retrieval query processing","Information integration is highly relevant as the paper focuses on ontology alignment (matching entities across ontologies). Information retrieval query processing is relevant for the structural measures and mapping techniques discussed. Other categories are rejected because the focus is on semantic alignment rather than databases, digital libraries, or user interaction."
4269,ModelingWeb Services Policy with Corporate Knowledge,"Web services policy is used to specify service constraints. Although many efforts like WS-policy have been done to provide declarative configuration languages, it is still hard to discover and exchange the configuration information in the service computing environment. A novel approach to share service knowledge and application-specific information is needed. In this paper, we model Web service policy with corporate knowledge, which is defined as the amount of knowledge provided by individual agents. This approach provides a distributed knowledge management method to our proposed semantic policy framework that enables reasoning, which is necessary for policy creation, conflicts resolution and negotiation","General and reference:0.05,Hardware:0.05,Computer systems organization:0.05,Networks:0.05,Software and its engineering:0.1,Theory of computation:0.05,Mathematics of computing:0.2,Information systems:0.7,Security and privacy:0.1,Human-centered computing:0.05,Computing methodologies:0.3,Applied computing:0.2,Social and professional topics:0.05",Information systems,Information systems: The paper models web service policies using knowledge management and semantic frameworks. Other categories like Security and privacy are less relevant as the focus is on policy representation rather than security mechanisms.,"Data management systems:0.0,Information retrieval:0.0,Information storage systems:0.0,Information systems applications:0.9,World Wide Web:0.8","Information systems applications,World Wide Web",Information systems applications is relevant for policy modeling and knowledge management in Web services. World Wide Web is relevant as the focus is on Web service policy frameworks. Other categories like data management systems are not directly discussed.,"Collaborative and social computing systems and tools:0.0,Computational advertising:0.0,Computing platforms:0.0,Data mining:0.0,Decision support systems:0.0,Digital libraries and archives:0.0,Enterprise information systems:0.5,Mobile information processing systems:0.0,Multimedia information systems:0.0,Online advertising:0.0,Process control systems:0.0,Spatial-temporal systems:0.0,Web applications:0.0,Web data description languages:0.5,Web interfaces:0.0,Web mining:0.0,Web searching and information discovery:0.0,Web services:1.0","Web services,Enterprise information systems",Web services: The paper directly addresses policy modeling for web services. Enterprise information systems: The focus on distributed knowledge management aligns with enterprise contexts. Web data description languages are secondary due to policy representation.
3971,Information retrieval for question answering a SIGIR 2004 workshop,"Open domain question answering has become a very active research area over the past few years, due in large measure to the stimulus of the TREC Question Answering track. This track addresses the task of finding answers to natural language (NL) questions (e.g. How tall is the Eiffel Tower? Who is Aaron Copland?) from large text collections. This task stands in contrast to the more conventional IR task of retrieving documents relevant to a query, where the query may be simply a collection of keywords (e.g. Eiffel Tower, American composer, born Brooklyn NY 1900, ...).","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.3,Software and its engineering:0.2,Theory of computation:0.2,Mathematics of computing:0.2,Information systems:1.0,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.6,Applied computing:0.3,Social and professional topics:0.1",Information systems,Information systems: The paper addresses question answering as part of information retrieval systems. Other options like 'Computing methodologies' are secondary as the focus is on IR systems rather than novel algorithms.,"Data management systems:0.4,Information retrieval:0.9,Information storage systems:0.3,Information systems applications:0.6,World Wide Web:0.2","Information retrieval,Information systems applications",Information retrieval: The paper focuses on retrieving answers from text collections for question-answering systems. Information systems applications: The application of IR techniques to real-world question-answering tasks. Other categories like Data management systems are secondary to the primary focus on retrieval methods.,"Collaborative and social computing systems and tools:0.1,Computational advertising:0.1,Computing platforms:0.1,Data mining:0.3,Decision support systems:0.2,Digital libraries and archives:0.1,Document representation:0.3,Enterprise information systems:0.1,Evaluation of retrieval results:0.5,Information retrieval query processing:1.0,Mobile information processing systems:0.1,Multimedia information systems:0.1,Process control systems:0.1,Retrieval models and ranking:1.0,Retrieval tasks and goals:1.0,Search engine architectures and scalability:0.3,Spatial-temporal systems:0.1,Specialized information retrieval:0.5,Users and interactive retrieval:0.3","Information retrieval query processing,Retrieval models and ranking,Retrieval tasks and goals",Information retrieval query processing is relevant because the paper focuses on processing natural language questions for retrieval. Retrieval models and ranking is relevant as the paper addresses the challenge of ranking answers in QA systems. Retrieval tasks and goals is relevant because open-domain QA is a specific retrieval task. Other categories like Data mining or Multimedia are less relevant as the paper's focus is on QA rather than general data mining or multimedia-specific retrieval.
4218,Deriving Concept-Based User Profiles from Search Engine Logs,"User profiling is a fundamental component of any personalization applications. Most existing user profiling strategies are based on objects that users are interested in (i.e., positive preferences), but not the objects that users dislike (i.e., negative preferences). In this paper, we focus on search engine personalization and develop several concept-based user profiling methods that are based on both positive and negative preferences. We evaluate the proposed methods against our previously proposed personalized query clustering method. Experimental results show that profiles which capture and utilize both of the user's positive and negative preferences perform the best. An important result from the experiments is that profiles with negative preferences can increase the separation between similar and dissimilar queries. The separation provides a clear threshold for an agglomerative clustering algorithm to terminate and improve the overall quality of the resulting query clusters.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.1,Information systems:0.85,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.6,Applied computing:0.0,Social and professional topics:0.0",Information systems,Information systems: The paper focuses on user profiling from search logs for personalization. Computing methodologies: Marginal relevance for clustering algorithms. Other categories like 'Mathematics of computing' are less central as the focus is on data-driven profiling rather than mathematical theory.,"Data management systems:0.3,Information retrieval:0.9,Information storage systems:0.2,Information systems applications:0.8,World Wide Web:0.4","Information retrieval,Information systems applications",Information retrieval is highly relevant because the paper focuses on query clustering and personalization for search engines. Information systems applications is relevant as it addresses user profiling for system optimization. Other categories like Data management or World Wide Web are less central to the core contribution of leveraging search logs for personalization.,"Collaborative and social computing systems and tools:0,Computational advertising:0,Computing platforms:0,Data mining:1,Decision support systems:0,Digital libraries and archives:0,Document representation:0,Enterprise information systems:0,Evaluation of retrieval results:0,Information retrieval query processing:1,Mobile information processing systems:0,Multimedia information systems:0,Process control systems:0,Retrieval models and ranking:0,Retrieval tasks and goals:0,Search engine architectures and scalability:0,Spatial-temporal systems:0,Specialized information retrieval:0,Users and interactive retrieval:1","Data mining,Users and interactive retrieval",Data mining is relevant because the paper analyzes search engine logs to derive user profiles. Users and interactive retrieval is relevant as the focus is on modeling user preferences. Other categories like decision support systems or spatial-temporal systems are not directly addressed.
2913,Customising geoparsing and georeferencing for historical texts,"In order to better support the text mining of historical texts, we propose a combination of complementary techniques from Geographical Information Systems, computational and corpus linguistics. In previous work, we have described this as `visual gisting' to extract important themes from text and locate those themes on a map representing geographical information contained in the text. Here, we describe the steps that were found necessary to apply standard analysis and resolution tools to identify place names in a specific corpus of historical texts. This task is seen as an initial and prerequisite step for further analysis and comparison by combining the information we extract from a corpus with information from other sources, including other text corpora. The process is intended to support close reading of historical texts on a much larger scale by highlighting using exploratory and data-driven approaches which parts of the corpus warrant further close analysis. Our case study presented here is from a corpus of Lake District travel literature. We discuss the customisations that we have to make to existing tools to extract placename information and visualise it on a map.","General and reference:0.05,Hardware:0.05,Computer systems organization:0.05,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.05,Mathematics of computing:0.1,Information systems:0.8,Security and privacy:0.05,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.05",Information systems,"Information systems is relevant for geoparsing, GIS integration, and text mining workflows. Other categories like Human-centered computing are less relevant as the focus is on data processing rather than user interaction.","Data management systems:0.2,Information retrieval:0.8,Information storage systems:0.3,Information systems applications:0.6,World Wide Web:0.5","Information retrieval,Information systems applications","Information retrieval: The paper addresses text mining and geoparsing of historical texts. Information systems applications: The system integrates GIS, NLP, and corpus linguistics for historical analysis. World Wide Web is partially relevant via DBpedia but less central than the core text mining focus.","Collaborative and social computing systems and tools:0.1,Computational advertising:0.1,Computing platforms:0.1,Data mining:1.0,Decision support systems:0.2,Digital libraries and archives:1.0,Document representation:0.1,Enterprise information systems:0.1,Evaluation of retrieval results:0.1,Information retrieval query processing:0.1,Mobile information processing systems:0.1,Multimedia information systems:0.1,Process control systems:0.1,Retrieval models and ranking:0.1,Retrieval tasks and goals:0.1,Search engine architectures and scalability:0.1,Spatial-temporal systems:0.3,Specialized information retrieval:0.1,Users and interactive retrieval:0.1","Data mining,Digital libraries and archives",Data mining: Geoparsing and georeferencing involve data mining techniques for text analysis. Digital libraries and archives: The focus on historical texts and archival analysis aligns with digital library applications. Other categories like 'Spatial-temporal systems' are less central to the text-mining focus.
54,Digital Archeology: Recovering Digital Objects from Audio Waveforms,"Specimens of early computer systems stop working every day. One storage medium that was popular for home computers in the 1980s was the audio tape. The first home computer systems allowed the use of standard cassette players to record and replay data. Audio tapes are more durable than old home computers when properly stored. Devices playing this medium (i.e. tape recorders) can be found in working condition or can be repaired as they are made out of standard components. By re-engineering the format of the waveform the data on such media can then be extracted from a digitized audio stream. This work presents a case study of extracting data created on an early home computer system, the Philips G7400. Results show that with some error correction methods parts of the tapes are still readable, even without the original system. It also becomes clear, that it is easier to build solutions now when the original systems are still available.","General and reference:0.1,Hardware:0.3,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.9,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Information systems,"Information systems is highly relevant for data recovery from storage media. Hardware gets moderate relevance for tape recorder context, but the core is data preservation.","Data management systems:0.1,Information retrieval:0.4,Information storage systems:1.0,Information systems applications:0.2,World Wide Web:0.1",Information storage systems,Information storage systems is highly relevant as the paper focuses on recovering data from audio tapes. Information retrieval is partially relevant but secondary to the core contribution.,"Information storage technologies:1.0,Record storage systems:0.0,Storage architectures:0.0,Storage management:0.5,Storage replication:0.0","Information storage technologies,Storage management",Information storage technologies is relevant for the focus on audio tape data recovery. Storage management is relevant for error correction and data extraction techniques. Other categories like Storage architectures are not central.
4794,Deadlock detection in a multidatabase,"Previous results [ACM87] show that the timeout strategy of deadlock detection performs poorly in a single site database. But because of the restrictions imposed on heterogeneous multidatabases, the traditional alternatives are not possible. This paper proposes a new deadlock detection strategy within the restrictions of heterogeneous multidatabases, one that we show performs better than simple timeouts. It is proven that in at least one sense, this algorithm is optimal.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.3,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.2,Information systems:0.9,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.3,Applied computing:0.2,Social and professional topics:0.1",Information systems,Information systems: The paper addresses deadlock detection algorithms in heterogeneous multidatabase systems. Other options are irrelevant as the focus is on database system algorithms rather than security or networks.,"Data management systems:0.9,Information retrieval:0.3,Information storage systems:0.5,Information systems applications:0.8,World Wide Web:0.2","Data management systems,Information systems applications",Data management systems is relevant for the focus on deadlock detection in multidatabases. Information systems applications is relevant for the application of these systems in real-world scenarios. Other categories like World Wide Web are not directly related to the paper's core contribution.,"Collaborative and social computing systems and tools:0,Computational advertising:0,Computing platforms:0,Data mining:0,Data structures:0,Database administration:1,Database design and models:1,Database management system engines:1,Decision support systems:0,Digital libraries and archives:0,Enterprise information systems:0,Information integration:0,Middleware for databases:0,Mobile information processing systems:0,Multimedia information systems:0,Process control systems:0,Query languages:0,Spatial-temporal systems:0","Database administration,Database design and models,Database management system engines",Database administration is relevant as the paper discusses deadlock detection in multidatabases. Database design and models is relevant as it addresses database system design considerations. Database management system engines is relevant as it focuses on engine-level operations. Other categories are not relevant to the topic of deadlock detection in databases.
3003,A light-weight feedback method for reconstructing a document vector space on a feature extraction model,"In this paper, we propose a document retrieval system with a light-weight feedback method for reconstructing a document vector space, which is developed on a Feature Extraction Model (FEM). FEM makes it possible to realize a light-weight creation of vector spaces by feature terms extracted from the pre-prepared documents and we can apply the feedback method dynamically to reconstruct the vector spaces based on intensions of users. Retrieval results can be improved through the proposed feedback process because the distributions of documents on the reconstructed vector space are arranged properly according to purposes and interests of users.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.9,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Information systems,"The paper focuses on document retrieval systems and vector space reconstruction, which are core topics in information systems. Other categories like 'Software and its engineering' or 'Computing methodologies' are not the primary focus.","Data management systems:0.2,Information retrieval:0.9,Information storage systems:0.3,Information systems applications:0.4,World Wide Web:0.1",Information retrieval,Information retrieval is core to document vector space reconstruction. Other categories are secondary to this text mining application.,"Document representation:0.8,Evaluation of retrieval results:0.2,Information retrieval query processing:0.3,Retrieval models and ranking:0.7,Retrieval tasks and goals:0.2,Search engine architectures and scalability:0.2,Specialized information retrieval:0.2,Users and interactive retrieval:0.9","Document representation,Users and interactive retrieval",Document representation is relevant because the paper discusses reconstructing document vector spaces using a Feature Extraction Model. Users and interactive retrieval is relevant because the feedback method dynamically adjusts vector spaces based on user intent. Other categories like Retrieval models or Query processing are less directly relevant as the focus is on vector space reconstruction through user feedback.
1204,Community Preserving Lossy Compression of Social Networks,"Compression plays an important role in social network analysis from both practical and theoretical points of view. Although there are a few pioneering studies on social network compression, they mainly focus on lossless approaches. In this paper, we tackle the novel problem of community preserving lossy compression of social networks. The trade-off between space and information preserved in a lossy compression presents an interesting angle for social network analysis, and, at the same time, makes the problem very challenging. We propose a sequence graph compression approach, discuss the design of objective functions towards community preservation, and present an interesting and practically effective greedy algorithm. Our experimental results on both real data sets and synthetic data sets demonstrate the promise of our method.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.8,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Information systems,Information systems is relevant because the paper addresses data compression techniques for social networks. Other categories like 'Computing methodologies' are less directly relevant as the focus is on data management rather than algorithmic theory.,"Data management systems:1.0,Information systems applications:0.9,Information retrieval:0.3,Information storage systems:0.2,World Wide Web:0.1","Data management systems,Information systems applications",Data management systems is central to network compression techniques. Information systems applications is relevant for social network analysis. Other categories like World Wide Web are not core to the compression framework.,"Collaborative and social computing systems and tools:0.4,Computational advertising:0,Computing platforms:0.1,Data mining:1,Data structures:0.2,Database administration:0.1,Database design and models:0.3,Database management system engines:0.2,Decision support systems:0.3,Digital libraries and archives:0.1,Enterprise information systems:0.2,Information integration:0.3,Middleware for databases:0.1,Mobile information processing systems:0.1,Multimedia information systems:0.1,Process control systems:0.1,Query languages:0.1,Spatial-temporal systems:0.1",Data mining,Data mining is relevant as the paper focuses on social network compression and community analysis. Other categories like collaborative systems or information integration are secondary.
1706,The uRNA database,"The uRNADB offers aligned, annotated and phylogenetically ordered sequences of several U RNAs. New to this release are RNAs from U7 (14 sequences), U8 (two sequences), U11 (three sequences), U12 (two sequences), U14 (11 sequences), U18, U48 and U49. A total of 34 new sequences were aligned with the previously compiled snRNAs U1, U2, U3, U4, U5 and U6.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.9,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.3,Social and professional topics:0.1",Information systems,Information systems is directly relevant for database development and sequence alignment. Other categories do not address core contributions in biological data organization and annotation.,"Data management systems:1.0,Information retrieval:0.0,Information storage systems:1.0,Information systems applications:0.75,World Wide Web:0.0","Data management systems,Information storage systems,Information systems applications",Data management systems (1.0) and Information storage systems (1.0) are directly relevant as the paper describes a database for RNA sequences. Information systems applications (0.75) applies as it's a specialized bioinformatics application. Other categories like World Wide Web are irrelevant since the paper focuses on database structure rather than web accessibility.,"Collaborative and social computing systems and tools:0.0,Computational advertising:0.0,Computing platforms:0.0,Data mining:0.0,Data structures:0.0,Database administration:1.0,Database design and models:0.0,Database management system engines:0.0,Decision support systems:0.0,Digital libraries and archives:0.0,Enterprise information systems:0.0,Information integration:0.0,Information storage technologies:0.5,Middleware for databases:0.0,Mobile information processing systems:0.0,Multimedia information systems:0.0,Process control systems:0.0,Query languages:0.0,Record storage systems:0.0,Spatial-temporal systems:0.0,Storage architectures:0.0,Storage management:0.0,Storage replication:0.0",Database administration,Database administration: The paper describes a database of RNA sequences. Information storage technologies (0.5) is relevant for sequence storage but not the core focus.
1854,Identification of arabic word from bilingual text using character features,"The identification of the language of the script is an important stage in the process of recognition of the writing. There are several works in this research area, which treat various languages. Most of the used methods are global or statistical. In this present paper, we study the possibility of using the features of scripts to identify the language. The identification of the language of the script by characteristics returns the identification in the case of multilingual documents less difficult. We present by this work, a study on the possibility of using the structural features to identify the Arabic language from an Arabic / Latin text.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.2,Information systems:0.9,Security and privacy:0.1,Human-centered computing:0.2,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Information systems,Information systems is relevant for the text analysis and language identification approach. Other fields like human-centered computing are not central to this document processing work.,"Data management systems:0.3,Information retrieval:0.8,Information storage systems:0.2,Information systems applications:0.4,World Wide Web:0.3",Information retrieval,Information retrieval is relevant for language identification in multilingual documents. Data management systems is less central to the script feature analysis approach.,"Document representation:1,Specialized information retrieval:1,Evaluation of retrieval results:0.2,Information retrieval query processing:0.1,Retrieval models and ranking:0.1,Retrieval tasks and goals:0.1,Search engine architectures and scalability:0.1,Users and interactive retrieval:0.1","Document representation,Specialized information retrieval","Document representation is relevant because the paper discusses using structural features of scripts for language identification. Specialized information retrieval is relevant as the work focuses on language identification in multilingual documents, a specific IR application. Other options like query processing or ranking are not core to the paper's focus."
2600,A Topic-Map-based Framework to Enhance Components' Retrieval in a Process Control,"Controlling a manufacturing process to meet customers’ requirements is crucial for many industrial companies to remain at the forefront. The control of a manufacturing process requires for each actor involved in this process to be reactive and effective in monitoring the process in order to meet company objectives. Enhancing the sharing and the retrieval of the components that support the control of such a process is necessary to effectively meet actors’ requirements for the accomplishment of their business tasks. Semantic description with knowledge representation techniques is one of the widely used techniques to address components’ retrieval. Existing methods focus on describing components according to their functionalities, thus they don’t really tackle user requirements that the components meet. We propose in this paper a layered conceptual framework to add semantic description to the components that support the manufacturing process control starting from the users’ requirements. A Topic-Map-based solution is proposed to support the implementation of this framework.","General and reference:0,Hardware:0,Computer systems organization:0,Networks:0,Software and its engineering:0.3,Theory of computation:0,Mathematics of computing:0,Information systems:1,Security and privacy:0,Human-centered computing:0,Computing methodologies:0.2,Applied computing:0,Social and professional topics:0",Information systems,"Information systems is directly relevant as the paper addresses semantic component retrieval in manufacturing processes. Software engineering receives partial relevance for component design, but the focus is on information systems semantics.","Data management systems:0.2,Information retrieval:1.0,Information storage systems:0.1,Information systems applications:0.6,World Wide Web:0.1",Information retrieval,Information retrieval is highly relevant due to the focus on semantic component retrieval using Topic-Maps. Information systems applications is secondary as the framework supports process control. Other categories like data management or storage are less central to the retrieval-focused contribution.,"Document representation:0.7,Evaluation of retrieval results:0.6,Information retrieval query processing:0.5,Retrieval models and ranking:0.8,Retrieval tasks and goals:0.7,Search engine architectures and scalability:0.4,Specialized information retrieval:0.6,Users and interactive retrieval:0.5","Retrieval models and ranking,Document representation,Retrieval tasks and goals","Retrieval models and ranking (semantic description framework), Document representation (Topic-Map-based implementation), Retrieval tasks and goals (component retrieval in manufacturing). Query processing and scalability are secondary."
4797,Spatial Databases - Accomplishments and Research Needs,"Spatial databases, addressing the growing data management and analysis needs of spatial applications such as geographic information systems, have been an active area of research for more than two decades. This research has produced a taxonomy of models for space, spatial data types and operators, spatial query languages and processing strategies, as well as spatial indexes and clustering techniques. However, more research is needed to improve support for network and field data, as well as query processing (e.g., cost models, bulk load). Another important need is to apply spatial data management accomplishments to newer applications, such as data warehouses and multimedia information systems. The objective of this paper is to identify recent accomplishments and associated research needs of the near term.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.2,Networks:0.2,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.2,Information systems:0.9,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.3,Applied computing:0.4,Social and professional topics:0.1",Information systems,Information systems is highly relevant as the paper focuses on spatial database systems. Applied computing gets moderate relevance for the application context. Other categories like Computing methodologies are mentioned in the abstract but not central to the core contribution.,"Data management systems:1.0,Information systems applications:0.75,Information retrieval:0.2,Information storage systems:0.3,World Wide Web:0.1","Data management systems,Information systems applications",Data management systems is core as the paper focuses on spatial database architecture and management. Information systems applications is relevant for its application to GIS and data warehouses. Other fields like information retrieval lack direct connection to the paper's focus on data management systems.,"Collaborative and social computing systems and tools:0,Computational advertising:0,Computing platforms:0,Data mining:0,Data structures:0,Database administration:0.5,Database design and models:1,Database management system engines:1,Decision support systems:0,Digital libraries and archives:0,Enterprise information systems:0,Information integration:0,Middleware for databases:0,Mobile information processing systems:0,Multimedia information systems:0,Process control systems:0,Query languages:0,Spatial-temporal systems:1","Database design and models,Database management system engines,Spatial-temporal systems",Database design and models is relevant as the paper reviews spatial database models. Database management system engines is relevant as it discusses management systems. Spatial-temporal systems is relevant as it focuses on spatial databases. Other categories are not directly relevant to the paper's focus.
1081,Proving Consistency of Database Transactions Written in Extended Pascal,The purpose of this correspondence is to present an approach for verifying that explicitly stated integrity constraints are not violated by certain transactions. We utilize a relational model wherein constraints are given in a language based on the first-order predicate calculus. Transactions are written in terms of a Pascal-like host language with embedded first-order predicate calculus capabilities allowing queries and updates.,"General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.3,Theory of computation:0.2,Mathematics of computing:0.1,Information systems:0.8,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.2,Social and professional topics:0.1",Information systems,Information systems is highly relevant as the paper addresses database transaction consistency verification. Software and its engineering is less relevant because the focus is on database integrity constraints rather than software development processes.,"Data management systems:1.0,Information retrieval:0.0,Information storage systems:0.0,Information systems applications:0.0,World Wide Web:0.0",Data management systems,"Data management systems: The paper directly addresses database transaction verification and constraint checking, which are core aspects of database management systems research.","Data structures:0,Database administration:0,Database design and models:1,Database management system engines:1,Information integration:0,Middleware for databases:0,Query languages:0","Database design and models,Database management system engines",Database design and models are critical for transaction integrity verification. Database management system engines enable transaction processing and constraint enforcement.
1140,CATdb: a public access to Arabidopsis transcriptome data from the URGV-CATMA platform,"CATdb is a free resource available at http://urgv.evry.inra.fr/CATdb that provides public access to a large collection of transcriptome data for Arabidopsis thaliana produced by a single Complete Arabidopsis Transcriptome Micro Array (CATMA) platform. CATMA probes consist of gene-specific sequence tags (GSTs) of 150–500 bp. The v2 version of CATMA contains 24 576 GST probes representing most of the predicted A. thaliana genes, and 615 probes tiling the chloroplastic and mitochondrial genomes. Data in CATdb are entirely processed with the same standardized protocol, from microarray printing to data analyses. CATdb contains the results of 53 projects including 1724 hybridized samples distributed between 13 different organs, 49 different developmental conditions, 45 mutants and 63 environmental conditions. All the data contained in CATdb can be downloaded from the web site and subsets of data can be sorted out and displayed either by keywords, by experiments, genes or lists of genes up to 100. CATdb gives an easy access to the complete description of experiments with a picture of the experiment design.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.9,Security and privacy:0.1,Human-centered computing:0.2,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Information systems,"Information systems is highly relevant as the paper introduces a database system (CATdb) for transcriptome data, focusing on data management, accessibility, and standardized processing. Other categories like Software and its engineering (0.2) are less relevant because the core contribution is about data organization, not software engineering practices.","Data management systems:0.9,Information retrieval:0.7,Information storage systems:0.6,Information systems applications:0.5,World Wide Web:0.3","Data management systems,Information retrieval,Information storage systems",Data management systems is core to the database implementation. Information retrieval and storage apply to query and archiving capabilities. Web access is secondary.,"Data structures:0,Database administration:0,Database design and models:1,Database management system engines:1,Document representation:0,Evaluation of retrieval results:0,Information integration:0,Information retrieval query processing:0.5,Information storage technologies:0.5,Middleware for databases:0,Query languages:0.5,Record storage systems:0,Retrieval models and ranking:0,Retrieval tasks and goals:0,Search engine architectures and scalability:0,Specialized information retrieval:0,Storage architectures:0,Storage management:0.5,Storage replication:0,Users and interactive retrieval:0","Database design and models,Database management system engines",Database design and models: The paper describes a public transcriptome database with standardized design. Database management system engines: The system provides query capabilities and data storage. Information retrieval is secondary as the focus is on database infrastructure.
1061,An efficient and optimised frequent pattern mining using novel multipath-graph structure,"The frequent pattern mining is one of the most focused areas in the data mining domain. The frequent pattern growth (FP-growth) algorithm introduced compact prefix-based data structure, frequent pattern tree (FP-Tree), to store frequent itemsets in compressed format. The FP-growth attempts to overcome drawback of candidate generation approach of multiple database scan. This work aims to propose a novel optimised data structure multipath-Graph (MP-Graph) for improving memory utilisation and efficiency of mining algorithms. The MP-Graph is a compact graph structure to store frequent patterns in memory. It generates graph nodes equal to number of frequent 1-itemsets of transaction database. Further, it stores multiple occurrences of prefix subpaths along the edges of the graph in the form of transaction bitmap instead of storing frequency of individual item node. The proposed structure helps to mine frequent patterns without constructing conditional FP-Trees. The performance of the MP-Graph mining algorithm is compared with FP-growth, CT-PRO and IFP-growth. The experimental results show order of magnitude improvement in memory consumption to store frequent patterns, nodes generated and time complexity.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.9,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Information systems,Information systems: The paper presents a data mining algorithm for frequent pattern discovery. Other categories are irrelevant as the focus is on information retrieval and analysis.,"Data management systems:0.9,Information systems applications:0.75,Information retrieval:0.3,Information storage systems:0.2,World Wide Web:0.1","Data management systems,Information systems applications",Data management systems: Proposes MP-Graph for efficient pattern mining. Information systems applications: Applies the technique to data mining tasks. Other categories: Less central to the core contribution.,"Collaborative and social computing systems and tools:0.1,Computational advertising:0.1,Computing platforms:0.2,Data mining:1,Data structures:1,Database administration:0.1,Database design and models:0.2,Database management system engines:0.1,Decision support systems:0.1,Digital libraries and archives:0.1,Enterprise information systems:0.1,Information integration:0.2,Middleware for databases:0.1,Mobile information processing systems:0.1,Multimedia information systems:0.1,Process control systems:0.1,Query languages:0.1,Spatial-temporal systems:0.1","Data mining,Data structures",Data mining is relevant for frequent pattern mining. Data structures are relevant for the novel MP-Graph structure. Other options are less directly related to the core contributions.
3836,Querying data across different legal domains,"The management of legal domains is gaining great importance in the context of data management. In fact, the geographical distribution of data as implied -- for example -- by cloud-based services requires that the legal restrictions and obligations are to be taken into account whenever data circulates across different legal domains. In this paper, we start to investigate an approach for coping with the complex issues that arise when dealing with data spanning different legal domains. Our approach consists of a conceptual model that takes into account the notion of legal domain (to be paired with the corresponding data) and a reference architecture for implementing our approach in an actual relational DBMS.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.95,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Information systems,Information systems is highly relevant as the paper addresses data management across legal domains. Other categories like Applied computing are secondary but less directly connected to the database system focus.,"Data management systems:1.0,Information systems applications:0.7,Information retrieval:0.5,Information storage systems:0.2,World Wide Web:0.3",Data management systems,Data management systems is primary as the paper focuses on legal domain-aware data querying. Applications and retrieval are secondary considerations.,"Data structures:0.2,Database administration:0.8,Database design and models:0.7,Database management system engines:0.3,Information integration:0.9,Middleware for databases:0.6,Query languages:0.4","Information integration,Database administration,Middleware for databases",Information integration is critical for handling legal domain restrictions across data. Database administration applies to the conceptual model implementation. Middleware is relevant for connecting legal domains. Query languages are less central to the conceptual architecture.
1608,SigReannot-mart: a query environment for expression microarray probe re-annotations,"Expression microarrays are commonly used to study transcriptomes. Most of the arrays are now based on oligo-nucleotide probes. Probe design being a tedious task, it often takes place once at the beginning of the project. The oligo set is then used for several years. During this time period, the knowledge gathered by the community on the genome and the transcriptome increases and gets more precise. Therefore re-annotating the set is essential to supply the biologists with up-to-date annotations. SigReannot-mart is a query environment populated with regularly updated annotations for different oligo sets. It stores the results of the SigReannot pipeline that has mainly been used on farm and aquaculture species. It permits easy extraction in different formats using filters. It is used to compare probe sets on different criteria, to choose the set for a given experiment to mix probe sets in order to create a new one. Database URL: http://sigreannot-mart.toulouse.inra.fr/","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.8,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.3,Social and professional topics:0.1",Information systems,Information systems is relevant as the paper presents a database system for microarray data management. Applied computing is secondary as it supports biological research but is not the core contribution.,"Data management systems:1.0,Information retrieval:0.7,Information storage systems:0.5,Information systems applications:0.6,World Wide Web:0.3","Data management systems,Information retrieval",Data management systems: The paper describes a query environment for microarray data. Information retrieval: The system enables data filtering and extraction. Other categories are less directly related to the core database functionality.,"Data structures:0.2,Database administration:0.8,Database design and models:0.7,Database management system engines:0.6,Document representation:0.1,Evaluation of retrieval results:0.1,Information integration:0.5,Information retrieval query processing:0.4,Middleware for databases:0.3,Query languages:0.4,Retrieval models and ranking:0.2,Retrieval tasks and goals:0.3,Search engine architectures and scalability:0.1,Specialized information retrieval:0.2,Users and interactive retrieval:0.1","Database administration,Database design and models,Information integration",Database administration is primary as the system is a query environment for managing annotations. Database design and models are relevant for the framework's structure. Information integration is secondary due to combining multiple datasets. Other categories like Data structures are too general.
3959,An object oriented approach to multidimensional database conceptual modeling (OOMD),"In the recent past, there has been an increasing interest in multidimensional databases (MDB) and On-line Analytical Processing (OLAP) scenarios. Several multidimensional models have been proposed in the last days. However, very few works have been focused on the area of multidimensional database conceptual modeling. Moreover, they are either conceptual extensions to the classical multidimensional model or translations from classical database conceptual models (such as the EntityRelationship model). Nevertheless, we take the concepts and basic ideas of the classical multidimensional model (dimensions and facts) to propose a revolutionary approach based on the Object Oriented (OO) Paradigm to MDB conceptual modeling. Then, the basic elements of our Object Oriented Multidimensional Model (OOMD) such as dimension classes and fact classes are introduced. We then present cube classes as the basic structure to allow a subsequent analysis of the data stored in the system. We fairly believe that the utilization of the OO Paradigm will provide us a general conceptual model to MDB conceptual modeling in a more flexible, natural and simple way than the models proposed until now.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.0,Information systems:0.9,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.3,Applied computing:0.1,Social and professional topics:0.0",Information systems,"Information systems is highly relevant as the paper introduces an object-oriented model for multidimensional databases (OOMD), focusing on conceptual database modeling. Other categories like Computing methodologies receive moderate scores for the OO approach but are secondary to the primary database systems focus.","Data management systems:0.9,Information retrieval:0.3,Information storage systems:0.25,Information systems applications:0.2,World Wide Web:0.1",Data management systems,Data management systems is directly relevant to the multidimensional database modeling approach. Other categories like Information retrieval or Web are not central to the paper's focus on conceptual modeling.,"Data structures:0.6,Database administration:0.2,Database design and models:1,Database management system engines:0.3,Information integration:0.5,Middleware for databases:0.4,Query languages:0.3",Database design and models,Database design and models is highly relevant for the object-oriented multidimensional database approach. Other options like Data structures are secondary.
4572,Morphological analysis of the corpus of spontaneous Japanese,"This paper describes two methods for detecting word segments and their morphological information in a Japanese spontaneous speech corpus, and describes how to tag a large spontaneous speech corpus accurately by using the two methods. The first method is used to detect any type of word segments. The second method is used when there are several definitions for word segments and their POS categories, and when one type of word segments includes another type of word segments. In this paper, we show that by using semi-automatic analysis, we achieve a precision of better than 99% for detecting and tagging short-unit words and 97% for long-unit words; the two types of words that comprise the corpus. We also show that better accuracy is achieved by using both methods than by using only the first.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.8,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Information systems,"Information systems is relevant because the paper presents a method for morphological analysis of Japanese speech corpora, which is a data processing and information management task. Other categories are irrelevant as the work focuses on linguistic data analysis rather than software engineering or computing methodologies.","Data management systems:0.8,Information retrieval:0.7,Information storage systems:0.6,Information systems applications:0.5,World Wide Web:0.3","Data management systems,Information retrieval",Data management systems is relevant because the paper presents methods for corpus segmentation and tagging. Information retrieval is relevant as the techniques enable accurate tagging of speech data. Other categories like Information storage systems are secondary.,"Data structures:0.1,Database administration:0.1,Database design and models:0.2,Database management system engines:0.1,Document representation:1,Evaluation of retrieval results:0.3,Information integration:0.1,Information retrieval query processing:0.5,Middleware for databases:0.1,Query languages:0.1,Retrieval models and ranking:0.5,Retrieval tasks and goals:0.2,Search engine architectures and scalability:0.1,Specialized information retrieval:0.2,Users and interactive retrieval:0.2","Document representation,Information retrieval query processing,Retrieval models and ranking",Document representation: The paper focuses on representing and tagging Japanese text. Information retrieval query processing: The methods are used for corpus processing. Retrieval models and ranking: The model improves information retrieval performance. Other options like Database design are less relevant.
1920,Extracting Core Knowledge from Linked Data,"Recent research has shown the Linked Data cloud to be a potentially ideal basis for improving user experience when interacting with Web content across different applications and domains. Using the explicit knowledge of datasets, however, is neither sufficient nor straightforward. Dataset knowledge is often not uniformly organized, thus it is generally unknown how to query for it. To deal with these issues, we propose a dataset analysis approach based on knowledge patterns, and show how the recognition of patterns can support querying datasets even if their vocabularies are previously unknown. Finally, we discuss results from experimenting on three multimedia-related datasets.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:1.0,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Information systems,Information systems is highly relevant as the paper addresses data querying and knowledge extraction from linked datasets. Categories like Computing methodologies are secondary and do not capture the primary focus on information management.,"Data management systems:1.0,Information retrieval:0.5,Information storage systems:0.0,Information systems applications:0.3,World Wide Web:0.3","Data management systems,Information retrieval",Data management systems is highly relevant because the paper focuses on querying and organizing Linked Data. Information retrieval is relevant for the pattern-based querying approach. Other categories like World Wide Web are less directly tied to the core contribution of dataset analysis and querying.,"Database management system engines:1,Information retrieval query processing:1,Information integration:0.8,Query languages:0.7,Retrieval models and ranking:0.6,Retrieval tasks and goals:0.5,Specialized information retrieval:0.4,Document representation:0.3,Database design and models:0.3,Evaluation of retrieval results:0.2,Search engine architectures and scalability:0.2,Users and interactive retrieval:0.1","Database management system engines,Information retrieval query processing",Database management system engines is relevant as the paper addresses scalable querying of linked datasets. Information retrieval query processing is relevant due to the focus on pattern-based querying. Other categories like information integration or retrieval models are secondary as they are aspects of the solution rather than the primary domain.
3991,Functions of the database workbench,"A powerful database system can be developed by a combination of a central relational database system and intelligent terminals. In such an organization a typical function of a terminal is to offer high-level user interfaces. In this paper the concept of the database workbench is introduced and shown to be suitable for development by such terminals. As design problems usually require a large amount of interaction, typical functions of the workbench are (1) the design of database schemas, (2) the design of conversion procedures between real-world data and data in the system, and (3) the design of queries. For the first function we focus on the relational database design under the assumption that set values are permitted. Problems of set values, especially conversion problems of dependencies, are discussed. Various facilities for design conversion procedures and the design of queries are also discussed.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.2,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.9,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Information systems,Information systems is relevant for database schema design and query mechanisms. Other categories like Software and its engineering are less directly related as the focus is on database systems rather than software development processes.,"Data management systems:1.0,Information retrieval:0.0,Information storage systems:0.0,Information systems applications:0.3,World Wide Web:0.0",Data management systems,Data management systems: The paper introduces a database workbench for schema and query design. Information systems applications is weakly relevant as the workbench could support applications but is not the primary focus.,"Data structures:0,Database administration:0,Database design and models:1,Database management system engines:0,Information integration:0,Middleware for databases:0,Query languages:1","Database design and models,Query languages",Database design and models are central to the workbench's schema and conversion design. Query languages are relevant for the discussed query design functions. Other categories are secondary.
4441,A Comparison of the Two Traditions of Metadata Development,"Metadata has taken on a more significant role than ever before in the emerging digital library context because the effective organization of networked information clearly depends on the effective management and organization of metadata. The issue of metadata has been approached variously by different intellectual communities. The two main approaches may be characterized as: (1) the bibliographic control approach (origins and major proponents in library science); and (2) data management approach (origins and major proponents in computer science). This article examines the different conceptual foundations and orientations of the two major approaches contributing to the metadata discussion. An examination of the on-going efforts to establish metadata standards, and comparison of different metadata formats, supports a proposal for an integrated concept of metadata to facilitate the merging of the two approaches.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.9,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Information systems,"Information systems: The paper discusses metadata standards and their integration in digital libraries, which is a core topic in information systems and data management. Other categories like 'Social and professional topics' are irrelevant as the focus is on technical metadata standards rather than community or professional issues.","Data management systems:0.2,Information retrieval:0.2,Information storage systems:0.8,Information systems applications:0.3,World Wide Web:0.2",Information storage systems,Information storage systems (metadata management for digital libraries). Other categories irrelevant as paper focuses on metadata conceptual frameworks for information organization.,"Information storage technologies:1.0,Record storage systems:0.6,Storage architectures:0.3,Storage management:1.0,Storage replication:0.2","Information storage technologies,Storage management",Information storage technologies (1.0): Metadata management is central to digital libraries. Storage management (1.0): The paper addresses metadata integration and standardization. Record storage systems (0.6) and Storage architectures (0.3) are less directly relevant. Storage replication (0.2) is not discussed.
1443,SIRA: TREC Session Track 2014,"1. Abstract This paper discusses Siena’s Interactive Research Assistant’s (SIRA) participation in the Text Retrieval Conference (TREC) Session Track of 2014. The overall goal of this track is to improve search results during query sessions based on a user’s behavior. Query sessions include many aspects of a search, including query topics, initial retrieved webpages, clicked on links, visit times, etc. SIRA has used several methods to improve search results that will be discussed in this paper. Each method of query expansion utilized clicked-on and non-clicked-on links, pages with the longest visited time, and N-Percent (N%) of each page. Two of our three submissions improved over our baseline results and both of these were equal to the median submission for all participants in the track.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:1.0,Security and privacy:0.1,Human-centered computing:0.3,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Information systems,"Information systems: The paper addresses query session tracking for improved search relevance, a core information retrieval problem. Human-centered computing is tangential to the technical system evaluation.","Data management systems:0.2,Information retrieval:1.0,Information storage systems:0.2,Information systems applications:0.5,World Wide Web:0.3",Information retrieval,Information retrieval is directly relevant as the paper focuses on improving search results during query sessions using techniques like query expansion and user behavior analysis. Other categories like Data management systems or Information storage systems are not the primary focus.,"Document representation:0.5,Evaluation of retrieval results:0,Information retrieval query processing:1,Retrieval models and ranking:0.5,Retrieval tasks and goals:0,Search engine architectures and scalability:0,Specialized information retrieval:0,Users and interactive retrieval:1","Users and interactive retrieval,Information retrieval query processing","Users and interactive retrieval: The paper focuses on improving search during sessions using user behavior. Information retrieval query processing: Query expansion using clicked links and N% of pages is a core technique. Retrieval models was rejected as the focus is on session dynamics, not ranking models."
5505,TrendFashion - A Framework for the Identification of Fashion Trends,"The fashion industry faces different challenges regarding accurate forecasts for future fashion products. The consumer demand is volatile and sales periods of fashion products are short due to production plants in Asia and target markets in Europe. Besides standard statistical approaches based on historical data and advanced methods such as the application of artificial neural networks or fuzzy logic, there are fashion experts, who use different information sources, e.g. fairs, social media, fashion websites, to predict design-trends as well as sales volumes. In this paper we follow this expert-driven approach by collecting data from fashion weblogs, news sites and fashion magazines, in order to identify actual and future designtrends. For this aim, we develop the TrendFashion Tool which collects data from these fashion sources and analyse them. On a higher level, this tool successfully separates fashion related posts from non-fashion related posts. And on a lower level, it identifies fashion related words and weights them according to an index.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:1.0,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Information systems,Information systems is highly relevant as the paper focuses on data collection and analysis for fashion trend identification. Other categories like Software engineering or Human-centered computing are secondary to the primary focus on information processing.,"Data management systems:0.7,Information retrieval:0.9,Information storage systems:0.5,Information systems applications:0.6,World Wide Web:0.8","Information retrieval,World Wide Web","Information retrieval (0.9) - The paper's core contribution is developing a tool to identify fashion trends from web content through data analysis. World Wide Web (0.8) - The system collects and analyzes data from weblogs, news sites, and fashion magazines. Other children were rejected because the focus is on information analysis (0.9) rather than storage systems (0.5) or general information systems (0.6).","Document representation:0.3,Evaluation of retrieval results:0.2,Information retrieval query processing:1.0,Online advertising:0.0,Retrieval models and ranking:0.7,Retrieval tasks and goals:0.6,Search engine architectures and scalability:0.4,Specialized information retrieval:1.0,Users and interactive retrieval:0.3,Web applications:0.2,Web data description languages:0.1,Web interfaces:0.1,Web mining:1.0,Web searching and information discovery:0.8,Web services:0.1","Information retrieval query processing,Web mining,Specialized information retrieval",Information retrieval query processing: The paper's core contribution is a system for fashion trend identification through web content analysis. Web mining: The tool collects and analyzes data from fashion weblogs/magazines. Specialized information retrieval: The system is tailored for fashion trend extraction. Other children like Web searching (0.8) are secondary but still relevant.
4145,Measure-driven Keyword-Query Expansion,"User generated content has been fueling an explosion in the amount of available textual data. In this context, it is also common for users to express, either explicitly (through numerical ratings) or implicitly, their views and opinions on products, events, etc. This wealth of textual information necessitates the development of novel searching and data exploration paradigms. 
 
In this paper we propose a new searching model, similar in spirit to faceted search, that enables the progressive refinement of a keyword-query result. However, in contrast to faceted search which utilizes domain-specific and hard-to-extract document attributes, the refinement process is driven by suggesting interesting expansions of the original query with additional search terms. Our query-driven and domain-neutral approach employs surprising word co-occurrence patterns and (optionally) numerical user ratings in order to identify meaningful top-k query expansions and allow one to focus on a particularly interesting subset of the original result set. 
 
The proposed functionality is supported by a framework that is computationally efficient and nimble in terms of storage requirements. Our solution is grounded on Convex Optimization principles that allow us to exploit the pruning opportunities offered by the natural top-k formulation of our problem. The performance benefits offered by our solution are verified using both synthetic data and large real data sets comprised of blog posts.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.2,Mathematics of computing:0.2,Information systems:0.9,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.4,Applied computing:0.3,Social and professional topics:0.1",Information systems,Information systems is directly relevant as the paper focuses on query expansion for data exploration. Computing methodologies is secondary for the algorithmic approach. Other categories like Applied computing are not central to the core contribution.,"Data management systems:0.75,Information retrieval:1.0,Information storage systems:0.0,Information systems applications:0.25,World Wide Web:0.0",Information retrieval,Information retrieval is highly relevant for the query expansion framework. Data management systems receives a moderate score for potential data storage implications.,"Document representation:0,Evaluation of retrieval results:0,Information retrieval query processing:1,Retrieval models and ranking:1,Retrieval tasks and goals:0.5,Search engine architectures and scalability:0.5,Specialized information retrieval:0,Users and interactive retrieval:0.5","Information retrieval query processing,Retrieval models and ranking",Information retrieval query processing is central to the proposed query expansion mechanism. Retrieval models and ranking are relevant due to the use of co-occurrence patterns for query refinement. Other categories like search engine scalability are secondary due to efficiency claims.
2623,Efficient phrase querying with an auxiliary index,"Search engines need to evaluate queries extremely fast, a challenging task given the vast quantities of data being indexed. A significant proportion of the queries posed to search engines involve phrases. In this paper we consider how phrase queries can be efficiently supported with low disk overheads. Previous research has shown that phrase queries can be rapidly evaluated using nextword indexes, but these indexes are twice as large as conventional inverted files. We propose a combination of nextword indexes with inverted files as a solution to this problem. Our experiments show that combined use of an auxiliary nextword index and a conventional inverted file allow evaluation of phrase queries in half the time required to evaluate such queries with an inverted file alone, and the space overhead is only 10% of the size of the inverted file. Further time savings are available with only slight increases in disk requirements.","General and reference:0,Hardware:0,Computer systems organization:0.1,Networks:0,Software and its engineering:0.2,Theory of computation:0,Mathematics of computing:0.1,Information systems:1,Security and privacy:0,Human-centered computing:0,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0",Information systems,Information systems is highly relevant as the paper focuses on optimizing search engines for phrase queries using data indexing. Other categories like Computer systems organization or Software engineering are less directly tied to the core contribution of information retrieval methods.,"Data management systems:0.2,Information retrieval:0.9,Information storage systems:0.3,Information systems applications:0.4,World Wide Web:0.5",Information retrieval,Information retrieval: The paper proposes methods to optimize phrase query evaluation in search engines. Data management systems and Information storage systems are less relevant as the focus is on query processing rather than data storage. World Wide Web is contextually related but not the core contribution.,"Document representation:0.2,Evaluation of retrieval results:0.1,Information retrieval query processing:1.0,Retrieval models and ranking:0.3,Retrieval tasks and goals:0.2,Search engine architectures and scalability:0.8,Specialized information retrieval:0.1,Users and interactive retrieval:0.1","Information retrieval query processing,Search engine architectures and scalability",Information retrieval query processing: The paper focuses on optimizing phrase query evaluation methods. Search engine architectures and scalability: The study addresses disk overheads and scalability in search systems. Other categories like 'Document representation' are less relevant as the paper focuses on query execution rather than document modeling.
3695,Weight Adjusment for Multi-criteria Ratings in Items Recommendation,"In this paper we propose to use implicit ratings of multiple criteria to mitigate the data sparsity problem. The intuition is to predict the overall relevance of an item for a given user, based on her/his own implicit feedback instead of using similar users ratings (commonly used in collaborative filtering). Furthermore, since we believe one criterion may be more important than others, we propose a weighting schema, in which we estimate how interesting is each criterion for a given user, in order to generate a personalized ranking. The weighting schema do not suppose the generation of predicted explicit ratings. Instead, we reorganize the weights in such a way that just the criterion that has rating are weighted. For predicting the weight of each criterion to each user, we propose a genetic programming to predict how interesting is each criterion for a user, in which the initial weight values are randomly generated. In our experiments, we show that when having a sufficient corpus of historical user implicit feedback we can obtain higher precision for ranking items to a user, considering a predicted set of weight.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.8,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.3,Applied computing:0.2,Social and professional topics:0.1",Information systems,"Information systems is highly relevant as the paper addresses recommendation systems and multi-criteria ratings, which are core to information systems research. Computing methodologies and applied computing receive moderate scores for algorithmic contributions, but the primary focus remains information systems.","Data management systems:0.4,Information retrieval:0.7,Information storage systems:0.2,Information systems applications:1.0,World Wide Web:0.3","Information systems applications,Information retrieval",Information systems applications and Information retrieval are highly relevant as the paper addresses personalized ranking and data sparsity in recommendation systems. Data management systems receives a lower score due to limited focus on storage mechanisms.,"Collaborative and social computing systems and tools:0,Computational advertising:0,Computing platforms:0,Data mining:1,Decision support systems:0,Digital libraries and archives:0,Document representation:0,Enterprise information systems:0,Evaluation of retrieval results:0,Information retrieval query processing:0,Mobile information processing systems:0,Multimedia information systems:0,Process control systems:0,Retrieval models and ranking:1,Retrieval tasks and goals:0,Search engine architectures and scalability:0,Spatial-temporal systems:0,Specialized information retrieval:0,Users and interactive retrieval:0","Data mining,Retrieval models and ranking",Data mining: The paper uses implicit feedback for recommendation systems. Retrieval models and ranking: The weighting schema affects item ranking. Other categories are irrelevant to the core methods.
6023,Uncloaking Terrorist Networks,"This paper looks at mapping covert networks using data available from news sources on the World Wide Web. Specifically, we examine the network surrounding the tragic events of September 11th 2001. Through public data we are able to map a portion of the network centered on the 19 dead hijackers. This map gives us some insight into the terrorist organization, yet it is incomplete. Suggestions for further work and research are offered.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.8,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Information systems,Information systems is relevant as the paper discusses data mapping and network analysis using public data. Other categories like 'Applied computing' are less directly related to the focus on information network modeling.,"Data management systems:0.0,Information retrieval:1.0,Information storage systems:0.0,Information systems applications:0.0,World Wide Web:0.0",Information retrieval,Information retrieval is highly relevant as the paper focuses on extracting and mapping networks from publicly available news data. Other categories like Data management systems are less directly related to the core task of data extraction and network mapping.,"Document representation:0.2,Evaluation of retrieval results:0,Information retrieval query processing:0.3,Retrieval models and ranking:0.4,Retrieval tasks and goals:1,Search engine architectures and scalability:0.1,Specialized information retrieval:1,Users and interactive retrieval:0","Retrieval tasks and goals,Specialized information retrieval","Retrieval tasks and goals is relevant because the paper addresses mapping networks from public data, a specialized retrieval task. Specialized information retrieval is directly relevant due to the focus on mapping covert networks. Other categories like document representation or query processing are less central to the paper's contributions."
2290,Free-text medical document retrieval via phrase-based vector space model,"Many information retrieval systems are based on vector space model (VSM) that represents a document as a vector of index terms. Concepts have been proposed to replace word stems as the index terms to improve retrieval accuracy. However, past research revealed that such systems did not outperform the traditional stem-based systems. Incorporating conceptual similarity derived from knowledge sources should have the potential to improve retrieval accuracy. Yet the incompleteness of the knowledge source precludes significant improvement. To remedy this problem, we propose to represent documents using phrases. A phrase consists of multiple concepts and word stems. The similarity between two phrases is jointly determined by their conceptual similarity and their common word stems. The document similarity can in turn be derived from phrase similarities. Using OHSUMED as a test collection and UMLS as the knowledge source, our experiment results reveal that phrase-based VSM yields a 16% increase of retrieval accuracy compared to the stem-based model.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.9,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Information systems,Information systems is directly relevant because the paper focuses on medical document retrieval systems and information retrieval techniques. Other categories like Hardware or Software engineering are not central to the core contribution of improving document retrieval accuracy through phrase-based models.,"Information retrieval:1.0,Data management systems:0.75,Information storage systems:0.2,World Wide Web:0.2,Information systems applications:0.2","Information retrieval,Data management systems",Information retrieval is highly relevant for the phrase-based document retrieval method. Data management systems is relevant for the storage and processing aspects. Other categories like WWW are not central to the technical focus.,"Data structures:0.3,Database administration:0.2,Database design and models:0.4,Database management system engines:0.3,Document representation:0.9,Evaluation of retrieval results:0.8,Information integration:0.5,Information retrieval query processing:0.6,Middleware for databases:0.1,Query languages:0.2,Retrieval models and ranking:1.0,Retrieval tasks and goals:0.7,Search engine architectures and scalability:0.3,Specialized information retrieval:0.9,Users and interactive retrieval:0.4","Retrieval models and ranking,Specialized information retrieval",Retrieval models and ranking are central to the phrase-based VSM approach. Specialized information retrieval is relevant for medical document retrieval. Document representation is secondary but included.
215,Modelling and Managing Time in Database Systems,"This paper describes a new architecture for a system to handle temporal data, called Temporal Data Management System, TDMS. TDMS represents an approach to the modelling of temporal semantics which is pragmatic because it takes into account practical issues such as the time /space trade-offs. We consider the technical aspects of the system i.e. how time domain data is managed in TDMS. The design of temporal databases is not addressed here. We introduce a conceptual framework which permits issues such as handling schema anomalies due to updates, query validation and correct semantics modelling to be handled in a consistent and usable manner. It incorporates the time values of each of three 'data constructs' organised in a hierarchy. This is combined at the implementation level with a well-established multi-database architecture to give a comprehensive and efficient temporal model which overcomes several weaknesses of current temporal models. Modest extensions to the SQL syntax to handle time are proposed and have been implemented. Both tuple-stamping and value-stamping are provided by the integration of TDMS and the relational data model. TDMS has been prototyped under the UNIX operating system using the C programming language. Presently, TDMS interfaces with the Ingres DBMS. Received October 1989, revised March 1992","General and reference:0.1,Hardware:0.1,Computer systems organization:0.2,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.3,Information systems:0.8,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.3,Social and professional topics:0.1",Information systems,Information systems is highly relevant as the paper focuses on temporal data management systems and their implementation. Applied computing (0.3) and Mathematics of computing (0.3) are secondary due to practical and mathematical modeling aspects. Other categories like Software engineering or Hardware are not central to the work.,"Data management systems:0.9,Information retrieval:0.1,Information storage systems:0.2,Information systems applications:0.1,World Wide Web:0.1",Data management systems,Data management systems is highly relevant as the paper describes a temporal data management system (TDMS) for handling temporal data. Other categories like information retrieval or web are not directly related to the core contribution of temporal database architecture.,"Database management system engines:1,Query languages:1,Data structures:0.4,Database administration:0.3,Database design and models:0.5,Information integration:0.2,Middleware for databases:0.3","Database management system engines,Query languages",Database management system engines: TDMS is a temporal data management system with a focus on engine design. Query languages: The paper introduces SQL syntax extensions for temporal queries. Other categories like Database administration are not the primary focus of the paper.
3833,Topic-Aware Social Sensing with Arbitrary Source Dependency Graphs,"This work is motivated by the emergence of social sensing as a new paradigm of collecting observations about the physical environment from humans or devices on their behalf. These observations may be true or false, and hence are viewed as binary claims. A fundamental problem in social sensing applications lies in ascertaining the correctness of claims and the reliability of data sources without knowing either of them a priori. We refer to this problem as truth discovery. Prior works have made significant progress to addressing the truth discovery problem, but two significant limitations exist: (i) they ignored the fact that claims reported in social sensing applications can be either relevant or irrelevant to the topic of interests. (ii) They either assumed the data sources to be independent or the source dependency graphs can be represented as a set of disjoint trees. These limitations led to suboptimal truth discovery results. In contrast, this paper presents the first social sensing framework that explicitly incorporates the topic relevance feature of claims and arbitrary source dependency graphs into the solutions of truth discovery problem. The new framework solves a multidimensional maximum likelihood estimation problem to jointly estimate the truthfulness and topic relevance of claims as well as the reliability and topic awareness of sources. We compared our new scheme with the state-of-the-art truth discovery solutions using three real world data traces collected from Twitter in the aftermath of Paris Shooting event (2015), Hurricane Arthur (2014) and Boston Bombing event (2013) respectively. The evaluation results showed that our schemes significantly outperform the compared baselines by identifying more relevant and truthful claims in the truth discovery results.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.9,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Information systems,Information systems is highly relevant due to the paper's focus on social sensing frameworks and data management across legal domains. Other categories lack direct connection to the data analysis and system design context.,"Data management systems:1.0,Information retrieval:0.8,Information storage systems:0.2,Information systems applications:0.6,World Wide Web:0.3","Data management systems,Information retrieval",Data management systems is primary as the paper focuses on truth discovery in social sensing data. Information retrieval is relevant due to the focus on extracting truthful claims. Storage systems and WWW are less central.,"Data structures:0.2,Database administration:0.8,Database design and models:0.7,Database management system engines:0.3,Document representation:0.1,Evaluation of retrieval results:0.7,Information integration:0.9,Information retrieval query processing:0.4,Middleware for databases:0.3,Query languages:0.2,Retrieval models and ranking:0.4,Retrieval tasks and goals:0.3,Search engine architectures and scalability:0.2,Specialized information retrieval:0.3,Users and interactive retrieval:0.2","Information integration,Database administration,Evaluation of retrieval results",Information integration is relevant due to the paper's focus on combining topic relevance and source dependencies for truth discovery. Database administration applies to managing social sensing data. Evaluation of retrieval results is relevant for assessing truth discovery effectiveness. Other categories like Data structures or Query languages are less central to the core contribution.
4350,A Path-based Relational RDF Database,"We propose a path-based scheme for storage and retrieval of RDF data using a relational database. The Semantic Web is much anticipated as the next-generation web where high-level processing of web resources are enabled by underlying metadata described in RDF format. A typical application of RDF is to describe ontologies or dictionaries, but in such applications, the size of RDF data is large. As large-size RDF data are emerging and their number is increasing, RDF databases that can manage large-size RDF data are becoming ever more important. To date, some RDF databases have already been proposed; however, they have critical problems: the performance of path queries is insufficient and they cannot discriminate between schema data and instance data. In this paper, as a solution to these problems, we propose a path-based relation RDF database. In our approach, we first divide the RDF graph into subgraphs, and then store each subgraph by applicable techniques into distinct relational tables. More precisely, all classes and properties are extracted from RDF schema data, and all resources are also extracted from RDF data. Each is assigned an identifier and a path expression, and stored in corresponding relational table. Because our proposed scheme retains schema information and path expressions of each resource, unlike most conventional RDF databases, it is possible to process path-based queries efficiently and store RDF instance data without schema information. The effectiveness of this approach is demonstrated through several experiments.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.9,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.3,Applied computing:0.1,Social and professional topics:0.1",Information systems,Information systems is highly relevant for the RDF database design and path-based query optimization. Other categories like Networks are not directly connected to the core database management focus.,"Data management systems:0.85,Information retrieval:0.3,Information storage systems:0.6,Information systems applications:0.4,World Wide Web:0.3",Data management systems,Data management systems is relevant for the proposed RDF storage and retrieval scheme. Information storage systems is partially relevant but less central than the core contribution in database design. Other categories are less directly related to the paper's focus on RDF data management.,"Data structures:1,Database administration:0,Database design and models:1,Database management system engines:1,Information integration:0,Middleware for databases:0,Query languages:0","Database design and models,Database management system engines,Data structures",Database design and models are relevant to the path-based relational approach for RDF. Database management system engines apply to the storage and retrieval implementation. Data structures are relevant for subgraph partitioning. Other categories like query languages are not the primary focus.
2141,Predictive ranking of computer scientists using CiteSeer data,"The increasing availability of digital libraries with cross‐citation data on the Internet enables new studies in bibliometrics. The paper focuses on the list of 10,000 top‐cited authors in computer science available as part of CiteSeer. Using data from several consecutive lists a model of how authors accrue citations with time is constructed. By comparing the rate at which individual authors accrue citations with the average rate, predictions are made of how their ranking in the list will change in the future.","General and reference:0.1,Hardware:0.0,Computer systems organization:0.1,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.2,Information systems:1.0,Security and privacy:0.3,Human-centered computing:0.0,Computing methodologies:0.5,Applied computing:0.4,Social and professional topics:0.0",Information systems,"Information systems is highly relevant because the paper focuses on modeling citation data and predicting author rankings using bibliometric analysis. Other categories like Computing methodologies or Applied computing received moderate scores due to indirect relevance in data analysis techniques, but the primary domain is information systems.","Data management systems:0.8,Information retrieval:0.9,Information storage systems:0.5,Information systems applications:0.6,World Wide Web:0.0","Information retrieval,Data management systems","Information retrieval (citation data analysis for ranking), Data management systems (CiteSeer data infrastructure). World Wide Web is irrelevant as the study doesn't focus on web technologies.","Data structures:0,Database administration:0,Database design and models:0.2,Database management system engines:0,Document representation:0,Evaluation of retrieval results:0.5,Information integration:0.3,Information retrieval query processing:0,Middleware for databases:0,Query languages:0,Retrieval models and ranking:1,Retrieval tasks and goals:0.4,Search engine architectures and scalability:0.3,Specialized information retrieval:0.2,Users and interactive retrieval:0.2",Retrieval models and ranking,Retrieval models and ranking are primary as the paper builds a predictive ranking model for authors. Evaluation of retrieval results is secondary as they assess prediction accuracy.
1490,Flexible and efficient XML search with complex full-text predicates,"Recently, there has been extensive research that generated a wealth of new XML full-text query languages, ranging from simple Boolean search to combining sophisticated proximity and order predicates on keywords. While computing least common ancestors of query terms was proposed for efficient evaluation of conjunctive keyword queries by exploiting the document structure, no such solution was developed to evaluate complex full-text queries. We present efficient evaluation algorithms based on a formalization of XML queries in terms of keyword patterns and an algebra which manipulates pattern matches. Our algebra captures most existing languages and their varying semantics and our algorithms combine relational query evaluation techniques with the exploitation of document structure to process queries with complex full-text predicates. We show how scoring can be incorporated into our framework without compromising the algorithms complexity. Our experiments show that considering element nesting dramatically improves the performance of queries with complex full-text predicates.","General and reference:0.1,Hardware:0.05,Computer systems organization:0.05,Networks:0.1,Software and its engineering:0.3,Theory of computation:0.2,Mathematics of computing:0.1,Information systems:0.9,Security and privacy:0.1,Human-centered computing:0.2,Computing methodologies:0.6,Applied computing:0.2,Social and professional topics:0.1",Information systems,"Information systems is highly relevant as the paper centers on XML full-text query evaluation and data retrieval. Computing methodologies is moderately relevant for algorithm design, but the primary focus is on information systems. Other categories like Software and its engineering are less central to the core contribution.","Data management systems:0.9,Information retrieval:0.8,Information storage systems:0.7,Information systems applications:0.3,World Wide Web:0.2","Data management systems,Information retrieval,Information storage systems",Data management systems is relevant due to XML data handling and query algorithms. Information retrieval is relevant for full-text query processing. Information storage systems is relevant for nested element performance. Other categories are less relevant as the focus is on query evaluation rather than web or application systems.,"Data structures:0.7,Database administration:0.3,Database design and models:0.9,Database management system engines:0.4,Document representation:0.8,Evaluation of retrieval results:0.4,Information integration:0.3,Information retrieval query processing:1.0,Information storage technologies:0.5,Middleware for databases:0.4,Query languages:0.9,Record storage systems:0.3,Retrieval models and ranking:0.6,Retrieval tasks and goals:0.5,Search engine architectures and scalability:0.4,Specialized information retrieval:0.5,Storage architectures:0.3,Storage management:0.3,Storage replication:0.2,Users and interactive retrieval:0.4","Database design and models,Information retrieval query processing,Query languages",Database design and models: XML structure and querying. Information retrieval query processing: Full-text query evaluation. Query languages: XML query language design. Other options: Storage aspects secondary.
3587,Merging DAML+OIL Bio-ontologies,Ontologies are being used nowadays in many areas. Within the bioinformatics area there are a number of bio-ontologies that cover different aspects in molecular biology. Many of these ontologies con ...,"General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:1.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Information systems,"Information systems is directly relevant because the paper focuses on merging bio-ontologies, a core challenge in semantic data integration.","Data management systems:1,Information retrieval:0.4,Information storage systems:0.3,Information systems applications:0.5,World Wide Web:0.2",Data management systems,Data management systems is relevant as the paper focuses on ontology merging for bioinformatics. Other categories are less directly connected to the core contribution of ontology integration.,"Data structures:0,Database administration:0,Database design and models:0.5,Database management system engines:0,Information integration:1,Middleware for databases:0,Query languages:0.5","Information integration,Database design and models","Information integration is relevant as the paper addresses merging bio-ontologies. Database design and models is moderately relevant if the paper discusses ontology structure. Other categories (e.g., query languages) are irrelevant as the focus is on integration rather than querying."
3551,After the search is over ... the work begins,"Even though much research work has been done on the low-level feature-extraction and matching tasks of music information retrieval (MIR), relatively little effort goes into matching music at a higher ‘semantic’ level. Finding appropriate semantic levels for particular musical tasks is an open research field, one that will become increasingly important as low-level retrieval tools become more sophisticated and efficient, as they show every sign of doing, largely thanks to the ISMIR conference and its companion MIREX contest.","General and reference:0.1,Hardware:0.2,Computer systems organization:0.3,Networks:0.1,Software and its engineering:0.2,Theory of computation:0.3,Mathematics of computing:0.4,Information systems:1.0,Security and privacy:0.1,Human-centered computing:0.5,Computing methodologies:0.3,Applied computing:0.2,Social and professional topics:0.1",Information systems,Information systems is highly relevant as the paper discusses music information retrieval and semantic levels. Human-centered computing is partially relevant but secondary to the information systems focus. Other categories like Theory are irrelevant to the discussion.,"Data management systems:0.0,Information retrieval:0.9,Information storage systems:0.0,Information systems applications:0.0,World Wide Web:0.0",Information retrieval,Information retrieval is relevant for the semantic-level music matching research. Other categories like Data management systems were rejected as the paper focuses on retrieval methodologies rather than storage systems.,"Document representation:0.0,Evaluation of retrieval results:0.2,Information retrieval query processing:0.0,Retrieval models and ranking:1.0,Retrieval tasks and goals:0.0,Search engine architectures and scalability:0.0,Specialized information retrieval:1.0,Users and interactive retrieval:0.0","Retrieval models and ranking,Specialized information retrieval",Retrieval models and ranking is relevant as the paper discusses semantic level modeling in MIR. Specialized information retrieval is relevant due to the focus on music-specific retrieval challenges.
1477,Dynamic ranked retrieval,"We present a theoretically well-founded retrieval model for dynamically generating rankings based on interactive user feedback. Unlike conventional rankings that remain static after the query was issued, dynamic rankings allow and anticipate user activity, thus providing a way to combine the otherwise contradictory goals of result diversification and high recall. We develop a decision-theoretic framework to guide the design and evaluation of algorithms for this interactive retrieval setting. Furthermore, we propose two dynamic ranking algorithms, both of which are computationally efficient. We prove that these algorithms provide retrieval performance that is guaranteed to be at least as good as the optimal static ranking algorithm. In empirical evaluations, dynamic ranking shows substantial improvements in retrieval performance over conventional static rankings.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.8,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Information systems,Information systems is highly relevant because the paper presents a dynamic ranking model for interactive retrieval systems. Other fields are irrelevant as the focus is on information retrieval algorithms.,"Data management systems:0.25,Information retrieval:1.0,Information storage systems:0.25,Information systems applications:0.25,World Wide Web:0.25",Information retrieval,"Information retrieval (1.0): The paper introduces dynamic ranking algorithms for interactive retrieval. Other categories are irrelevant as the work focuses on retrieval systems rather than data storage, Web applications, or general information systems.","Document representation:0.1,Evaluation of retrieval results:0.3,Information retrieval query processing:0.2,Retrieval models and ranking:0.8,Retrieval tasks and goals:0.3,Search engine architectures and scalability:0.2,Specialized information retrieval:0.2,Users and interactive retrieval:0.7","Retrieval models and ranking,Users and interactive retrieval",Retrieval models and ranking are central to the dynamic ranking framework. Users and interactive retrieval are relevant due to user feedback integration. Other categories like Query processing are less directly related to the core contribution.
4449,deepschema.org: An Ontology for Typing Entities in the Web of Data,"Discovering the appropriate type of an entity in the Web of Data is still considered an open challenge, given the complexity of the many tasks it entails. Among them, the most notable is the definition of a generic and cross-domain ontology. While the ontologies proposed in the past function mostly as schemata for knowledge bases of different sizes, an ontology for entity typing requires a rich, accurate and easily-traversable type hierarchy. Likewise, it is desirable that the hierarchy contains thousands of nodes and multiple levels, contrary to what a manually curated ontology can offer. Such level of detail is required to describe all the possible environments in which an entity exists in. Furthermore, the generation of the ontology must follow an automated fashion, combining the most widely used data sources and following the speed of the Web. In this paper we propose deepschema.org, the first ontology that combines two well-known ontological resources, Wikidata and schema.org, to obtain a highly-accurate, generic type ontology which is at the same time a first-class citizen in the Web of Data. We describe the automated procedure we used for extracting a class hierarchy from Wikidata and analyze the main characteristics of this hierarchy. We also provide a novel technique for integrating the extracted hierarchy with schema.org, which exploits external dictionary corpora and is based on word embeddings. Finally, we present a crowdsourcing evaluation which showcases the three main aspects of our ontology, namely the accuracy, the traversability and the genericity. The outcome of this paper is published under the portal: http://deepschema.github.io.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.3,Theory of computation:0.4,Mathematics of computing:0.2,Information systems:0.9,Security and privacy:0.1,Human-centered computing:0.3,Computing methodologies:0.5,Applied computing:0.4,Social and professional topics:0.1",Information systems,"Information systems: The paper centers on ontology design for the Web of Data, a core area in information systems. Other categories like Computing methodologies are secondary as the focus is on the system rather than the methods.","Data management systems:0.8,World Wide Web:0.75,Information systems applications:0.7,others:0.05","Data management systems,World Wide Web,Information systems applications",Data management systems is relevant for ontology structure. World Wide Web is relevant for Web of Data integration. Information systems applications is relevant for entity typing. Other categories are less central.,"Collaborative and social computing systems and tools:0.1,Computational advertising:0.1,Computing platforms:0.1,Data mining:0.3,Data structures:0.2,Database administration:0.1,Database design and models:1.0,Database management system engines:0.1,Decision support systems:0.2,Digital libraries and archives:0.3,Enterprise information systems:0.2,Information integration:1.0,Middleware for databases:0.1,Mobile information processing systems:0.1,Multimedia information systems:0.1,Online advertising:0.1,Process control systems:0.1,Query languages:0.2,Spatial-temporal systems:0.1,Web applications:0.3,Web data description languages:1.0,Web interfaces:0.2,Web mining:0.3,Web searching and information discovery:0.2,Web services:0.1","Database design and models,Information integration,Web data description languages",Database design and models are central to the ontology creation. Information integration is key for combining Wikidata and schema.org. Web data description languages apply to the ontology's role in the Web of Data. Other categories like Data mining or Web applications are secondary.
234,A method for ontologies structures and contents comparison,"Growth of an internet information source makes harder to user effective finding relevant information. Search engines and category hierarchical systems deal mainly with human purpose. But if there are the machines for integration or for adaptation used, then some meta-data information (data about data) have to be added to the processed data. There are some possibilities to store the meta-data information - the very effective one is an ontology representation.
 The ontology (e.g. described in OWL) at least provides a hierarchical concept structure and restrictions for its instances. Ontologies can be integrated (mapped, merged) to a one complex ontology. This integration of ontologies brings many advantages. On the other hand, the integration brings many problems in case when input ontologies are not fully compatible. We want to find conditions for semi-automatic or fully automatic ontology integration.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.0,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.9,Security and privacy:0.0,Human-centered computing:0.2,Computing methodologies:0.3,Applied computing:0.2,Social and professional topics:0.1",Information systems,"Information systems is highly relevant as the paper focuses on ontology-based information organization. Computing methodologies (0.3) receives a moderate score for the algorithmic approach to ontology integration, but the core contribution is in information organization.","Data management systems:1.0,Information retrieval:0.6,Information storage systems:0.5,Information systems applications:0.4,World Wide Web:0.3",Data management systems,Data management systems is highly relevant as the paper addresses ontology integration for data organization. Other categories like Information retrieval are less central since the focus is on data structure integration rather than retrieval algorithms.,"Data structures:0,Database administration:0,Database design and models:0,Database management system engines:0,Information integration:1,Middleware for databases:0,Query languages:0",Information integration,Information integration is directly relevant as the paper focuses on ontology integration challenges and methods. Other database-related categories are less relevant as the focus is on semantic integration rather than database systems.
1787,An Architecture Framework for Complex Data Warehouses,"Nowadays, many decision support applications need to exploit data that are not only numerical or symbolic, but also multimedia, multistructure, multisource, multimodal, and/or multiversion. We term such data complex data. Managing and analyzing complex data involves a lot of different issues regarding their structure, storage and processing, and metadata are a key element in all these processes. Such problems have been addressed by classical data warehousing (i.e., applied to “simple” data). However, data warehousing approaches need to be adapted for complex data. In this paper, we first propose a precise, though open, definition of complex data. Then we present a general architecture framework for warehousing complex data. This architecture heavily relies on metadata and domain-related knowledge, and rests on the XML language, which helps storing data, metadata and domain-specific knowledge altogether, and facilitates communication between the various warehousing processes.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:1.0,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Information systems,Information systems is highly relevant as the paper proposes a data warehouse architecture for complex data. Other categories lack direct connection to data management systems.,"Data management systems:1,Information storage systems:0.8,Information retrieval:0.5,Information systems applications:0.6,World Wide Web:0.3","Data management systems,Information storage systems",Data management systems is core to the architecture for complex data. Information storage systems applies to XML-based storage. Other categories are less aligned with the framework's focus on metadata and storage.,"Data structures:0,Database administration:0,Database design and models:1,Database management system engines:0.5,Information integration:0.5,Information storage technologies:0.5,Middleware for databases:0,Query languages:0,Record storage systems:0,Storage architectures:0.5,Storage management:0.5,Storage replication:0","Database design and models,Information storage technologies","Database design and models: The paper proposes a framework for complex data warehouses, focusing on architectural design. Information storage technologies: XML-based storage and metadata management are central to the approach. Other fields like Database administration are not the primary focus."
3120,Relevance of ASR for the Automatic Generation of Keywords Suggestions for TV programs,"Semantic access to multimedia content in audiovisual archives is to a large extent dependent on quantity and quality of the metadata, and particularly the content descriptions that are attached to the individual items. However, the manual annotation of collections puts heavy demands on resources. A large number of archives are introducing (semi) automatic annotation techniques for generating and/or enhancing metadata. The NWO funded CATCH-CHOICE project has investigated the extraction of keywords from textual resources related to TV programs to be archived (context documents), in collaboration with the Dutch audiovisual archives, Sound and Vision. This paper investigates the suitability of Automatic Speech Recognition transcripts produced in the CATCH-CHoral project for generating such keywords, which we evaluate against manual annotations of the documents, and against keywords automatically generated from context documents describing the TV programs’ content.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.95,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Information systems,Information systems is highly relevant for metadata generation and semantic access to multimedia content. Other categories like Computing methodologies are less relevant as the focus is on application-specific annotation techniques rather than general methods.,"Data management systems:0.0,Information retrieval:0.85,Information storage systems:0.0,Information systems applications:0.1,World Wide Web:0.0",Information retrieval,"Information retrieval is highly relevant because the paper evaluates keyword generation from ASR transcripts. Information systems applications is marginally relevant as it relates to metadata generation, but the core focus is retrieval techniques.","Retrieval tasks and goals:1,Specialized information retrieval:0.8,Document representation:0.6,Evaluation of retrieval results:0.5,Information retrieval query processing:0.4,Retrieval models and ranking:0.3,Search engine architectures and scalability:0.2,Users and interactive retrieval:0.1","Retrieval tasks and goals,Specialized information retrieval",Retrieval tasks and goals is relevant as the paper focuses on keyword generation for TV programs. Specialized information retrieval is relevant due to the domain-specific application. Other categories like document representation are secondary but still applicable.
4035,A fast query by humming system based on notes,"Query by humming (QBH), a content-based retrieval method, is an efficient way to search the song from a large database. The frame-based systems can achieve a good performance, but it is time-consuming. In this paper, we proposed an efficient note-based system, which is mainly comprised of noted-based linear scaling (NLS) and noted-based recursive align (NRA). The system after post-processing can achieve 96.1% in Top5 and 0.211s in time.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.8,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Information systems,Information systems: The paper presents a query-by-humming system for music retrieval. Other categories like Computing Methodologies are secondary as the focus is on content-based retrieval systems.,"Data management systems:0.2,Information retrieval:1.0,Information storage systems:0.2,Information systems applications:0.3,World Wide Web:0.2",Information retrieval,"Information retrieval is directly relevant as the paper focuses on query-by-humming, a content-based retrieval method. Other categories like data management or web systems are not central to the paper's core contribution of improving retrieval efficiency.","Document representation:0.0,Evaluation of retrieval results:0.0,Information retrieval query processing:1.0,Retrieval models and ranking:1.0,Retrieval tasks and goals:0.0,Search engine architectures and scalability:0.0,Specialized information retrieval:0.0,Users and interactive retrieval:0.0","Information retrieval query processing,Retrieval models and ranking",Information retrieval query processing is relevant as the paper describes a query-by-humming system for song retrieval. Retrieval models and ranking is relevant due to the focus on aligning notes for retrieval efficiency. Other categories like Document representation are unrelated to the content-based retrieval approach.
4726,International Standard for a Linguistic Annotation Framework,This paper describes the Linguistic Annotation Framework under development within ISO TC37 SC4 WG1. The Linguistic Annotation Framework is intended to serve as a basis for harmonizing existing language resources as well as developing new ones.,"General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:1.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Information systems,Information systems is highly relevant as the paper discusses a linguistic annotation standard framework for language resources. Other categories like Computing Methodologies are less relevant since the focus is on data standards rather than algorithms.,"Data management systems:0.9,Information retrieval:0.3,Information storage systems:0.8,Information systems applications:0.5,World Wide Web:0.2","Data management systems,Information storage systems","Data management systems and Information storage systems are relevant for structuring linguistic annotations. Information retrieval is less central, while others are not directly connected.","Data structures:0.7,Database administration:0.2,Database design and models:0.8,Database management system engines:0.2,Information integration:0.3,Information storage technologies:0.4,Middleware for databases:0.2,Query languages:0.3,Record storage systems:0.2,Storage architectures:0.3,Storage management:0.3,Storage replication:0.2","Data structures,Database design and models",Data structures is relevant for the framework's annotation design. Database design and models is relevant for harmonizing language resources. Other categories like Query languages are not discussed.
3681,Overview of the Patent Mining Task at the NTCIR-7 Workshop,"This paper introduces the Patent Mining Task at the Eighth NTCIR Workshop and the test collections produced in this task. The purpose of the Patent Mining Task is to create technical trend maps from a set of research papers and patents. We performed two subtasks: (1) the subtask of research papers classification and (2) the subtask of technical trend map creation. For the subtask of research papers classification, six participant groups submitted 101 runs. For the subtask of technical trend map creation, nine participant groups submitted 40 runs. In this paper, we also report on the evaluation results of the task.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.2,Networks:0.2,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.9,Security and privacy:0.1,Human-centered computing:0.2,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Information systems,"The paper focuses on patent mining tasks involving information retrieval and classification, central to 'Information systems.' Other categories like 'Applied computing' (0.1) are less relevant as the core is information system techniques.","Data management systems:0.0,Information retrieval:0.9,Information storage systems:0.0,Information systems applications:0.0,World Wide Web:0.0",Information retrieval,Information retrieval is highly relevant for patent mining and technical trend mapping from research papers. Other categories are not directly related to the task's focus on classification and information organization.,"Document representation:1,Evaluation of retrieval results:1,Information retrieval query processing:0.2,Retrieval models and ranking:0.2,Retrieval tasks and goals:1,Search engine architectures and scalability:0.1,Specialized information retrieval:1,Users and interactive retrieval:0.1","Document representation,Evaluation of retrieval results,Retrieval tasks and goals",Document representation is relevant for classifying patents and papers. Evaluation of retrieval results is directly mentioned in the paper. Retrieval tasks and goals align with the technical trend mapping objective. Other options like query processing or ranking are less central to the paper's focus on task design and evaluation.
5237,Integrating time series mining and fractals to discover patterns and extreme events in climate and remote sensing databases,"This thesis presents new methods based on fractal theory and data mining techniques to support agricultural monitoring in regional scale, specifically regions with sugar cane fields. This commodity greatly contributes to the Brazilian economy since it is a viable alternative to replace fossil fuels. Since climate influences the national agricultural production, researchers use climate data associated to agrometeorological indexes, and recently they also employed data from satellites to support decision making processes. In this context, we proposed a method that uses the fractal dimension to identify trend changes in climate series jointly with a statistical analysis module to define which attributes are responsible for the behavior alteration in the series. Moreover, we also proposed two methods of similarity measure to allow comparisons among different agricultural regions represented by multiples variables from meteorological data and remote sensing images. Given the importance of studying the extreme weather events, which could increase in intensity, duration and frequency according to different scenarios indicated by climate forecasting models, we proposed the CLIPSMiner algorithm to identify relevant patterns and extremes in climate series. CLIPSMiner also detects correlations among multiple time series considering time lag and finds patterns according to parameters, which can be calibrated by the users. We applied two distinct approaches in order to discover association patterns on time series. The first one is the Apriori-FD method that integrates an algorithm to perform attribute selection through applying the correlation fractal dimension, an algorithm of discretization to convert continuous values of series into discrete intervals, and a well-known association rules algorithm (Apriori). Although Apriori-FD has identified interesting patterns related to temperature, this method failed to appropriately deal with time lag. As a solution, we proposed CLEARMiner that is an unsupervised algorithm in order to mine the association patterns in one time series relating them to patterns in other series considering the possibility of time lag. The proposed methods were compared with similar techniques as well as assessed by a group of meteorologists, and specialists in agrometeorology and remote sensing. The experiments showed that applying data mining techniques and fractal theory can contribute to improve the analyses of agrometeorological and satellite data. These new techniques can aid researchers in their work on decision making and become important tools to support decision making in agribusiness.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.3,Information systems:0.9,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Information systems,Information systems is highly relevant because the paper focuses on data mining techniques for climate and remote sensing databases. Other fields like Applied computing or Mathematics of computing are not central to the core contribution of database-driven decision support.,"Data management systems:0.85,Information retrieval:0.75,Information systems applications:0.65,World Wide Web:0.25,Information storage systems:0.3","Data management systems,Information retrieval","Data management systems: The paper integrates fractal theory and data mining for climate and remote sensing databases, which is a key area in data management. Information retrieval: The focus on discovering patterns in time series aligns with information retrieval. World Wide Web is less relevant as the work is not web-focused.","Data structures:0.1,Database administration:0.5,Database design and models:0.3,Database management system engines:0.2,Document representation:0.1,Evaluation of retrieval results:0.1,Information integration:0.8,Information retrieval query processing:0.6,Middleware for databases:0.2,Query languages:0.1,Retrieval models and ranking:0.1,Retrieval tasks and goals:0.3,Search engine architectures and scalability:0.1,Specialized information retrieval:0.1,Users and interactive retrieval:0.4","Information integration,Database administration",Information integration is relevant for the fractal-based data analysis methods. Database administration is relevant for managing climate and remote sensing datasets. Other categories like query processing are less central than the system's core integration and administrative focus.
5239,An Algebraic Framework for Physical OODB Design,"Physical design for object-oriented databases is still in its infancy. Implementation decisions often intrude into the conceptual design (such as inverse links and object decomposition). Furthermore, query optimizers do not always take full advantage of physical design information. This paper proposes a formal framework for physical database design that automates the query translation process. In this framework, the physical database design is specified in a declarative manner. This specification is used for generating an efficient query transformer that translates logical queries into programs that manipulate the physical database. Alternative access paths to physical data are captured as simple rewrite rules that are used for generating alternative plans for a query.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.9,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Information systems,Information systems is highly relevant because the paper focuses on physical design for object-oriented databases and query optimization. Other fields like Software and its engineering or Mathematics of computing are not central to the core contribution of database systems.,"Data management systems:0.9,Information storage systems:0.8,Information systems applications:0.6,World Wide Web:0.3,Information retrieval:0.4","Data management systems,Information storage systems","Data management systems: The paper proposes a framework for physical OODB design, a core data management topic. Information storage systems: The focus on physical design aligns with storage systems. Information systems applications is less relevant as the work is about design principles rather than applications.","Data structures:0.0,Database administration:0.0,Database design and models:1.0,Database management system engines:0.3,Information integration:0.1,Information storage technologies:0.2,Middleware for databases:0.0,Query languages:0.8,Record storage systems:0.1,Storage architectures:0.0,Storage management:0.0,Storage replication:0.0","Database design and models,Query languages",Database design and models: The paper proposes a formal framework for physical database design with declarative specifications. Query languages: The framework generates query transformers for translating logical queries into physical database programs.
3747,DAEDALUS at PAN 2014: Guessing Tweet Author's Gender and Age,"This paper describes our participation at PAN 2014 author profiling task. Our idea was to define, develop and evaluate a simple machine learning classifier able to guess the gender and the age of a given user based on his/her texts, which could become part of the solution portfolio of the company. We were interested in finding not the best possible classifier that achieves the highest accuracy, but to find the optimum balance between performance and throughput using the most simple strategy and less dependent of external systems. Results show that our software using Naive Bayes Multinomial with a term vector model representation of the text is ranked quite well among the rest of participants in terms of accuracy.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.75,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.5,Applied computing:0.1,Social and professional topics:0.1",Information systems,"Information systems: The paper uses machine learning for author profiling (gender and age prediction from tweets), a classic text mining task in information systems. Computing methodologies is secondary due to the algorithmic approach. Other children are irrelevant.","Data management systems:0.2,Information retrieval:0.8,Information storage systems:0.1,Information systems applications:0.3,World Wide Web:0.2",Information retrieval,"Information retrieval (0.8): The paper focuses on text analysis for author profiling, a core information retrieval task. Other categories like Data management systems (0.2) and Information systems applications (0.3) are only tangentially related.","Document representation:1,Evaluation of retrieval results:0.2,Information retrieval query processing:0.3,Retrieval models and ranking:1,Retrieval tasks and goals:0.2,Search engine architectures and scalability:0.1,Specialized information retrieval:0.3,Users and interactive retrieval:0.2","Document representation,Retrieval models and ranking",Document representation is relevant for text modeling in author profiling. Retrieval models and ranking apply to the machine learning approach. Other categories like 'query processing' or 'specialized retrieval' are less central to the core methodology.
256,Web site personalization based on link analysis and navigational patterns,"The continuous growth in the size and use of the World Wide Web imposes new methods of design and development of online information services. The need for predicting the users' needs in order to improve the usability and user retention of a Web site is more than evident and can be addressed by personalizing it. Recommendation algorithms aim at proposing “next” pages to users based on their current visit and past users' navigational patterns. In the vast majority of related algorithms, however, only the usage data is used to produce recommendations, disregarding the structural properties of the Web graph. Thus important—in terms of PageRank authority score—pages may be underrated. In this work, we present UPR, a PageRank-style algorithm which combines usage data and link analysis techniques for assigning probabilities to Web pages based on their importance in the Web site's navigational graph. We propose the application of a localized version of UPR (l-UPR) to personalized navigational subgraphs for online Web page ranking and recommendation. Moreover, we propose a hybrid probabilistic predictive model based on Markov models and link analysis for assigning prior probabilities in a hybrid probabilistic model. We prove, through experimentation, that this approach results in more objective and representative predictions than the ones produced from the pure usage-based approaches.","General and reference:0.05,Hardware:0.05,Computer systems organization:0.05,Networks:0.05,Software and its engineering:0.1,Theory of computation:0.05,Mathematics of computing:0.1,Information systems:0.8,Security and privacy:0.05,Human-centered computing:0.05,Computing methodologies:0.3,Applied computing:0.05,Social and professional topics:0.05",Information systems,Information systems is highly relevant as the paper addresses web personalization and recommendation systems. Computing methodologies is secondary as it involves algorithm design but not the primary domain.,"Data management systems:0.1,Information retrieval:0.3,Information storage systems:0.05,Information systems applications:0.1,World Wide Web:0.8","World Wide Web,Information retrieval",World Wide Web: The paper focuses on web personalization using navigational patterns. Information retrieval: It discusses link analysis techniques for recommendation. Data management systems is secondary as the focus is on algorithm design rather than storage.,"Document representation:0.2,Evaluation of retrieval results:0.1,Information retrieval query processing:0.1,Online advertising:0.05,Retrieval models and ranking:1.0,Retrieval tasks and goals:0.3,Search engine architectures and scalability:0.4,Specialized information retrieval:0.2,Users and interactive retrieval:0.6,Web applications:0.3,Web data description languages:0.1,Web interfaces:0.2,Web mining:1.0,Web searching and information discovery:1.0,Web services:0.1","Retrieval models and ranking,Web mining,Web searching and information discovery",Retrieval models and ranking: The paper introduces a PageRank-style algorithm (UPR) for web page ranking. Web mining: Combines usage data and link analysis for personalization. Web searching and information discovery: Focuses on improving navigation and recommendation systems. Other options like Document representation or Web services are not central to the core contribution.
510,A Representation Framework for Cross-lingual/Interlingual Lexical Semantic Correspondences,"This paper proposes a framework for representing cross-lingual/interlingual lexical semantic correspondences that are expected to be recovered through a series of on-demand/on-the-fly invocations of a lexical semantic matching process. One of the central notions of the proposed framework is a pseudo synset, which is introduced to represent a cross-lingual/multilingual lexical concept, jointly denoted by word senses in more than one language. Another important ingredient of the proposed framework is a framework for semantifying bilingual lexical resource entries. This is a necessary substep when associating and representing corresponding lexical concepts in different languages by using bilingual lexical resources. Based on these devices, this paper further discusses possible extensions to the ISO standard lexical markup framework (LMF). These extensions would enable recovered correspondences to be organized as a dynamic secondary language resource, while keeping the existing primary language resources intact.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:1.0,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.5,Applied computing:0.1,Social and professional topics:0.1",Information systems,The paper presents a framework for cross-lingual lexical semantic correspondences in information systems. Computing methodologies gets partial relevance for the framework design. Other categories like Networks are not discussed.,"Data management systems:0.2,Information retrieval:0.9,Information storage systems:0.1,Information systems applications:0.3,World Wide Web:0.1",Information retrieval,"Information retrieval is relevant as the paper addresses cross-lingual lexical semantic matching for organizing lexical resources. Other categories like Data management systems are less relevant, as the focus is not on data storage or management infrastructure.","Document representation:1.0,Evaluation of retrieval results:0.1,Information retrieval query processing:0.0,Retrieval models and ranking:0.2,Retrieval tasks and goals:0.0,Search engine architectures and scalability:0.0,Specialized information retrieval:1.0,Users and interactive retrieval:0.0","Document representation,Specialized information retrieval","Document representation is relevant as the paper proposes a framework for representing cross-lingual lexical concepts. Specialized information retrieval is relevant because the framework addresses cross-lingual/interlingual semantic correspondences, a specialized retrieval domain. Other options are less relevant as the paper focuses on representation rather than retrieval processes."
4673,Answering Multiple Queries in Compressed Texts,"With the exponential increment of data, compression technology becomes an important tool in the field of data management, especially in text management. An increasing pressing challenge is how to efficiently query these massive amounts of sequence data in their compressed format. In this paper we study the problem of answering subsequence-search queries on LZ78 format of texts. We propose the concept of conditional common sub strings of queries to improve query performance. We present a techniques to find minimal conditional common sub strings in compressed text and a local uncompressing technique to verify and locate positions of answers in text. Finally, the experimental results over real data demonstrate the efficiency of our algorithm.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.9,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.7,Applied computing:0.1,Social and professional topics:0.1",Information systems,Information systems is central for compressed text query algorithms. Computing methodologies is secondary for the specific algorithms but not the primary domain.,"Data management systems:0.8,Information retrieval:0.7,Information storage systems:0.3,Information systems applications:0.1,World Wide Web:0.1","Data management systems,Information retrieval",Data management systems: The paper addresses compressed text management techniques. Information retrieval: It focuses on subsequence search queries in compressed data. Storage systems and web categories are only peripherally related.,"Data structures:0.4,Database administration:0.2,Database design and models:0.3,Database management system engines:0.4,Document representation:0.2,Evaluation of retrieval results:0.2,Information integration:0.2,Information retrieval query processing:1,Middleware for databases:0.2,Query languages:1,Retrieval models and ranking:0.3,Retrieval tasks and goals:0.3,Search engine architectures and scalability:0.3,Specialized information retrieval:0.3,Users and interactive retrieval:0.2","Information retrieval query processing,Query languages",Information retrieval query processing is relevant for the compressed text querying approach. Query languages are relevant as the paper introduces specific query techniques. Database management system engines and data structures are less directly relevant as the focus is on query processing rather than storage systems.
4810,Centrality-as-Relevance: Support Sets and Similarity as Geometric Proximity,"In automatic summarization, centrality-as-relevance means that the most important content of an information source, or a collection of information sources, corresponds to the most central passages, considering a representation where such notion makes sense (graph, spatial, etc.). We assess the main paradigms, and introduce a new centrality-based relevance model for automatic summarization that relies on the use of support sets to better estimate the relevant content. Geometric proximity is used to compute semantic relatedness. Centrality (relevance) is determined by considering the whole input source (and not only local information), and by taking into account the existence of minor topics or lateral subjects in the information sources to be summarized. The method consists in creating, for each passage of the input source, a support set consisting only of the most semantically related passages. Then, the determination of the most relevant content is achieved by selecting the passages that occur in the largest number of support sets. This model produces extractive summaries that are generic, and language- and domainindependent. Thorough automatic evaluation shows that the method achieves state-of-the art performance, both in written text, and automatically transcribed speech summarization, including when compared to considerably more complex approaches.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.9,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.3,Applied computing:0.1,Social and professional topics:0.1",Information systems,"Information systems is highly relevant as the paper discusses centrality-based relevance models for summarization. Computing methodologies is moderately relevant for the algorithmic approach, but the focus is on information retrieval. Other categories are irrelevant.","Data management systems:0.5,Information retrieval:0.9,Information storage systems:0.2,Information systems applications:0.4,World Wide Web:0.1",Information retrieval,Information retrieval: The paper introduces a centrality-based relevance model for automatic summarization. Data management systems is moderately relevant as it's about information management techniques.,"Document representation:1,Retrieval models and ranking:1,Machine learning approaches:0.7","Document representation,Retrieval models and ranking",Document representation: The paper uses geometric proximity and support sets for semantic representation. Retrieval models and ranking: The method defines relevance via centrality in a graph-based model. Machine learning is less central as the focus is on a specific relevance model rather than broader ML techniques.
411,Implicit Incompressible SPH on the GPU,"This paper presents CUDA-based parallelization of implicit incompressible SPH (IISPH) on the GPU. Along with the detailed exposition of our implementation, we analyze various components involved for their costs. We show that our CUDA version achieves near linear scaling with the number of particles and is faster than the multi-core parallelized IISPH on the CPU. We also present a basic comparison of IISPH with the standard SPH on GPU.","General and reference:0.1,Hardware:0.1,Computer systems organization:1.0,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Computer systems organization,"Computer systems organization is highly relevant because the paper discusses GPU-based parallelization of IISPH, a systems-level optimization. Other categories like 'Hardware' are irrelevant as the focus is on software implementation, not hardware design.","Architectures:1.0,Real-time systems:0.2,Embedded and cyber-physical systems:0.3",Architectures,Architectures is directly relevant as the paper focuses on GPU-based parallelization. Other categories are not discussed in the core contribution.,"Distributed architectures:0.3,Other architectures:0.0,Parallel architectures:1.0,Serial architectures:0.0",Parallel architectures,Parallel architectures is directly relevant for GPU-based IISPH implementation. Distributed architectures is partially relevant but less so compared to parallelism on a single GPU.
465,Changing Persistent Objects in a Distributed Environment,"Abstract : In distributed systems, it is useful to classify persistent objects as either immutable or mutable. The contents of an immutable object cannot be changed while the contents of a mutable object can. In a distributed system, multiple copies of an immutable object can exist at different places and can be used freely without the need for any special synchronization. Mutable objects, however, require synchronization. When and object is about to be changed, all current users need to be notified. When two users both try to change the same object, only one should be permitted to succeed. This kind of synchronization requires that for each mutable object there be a single point in the network that controls use for that object. If a network becomes temporarily partitioned into two isolated subnetworks, only one will have control over each mutable object. This paper considers a class of objects called incrementally mutable objects that are intermediate between mutable and immutable objects. Intuitively the only permitted modifications to an incrementally mutable object are those that add new information to the object while preserving existing information. Changes to incrementally mutable objects do not require central synchronization. When a network becomes partitioned, the same incrementally mutable object can be safely modified in each subnetwork. A mutable object can be modeled by a set of immutable objects that represent each value of the object over time and an incrementally mutable object that relates each immutable object to its successor. Multiple successors are permitted to represent parallel changes. Keywords: Computer programs; Computer applications.","General and reference:0.0,Hardware:0.0,Computer systems organization:1.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Computer systems organization,Computer systems organization: The paper discusses distributed systems and persistent object synchronization in a networked environment. Other categories like Software and its engineering are less directly relevant.,"Architectures:0.2,Dependable and fault-tolerant systems and networks:0.95,Embedded and cyber-physical systems:0.1,Real-time systems:0.1",Dependable and fault-tolerant systems and networks,Dependable and fault-tolerant systems and networks is highly relevant because the paper addresses synchronization challenges in distributed systems during network partitions. Other options like Architectures or Real-time systems are too general or unrelated to the core topic of fault tolerance.,"Availability:0,Fault-tolerant network topologies:1,Maintainability and maintenance:0,Processors and memory architectures:0,Redundancy:0,Reliability:1,Secondary storage organization:0","Fault-tolerant network topologies,Reliability","Fault-tolerant network topologies: The paper introduces incrementally mutable objects to handle network partitions, a key aspect of fault tolerance in distributed systems. Reliability: Ensuring safe modifications during network partitions directly relates to system reliability. Other categories like Processors and memory architectures are unrelated."
566,Experimental verification of Hyper-V performance isolation level,"The need for cost optimization in a broad sense constitutes the basis of opera- tion of every enterprise. In the case of IT structure (which is present in almost every field of activity these days), one of the most commonly applied technolo- gies leading to good cost-to-profit adjustment is virtualization. This allows the location of several operational systems with IT systems on a single server. In order for such an optimization to be carried out correctly, it has to be stric- tly controlled by means of allocating access to resources (which is known as performance isolation). Modern virtualizers allow us to set up this allocation in quantitative terms (the number of processors, size of RAM, or disc space). It appears, however, that in qualitative terms (processor time, RAM, or hard disc bandwidth), the actual allocation of resources does not always correspond with this configuration. This paper provides an experimental presentation of the achievable quality level of performance isolation of the Hyper-V virtualizer.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.75,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Computer systems organization,Computer systems organization is relevant for the study of virtualization performance isolation. Other categories like Software and its engineering are less specific to the system architecture focus.,"Architectures:1.0,Dependable and fault-tolerant systems and networks:0.0,Embedded and cyber-physical systems:0.0,Real-time systems:0.0",Architectures,Architectures is highly relevant as the paper evaluates performance isolation in Hyper-V virtualization. Other categories like Real-time systems are not central to the study of resource allocation in virtual machines.,"Distributed architectures:1,Other architectures:0.8,Parallel architectures:0.6,Serial architectures:0.3,Other:0","Distributed architectures,Other architectures",Distributed architectures is relevant for virtualized resource allocation. Other architectures applies to Hyper-V's specific isolation model. Parallel/Serial architectures are less directly relevant to the performance isolation focus.
590,Efficient Task Allocation Method to Improve Network Processor Throughput,"Ubiquitous computing involves large number of devices which are connected via networks. This requires packet processing service to guarantee privacy, security, and high quality. We study to provide ubiquitous computing with stable and satisfied services through improving packet processing performance. Since the applications become more and more complicated, the task allocation among multi-cores for pipelined architecture becomes important and difficult. In order to map tasks onto pipelined architecture and maximize the overall throughput, we propose a task allocation scheme incorporated with profiling and globally thread refinement. This scheme relies on a performance model which determines the system throughput considering multi-thread, memory access and the effect of communications between stages. We evaluate the technique by implementing representative network processing applications on the Intel IXP architecture.  Experimental results show that our scheme is able to generate mapping of realistic applications to balance the stages and obtain high throughput. Furthermore, it outperforms other methods even when the PE number is reduced.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.9,Networks:0.2,Software and its engineering:0.3,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Computer systems organization,Computer systems organization is relevant for multi-core task allocation in network processors. Networks is mentioned but not central to the core contribution of optimizing throughput in pipelined architectures. Software and its engineering is secondary to system-level design.,"Architectures:1.0,Embedded and cyber-physical systems:0.8,Dependable and fault-tolerant systems and networks:0.5,Real-time systems:0.3","Architectures,Embedded and cyber-physical systems",Architectures: The paper focuses on multi-core task allocation for network processors. Embedded and cyber-physical systems: Network processors are relevant to embedded systems. Other categories are less central.,"Parallel architectures:0.9,Embedded systems:0.7,System on a chip:0.6,Other architectures:0.4,Serial architectures:0.2,Distributed architectures:0.2,Sensor networks:0.1,Sensors and actuators:0.1,Robotics:0.1","Parallel architectures,Embedded systems",Parallel architectures: The task allocation method targets multi-core pipelined architectures. Embedded systems: Network processors are embedded systems. System on a chip is relevant but secondary to the architectural focus.
592,Connecting client objectives with resource capabilities: an essential component for grid service managent infrastructures,"In large-scale, distributed systs such as Grids, an agreent between a client and a service provider specifies service level objectives both as expressions of client requirents and as provider assurances. Ideally, these objectives are expressed in a high-level, service- or application-specific manner rather than requiring clients to detail the necessary resources. Resource providers on the other hand, expect low-level, resource specific performance criteria that are uniform across applications and can easily be interpreted and provisioned.
 This paper presents a framework for Grid service managent that addresses this gap between high-level specification of client performance objectives and existing resource managent infrastructures It identifies three levels of abstraction for resource requirents that a service provider needs to manage, namely: detailed specification of raw resources, virtualization of heterogeneous resources as abstract resources, and performance objectives at an application level. The paper also identifies three key functions for managing service level agreents, namely: <i>translation</i> of resource requirents across abstraction layers, <i>arbitration</i> in allocating resources to client requests, and <i>aggregation and allocation</i> of resources from multiple lower level resource managers. One or more of these key functions may be present at each abstraction layer of a service level manager. Thus, the composition of these functions across resource abstraction layers enables modeling of a wide array of managent scenarios. We present a framework that supports these functions: it uses the service metadata and/or service performance models to map client requirents to resource capabilities, it uses business value associated with objectives in allocation decisions to arbitrate between competing requests, and it allocates resources based on previously negotiated agreents.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.9,Networks:0.2,Software and its engineering:0.3,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Computer systems organization,Computer systems organization is relevant for distributed resource management in Grid infrastructures. Software and its engineering is secondary for service management frameworks. Networks is mentioned but not central to the core contribution of abstraction layers.,"Architectures:0.9,Dependable and fault-tolerant systems and networks:0.3,Embedded and cyber-physical systems:0.2,Real-time systems:0.2",Architectures,Architectures is relevant for the resource abstraction framework in Grid systems. Other categories like dependable systems or real-time systems are not directly addressed in the core contribution.,"Distributed architectures:1.0,Other architectures:0.1,Parallel architectures:0.2,Serial architectures:0.1",Distributed architectures,Distributed architectures is highly relevant as the paper addresses Grid service management in distributed systems. Other categories like parallel architectures are irrelevant as the focus is on distributed resource management rather than parallel processing.
686,Phase-aware remote profiling,"Recent advances in networking and embedded device technology have made the vision of ubiquitous computing a reality; users can access the Internet's vast offerings anytime and anywhere. Moreover, battery-powered devices such as personal digital assistants and Web-enabled mobile phones have successfully emerged as new access points to the world's digital, infrastructure. This ubiquity offers a new opportunity for software developers: users can now participate in the software development, optimization, and evolution process while they use their software. Such participation requires effective techniques for gathering profile information from remote, resource-constrained devices. Further, these techniques must be unobtrusive and transparent to the user; profiles must be gathered using minimal computation, communication, and power. Toward this end, we present a flexible hardware-software scheme for efficient remote profiling. We rely on the extraction of meta information from executing programs in the form of phases, and then use this information to guide intelligent online sampling and to manage the communication of those samples. Our results indicate that phase-based remote profiling can reduce the communication, computation, and energy consumption overheads by 50-75% over random and periodic sampling.","General and reference:0.1,Hardware:0.5,Computer systems organization:0.75,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Computer systems organization,Computer systems organization is relevant for the hardware-software profiling system optimization. Hardware is partially relevant but secondary. Other categories are rejected because the core focus is system-level profiling rather than pure software or theory.,"Architectures:0.2,Dependable and fault-tolerant systems and networks:0.3,Embedded and cyber-physical systems:0.8,Real-time systems:0.5",Embedded and cyber-physical systems,Embedded and cyber-physical systems is relevant because the paper focuses on remote profiling for resource-constrained devices. Real-time systems is partially relevant due to overhead reduction but lacks explicit temporal constraints. The other options are irrelevant as they do not address profiling or resource-constrained device optimization.,"Embedded systems:0.9,Robotics:0.1,Sensor networks:0.3,Sensors and actuators:0.2,System on a chip:0.0",Embedded systems,Embedded systems is directly addressed through resource-constrained device profiling. Sensor networks are secondary but less relevant.
706,Evaluation of Design Choices for Gang Scheduling Using Distributed Hierarchical Control,"Gang scheduling?the scheduling of a number of related threads to execute simultaneously on distinct processors?appears to meet the requirements of interactive, multiuser, general-purpose parallel systems.Distributed hierarchical control(DHC) has been proposed as an efficient mechanism for coping with the dynamic processor partitioning necessary to support gang scheduling on massively parallel machines. In this paper, we compare and evaluate different algorithms that can be used within the DHC framework. Regrettably, gang scheduling can leave processors idle if the sizes of the gangs do not match the number of available processors. We show that in DHC this effect can be reduced by reclaiming the leftover processors when the gang size is smaller than the allocated block of processors, and by adjusting the scheduling time quantum to control the adverse effect of badly matched gangs. Consequently, the on-line mapping and scheduling algorithms developed for DHC are optimal in the sense that asymptotically they achieve performance commensurate with off-line algorithms.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.9,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Computer systems organization,Computer systems organization is highly relevant as the paper evaluates gang scheduling algorithms in parallel systems. Other fields are irrelevant as the focus is on system-level scheduling mechanisms.,"Architectures:0.9,Dependable and fault-tolerant systems and networks:0.3,Embedded and cyber-physical systems:0.2,Real-time systems:0.7","Architectures,Real-time systems",Architectures is relevant for gang scheduling in parallel systems. Real-time systems apply to the scheduling algorithms. Dependable systems are less directly addressed.,"Distributed architectures:1.0,Other architectures:0.1,Parallel architectures:1.0,Real-time languages:0.1,Real-time operating systems:0.2,Real-time system architecture:0.3,Real-time system specification:0.1,Serial architectures:0.1","Distributed architectures,Parallel architectures",Distributed architectures and Parallel architectures are directly addressed in the gang scheduling context. Other fields like Real-time systems are less central.
1292,A Fault Tolerant and Multi-Paradigm Grid Architecture for Time Constrained Problems. Application to Option Pricing in Finance.,"This paper introduces a Grid software architecture offering fault tolerance, dynamic and aggressive load balancing and two complementary parallel programming paradigms. Experiments with financial applications on a real multi-site Grid assess this solution. This architecture has been designed to run industrial and financial applications, that are frequently time constrained and CPU consuming, feature both tightly and loosely coupled parallelism requiring generic programming paradigm, and adopt client-server business architecture.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.9,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Computer systems organization,Computer systems organization: The paper presents a Grid architecture with fault tolerance and load balancing for time-constrained applications. The focus is on system design and organization for distributed computing. Networks or Applied computing are secondary to the core contribution.,"Architectures:1.0,Dependable and fault-tolerant systems and networks:1.0,Embedded and cyber-physical systems:0.25,Real-time systems:0.75","Dependable and fault-tolerant systems and networks,Architectures,Real-time systems",Dependable and fault-tolerant systems and networks is highly relevant due to the explicit focus on fault tolerance. Architectures is relevant because the paper introduces a novel grid architecture. Real-time systems is relevant as the architecture targets time-constrained problems. Embedded and cyber-physical systems is irrelevant as the paper does not address embedded systems.,"Availability:0.3,Distributed architectures:1,Fault-tolerant network topologies:1,Maintainability and maintenance:0,Other architectures:0,Parallel architectures:0.5,Processors and memory architectures:0,Real-time languages:0,Real-time operating systems:0,Real-time system architecture:0,Real-time system specification:0,Redundancy:0.5,Reliability:0.3,Secondary storage organization:0,Serial architectures:0","Distributed architectures,Fault-tolerant network topologies",Distributed architectures: The paper presents a Grid architecture with multiple programming paradigms. Fault-tolerant network topologies: The architecture includes fault tolerance. Parallel architectures is only marginally relevant.
1306,Combining low-latency communication protocols with multithreading for high performance DSM systems on clusters,"With the development of clusters based on high performance networks, it is now possible to design efficient Distributed Shared Memory systems. In this paper we present the approach we choose to implement a high performance DSM system on top of a cluster by combining the use of low-latency communication protocols (MPI-BIP on Myrinet networks) with multithreading approach (PM2). We present our approach called Distributed Objects Shared MemOry System (DOSMOS system), its design and experiments performed on various communication libraries (PVM, MPI) and on various networks (Ethernet, Myrinet).","General and reference:0.0,Hardware:0.0,Computer systems organization:1.0,Networks:0.5,Software and its engineering:0.2,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Computer systems organization,Computer systems organization is directly relevant as the paper focuses on implementing a high-performance distributed shared memory system on clusters. Networks is partially relevant due to communication protocols but not the primary focus. Other categories are not applicable.,"Architectures:0.9,Dependable and fault-tolerant systems and networks:0.3,Embedded and cyber-physical systems:0.2,Real-time systems:0.2",Architectures,Architectures is highly relevant for the DSM system design on clusters. Other fields like Real-time systems are not central to the paper's focus on protocol integration.,"Distributed architectures:1,Other architectures:0,Parallel architectures:1,Serial architectures:0","Distributed architectures,Parallel architectures",Distributed architectures is relevant for the cluster-based DSM system. Parallel architectures is relevant for the multithreading implementation. Other options are not applicable to the paper's focus on distributed memory systems.
1405,The ABCIT research platform,"We present a portable, binaural, real-time research platform for cochlear implant devices. The platform consists of hardware and software parts. The research hardware is responsible for the 4-channel input audio signal as well as for the electric stimulation of the cochlea. The software includes a highly flexible research environment for the development of sound pre-processing (“speech processing”) and stimulation strategies. The interface between hardware and software is fully bi-directional, supporting recording capabilities of objective measures specific to the connected implant device. Hence, this platform enables the researchers to perform a diversity of experiments related to cochlear implant research.","General and reference:0.0,Hardware:0.75,Computer systems organization:1.0,Networks:0.0,Software and its engineering:0.75,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Computer systems organization,Computer systems organization: The paper describes a complete hardware-software platform for cochlear implants. Hardware: The physical components are relevant but secondary to the system-level design.,"Architectures:0.8,Dependable and fault-tolerant systems and networks:0.2,Embedded and cyber-physical systems:0.75,Real-time systems:0.9","Real-time systems,Embedded and cyber-physical systems",Real-time systems is relevant due to the platform's real-time audio processing and stimulation requirements. Embedded systems applies to the hardware-software integration for a specific application. Dependable systems is irrelevant as fault tolerance is not discussed. Architectures is only moderately relevant for system design aspects.,"Embedded systems:0.9,Real-time languages:0.3,Real-time operating systems:0.5,Real-time system architecture:0.8,Real-time system specification:0.4,Robotics:0.2,Sensor networks:0.3,Sensors and actuators:0.7,System on a chip:0.6","Embedded systems,Real-time system architecture",Embedded systems is primary as the platform is hardware/software for cochlear implants. Real-time system architecture is secondary for real-time processing requirements. Other options like robotics are less relevant.
1409,DARC2: 2nd generation DLX architecture simulator,"Renewed interest in computer architecture education in our university started three years ago. Since then, research framework in computer architecture has been established with emphasis on simulation of different computer architecture concepts. One of the concepts, which have generated a lot of excitement, is the topic on pipelining. Our research group had already developed a pipeline simulator based on the DLX architecture called DARC [1]. The simulator was used as a supplementary tool for both undergraduate and graduate students. It was received favorably and at the same time, they gave feedbacks and suggestions on improving the simulator. With those suggestions, DARC2, the 2nd generation pipeline simulator based on DLX architecture was developed. This paper describes the DARC2 system.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.9,Networks:0.2,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Computer systems organization,Computer systems organization is highly relevant because the paper describes a DLX architecture simulator (DARC2) for teaching computer architecture concepts like pipelining. Other categories like Hardware or Software are not directly relevant as the focus is on educational simulation tools rather than physical components or software engineering.,"Architectures:0.95,Dependable and fault-tolerant systems and networks:0.1,Embedded and cyber-physical systems:0.3,Real-time systems:0.2",Architectures,Architectures is directly relevant to the DLX processor architecture simulation. Real-time systems is irrelevant as the paper focuses on educational simulation rather than real-time constraints. Embedded systems is secondary to the architectural focus.,"Distributed architectures:0.0,Other architectures:1.0,Parallel architectures:0.0,Serial architectures:1.0","Other architectures,Serial architectures","Other architectures is relevant as the paper describes a DLX architecture simulator. Serial architectures is relevant due to the focus on pipelining, a serial processing technique. Distributed/parallel architectures are not discussed."
1550,Dynamic verification of sequential consistency,"In this paper, we develop the first feasibly implemental scheme for end-to-end dynamic verification of multithreaded memory systems. For multithreaded (including multiprocessor) memory systems, end-to-end correctness is defined by its memory consistency model. One such consistency model is sequential consistency (SC), which specifies that all loads and stores appear to execute in a total order that respects program order for each thread. Our design, DVSC-Indirect, performs dynamic verification of SC (DVSC) by dynamically verifying a set of sub-invariants that, when taken together, have been proven equivalent to SC. We evaluate DVSC-Indirect with full-system simulation and commercial workloads. Our results for multiprocessor systems with both directory and snooping cache coherence show that DVSC-Indirect detects all injected errors that affect system correctness (i.e., SC). We show that it uses only a small amount more bandwidth (less than 25%) than an unprotected system and thus can achieve comparable performance when provided with only modest additional link bandwidth.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.85,Networks:0.3,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Computer systems organization,Computer systems organization: The paper focuses on memory consistency models in multithreaded systems. Networks and Software are less relevant as the focus is on system-level correctness verification rather than networking or software development.,"Dependable and fault-tolerant systems and networks:0.9,Architectures:0.5,Embedded and cyber-physical systems:0.3,Real-time systems:0.2",Dependable and fault-tolerant systems and networks,Dependable and fault-tolerant systems and networks: The paper presents a dynamic verification scheme for memory consistency (SC) in multithreaded systems. Architectures is secondary as the focus is on verification rather than architectural design. Embedded systems and Real-time systems are not central to the research.,"Availability:0.5,Fault-tolerant network topologies:0.3,Maintainability and maintenance:0.1,Processors and memory architectures:1.0,Redundancy:0.6,Reliability:0.8,Secondary storage organization:0.2","Processors and memory architectures,Reliability",Processors and memory architectures is highly relevant as the paper focuses on memory system verification and simulation. Reliability is relevant due to error detection and system correctness. Other categories like Availability or Fault-tolerant topologies are less directly related to memory architecture verification.
1562,Efficient Matrix Multiplication in Hadoop,"In a typical MapReduce job, each map task processing one piece of the input file. If two input matrices are stored in separate HDFS files, one map task would not be able to access the two input matrices at the same time. To deal with this problem, we propose a efficient matrix multiplication in Hadoop. For dense matrices, we use plain row major order to store the matrices on HDFS; For sparse matrices, we use the row-major-like strategy. So, a mapper can get the rows and columns by only scannig through a consecutive part of a file. We modify the Hadoop MapReduce input format, add two file paths to the two input matrices and store the input matrices in row major order. With the new file split structure, all data are distributed properly to the mappers. Finally, we propose a user feedback method to avoid the overheads of starting multiple map waves. A number of comparative experiments are conducted, the result show that our method observably improve the performance of dense matrix multiplication in MapReduce.","General and reference:0.0,Hardware:0.0,Computer systems organization:1.0,Networks:0.0,Software and its engineering:0.2,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Computer systems organization,"Computer systems organization is highly relevant as the paper addresses distributed computing and Hadoop optimizations. Software and its engineering is marginally relevant due to implementation aspects, but the core focus is systems-level optimization.","Architectures:0.75,Dependable and fault-tolerant systems and networks:0.75,Embedded and cyber-physical systems:0.0,Real-time systems:0.0","Architectures,Dependable and fault-tolerant systems and networks",Architectures is relevant as the paper focuses on optimizing matrix multiplication in Hadoop (a distributed system architecture). Dependable systems is relevant for improving system performance and fault tolerance. Real-time systems is less relevant as the focus is on batch processing in Hadoop.,"Availability:0.2,Distributed architectures:1.0,Fault-tolerant network topologies:0.1,Maintainability and maintenance:0.1,Other architectures:0.1,Parallel architectures:0.8,Processors and memory architectures:0.1,Redundancy:0.1,Reliability:0.2,Secondary storage organization:0.1,Serial architectures:0.1","Distributed architectures,Parallel architectures",Distributed architectures: The paper focuses on optimizing Hadoop for matrix multiplication. Parallel architectures: The solution leverages parallel processing for efficiency. Serial architectures are not discussed.
1564,ELIAD: Efficient Lithography Aware Detailed Routing Algorithm With Compact and Macro Post-OPC Printability Prediction,"In this paper, we present an efficient lithography aware detailed (ELIAD) router to enhance silicon image after optical proximity correction (OPC) in a correct-by-construction manner. We first quantitatively show that a pre-OPC litho-metric is highly uncorrelated with a post-OPC metric, which stresses the importance of a post-OPC litho-metric for design-time optimization. We then propose a compact post-OPC litho-metric for a detailed router (DR) based on statistical characterization, where the interferences among predefined litho-prone shapes are captured as a lookup table. Our litho-metric derived from the characterization shows high fidelity to the total edge placement error (EPE) in large scale, compared with Calibre OPC/optical rule check. Therefore, ELIAD powered by the proposed litho-metric can enhance the overall post-OPC printed silicon image. Experimental results on 65-nm industrial circuits show that ELIAD outperforms a rip-up/rerouting approach such as Resolution-enhancement-technique-Aware Detailed Routing with 8times more EPE hot spot reduction and 12times speedup. Moreover, compared with a conventional DR, ELIAD is only about 50% slower.","General and reference:0.0,Hardware:0.2,Computer systems organization:1.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Computer systems organization,"Computer systems organization is highly relevant as the paper addresses VLSI design and lithography-aware routing. Hardware is marginally relevant due to physical design aspects, but the core focus is systems-level optimization.","Architectures:0.75,Dependable and fault-tolerant systems and networks:0.0,Embedded and cyber-physical systems:0.0,Real-time systems:0.0",Architectures,Architectures is relevant as the paper presents a new lithography-aware routing algorithm for VLSI design. Other categories like Embedded systems are less relevant as the focus is on routing algorithms in semiconductor design.,"Distributed architectures:0.1,Other architectures:0.1,Parallel architectures:0.1,Serial architectures:1.0",Serial architectures,"Serial architectures: The paper focuses on a routing algorithm implementation, likely sequential in nature. Other architecture types are not discussed in the abstract."
1568,A Framework for the Derivation of WCET Analyses for Multi-core Processors,"Multi-core processors share common hardware resources between several processor cores. As a consequence, the performance of one processor core is influenced by the programs executed on the concurrent cores. We refer to this phenomenon as shared-resource interference. An explicit consideration of all such interference effects is in general combinatorially infeasible. This makes a precise worst-case execution time (WCET) analysis for multi-core processors challenging. In order to reduce the complexity, WCET analyses for multi-core processors coarsely approximate the behavior of the considered applications. However, current approaches are only applicable to rather restricted classes of hardware platforms. We propose a framework for the derivation of WCET analyses for multi-core processors. It relaxes the restricting assumptions that existing approaches are based on. The derivation starts from a WCET analysis that makes maximally pessimistic assumptions about the shared-resource interference. More precise interference bounds for the concrete system are subsequently lifted to the approximation of the analysis. The lifted bounds are finally incorporated in the analysis in order to model the interference in a more precise way.","General and reference:0.0,Hardware:0.0,Computer systems organization:1.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.2,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Computer systems organization,"Computer systems organization is highly relevant as the paper addresses multi-core processor WCET analysis. Theory of computation is marginally relevant due to theoretical analysis, but the core focus is systems architecture.","Architectures:0.4,Dependable and fault-tolerant systems and networks:0.2,Embedded and cyber-physical systems:0.1,Real-time systems:0.9",Real-time systems,Real-time systems is highly relevant as the paper addresses worst-case execution time (WCET) analysis for multi-core processors. Architectures is secondary due to multi-core processor context.,"Real-time languages:0.1,Real-time operating systems:0.3,Real-time system architecture:0.9,Real-time system specification:0.7","Real-time system architecture,Real-time system specification",The framework addresses multi-core WCET analysis architecture ('Real-time system architecture') and provides methodological improvements for specification ('Real-time system specification'). 'Operating systems' is less relevant as the focus is on analysis frameworks.
1758,Systematic evaluation of workload clustering for extremely energy-efficient architectures,"Chip power consumption has reached its limits, leading to the flattening of single-core performance. We propose the 10x10 processor, a federated heterogeneous multi-core architecture, where each core is an ensemble of u-engines (micro-engines, similar to accelerators) specialized for different workload groups to achieve dramatically higher energy efficiency. The u-engines collectively target the entire general-purpose workload space.
 The problem we study in this article is selecting the set of workloads that each u-engine should be customized for. For this problem we study the computation structure of a wide variety of workloads and cluster together workloads with similar computation structures, the idea being that each u-engine will be customized for the compute structures exhibited by a particular cluster. The constraint on this problem is the silicon budget of a processor. Lower silicon budgets accommodate fewer uengines and require individual u-engines to target larger segments of the workload space which leads to lower energy efficiency benefits from customization, because there is more variation among the compute structures making up each cluster. Therefore, we also study how workload coverage and benefit can be maximized for a given silicon budget.
 We study a broad general-purpose workload that includes 34 codes from 6 benchmark suites, identifying the most frequent functions, and clustering them based on two sets of instruction usage features (high-resolution and low-resolution) into 8, 16, 32, 64, 128 clusters respectively. We develop abstract metrics (coverage and weighted customization benefit) to evaluate the clusters. We show significant potential payoffs with four benefit models: 2-3x (square root model), 4-10x (linear model), 12-24x (quadratic model), and 22-26x (cubic model).","General and reference:0.0,Hardware:0.0,Computer systems organization:1.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Computer systems organization,Computer systems organization is highly relevant as the paper analyzes processor architecture for energy efficiency. Other categories are irrelevant as the focus is on workload clustering in hardware design.,"Architectures:1.0,Dependable and fault-tolerant systems and networks:0.0,Embedded and cyber-physical systems:0.5,Real-time systems:0.0",Architectures,Architectures is directly relevant as the paper discusses processor design for energy efficiency. Embedded and cyber-physical systems receive a moderate score due to the focus on workload clustering for specialized cores.,"Distributed architectures:0.2,Other architectures:0.9,Parallel architectures:0.5,Serial architectures:0.1",Other architectures,"The paper introduces a novel heterogeneous multi-core architecture (10x10 processor) with specialized micro-engines, which falls under 'Other architectures' due to its unconventional design. 'Distributed architectures' is less relevant as the focus is on a single processor's internal structure. 'Parallel architectures' is partially relevant but lacks specificity compared to 'Other architectures'."
1872,Multiple coherence and coordinated checkpointing protocols for DSM systems,"In this article, we address two important issues in DSM research: improving performance and providing reliability. To improve performance, we designed a low-overhead multiple coherence protocol mechanism and to augment the reliability of the system, we propose a coordinated checkpointing/recovery mechanism. Both mechanisms were implemented and incorporated in JIAJIA, a DSM system that implements scope consistency with a write-invalidate protocol. Our results on an eight machine cluster with some popular benchmarks show, for the multiple coherence protocol strategy, a significant reduction on the number of messages exchanged, leading to better performance results. Also, our results for the checkpointing strategy show that the overhead introduced in failure-free executions is small when considering the benefits obtained.","General and reference:0.1,Hardware:0.1,Computer systems organization:1.0,Networks:0.2,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Computer systems organization,"The paper addresses DSM systems, coherence protocols, and checkpointing, which are core topics in computer systems organization.","Architectures:0.9,Dependable and fault-tolerant systems and networks:0.85,Embedded and cyber-physical systems:0.2,Real-time systems:0.3","Architectures,Dependable and fault-tolerant systems and networks",Architectures is relevant for designing low-overhead coherence protocols in DSM systems. Dependable and fault-tolerant systems and networks applies to the coordinated checkpointing mechanism for reliability. Embedded systems and Real-time systems are not central to this work.,"Availability:0,Distributed architectures:1,Fault-tolerant network topologies:1,Maintainability and maintenance:0,Other architectures:0,Parallel architectures:0,Processors and memory architectures:0,Redundancy:0,Reliability:0,Secondary storage organization:0,Serial architectures:0","Distributed architectures,Fault-tolerant network topologies",Distributed architectures and Fault-tolerant network topologies are directly addressed through DSM systems and checkpointing protocols. Other categories like Reliability are implicitly covered but not explicitly focused.
1956,Limits on multiple instruction issue,"This paper investigates the limitations on designing a processor which can sustain an execution rate of greater than one instruction per cycle on highly-optimized, non-scientific applications. We have used trace-driven simulations to determine that these applications contain enough instruction independence to sustain an instruction rate of about two instructions per cycle. In a straightforward implementation, cost considerations argue strongly against decoding more than two instructions in one cycle. Given this constraint, the efficiency in instruction fetching rather than the complexity of the execution hardware limits the concurrency attainable at the instruction level.","General and reference:0.3,Hardware:0.3,Computer systems organization:1.0,Networks:0.3,Software and its engineering:0.3,Theory of computation:0.3,Mathematics of computing:0.3,Information systems:0.3,Security and privacy:0.3,Human-centered computing:0.3,Computing methodologies:0.3,Applied computing:0.3,Social and professional topics:0.3",Computer systems organization,Computer systems organization is directly relevant for analyzing processor instruction issue limitations. Other categories like Hardware are less central as the paper focuses on system-level organization rather than physical design.,"Architectures:1.0,Dependable and fault-tolerant systems and networks:0.0,Embedded and cyber-physical systems:0.0,Real-time systems:0.0",Architectures,Architectures is highly relevant for processor design and instruction-level parallelism analysis. Other options are unrelated to the paper's focus on processor execution limits.,"Distributed architectures:0.1,Other architectures:0.1,Parallel architectures:0.2,Serial architectures:0.8",Serial architectures,Serial architectures is highly relevant as the paper discusses instruction issue limitations in processors. Other categories like Parallel architectures are less relevant as the focus is on serial instruction processing rather than parallelism.
2140,Intrinsic references in distributed systems,"The notion of intrinsic references, i.e. references based on the hash digest of the referent, is introduced and contrasted with that of physical references, where the referent is defined relative to the state of a physical system. A retrieval mechanism using intrinsic references, the Elephant Store, is presented. The use of intrinsic references in hierarchical data structures is discussed, and the advantages regarding version management, consistency and distributed storage are argued.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.8,Networks:0.1,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Computer systems organization,Computer systems organization is relevant for distributed systems and data structures. Other categories are less aligned with the focus on intrinsic references and system design.,"Architectures:0.4,Dependable and fault-tolerant systems and networks:0.9,Embedded and cyber-physical systems:0.0,Real-time systems:0.0",Dependable and fault-tolerant systems and networks,Dependable and fault-tolerant systems and networks (distributed storage consistency and version management). Architectures is lower relevance as the focus is on reference semantics rather than system architecture design.,"Availability:0.3,Fault-tolerant network topologies:0.5,Maintainability and maintenance:0,Processors and memory architectures:0,Redundancy:0.4,Reliability:1,Secondary storage organization:0",Reliability,Reliability is primary as the paper discusses intrinsic references for distributed storage consistency. Fault-tolerant topologies and redundancy are secondary to the core focus on reliability through intrinsic references.
2257,Unified assign and schedule: a new approach to scheduling for clustered register file microarchitectures,"Recently, there has been a trend towards clustered microarchitectures to reduce the cycle time for wide issue microprocessors. In such processors, the register file and functional units are partitioned and grouped into clusters. Instruction scheduling for a clustered machine requires assignment and scheduling of operations to the clusters. In this paper, a new scheduling algorithm named unified-assign-and-schedule (UAS) is proposed for clustered, statically-scheduled architectures. UAS merges the cluster assignment and instruction scheduling phases in a natural and straightforward fashion. We compared the performance of UAS with various heuristics to the well-known Bottom-up Greedy (BUG) algorithm and to an optimal cluster scheduling algorithm, measuring the schedule lengths produced by all of the schedulers. Our results show that UAS gives better performance than the BUG algorithm and is quite close to optimal.","General and reference:0.1,Hardware:0.2,Computer systems organization:0.9,Networks:0.1,Software and its engineering:0.5,Theory of computation:0.1,Mathematics of computing:0.2,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Computer systems organization,Computer systems organization is relevant for clustered register file microarchitectures. Software & Engineering is secondary for scheduling algorithm implementation.,"Architectures:1.0,Dependable and fault-tolerant systems and networks:0.0,Embedded and cyber-physical systems:0.0,Real-time systems:0.0",Architectures,Architectures is highly relevant as the paper introduces a scheduling algorithm for clustered microarchitectures. Other categories are irrelevant as the paper's focus is on microarchitecture scheduling.,"Parallel architectures:1,Other architectures:0.5,Distributed architectures:0",Parallel architectures,Parallel architectures are directly relevant as the paper discusses clustered microarchitectures for scheduling. 'Other architectures' is moderately relevant due to the novel approach. 'Distributed architectures' are irrelevant as the focus is on static scheduling within clusters.
2297,Vectorized deblocking filter for HD H.264 decoding on Cell/B.E.,"For high definition (HD) H.264 decoding, deblocking filter is one of the most time-consuming modules. This paper proposes several vectorization approaches to speed up it on a single synergistic processor element (SPE) of IBM Cell Broadband Engine (Cell/B.E.) processor, by which great performance improvements are achieved. The average deblocking speed is 42.4 frames per second (fps) for 141 1080p H.264 video streams. The steady-going performance is obtained for different streams of various contents and bitrates. With the vectorized deblocking filter, the new HD H.264 decoder is able to decode two 1080p H.264 video streams simultaneously on PlayStation® 3 (PS3) in real-time. The proposed approaches are practical on many other vector processor platforms, but not limited to Cell/B.E. processor.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.9,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Computer systems organization,Computer systems organization is directly relevant because the paper focuses on optimizing video decoding algorithms for specific processor architectures. Categories like Software engineering are not central to the core contribution of system-level hardware-software optimization.,"Architectures:0.9,Dependable and fault-tolerant systems and networks:0.2,Embedded and cyber-physical systems:0.5,Real-time systems:0.8","Architectures,Real-time systems",Architectures: The paper focuses on vectorization techniques for the Cell/B.E. processor architecture. Real-time systems: The optimization enables real-time decoding of HD video streams. Embedded systems are less central as the application is not explicitly tied to embedded contexts.,"Distributed architectures:0.5,Other architectures:0,Parallel architectures:1,Real-time languages:0,Real-time operating systems:0,Real-time system architecture:0,Real-time system specification:0,Serial architectures:0",Parallel architectures,Parallel architectures is highly relevant because the paper optimizes deblocking filters using vectorization on the Cell/B.E. processor's SPE. Distributed architectures is moderately relevant but less central. Other options like real-time systems are not discussed.
2336,Asynchronous message-passing binary consensus over non-complete graphs,"While the fundamental problem of consensus in distrbuted systems has been studied extensively, this has mostly focused on shared-memory and to a lesser extent on asynchronous message-passing models. However, a natural extension is to consider the case of a asynchronous message-passing model over non-complete graphs. In this paper, we study the problem of binary consensus problem over non-complete graphs using Erdfüs-Rényi and describe an algorithm which not only yields the desired primary result, but also achieves this with the stronger constraint of messages from a given source reaching their respective sinks over k edge-disjoint spanning trees by extending Correia et al.'s variant of Ben-Or's algorithm.","General and reference:0.0,Hardware:0.0,Computer systems organization:1.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.5,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Computer systems organization,"Computer systems organization is highly relevant for asynchronous consensus algorithms in distributed systems. Theory of computation (0.5) relates to the algorithmic analysis but is secondary to the systems-level focus. Networks is irrelevant as the paper is about consensus models, not communication protocols.","Dependable and fault-tolerant systems and networks:1.0,Architectures:0.3,Embedded and cyber-physical systems:0.2,Real-time systems:0.1",Dependable and fault-tolerant systems and networks,Dependable and fault-tolerant systems is directly relevant for the consensus algorithm in distributed systems. Architectures and embedded systems are less relevant as the focus is on algorithmic properties rather than system design.,"Availability:0.5,Fault-tolerant network topologies:1,Maintainability and maintenance:0,Processors and memory architectures:0,Redundancy:0.8,Reliability:0.7,Secondary storage organization:0","Fault-tolerant network topologies,Redundancy",Fault-tolerant network topologies and redundancy are directly relevant as the paper studies consensus algorithms over non-complete graphs with edge-disjoint spanning trees. Availability and reliability receive lower scores as the focus is on topological constraints rather than system availability metrics.
2429,Efficient architecture for large-scale video on demand storage server,"Video-on-demand (VOD) system allows users to access media any time without the need to leave their home. Hard disk drive (HDD) has become popular for VOD storage to store and deal with a large amount of data. In practical cases, disk storage throughput is limited by a slow HDD and disk operating degrades drastically when the server needs to serve many simultaneous video streams. On the other hand, solid-state drive (SSD) are a appropriate features which does not has long access delay and support more IOPS that is greatly desirable for VOD storage. Unfortunately, using SSD to entirely replace HDD is not cost-effective. For this reason, hybrid storage is an economical way and offer better performance to treat with large scale VOD storage server. In this paper, SSD is divided into two different regions in size. The large part is used to store popular contents and the other is used as a buffer cache instead of RAM. The target of this research is to exploit fast access of SSD and its greater number of IOPS to maximize the number of simultaneously users and to minimize the service time. Experimental results of the proposed work enhance the response time for VOD storage server.","General and reference:0.1,Hardware:0.3,Computer systems organization:0.8,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Computer systems organization,"Computer systems organization: The paper designs a hybrid storage architecture for VOD servers, focusing on system-level performance optimization. Hardware is secondary as the paper emphasizes architectural design over physical components.","Architectures:1.0,Dependable and fault-tolerant systems and networks:0.2,Embedded and cyber-physical systems:0.2,Real-time systems:0.3",Architectures,Architectures is relevant for the hybrid SSD/HDD storage design. Other categories like Real-time systems are irrelevant as the focus is on storage throughput rather than timing constraints.,"Distributed architectures:1.0,Other architectures:0.3,Parallel architectures:0.5,Serial architectures:0.2",Distributed architectures,Distributed architectures is relevant as the paper discusses hybrid storage for large-scale VOD servers. Other architecture categories are less specific to the core contribution of distributed storage optimization.
2498,Improving metadata management for small files in HDFS,"Scientific applications are adapting HDFS/MapReduce to perform large scale data analytics. One of the major challenges is that an overabundance of small files is common in these applications, and HDFS manages all its files through a single server, the Namenode. It is anticipated that small files can significantly impact the performance of Namenode. In this work we propose a mechanism to store small files in HDFS efficiently and improve the space utilization for metadata. Our scheme is based on the assumption that each client is assigned a quota in the file system, for both the space and number of files. In our approach, we utilize the compression method ‘harballing', provided by Hadoop, to better utilize the HDFS. We provide for new job functionality to allow for in-job archival of directories and files so that running MapReduce programs may complete without being killed by the JobTracker due to quota policies. This approach leads to better functionality of metadata operations and more efficient usage of the HDFS. Our analysis results show that we can reduce the metadata footprint in main memory by a factor of 42.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.9,Networks:0.3,Software and its engineering:0.3,Theory of computation:0.1,Mathematics of computing:0.2,Information systems:0.2,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.3,Applied computing:0.1,Social and professional topics:0.1",Computer systems organization,"Computer systems organization: The paper focuses on optimizing HDFS metadata management for small files, a core systems-level problem in distributed file systems. Networks and Software are less relevant as the work addresses system architecture rather than communication or application-level programming.","Architectures:0.8,Dependable and fault-tolerant systems and networks:0.5,Embedded and cyber-physical systems:0.3,Real-time systems:0.4",Architectures,"Architectures receives top score because the paper proposes a new metadata management architecture for HDFS. Other categories are less relevant as the paper doesn't focus on fault tolerance, embedded systems, or real-time requirements.","Distributed architectures:1.0,Other architectures:0.2,Parallel architectures:0.3,Serial architectures:0.1",Distributed architectures,Distributed architectures is directly relevant for HDFS metadata management. Other architecture categories are not applicable to this specific file system context.
2564,"Supporting High Performance Molecular Dynamics in Virtualized Clusters using IOMMU, SR-IOV, and GPUDirect","Cloud Infrastructure-as-a-Service paradigms have recently shown their utility for a vast array of computational problems, ranging from advanced web service architectures to high throughput computing. However, many scientific computing applications have been slow to adapt to virtualized cloud frameworks. This is due to performance impacts of virtualization technologies, coupled with the lack of advanced hardware support necessary for running many high performance scientific applications at scale. By using KVM virtual machines that leverage both Nvidia GPUs and InfiniBand, we show that molecular dynamics simulations with LAMMPS and HOOMD run at near-native speeds. This experiment also illustrates how virtualized environments can support the latest parallel computing paradigms, including both MPI+CUDA and new GPUDirect RDMA functionality. Specific findings show initial promise in scaling of such applications to larger production deployments targeting large scale computational workloads.","General and reference:0.1,Hardware:0.3,Computer systems organization:1.0,Networks:0.2,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.3,Applied computing:0.1,Social and professional topics:0.1",Computer systems organization,"Computer systems organization is highly relevant as the paper addresses virtualized cluster performance for molecular dynamics simulations. Hardware is less central because the focus is on virtualization frameworks, not hardware design.","Architectures:0.9,Dependable and fault-tolerant systems and networks:0.7,Embedded and cyber-physical systems:0.1,Real-time systems:0.1","Architectures,Dependable and fault-tolerant systems and networks",Architectures (0.9) is central to virtualized cluster design. Dependable and fault-tolerant systems (0.7) relates to high-performance stability. Embedded systems (0.1) and Real-time systems (0.1) are irrelevant as the paper does not address embedded or real-time constraints.,"Availability:0.0,Distributed architectures:1.0,Fault-tolerant network topologies:0.0,Maintainability and maintenance:0.0,Other architectures:0.0,Parallel architectures:1.0,Processors and memory architectures:0.0,Redundancy:0.0,Reliability:0.0,Secondary storage organization:0.0,Serial architectures:0.0","Distributed architectures,Parallel architectures",Distributed architectures are relevant for virtualized clusters. Parallel architectures are central to GPU and InfiniBand-based molecular dynamics simulations. Other categories like Serial architectures are not discussed.
2850,"Objective, innovation and impact of the energy-efficient dome microdatacenter","The DOME MicroDataCenter, developed by IBM Zurich Research and ASTRON Netherlands Institute for Radio Astronomy, brings together the embedded and data-center computing worlds, resulting in the densest general-purpose computing capability with top energy-efficiency. This paper summarizes the entire 5 year project and illustrates how we went from an initial idea through obtaining funding for our small team and finally to building innovative hard- and software. We explain how we used first-principles from physics to motivate key decisions, highlight some of the practical technical obstacles we needed to overcome and summarize key lessons we learnt along our way. Our result, which we are currently bringing to market through a new startup company, addresses the needs of edge-computing (analytics) for the Internet of Things, an unforeseen opportunity that emerged during our project.","General and reference:0.0,Hardware:0.25,Computer systems organization:1.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Computer systems organization,Computer systems organization: The paper focuses on microdatacenter architecture and energy efficiency. Hardware has limited relevance to the system design.,"Architectures:0.75,Dependable and fault-tolerant systems and networks:0.2,Embedded and cyber-physical systems:1.0,Real-time systems:0.2",Embedded and cyber-physical systems,Embedded and cyber-physical systems is highly relevant as the paper focuses on energy-efficient edge computing for IoT. Architectures is secondary if the design of the microdatacenter is discussed. Other options are not directly addressed.,"Embedded systems:1,Robotics:0,Sensor networks:0,Sensors and actuators:0,System on a chip:1","Embedded systems,System on a chip",Embedded systems is relevant as the paper discusses dense computing for edge applications. System on a chip is relevant due to the integration of hardware and software for energy efficiency. Other categories like robotics are not addressed in the paper's scope.
2876,Analysis of performance limitations in multithreaded multiprocessor architectures,"The performance of modern multiprocessor systems is increasingly limited by interconnection delays or long latencies of memory subsystems. Instruction-level multithreading is a technique to tolerate such long latencies by switching from one instruction thread to another and continuing instruction execution concurrently with the long-latency operations. Using timed Petri net models, the paper analyzes performance limitations introduces by different components of distributed-memory multithreaded multiprocessor systems. Simulation results are used to compare performance improvements obtained by replicating critical components of the system to those obtained using components with better performance characteristics.","General and reference:0.1,Hardware:0.3,Computer systems organization:0.9,Networks:0.1,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.2,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.3,Applied computing:0.2,Social and professional topics:0.1",Computer systems organization,"Computer systems organization is highly relevant as the paper analyzes performance limitations in multiprocessor architectures using Petri net models. Hardware (0.3) receives moderate relevance for discussing physical system components, but the primary contribution is architectural analysis.","Architectures:1.0,Dependable and fault-tolerant systems and networks:0.3,Embedded and cyber-physical systems:0.4,Real-time systems:0.5",Architectures,Architectures is core as the paper analyzes performance limitations in multithreaded multiprocessor systems. Real-time systems is secondary but relevant. Other categories like fault tolerance are not the primary focus.,"Distributed architectures:0.8,Other architectures:0.1,Parallel architectures:0.7,Serial architectures:0.1","Distributed architectures,Parallel architectures",Distributed architectures is primary as the paper analyzes distributed-memory multiprocessor systems. Parallel architectures is relevant due to the focus on multithreading and parallel execution. Other categories like Serial architectures are not discussed.
2921,Performance Analysis of Four Memory Consistency Models for Multithreaded Multiprocessors,"Stochastic timed Petri nets are developed to evaluate the relative performance of distributed shared memory models for scalable multiprocessors, using multithreaded processors as building blocks. Four shared memory models are evaluated: the sequential consistency (SC) model by Lamport (1979), the weak consistency (WC) model by Dubois et al. (1986), the processor consistency (PC) model by Goodman (1989), and the release consistency (RC) model by Gharachorloo et al. (1990). We assumed a scalable network with a sufficient bandwidth to absorb the increased traffic from multithreading, coherent caches, and memory event reordering. The embedded Markov chains are solved to reveal the performance attributes. Under saturated conditions, we find that multithreading contributes more than 50% of the performance improvement, while the improvement from memory consistency models varies between 20% to 40% of the total performance gain. Petri net models are effective to predict the performance of processors with a larger number of contexts than that can be simulated in previous benchmark studies. The accuracy of these memory performance models was validated with the simulation results from Stanford University. Our analytical results reveal the lowest performance of the SC model amongst four memory consistency models. The PC model requires to use larger write buffers, while the WC and RC models require smaller write buffers. The PC model may perform even lower than the SC model, if a small buffer was used. The performance of the WC model depends heavily on the synchronization rate in user code. For a low synchronization rate, the WC model performs as well as the RC model. With sufficient multithreading and network bandwidth, the RC model shows the best performance among the four models. Furthermore, we discovered that cache interferences cause very little performance degradation in all relaxed memory consistency models; as long as the network is contention-free even when multithreading has saturated the system. >","General and reference:0.0,Hardware:0.0,Computer systems organization:1.0,Networks:0.5,Software and its engineering:0.25,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Computer systems organization,Computer systems organization is highly relevant as the paper evaluates memory consistency models for multiprocessors. Networks is partially relevant due to network bandwidth considerations but not central to the core contribution.,"Architectures:1.0,Dependable and fault-tolerant systems and networks:0.2,Embedded and cyber-physical systems:0.3,Real-time systems:0.4",Architectures,"Architectures receives 1.0 as the paper evaluates memory consistency models for multiprocessor architectures. Other categories are irrelevant since the study focuses on architectural evaluation rather than fault tolerance, embedded systems, or real-time constraints.","Distributed architectures:1.0,Other architectures:0.0,Parallel architectures:1.0,Serial architectures:0.0","Distributed architectures,Parallel architectures",Distributed architectures: Evaluates distributed memory models. Parallel architectures: Multithreaded multiprocessors are a key focus. Others are irrelevant.
3008,An analytical approach for fast and accurate design space exploration of instruction caches,"Application-specific system-on-chip platforms create the opportunity to customize the cache configuration for optimal performance with minimal chip area. Simulation, in particular trace-driven simulation, is widely used to estimate cache hit rates. However, simulation is too slow to be deployed in design space exploration, especially when there are hundreds of design points and the traces are huge. In this article, we propose a novel analytical approach for design space exploration of instruction caches. Given the program control flow graph (CFG) annotated only with basic block and control flow edge execution counts, we first model the cache states at each point of the CFG in a probabilistic manner. Then, we exploit the structural similarities among related cache configurations to estimate the cache hit rates for multiple cache configurations in one pass. Experimental results indicate that our analysis is 28--2,500 times faster compared to the fastest known cache simulator while maintaining high accuracy (0.2% average error) in estimating cache hit rates for a large set of popular benchmarks. Moreover, compared to a state-of-the-art cache design space exploration technique, our approach achieves 304--8,086 times speedup and saves up to 62% (average 7%) energy for the evaluated benchmarks.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.8,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Computer systems organization,"The paper focuses on cache design space exploration, a core topic in computer systems organization. Other categories like 'Software and its engineering' are not the primary focus.","Architectures:0.8,Dependable and fault-tolerant systems and networks:0.1,Embedded and cyber-physical systems:0.9,Real-time systems:0.3","Architectures,Embedded and cyber-physical systems",Architectures and embedded systems are core as the paper addresses instruction cache optimization for SoCs. Real-time systems are only marginally relevant through performance considerations.,"Distributed architectures:0.2,Embedded systems:0.6,Other architectures:0.1,Parallel architectures:0.1,Robotics:0.1,Sensor networks:0.1,Sensors and actuators:0.1,Serial architectures:0.2,System on a chip:0.8","System on a chip,Embedded systems",System on a chip: The paper addresses instruction cache design for SoC platforms. Embedded systems: Customizable caches are critical for embedded system-on-chip designs. 'Distributed architectures' and 'Parallel architectures' are less relevant as the focus is on single-chip cache optimization.
3070,Code Optimization for Tagged-Token Dataflow Machines,The efficiency of dataflow code generated from a high-level language can be considerably improved by both conventional and dataflow-specific optimizations. Such techniques are used in implementing the single-assignment language SISAL on the Manchester Dataflow Machine. The quality of code generated for numeric applications can be measured in terms of the ratio of total number of instructions executed to floating-point operations: the MIPS/MFLOPS ratio. Relevant features of the general-purpose single-assignment language SISAL and the Manchester Dataflow Machine are introduced. An assessment of the initial SISAL implementation shows it to be very expensive. A range of optimizations is then described. >,"General and reference:0.1,Hardware:0.2,Computer systems organization:0.9,Networks:0.1,Software and its engineering:0.3,Theory of computation:0.1,Mathematics of computing:0.2,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.4,Applied computing:0.2,Social and professional topics:0.1",Computer systems organization,Computer systems organization is highly relevant as the paper discusses code optimization for dataflow machines. Other categories like 'Hardware' or 'Software and its engineering' are less relevant as the focus is on system-level code generation rather than hardware design or general software development.,"Architectures:0.9,Dependable and fault-tolerant systems and networks:0.0,Embedded and cyber-physical systems:0.1,Real-time systems:0.1",Architectures,Architectures: The paper addresses code optimization for dataflow machine architectures. Embedded systems and Real-time systems are only marginally relevant as the focus is on general code optimization rather than domain-specific embedded systems.,"Distributed architectures:0.1,Other architectures:0.2,Parallel architectures:0.9,Serial architectures:0.3",Parallel architectures,"Parallel architectures is highly relevant because the paper focuses on code optimization techniques for dataflow machines, which are inherently parallel architectures. Other categories like Distributed/Serial architectures are less relevant as the paper specifically addresses dataflow machine optimizations."
3259,Obtaining Performance Measures through Microbenchmarking in a Peer-to-Peer Overlay Computer,"We address the problem of developing a suite of microbenchmarking experiments aimed at providing the basic functionalities of a measurement tool for a P2P-based globally distributed computing platform, usually referred to as overlay computer. We argue that such a measuring system should take into account the communication patterns generated by the applications in order to provide useful performance insights","General and reference:0.1,Hardware:0.1,Computer systems organization:1.0,Networks:0.2,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Computer systems organization,"Computer systems organization is highly relevant as the paper discusses microbenchmarking for performance evaluation in distributed computing platforms. Other categories are irrelevant as the focus is on system-level measurement rather than networks, software, or applications.","Architectures:1.0,Dependable and fault-tolerant systems and networks:0.4,Embedded and cyber-physical systems:0.1,Real-time systems:0.2",Architectures,Architectures is highly relevant as the paper focuses on benchmarking and performance evaluation of P2P overlay computer systems. Other fields like real-time systems are not core to the microbenchmarking methodology.,"Distributed architectures:1.0,Other architectures:0.0,Parallel architectures:0.0,Serial architectures:0.0",Distributed architectures,Distributed architectures is directly relevant as the paper focuses on P2P overlay computers. Other architecture types are not discussed.
3449,AMP: Adaptive Multi-stream Prefetching in a Shared Cache,"Prefetching is a widely used technique in modern data storage systems. We study the most widely used class of prefetching algorithms known as sequential prefetching. There are two problems that plague the state-of-the-art sequential prefetching algorithms: (i) cache pollution, which occurs when prefetched data replaces more useful prefetched or demand-paged data, and (ii) prefetch wastage, which happens when prefetched data is evicted from the cache before it can be used. 
 
A sequential prefetching algorithm can have a fixed or adaptive degree of prefetch and can be either synchronous (when it can prefetch only on a miss), or asynchronous (when it can also prefetch on a hit). To capture these distinctions we define four classes of prefetching algorithms: Fixed Synchronous (FS), Fixed Asynchronous (FA), Adaptive Synchronous (AS), and Adaptive Asynchronous (AA). We find that the relatively unexplored class of AA algorithms is in fact the most promising for sequential prefetching. We provide a first formal analysis of the criteria necessary for optimal throughput when using an AA algorithm in a cache shared by multiple steady sequential streams. We then provide a simple implementation called AMP, which adapts accordingly leading to near optimal performance for any kind of sequential workload and cache size. 
 
Our experimental set-up consisted of an IBM xSeries 345 dual processor server running Linux using five SCSI disks. We observe that AMP convincingly outperforms all the contending members of the FA, FS, and AS classes for any number of streams, and over all cache sizes. As anecdotal evidence, in an experiment with 100 concurrent sequential streams and varying cache sizes, AMP beats the FA, FS, and AS algorithms by 29-172%, 12-24%, and 21-210% respectively while outperforming OBL by a factor of 8. Even for complex workloads like SPC1-Read, AMP is consistently the best performing algorithm. For the SPC2 Video-on-Demand workload, AMP can sustain at least 25% more streams than the next best algorithm. Finally, for a workload consisting of short sequences, where optimality is more elusive, AMP is able to outperform all the other contenders in overall performance.","General and reference:0.0,Hardware:0.25,Computer systems organization:1.0,Networks:0.0,Software and its engineering:0.5,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Computer systems organization,Computer systems organization is highly relevant for the cache management and prefetching algorithm. Hardware receives a moderate score for the physical implementation aspects. Software and its engineering gets a moderate score for the algorithm's implementation.,"Architectures:1.0,Dependable and fault-tolerant systems and networks:1.0,Embedded and cyber-physical systems:0.0,Real-time systems:0.0","Architectures,Dependable and fault-tolerant systems and networks",Architectures is relevant as the paper discusses system design for prefetching. Dependable and fault-tolerant systems is relevant due to the focus on cache reliability. Other fields are not central.,"Availability:0.2,Distributed architectures:0.9,Fault-tolerant network topologies:0.1,Maintainability and maintenance:0.1,Other architectures:0.3,Parallel architectures:0.4,Processors and memory architectures:0.95,Redundancy:0.2,Reliability:0.3,Secondary storage organization:0.1,Serial architectures:0.2","Distributed architectures,Processors and memory architectures","Distributed architectures: The paper addresses multi-stream cache management in shared environments, a core distributed systems problem. Processors and memory architectures: Focuses on cache optimization and prefetching strategies, directly relevant to memory architecture design. Other categories like Serial architectures or Redundancy are less relevant as the paper doesn't address sequential execution models or fault tolerance."
3498,Analyzing and improving performance scalability of commercial server workloads on a chip multiprocessor,"A chip multiprocessor (CMP) with many low performance cores can achieve high performance or high performance/power for commercial server applications. The large number of hardware threads of a CMP with many low performance cores poses significant challenges to application developers in writing scalable applications. Many papers have assessed the architectural characteristics and the performance scalability, and some of them have identified lock contention as one of the scalability bottlenecks. However, there are few studies that resolved these problems, analyzed their causes, and compared the architectural characteristics before and after the scalability limitations were addressed. We analyzed and resolved some of the problems limiting the scalability of three commercial server applications with 64 hardware threads. We also did before and after comparisons of the architectural characteristics affected by the scalability enhancements, supporting the development of new processors. We addressed the lock contention with changes in the Java code. Our enhancements improved the performance scalability by up to 132%. We show that though the causes of lock contention are in different software layers, they share certain similarities and can be organized in three categories. Our comparisons reveal that the CPI and data TLB miss rates decrease, but the L2 data cache miss rates, L2 instruction cache miss rates, and memory traffic increase. These results suggest that we need to address the performance scalability problems of an application before we can accurately measure the architectural characteristics of a CMP.","General and reference:0.1,Hardware:0.3,Computer systems organization:1.0,Networks:0.3,Software and its engineering:0.5,Theory of computation:0.2,Mathematics of computing:0.2,Information systems:0.2,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.4,Applied computing:0.3,Social and professional topics:0.1",Computer systems organization,"Computer systems organization is highly relevant as the paper analyzes and improves performance scalability of chip multiprocessors (CMPs), a core topic in computer architecture. Software and its engineering (0.5) is somewhat relevant due to code changes, but the primary focus is on system-level analysis.","Architectures:1.0,Dependable and fault-tolerant systems and networks:0.1,Embedded and cyber-physical systems:0.2,Real-time systems:0.1",Architectures,Architectures is directly relevant as the paper analyzes CMP hardware-thread scalability and architectural characteristics like CPI and cache miss rates. Other categories are unrelated to CMP architecture or scalability analysis.,"Distributed architectures:0.2,Other architectures:0.1,Parallel architectures:1.0,Serial architectures:0.1",Parallel architectures,"Parallel architectures: The paper focuses on chip multiprocessors (CMPs) with many cores, directly addressing parallel hardware architecture challenges. Distributed architectures and Serial architectures are not relevant as the work centers on single-chip parallelism."
3589,Adaptive delay estimation for partitioning-driven PLD placement,"This paper describes the application of weighted partitioning techniques to timing-driven placement on a hierarchical programmable logic device. We discuss the nature of placement on these architectures, the details of applying weighted techniques specifically to the programmable logic device (PLD) CAD flow, and introduce the new concept of adaptive delay estimation using phase local to increase performance. Empirical results show that these techniques, in a fully complete system with large industrial designs, give an average 38.5% improvement over the unimproved partitioning-based placement tool. Approximately two-thirds of this benefit is due to our improvements over a straightforward weighted partitioning approach.","General and reference:0.0,Hardware:0.0,Computer systems organization:1.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Computer systems organization,"Computer systems organization is directly relevant because the paper discusses timing-driven placement algorithms for programmable logic devices, a core CAD topic in hardware design.","Real-time systems:1,Architectures:0.7,Embedded and cyber-physical systems:0.6,Dependable and fault-tolerant systems and networks:0.3",Real-time systems,Real-time systems is relevant due to timing-driven placement techniques. Embedded systems is partially relevant but less core than real-time considerations.,"Real-time languages:0,Real-time operating systems:0,Real-time system architecture:0,Real-time system specification:0",,"None of the options are relevant. The paper focuses on PLD placement techniques and timing-driven CAD tools, which do not align with real-time systems categories."
3601,Parallel depth first vs. work stealing schedulers on CMP architectures,"In chip multiprocessors (CMPs), limiting the number of off-chip cache misses is crucial for good performance. Many multithreaded programs provide opportunities for constructive cache sharing, in which concurrently scheduled threads share a largely overlapping working set. In this brief announcement, we highlight our ongoing study [4] comparing the performance of two schedulers designed for fine-grained multithreaded programs: Parallel Depth First (PDF) [2], which is designed for constructive sharing, and Work Stealing (WS) [3], which takes a more traditional approach.Overview of schedulers. In PDF, processing cores are allocated ready-to-execute program tasks such that higher scheduling priority is given to those tasks the sequential program would have executed earlier. As a result, PDF tends to co-schedule threads in a way that tracks the sequential execution. Hence, the aggregate working set is (provably) not much larger than the single thread working set [1]. In WS, each processing core maintains a local work queue of readyto-execute threads. Whenever its local queue is empty, the core steals a thread from the bottom of the first non-empty queue it finds. WS is an attractive scheduling policy because when there is plenty of parallelism, stealing is quite rare. However, WS is not designed for constructive cache sharing, because the cores tend to have disjoint working sets.CMP configurations studied. We evaluated the performance of PDF and WS across a range of simulated CMP configurations. We focused on designs that have fixed-size private L1 caches and a shared L2 cache on chip. For a fixed die size (240 mm2), we varied the number of cores from 1 to 32. For a given number of cores, we used a (default) configuration based on current CMPs and realistic projections of future CMPs, as process technologies decrease from 90nm to 32nm.Summary of findings. We studied a variety of benchmark programs to show the following findings.For several application classes, PDF enables significant constructive sharing between threads, leading to better utilization of the on-chip caches and reducing off-chip traffic compared to WS. In particular, bandwidth-limited irregular programs and parallel divide-and-conquer programs present a relative speedup of 1.3-1.6X over WS, observing a 13- 41% reduction in off-chip traffic. An example is shown in Figure 1, for parallel merge sort. For each schedule, the number of L2 misses (i.e., the off-chip traffic) is shown on the left and the speed-up over running on one core is shown on the right, for 1 to 32 cores. Note that reducing the offchip traffic has the additional benefit of reducing the power consumption. Moreover, PDF's smaller working sets provide opportunities to power down segments of the cache without increasing the running time. Furthermore, when multiple programs are active concurrently, the PDF version is also less of a cache hog and its smaller working set is more likely to remain in the cache across context switches.For several other applications classes, PDF and WS have roughly the same execution times, either because there is only limited data reuse that can be exploited or because the programs are not limited by off-chip bandwidth. In the latter case, the constructive sharing PDF enables does provide the power and multiprogramming benefits discussed above.Finally, most parallel benchmarks to date, written for SMPs, use such a coarse-grained threading that they cannot exploit the constructive cache behavior inherent in PDF.We find that mechanisms to finely grain multithreaded applications are crucial to achieving good performance on CMPs.","General and reference:0.1,Hardware:0.75,Computer systems organization:1.0,Networks:0.3,Software and its engineering:0.4,Theory of computation:0.2,Mathematics of computing:0.3,Information systems:0.2,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.3,Applied computing:0.3,Social and professional topics:0.1",Computer systems organization,"Computer systems organization: The paper evaluates schedulers for chip multiprocessors (CMPs) and their impact on cache sharing and performance, which is central to computer systems organization. 'Hardware' is relevant but secondary as the focus is on scheduling strategies rather than hardware design.","Architectures:1.0,Dependable and fault-tolerant systems and networks:0.05,Embedded and cyber-physical systems:0.1,Real-time systems:0.05",Architectures,Architectures is directly relevant as the paper evaluates CMP scheduling strategies (PDF vs. WS) for cache optimization. Other options are not discussed in the context of CMP design or scheduling.,"Distributed architectures:0.0,Other architectures:0.0,Parallel architectures:1.0,Serial architectures:0.0",Parallel architectures,"Parallel architectures is relevant because the paper compares scheduling strategies (PDF and WS) for fine-grained multithreading on CMPs, a core topic in parallel computing. Other options like Distributed architectures are irrelevant as the focus is on chip-level parallelism, not distributed systems."
3686,The Slide Simulator: A Facility for the Design and Analysis of Computer Interconnections,"Interconnection design can have a profound effect on the price and performance of a digital system. This paper describes a new simulation facility that is designed to allow the user to describe and simulate the behavior of an interconnected system. The simulator provides the capability to devise, debug, and evaluate digital interconnection schemes. The user first writes a description of the system interconnections using the hardware descriptive language SLIDE. The UNIBUS, for example, has been described in SLIDE. The description is then compiled into SIMULA code, and linked by the user with other SIMULA or SLIDE modules which probabilistically or deterministically modet the hardware that drives the interconnections. The simulation then proceeds under interactive user control.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.9,Networks:0.1,Software and its engineering:0.6,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Computer systems organization,"The Slide Simulator focuses on designing and analyzing computer interconnections, a core 'Computer systems organization' task. 'Software and its engineering' (0.6) is secondary due to the simulation tool's role.","Architectures:0.95,Dependable and fault-tolerant systems and networks:0.3,Embedded and cyber-physical systems:0.2,Real-time systems:0.2",Architectures,Architectures is relevant for interconnection system design. Dependable systems and others are less central to the simulation focus.,"Distributed architectures:0.7,Other architectures:0.2,Parallel architectures:1,Serial architectures:0.3","Parallel architectures,Distributed architectures",Parallel architectures are relevant for interconnection schemes in digital systems. Distributed architectures are applicable to the simulator's design. Serial architectures are less relevant as the paper focuses on general interconnection simulation.
3820,A Parallel Pruned Bit-Reversal Interleaver,"A parallel algorithm and architecture for pruned bit-reversal interleaving (PBRI) are proposed. For a pruned interleaver of size N with mother interleaver size M = 2n ges N, the proposed algorithm interleaves any number x isin [0, N - 1] in at most n - 1 steps, as opposed to x steps using existing PBRI algorithms. A parallel architecture of the proposed algorithm employing simple logic gates and having a short critical path delay is presented. The proposed architecture is valuable in reducing (de-)interleaving latency in emerging wireless standards that employ PBRI channel (de-)interleaving in their PHY layer such as the 3GPP2 ultra mobile broadband standard.","General and reference:0.1,Hardware:0.4,Computer systems organization:0.6,Networks:0.1,Software and its engineering:0.3,Theory of computation:0.1,Mathematics of computing:0.2,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Computer systems organization,"Computer systems organization is highly relevant as the paper presents a parallel architecture for bit-reversal interleaving. Hardware receives moderate relevance due to the design of the architecture, but the primary contribution is the system-level organization.","Architectures:0.9,Dependable and fault-tolerant systems and networks:0.2,Embedded and cyber-physical systems:0.5,Real-time systems:0.3",Architectures,Architectures: Proposes a parallel hardware architecture for interleaving. Embedded and real-time systems are tangential. Dependability is not a primary focus.,"Distributed architectures:0.1,Other architectures:0.1,Parallel architectures:1.0,Serial architectures:0.1",Parallel architectures,Parallel architectures are directly addressed in the paper's focus on parallel algorithm design. Other architecture types are not discussed in the context of this work.
3832,"Exploiting ILP, TLP, and DLP with the polymorphous TRIPS architecture","We describe the polymorphous TRIPS architecture, which can be configured for different granularities and types of parallelism. TRIPS contains mechanisms that enable the processing cores and the on-chip memory system to be configured and combined in different modes for instruction, data, or thread-level parallelism. To adapt to small and large-grain concurrency, the TRIPS architecture contains four out-of-order, 16-wide-issue grid processor cores, which can be partitioned when easily extractable fine-grained parallelism exists. This approach to polymorphism provides better performance across a wide range of application types than an approach in which many small processors are aggregated to run workloads with irregular parallelism. Our results show that high performance can be obtained in each of the three modes-ILP, TLP, and DLP-demonstrating the viability of the polymorphous coarse-grained approach for future microprocessors.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.95,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Computer systems organization,Computer systems organization is highly relevant as the paper presents a novel processor architecture design (TRIPS) for parallelism. Hardware is secondary but less directly connected to the system-level architecture focus.,"Architectures:1.0,Dependable and fault-tolerant systems and networks:0.2,Embedded and cyber-physical systems:0.1,Real-time systems:0.3",Architectures,Architectures is highly relevant as the paper introduces a novel polymorphous processor architecture for parallelism. Other options like Embedded systems or Real-time systems are not discussed as core contributions.,"Distributed architectures:0,Other architectures:0,Parallel architectures:1,Serial architectures:0",Parallel architectures,"Parallel architectures is relevant as the TRIPS architecture is designed for ILP, TLP, and DLP parallelism. Other options are irrelevant as the paper does not discuss serial or distributed architectures specifically."
3927,Design issues in the development of a JAVA-processor for small embedded applications (abstract only),This poster presents some design issues in the development of a JAVA-processor according SUN’s JavaCard 2.0 API for use in small embedded applications which could be realized with FPGAs. We employed this API because threads and garbage collection are not defined within this specification which leads to small area requirements. As our current solution is microcode-based we demonstrate that the footprint of the Java-processor can be reduced when using loosely coupled state machines (a microcode-sequencer and three slave state machines). Each slave state machine can HALT the microcode-sequencer while itself is still running. Furthermore we discuss some architecture details on implementing the stack on such systems as Java machine implementations are stack-based computer architectures.,"General and reference:0.0,Hardware:0.3,Computer systems organization:0.9,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Computer systems organization,Computer systems organization is relevant due to the processor design for embedded systems. Other categories do not focus on hardware architecture.,"Architectures:0.85,Dependable and fault-tolerant systems and networks:0.3,Embedded and cyber-physical systems:0.9,Real-time systems:0.4","Embedded and cyber-physical systems,Architectures","Embedded and cyber-physical systems: The paper discusses Java processor design for embedded applications. Architectures: Focuses on microcode-based architectures and state machine design. Other categories like Dependable systems are less relevant as the paper doesn't address fault tolerance, and Real-time systems are not explicitly discussed.","Distributed architectures:0.2,Embedded systems:1.0,Other architectures:0.3,Parallel architectures:0.2,Robotics:0.1,Sensor networks:0.2,Sensors and actuators:0.1,Serial architectures:0.3,System on a chip:1.0","Embedded systems,System on a chip",Embedded systems is relevant as the paper discusses Java processors for embedded applications. System on a chip is relevant due to the FPGA-based implementation focus. Distributed architectures is less relevant as the paper is about a single-processor design.
4054,Sit-to-stand task on a humanoid robot from human demonstration,"In this work, we perform the challenging task of a humanoid robot standing up from a chair. First we recorded demonstrations of sit-to-stand motions from normal human subjects as well as actors performing stylized standing motions (e.g. imitating an elderly person). Ground contact force information was also collected for these motions, in order to estimate the human's center of mass trajectory. We then mapped the demonstrated motions to the humanoid robot via an inverse kinematics procedure that attempts to track the human's kinematics as well as their center-of-mass trajectory. In order to estimate the robot's center-of-mass position accurately, we additionally used an inertial parameter identification technique that fit mass and center-of-mass link parameters from measured force data. We demonstrate the resulting motions on the Carnegie Mellon/Sarcos hydraulic humanoid robot.","General and reference:0.1,Hardware:0.2,Computer systems organization:0.9,Networks:0.1,Software and its engineering:0.3,Theory of computation:0.1,Mathematics of computing:0.3,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.2,Computing methodologies:0.2,Applied computing:0.3,Social and professional topics:0.1",Computer systems organization,Computer systems organization is relevant for the humanoid robot's system design and control methodologies. Other categories like Computing methodologies or Applied computing are less directly related to the core contribution.,"Architectures:0.1,Dependable and fault-tolerant systems and networks:0.1,Embedded and cyber-physical systems:0.9,Real-time systems:0.8","Embedded and cyber-physical systems,Real-time systems",Embedded and cyber-physical systems is highly relevant for the humanoid robot application. Real-time systems is relevant for the motion control requirements. Other fields like architectures are not central to the core contribution.,"Embedded systems:0.7,Real-time languages:0.3,Real-time operating systems:0.3,Real-time system architecture:0.8,Real-time system specification:0.3,Robotics:1,Sensor networks:0.3,Sensors and actuators:1,System on a chip:0.3","Robotics,Sensors and actuators,Real-time system architecture",Robotics is central to the humanoid robot's task. Sensors and actuators are used for motion tracking. Real-time system architecture is relevant for the inverse kinematics processing. Embedded systems is secondary.
4149,Janus: Optimal Flash Provisioning for Cloud Storage Workloads,"Janus is a system for partitioning the flash storage tier between workloads in a cloud-scale distributed file system with two tiers, flash storage and disk. The file system stores newly created files in the flash tier and moves them to the disk tier using either a First-In-First-Out (FIFO) policy or a Least-Recently-Used (LRU) policy, subject to per-workload allocations. Janus constructs compact metrics of the cacheability of the different workloads, using sampled distributed traces because of the large scale of the system. From these metrics, we formulate and solve an optimization problem to determine the flash allocation to workloads that maximizes the total reads sent to the flash tier, subject to operator-set priorities and bounds on flash write rates. Using measurements from production workloads in multiple data centers using these recommendations, as well as traces of other production workloads, we show that the resulting allocation improves the flash hit rate by 47-76% compared to a unified tier shared by all workloads. Based on these results and an analysis of several thousand production workloads, we conclude that flash storage is a cost-effective complement to disks in data centers.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.9,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Computer systems organization,Computer systems organization is relevant because the paper focuses on optimizing flash storage allocation in cloud systems. Other categories like Software or Networks are irrelevant as the work centers on storage management and system-level optimization.,"Architectures:0.85,Dependable and fault-tolerant systems and networks:0.2,Embedded and cyber-physical systems:0.1,Real-time systems:0.1",Architectures,"Architectures is relevant because the paper introduces Janus, a cloud storage architecture with optimized flash provisioning. Other categories lack direct connection to storage system design.","Distributed architectures:1,Other architectures:0,Parallel architectures:0.5,Serial architectures:0",Distributed architectures,The paper focuses on distributed flash provisioning in cloud storage. Parallel architectures are marginally relevant due to workload partitioning.
4221,Optimal job fragmentation,"It has been recently discovered that on an unreliable server, the job completion time distribution function (df) can be heavy-tailed (HT) even when job size df is light-tailed (LT) [1, 5]. A key to this phenomenon is the RESTART feature where if a job is interrupted in the middle of its processing, the entire job needs to restart from the beginning, i.e., the work that is partially completed is lost. 
 
A standard mechanism for reducing the job completion 
time in an unreliable service environment is checkpointing 
[3, 4, 6]. We view checkpointing as a job fragmentation operation, where the server processes one fragment of the job at a time. If the server becomes unavailable, say due to failure, then only the work corresponding to the fragment being processed at the time of failure is lost. In this paper, we are motivated by the question: Can fragmentation ‘lighten’ the tail df of the completion time? In Section 3, we provide sufficient conditions on the fragmentation policy that gives rise to LT completion time so long as the job size df is LT. We then characterize the optimal fragmentation policy seeking to minimize the expected job completion time. This policy 
requires a priori knowledge of the job size. We then describe a sub-optimal fragmentation policy that is blind to the job size and is provably very close to optimal. We also describe the asymptotic tail behavior of the job completion time df under both policies. Assuming the server unavailability periods are LT, both policies produce LT completion times when the job size df is LT. For the case of regularly varying job size df, the job completion time under both policies is regularly varying with the same degree - this is the lightest possible asymptotic tail behavior (in the degree sense).","General and reference:0.1,Hardware:0.1,Computer systems organization:0.9,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Computer systems organization,Computer systems organization is highly relevant as the paper focuses on job scheduling and resource management in unreliable systems. Other categories like Theory of computation are less directly relevant since the focus is on system-level optimization rather than algorithmic theory.,"Architectures:0.8,Dependable and fault-tolerant systems and networks:1.0,Embedded and cyber-physical systems:0.3,Real-time systems:0.7","Dependable and fault-tolerant systems and networks,Real-time systems",Dependable systems is highly relevant for checkpointing and fault tolerance. Real-time systems is relevant for optimizing job completion times. Architectures is secondary as the focus is on fault tolerance mechanisms rather than hardware design.,"Availability:1,Fault-tolerant network topologies:0,Maintainability and maintenance:1,Processors and memory architectures:0,Real-time languages:0,Real-time operating systems:0,Real-time system architecture:0,Real-time system specification:0,Redundancy:0,Reliability:1,Secondary storage organization:0","Reliability,Maintainability and maintenance",Reliability is relevant due to the focus on improving job completion under server failures. Maintainability and maintenance is relevant for checkpointing as a maintenance strategy. Other categories like fault-tolerant topologies are less directly relevant.
4242,Towards an efficient switch architecture for high-radix switches,"The interconnection network plays a key role in the overall performance achieved by high performance computing systems, also contributing an increasing fraction of its cost and power consumption. Current trends in interconnection network technology suggest that high-radix switches will be preferred as networks will become smaller (in terms of switch count) with the associated savings in packet latency, cost, and power consumption. Unfortunately, current switch architectures have scalability problems that prevent them from being effective when implemented with a high number of ports. In this paper, an efficient and cost-effective architecture for high-radix switches is proposed. The architecture, referred to as partitioned crossbar input queued (PCIQ), relies on three key components: a partitioned crossbar organization that allows the use of simple arbiters and crossbars, a packet-based arbiter, and a mechanism to eliminate the switch-level HOL blocking. Under uniform traffic, maximum switch efficiency is achieved. Furthermore, switch-level HOL blocking is completely eliminated under hot-spot traffic, again delivering maximum throughput. Additionally, PCIQ inherently implements an efficient congestion management technique that eliminates all the network-wide HOL blocking. On the contrary, the previously proposed architectures either show poor performance or they require significantly higher costs than PCIQ (in both components and complexity).","General and reference:0.0,Hardware:0.25,Computer systems organization:1.0,Networks:0.5,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Computer systems organization,"Computer systems organization is relevant for the switch architecture design. Hardware is partially relevant, and Networks is moderately relevant. Other categories are not applicable.","Architectures:1.0,Dependable and fault-tolerant systems and networks:0.2,Embedded and cyber-physical systems:0.3,Real-time systems:0.2",Architectures,"Architectures is directly relevant as the paper proposes a novel high-radix switch architecture (PCIQ) with detailed hardware design considerations. Other categories are rejected because the paper does not focus on fault tolerance, embedded systems, or real-time constraints.","Distributed architectures:0.3,Other architectures:1.0,Parallel architectures:0.4,Serial architectures:0.1",Other architectures,Other architectures (1.0) is directly relevant as the paper proposes a novel high-radix switch architecture (PCIQ). Distributed architectures (0.3) and Parallel architectures (0.4) are less relevant since the paper focuses on a specific switch design rather than broader distributed/parallel concepts. Serial architectures (0.1) is irrelevant.
4555,References to remote mobile objects in Thor,"Thor is a distributed object-oriented database where objects are stored persistently at highly available servers called object repositories, or ORs. In a large Thor system, performance tuning and system reconfiguration dictate that objects must be able to migrate among ORs. The paper describes two schemes for object references that support object migration, one using location-independent names and the other, location-dependent names. The paper analyzes the performance of the two schemes and concludes that location-dependent names are the right choice for systems like Thor, where we want fast access to objects that have migrated.","General and reference:0.0,Hardware:0.0,Computer systems organization:1.0,Networks:0.2,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Computer systems organization,Computer systems organization is highly relevant as the paper discusses distributed object migration schemes in a database system. Networks is marginally relevant due to remote object references but not core to the paper's contribution.,"Architectures:0.0,Dependable and fault-tolerant systems and networks:1.0,Embedded and cyber-physical systems:0.0,Real-time systems:0.2",Dependable and fault-tolerant systems and networks,Dependable and fault-tolerant systems and networks is relevant due to the paper's focus on object migration and system reconfiguration in distributed databases. Real-time systems is less relevant as timing constraints are not emphasized.,"Availability:1.0,Fault-tolerant network topologies:0.7,Maintainability and maintenance:0.2,Processors and memory architectures:0.1,Redundancy:0.3,Reliability:0.4,Secondary storage organization:0.1","Availability,Fault-tolerant network topologies",Availability is key due to the focus on object migration and fast access. Fault-tolerant topologies are relevant for handling communication constraints. Other categories like reliability are secondary to the paper's core focus.
4595,Incorporating implementation overheads in the analysis for the flexible spin-lock model,"The flexible spin-lock model (FSLM) unifies suspension-based and spin-based resource sharing protocols for partitioned fixed-priority preemptive scheduling based real-time multiprocessor platforms. Recent work has been done in defining the protocol for FSLM and providing a schedulability analysis without accounting for the implementation overheads. In this paper, we extend the analysis for FSLM with implementation overheads. Utilizing an initial implementation of FSLM in the OSEK/VDX-compliant Erika Enterprise RTOS on an Altera Nios II platform using 4 soft-core processors, we present an improved implementation. Given the design of the implementation, the overheads are characterized and incorporated in specific terms of the existing analysis. The paper also supplements the analysis with measurement results, enabling an analytical comparison of FSLM with the natively provided multiprocessor stack resource policy (MSRP), which may serve as a guideline for the choice of FSLM or MSRP for a specific application.","General and reference:0.1,Hardware:0.1,Computer systems organization:1.0,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Computer systems organization,Computer systems organization is relevant because the paper addresses real-time multiprocessor scheduling protocols and their implementation in RTOS. Other categories like Hardware or Software are less relevant as the focus is on system-level resource management.,"Architectures:0.2,Dependable and fault-tolerant systems and networks:0.5,Embedded and cyber-physical systems:0.8,Real-time systems:0.9","Real-time systems,Embedded and cyber-physical systems","Real-time systems: Focuses on scheduling for real-time multiprocessor platforms. Embedded and cyber-physical systems: Application in OSEK/VDX-compliant RTOS. 'Dependable systems' is less relevant as the paper is about scheduling, not fault tolerance.","Embedded systems:0.8,Real-time languages:0.1,Real-time operating systems:1,Real-time system architecture:0.3,Real-time system specification:0.2,Robotics:0,Sensor networks:0,Sensors and actuators:0,System on a chip:0.3","Real-time operating systems,Embedded systems",Real-time operating systems: The paper analyzes implementation overheads in the context of real-time OS scheduling. Embedded systems: The implementation on an Altera Nios II platform with soft-core processors directly relates to embedded systems. Other options like Robotics are not discussed.
4650,Coordinating open distributed systems,"Open Distributed Systems are the dominating intellectual issue of the end of this century. Figuring out how to build those systems will become a central issue in distributed system research in the next future. Although CORBA seems to provide all the necessary support to construct those systems, it provides a very limited support to the evolution of requirements in those systems. The main problem is that the description of the elements from which systems are built, and the way in which they are composed are mixed into the application code, making them difficult to understand, modify and customize. We think that a solution to this problem goes through the introduction of the so called coordination models and languages into the CORBA model. We propose in this paper the introduction of our object coordination model called CoLaS into the CORBA model.","General and reference:0.1,Hardware:0.1,Computer systems organization:1.0,Networks:0.3,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Computer systems organization,Computer systems organization is relevant for distributed system architecture coordination. Other categories lack system-level design focus.,"Architectures:0.95,Dependable and fault-tolerant systems and networks:0.0,Embedded and cyber-physical systems:0.0,Real-time systems:0.0",Architectures,Architectures: The paper addresses system architecture design for open distributed systems using coordination models. Other options like Real-time systems are irrelevant as the focus is on architectural evolution rather than timing constraints.,"Distributed architectures:1.0,Other architectures:0.1,Parallel architectures:0.2,Serial architectures:0.1",Distributed architectures,Distributed architectures is directly relevant as the paper focuses on coordination models for open distributed systems. Other architecture types are not discussed.
4652,Operating system support for improving data locality on CC-NUMA compute servers,"The dominant architecture for the next generation of shared-memory multiprocessors is CC-NUMA (cache-coherent non-uniform memory architecture). These machines are attractive as compute servers because they provide transparent access to local and remote memory. However, the access latency to remote memory is 3 to 5 times the latency to local memory. CC-NOW machines provide the benefits of cache coherence to networks of workstations, at the cost of even higher remote access latency. Given the large remote access latencies of these architectures, data locality is potentially the most important performance issue. Using realistic workloads, we study the performance improvements provided by OS supported dynamic page migration and replication. Analyzing our kernel-based implementation, we provide a detailed breakdown of the costs. We show that sampling of cache misses can be used to reduce cost without compromising performance, and that TLB misses may not be a consistent approximation for cache misses. Finally, our experiments show that dynamic page migration and replication can substantially increase application performance, as much as 30%, and reduce contention for resources in the NUMA memory system.","General and reference:0.1,Hardware:0.1,Computer systems organization:1.0,Networks:0.2,Software and its engineering:0.3,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Computer systems organization,Computer systems organization is relevant for NUMA architecture optimization. Other categories lack system-level hardware-software interaction focus.,"Architectures:0.95,Dependable and fault-tolerant systems and networks:0.0,Embedded and cyber-physical systems:0.0,Real-time systems:0.0",Architectures,Architectures: The paper focuses on OS-level optimizations for CC-NUMA architectures to improve data locality. Other options like Real-time systems are irrelevant as the focus is on memory architecture rather than timing constraints.,"Distributed architectures:0.2,Other architectures:0.1,Parallel architectures:1.0,Serial architectures:0.1",Parallel architectures,Parallel architectures is relevant as CC-NUMA is a shared-memory parallel architecture. Distributed architectures are less central here.
4694,A New Relaxed Memory Consistency Model for Shared-Memory Multiprocessors with Parallel-Multithreaded Processing Elements,"The release consistency model is the generally accepted hardware-centric relaxed memory consistency model because of its performance and implementation complexity. By extending the release consistency model, in this paper, we propose a hardware-centric memory consistency model particularly for shared-memory multiprocessor systems with parallel-multithreaded processing elements. The new model uses a new categorization for memory references and utilizes the feature of parallel multithreaded processors (PMPs). We further partition acquire and release references into three sub-categories: one for lock-unlock pairs, one for barrier synchronization, and the last for others. According to the semantic of each synchronization primitive, each sub-category has its own relaxed restrictions. On the other hand, the feature of a PMP is that it is capable of executing more than one thread at the same time, where all parallel threads share only one cache hierarchy. Under the new model, we can use dual write-caches to reduce write traffic and synchronization time. We have used five benchmarks in the SPLASH suite to evaluate the performance gain for the new model. According to the simulation results, the new model is superior to the release consistency model at best by about 11%.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.9,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Computer systems organization,"Computer systems organization: The paper introduces a new memory consistency model for shared-memory multiprocessors, a core topic in systems architecture. Other categories like Networks or Software Engineering are irrelevant as the focus is on system-level memory models.","Architectures:0.95,Dependable and fault-tolerant systems and networks:0.2,Embedded and cyber-physical systems:0.1,Real-time systems:0.1",Architectures,Architectures is highly relevant as the paper focuses on hardware-centric memory consistency models for shared-memory multiprocessors. Other fields do not address memory system design.,"Distributed architectures:0.3,Other architectures:0.0,Parallel architectures:1.0,Serial architectures:0.0",Parallel architectures,Parallel architectures is relevant because the paper introduces a memory consistency model for shared-memory multiprocessors with parallel multithreaded elements. Distributed architectures is less relevant as the focus is on shared-memory rather than distributed systems.
4892,Performance evaluation of scheduling applications with DAG topologies on multiclusters with independent local schedulers,"Before an application modelled as a directed acyclic graph (DAG) is executed on a heterogeneous system, a DAG mapping policy is often enacted. After mapping, the tasks (in the DAG-based application) to be executed at each computational resource are determined. The tasks are then sent to the corresponding resources, where they are orchestrated in the pre-designed pattern to complete the work. Most DAG mapping policies in the literature assume that each computational resource is a processing node of a single processor, i.e. the tasks mapped to a resource are to be run in sequence. Our studies demonstrate that if the resource is actually a cluster with multiple processing nodes, this assumption will cause a misperception in the tasks' execution time and execution order. This will disturb the pre-designed cooperation among tasks so that the expected performance cannot be achieved. In this paper, a DAG mapping algorithm is presented for multicluster architectures. Each constituent cluster in the multicluster is shared by background workload (from other users) and has its own independent local scheduler. The multicluster DAG mapping policy is based on theoretical analysis and its performance is evaluated through extensive experimental studies. The results show that compared with conventional DAG mapping policies, the new scheme that we present can significantly improve the scheduling performance of a DAG-based application in terms of the schedule length","General and reference:0.1,Hardware:0.1,Computer systems organization:0.9,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Computer systems organization,Computer systems organization is relevant because the paper addresses scheduling algorithms for distributed systems (multiclusters). Other categories are less relevant as the focus is on system-level scheduling.,"Architectures:1.0,Dependable and fault-tolerant systems and networks:0.2,Embedded and cyber-physical systems:0.1,Real-time systems:0.3",Architectures,Architectures is highly relevant as the paper focuses on DAG scheduling algorithms for multicluster architectures with independent local schedulers. Other options like Real-time systems lack explicit temporal constraints in the core contribution.,"Distributed architectures:1.0,Other architectures:0.1,Parallel architectures:0.5,Serial architectures:0.1",Distributed architectures,"Distributed architectures: The paper addresses scheduling on multiclusters with distributed execution. Parallel architectures: Marginally relevant due to cluster-level parallelism. Other categories (e.g., Serial architectures) are irrelevant."
4974,Versatile Task Assignment for Heterogeneous Soft Dual-Processor Platforms,"Heterogeneous soft multiprocessor systems are likely to find a larger share in the application-specific computing market due to increasing cost and defect rates in foreseeable manufacturing technologies. We study the problem of mapping streaming applications onto heterogeneous soft dual-processor systems, in which processors' limited memory resources and application throughput form the outstanding constraints and objective, respectively. A key step in the compilation process is task assignment, where tasks are assigned to the processors. We develop a provably-effective algorithm for task assignment. Our algorithm is versatile, in that its formal properties hold for, and hence it is applicable to, a variety of platforms. Measurement of generated code size, and throughput of emulated systems validate the effectiveness of our approach. We advance the state-of-the-art by considerably outperforming two recent competitors in terms of both versatility and application throughput.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.6,Networks:0.1,Software and its engineering:0.3,Theory of computation:0.1,Mathematics of computing:0.2,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.3,Social and professional topics:0.1",Computer systems organization,"Computer systems organization: The paper addresses task assignment in heterogeneous dual-processor systems, a core system architecture problem. Other categories are rejected as the focus is on system-level resource allocation rather than software, algorithms, or applied problems.","Architectures:0.1,Dependable and fault-tolerant systems and networks:0.1,Embedded and cyber-physical systems:0.7,Real-time systems:0.9","Real-time systems,Embedded and cyber-physical systems",Real-time systems is core to task assignment for throughput. Embedded systems are relevant for heterogeneous platforms. Architectures are less directly connected to the algorithmic contribution.,"Embedded systems:1.0,Real-time languages:0.1,Real-time operating systems:0.3,Real-time system architecture:0.5,Real-time system specification:0.2,Robotics:0.4,Sensor networks:0.2,Sensors and actuators:0.3,System on a chip:0.6","Embedded systems,System on a chip",Embedded systems is highly relevant as the paper addresses task assignment in heterogeneous soft multiprocessor systems. System on a chip is also relevant due to the focus on application-specific computing. Other categories like Robotics are less relevant as the paper's focus is on system architecture rather than robotic applications.
5022,A resiliency model for high performance infrastructure based on logical encapsulation,"An emerging trend in distributed systems is the creation of dynamically provisioned heterogeneous high performance platforms that include the co-allocation of both virtualized computing and network attached storage volumes offering NAS and SAN level data services. These high performance computing environments support parallel applications performing traditional file system operations. As with any parallel platform the ability to continue computation in the face of component failures is an important characteristic. Achieving resiliency in heterogeneous environments presents unique challenges and opportunities not found in homogeneous aggregations of computing resources. We present a logical encapsulation model for heterogeneous high performance infrastructure, which enables a reactive resiliency approach for federations of virtual machines and externally hosted physical storage volumes. Asynchronous state capture and restoration models are presented for individual resources, which are composed into non-blocking resiliency models for logical encapsulations. We perform an evaluation that demonstrates our methodology has greater overall flexibility and significant performance improvements when compared to current resiliency approaches in virtualized distributed execution environments.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.8,Networks:0.2,Software and its engineering:0.3,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Computer systems organization,Computer systems organization: The paper focuses on infrastructure resiliency models for distributed systems. Other categories are not directly related to system architecture and fault tolerance.,"Architectures:0.0,Dependable and fault-tolerant systems and networks:1.0,Embedded and cyber-physical systems:0.0,Real-time systems:0.0",Dependable and fault-tolerant systems and networks,Dependable and fault-tolerant systems is highly relevant as the paper presents a resiliency model for distributed systems.,"Availability:0.2,Fault-tolerant network topologies:0.8,Maintainability and maintenance:0.1,Processors and memory architectures:0.1,Redundancy:0.3,Reliability:1.0,Secondary storage organization:0.1","Reliability,Fault-tolerant network topologies",Reliability is central to the resiliency model. Fault-tolerant network topologies are relevant to the heterogeneous infrastructure analysis. Other fields like Redundancy are less directly addressed.
5238,HPC Supported Mission-Critical Cloud Architecture,"Tactical Operations Center (TOC) system in military field is an advanced computer system composed of multiple servers and desktops to interlock internal/external weapon systems processing mission-critical applications in combat situation. However, the current TOC system has several limitations such as difficulty of integrating tactical weapon systems including missile launch system and radar system into the single TOC system due to the heterogeneity of HW and SW between systems, and an inefficient computing resource management for the weapon systems. In this paper, we proposed a novel HPC supported mission-critical Cloud architecture as TOC for Surface-to-Air-Missile (SAM) system with OpenStack Cloud OS, Data Distribution Service (DDS), and GPU virtualization techniques. With this approach, our system provides elastic resource management over the weapon systems with virtual machines, integration of heterogeneous systems with different kinds of guest OS, real-time, reliable, and high-speed communication between the virtual machines and virtualized GPU resource over the virtual machines. Evaluation of our TOC system includes DDS performance measurement over 10Gbps Ethernet and QDR InfiniBand networks on the virtualized environment with OpenStack Cloud OS, and GPU virtualization performance evaluation with two different methods, PCI pass-through and remote-API. With the evaluation results, we conclude that our system provides reasonable performance in the combat situation compared to the previous TOC system while additionally supports scalable and elastic use of computing resource through the virtual machines.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.9,Networks:0.3,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Computer systems organization,"Computer systems organization is highly relevant because the paper focuses on cloud architecture, resource management, and virtualization in mission-critical systems. Other fields like Networks or Applied computing are not central to the core contribution of system design.","Dependable and fault-tolerant systems and networks:0.9,Real-time systems:0.85,Architectures:0.7,Embedded and cyber-physical systems:0.4","Dependable and fault-tolerant systems and networks,Real-time systems",Dependable and fault-tolerant systems and networks: The paper addresses mission-critical cloud architecture with fault-tolerant requirements. Real-time systems: The focus on real-time communication and reliability aligns with real-time systems. Architectures is less directly relevant as the paper emphasizes dependability over architectural design.,"Availability:0.3,Fault-tolerant network topologies:0.6,Maintainability and maintenance:0.2,Processors and memory architectures:0.1,Real-time languages:0.1,Real-time operating systems:0.5,Real-time system architecture:0.7,Real-time system specification:0.2,Redundancy:0.4,Reliability:0.4,Secondary storage organization:0.1","Real-time system architecture,Fault-tolerant network topologies",Real-time system architecture is relevant for the mission-critical cloud design. Fault-tolerant network topologies is relevant for the reliable communication infrastructure. Other categories like real-time operating systems are less central than the architectural and network reliability focus.
5549,A Single-Query Manipulation Planner,"In manipulation tasks, a robot interacts with movable object(s). The configuration space in manipulation planning is thus the Cartesian product of the configuration space of the robot with those of the movable objects. It is the complex structure of such a “composite configuration space” that makes manipulation planning particularly challenging. Previous works approximate the connectivity of the composite configuration space by means of discretization or by creating random roadmaps. Such approaches involve an extensive preprocessing phase, which furthermore has to be redone each time the environment changes. In this letter, we propose a high-level Grasp-Placement Table similar to that proposed by Tournassoud et al. (1987), but which does not require any discretization or heavy pre-processing. The table captures the potential connectivity of the composite configuration space while being specific only to the movable objects: in particular, it does not require to be recomputed when the environment changes. During the query phase, the table is used to guide a tree-based planner that explores the space systematically. Our simulations and experiments show that the proposed method enables improvements in both running time and trajectory quality as compared to existing approaches.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.9,Networks:0.0,Software and its engineering:0.3,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Computer systems organization,"Computer systems organization: The paper presents a manipulation planner for robotic systems, focusing on configuration space analysis and system-level planning. Other categories are rejected because the work is centered on robotic systems and their organization rather than software engineering, networks, or theoretical computation.","Architectures:0.4,Dependable and fault-tolerant systems and networks:0.1,Embedded and cyber-physical systems:0.9,Real-time systems:0.25",Embedded and cyber-physical systems,Embedded and cyber-physical systems receives high score as the paper focuses on robot manipulation planning in physical environments. Architectures is less relevant as the focus is on algorithms rather than system architecture design. Real-time systems is not emphasized in the abstract.,"Robotics:0.9,Sensor networks:0.5,Sensors and actuators:0.4",Robotics,Robotics is highly relevant as the paper introduces a manipulation planning algorithm. 'Sensor networks' is less relevant as the focus is on planning rather than sensor usage.
5637,On Correlating Performance Metrics,"Performance metrics and their measurements are the basis of identifying and addressing computer performance related issues, from monitoring to capacity planning. IT professionals and performance analysts often face the questions of what metrics to look at, whether those metrics are related, and how to group them in meaningful way for further analysis, e.g., workload characterization. In this paper we present a number of techniques for correlating performance metrics, which could be sampled at different rates, at different lengths and overlapping intervals, and with different response times to a common system event. We also discuss how to assess the reliability of the correlations we compute and how to efficiently find relationships of interest.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.7,Networks:0.2,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.2,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.3,Social and professional topics:0.1",Computer systems organization,Computer systems organization is highly relevant because the paper focuses on correlating performance metrics in computer systems for monitoring and capacity planning. Other categories like Mathematics of computing are less relevant as the focus is on practical performance analysis rather than theoretical methods.,"Architectures:0.1,Dependable and fault-tolerant systems and networks:0.8,Embedded and cyber-physical systems:0.05,Real-time systems:0.7","Dependable and fault-tolerant systems and networks,Real-time systems",Dependable systems is relevant for analyzing performance metric reliability. Real-time systems applies to system event response time analysis. Architectures is too general for the correlation techniques discussed.,"Availability:0.5,Fault-tolerant network topologies:0.0,Maintainability and maintenance:1.0,Processors and memory architectures:0.0,Real-time languages:0.0,Real-time operating systems:0.0,Real-time system architecture:0.0,Real-time system specification:0.0,Redundancy:0.0,Reliability:1.0,Secondary storage organization:0.0","Maintainability and maintenance,Reliability",Maintainability and maintenance are relevant as the paper focuses on correlating metrics for system analysis. Reliability is relevant because the work addresses system performance and trustworthiness.
5643,Extreme-scaling applications en route to exascale,"Feedback from the previous year's very successful workshop motivated the organisation of a three-day workshop from 1 to 3 February 2016, during which the 28-rack JUQUEEN BlueGene/Q system with 458 752 cores was reserved for over 50 hours. Eight international code teams were selected to use this opportunity to investigate and improve their application scalability, assisted by staff from JSC Simulation Laboratories and Cross-Sectional Teams. Ultimately seven teams had codes successfully run on the full JUQUEEN system. Strong scalability demonstrated by Code_Saturne and Seven-League Hydro, both using 4 OpenMP threads for 16 MPI processes on each compute node for a total of 1 835 008 threads, qualify them for High-Q Club membership. Existing members CIAO and iFETI were able to show that they had additional solvers which also scaled acceptably. Furthermore, large-scale in-situ interactive visualisation was demonstrated with a CIAO simulation using 458 752 MPI processes running on 28 racks coupled via JUSITU to VisIt. The two adaptive mesh refinement utilities, ICI and p4est, showed that they could respectively scale to run with 458 752 and 971 504 MPI ranks, but both encountered problems loading large meshes. Parallel file I/O issues also hindered large-scale executions of PFLOTRAN. Poor performance of a NEST-import module which loaded and connected 1.9 TiB of neuron and synapse data was tracked down to an internal data-structure mismatch with the HDF5 file objects that prevented use of MPI collective file reading, which when rectified is expected to enable large-scale neuronal network simulations. Comparative analysis is provided to the 25 codes in the High-Q Club at the start of 2016, which includes five codes that qualified from the previous workshop. Despite more mixed results, we learnt more about application file I/O limitations and inefficiencies which continue to be the primary inhibitor to large-scale simulations.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.9,Networks:0.1,Software and its engineering:0.3,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Computer systems organization,Computer systems organization: The paper focuses on extreme-scale application performance and system scalability on supercomputing architectures. Other categories lack direct relevance to the core system organization analysis.,"Architectures:0.95,Dependable and fault-tolerant systems and networks:0.2,Embedded and cyber-physical systems:0.1,Real-time systems:0.1",Architectures,"Architectures: The paper focuses on application scalability on large-scale systems (JUQUEEN), which directly relates to computer architectures. Other categories like fault-tolerant systems are mentioned only incidentally (no focus on dependability).","Distributed architectures:0.8,Other architectures:0.1,Parallel architectures:1,Serial architectures:0.1","Distributed architectures,Parallel architectures",Distributed architectures: The paper discusses scaling on a distributed BlueGene/Q system. Parallel architectures: The focus is on parallel computing for extreme-scale applications. Other categories are irrelevant.
5759,RAID10L: A high performance RAID10 storage architecture based on logging technique,"RAID10 storage system suffers the relatively poor write performance due to the write request must be served by both disks in a mirror set. To address this problem, in this paper we propose a novel RAID10 storage architecture, called RAID10L, which extends the data mirroring redundancy of RAID10 by incorporating a dedicated log disk. The goal of RAID10L is to significantly improve the write performance of RAID10 at some little expense of reliability. In RAID10L both read and write requests are processed in a balance scheme. For every write request, RAID10L keeps two copies of the write data: one in its normal place of data disk chosen by a write balance scheme and the other in the log disk by writing sequentially. The update to another data disk in a mirror set is delayed to the next quiet period between bursts of client activity. Reliability analysis shows that the reliability of RAID10L, in terms of MTTDL (mean time to data loss), is somewhat worse than RAID10 but much better than RAID5. On the other hand, our prototype implementation of RAID10L driven by Iometer benchmark shows that RAID10L outperforms RAID10 by up to 47.1% and RAID0 by 27.3% in terms of average response time. Driven by some real-life traces, RAID10L gains improvement up to 30.7% with an average of 27.7% than RAID10 in terms of average response time.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.9,Networks:0.1,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Computer systems organization,Computer systems organization is highly relevant for storage architecture design and performance analysis. Other categories lack direct connection to storage system innovations.,"Architectures:0.5,Dependable and fault-tolerant systems and networks:1.0,Embedded and cyber-physical systems:0.3,Real-time systems:0.4",Dependable and fault-tolerant systems and networks,Dependable and fault-tolerant systems and networks is highly relevant as the paper focuses on reliability and performance improvements in storage systems.,"Availability:0.9,Fault-tolerant network topologies:0.4,Maintainability and maintenance:0.5,Processors and memory architectures:0.5,Redundancy:1.0,Reliability:0.8,Secondary storage organization:0.7","Redundancy,Availability","Redundancy is directly relevant as the paper introduces a RAID10L architecture with data mirroring. Availability is relevant due to the performance improvements in write operations. Fault-tolerant network topologies was rejected as the focus is on disk storage redundancy, not network topology."
5829,Réseau de cellules intégré : mécanisme de communication inter-cellulaire et application à la simulation logique. (Integrated cell network : inter-cell communication mechanism and application to logical simulation),"Il existe une voie nouvelle differente du schema de calcul, par nature sequentiel de Von Neumann: celle du parallelisme massif. Nous proposons dans cette these une architecture reguliere hautement parallele basee sur un reseau de cellules asynchrones communiquant par messages. Chaque cellule execute une tache simple et integre un mecanisme de communication lui permettant d'echanger des informations avec n'importe quelle autre cellule du reseau. Cette architecture permet d'executer de maniere efficace bon nombre d'algorithmes tres paralleles. Nous avons etudie un accelerateur de simulation logique base sur cette architecture cellulaire. Le principe est d'associer a chaque cellule du reseau un element logique du circuit a simuler. Controlee par un systeme-hote, la simulation se deroule en deux temps: initialisation des cellules du reseau puis execution de l'algorithme reparti dans les cellules. Plusieurs algorithmes de simulation ainsi que differents modes de synchronisation sont presentes. La realisation d'un circuit integrant un reseau 2 x 2 et ses interfaces de communication est decrite. Enfin, une machine prototype de simulation logique basee sur ce circuit utilisant un ordinateur IBM PC/AT comme systeme-hote est presente","General and reference:0.1,Hardware:0.2,Computer systems organization:0.9,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Computer systems organization,Computer systems organization is relevant because the paper discusses a parallel computing architecture for logical simulation. Hardware is also somewhat relevant but less central than the system organization. Other categories like Networks are less relevant as the focus is on parallel processing architecture.,"Architectures:1.0,Dependable and fault-tolerant systems and networks:0.3,Embedded and cyber-physical systems:0.6,Real-time systems:0.2","Architectures,Embedded and cyber-physical systems",Architectures is directly relevant for the parallel processing cell network design. Embedded and cyber-physical systems is relevant for the hardware implementation and simulation application. Real-time systems is less relevant as the focus is on parallel processing rather than timing constraints.,"Distributed architectures:1,Embedded systems:0.3,Other architectures:0,Parallel architectures:1,Robotics:0,Sensor networks:0,Sensors and actuators:0,Serial architectures:0,System on a chip:1","Parallel architectures,Distributed architectures,System on a chip",Parallel architectures: The architecture is designed for massive parallelism. Distributed architectures: The system uses distributed cells communicating via messages. System on a chip: The prototype integrates a 2x2 cell network as a SoC. Categories like Robotics are not related.
5880,Achieving Better Load Balance in Distributed Storage System,"TheCPUprocessingspeedanddiskcapacityare increasing tremendously during the pastdecade. However, the evenfaster increasing numberof users generateshigher requirementsfor high performanceand huge capacity computer systems.More andmore applications are now running ondistributedsystems. Loadbalanceis animportant issuein distributedstorage systems.Currently, very few of themhas a finelytuned load balance scheme to achieve higher system throughput and shorter client responsetimes. We proposea new loadbalanceschemecalledCooperativeLoad BalanceScheme(CLBS)to solvethis problem.In CLBS, servers cooperatewith a client in its decisionof thenext requestby sendingtheir currentworkloadinformation to theclient everysmallamount of time. With theassistance of servers, a client can alwayschoose the light-loaded server . We did trace driven simulationexperimentsto compare CLBS with the schemeusedin a traditional distributedstorage system.Theresultsshowa shorteraverageresponsetimeanda smalleraveragequeuesizeunder bothlight andheavy workloads.","General and reference:0.25,Hardware:0.25,Computer systems organization:1.0,Networks:0.5,Software and its engineering:0.25,Theory of computation:0.25,Mathematics of computing:0.25,Information systems:0.25,Security and privacy:0.25,Human-centered computing:0.25,Computing methodologies:0.25,Applied computing:0.25,Social and professional topics:0.25",Computer systems organization,The paper focuses on load balancing algorithms in distributed storage systems (Computer systems organization). Networks is relevant due to distributed nature but not central. Other categories are irrelevant.,"Architectures:0.8,Dependable and fault-tolerant systems and networks:0.75,Real-time systems:0.5,Embedded and cyber-physical systems:0.4","Architectures,Dependable and fault-tolerant systems and networks",Architectures: Proposes a load balancing architecture for distributed storage. Dependable systems: Aims to improve system throughput and fault tolerance. 'Real-time systems' is less central.,"Availability:0.7,Distributed architectures:1.0,Fault-tolerant network topologies:0.4,Maintainability and maintenance:0.3,Other architectures:0.5,Parallel architectures:0.5,Processors and memory architectures:0.3,Redundancy:0.7,Reliability:0.5,Secondary storage organization:0.4,Serial architectures:0.2","Distributed architectures,Availability",Distributed architectures are central to the proposed load-balancing scheme. Availability is secondary due to improved system throughput. Other categories address unrelated design aspects.
2190,Hierarchical and Shared Key Assignment,"Access control ensures that only the authorized users of a system are allowed to access certain resources or tasks. Usually, according to their powers and responsibilities, users are organized in hierarchies formed by a certain number of disjoint classes. Such hierarchies are implemented by assigning a key to each class, so that the keys for descendant classes can be derived efficiently from classes higher in the hierarchy. However, in many cases the only hierarchical access represents a limitation. Indeed, sometimes it might be useful or even necessary to ensure the access to a resource or task to both his direct responsible (or owner) and a group of users possessing certain credentials. In this paper, we first propose a novel model that extends the conventional hierarchical access and allows such access even by certain sets of qualified users. Afterwards, we propose a construction for hierarchical key assignment schemes in this new model. The proposed construction is provably secure with respect to key indistinguishability and relies on both symmetric encryption and perfect secret sharing.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.2,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:1.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Security and privacy,"Security and privacy: The paper introduces a hierarchical key assignment scheme with security proofs, which is central to cryptographic access control. Software and its engineering receives a low score as the focus is on the security model rather than software implementation.","Cryptography:0.9,Database and storage security:0.3,Formal methods and theory of security:0.1,Human and societal aspects of security and privacy:0.1,Intrusion/anomaly detection and malware mitigation:0.1,Network security:0.1,Security in hardware:0.1,Security services:0.1,Software and application security:0.1,Systems security:0.1",Cryptography,Cryptography is relevant as the paper proposes a hierarchical key assignment scheme. Other categories like Database and storage security are less directly tied to the cryptographic construction.,"Cryptanalysis and other attacks:0,Information-theoretic techniques:1,Key management:1,Mathematical foundations of cryptography:0,Public key (asymmetric) techniques:0,Symmetric cryptography and hash functions:1","Key management,Symmetric cryptography and hash functions,Information-theoretic techniques",Key management is central to the paper's hierarchical access control model. Symmetric cryptography is used for encryption in the key assignment scheme. Information-theoretic techniques are applied through perfect secret sharing. Other categories like public key or cryptanalysis are not discussed.
1636,"Measuring Risk: Computer Security Metrics, Automation, and Learning","Risk management is widely seen as the basis for cybersecurity in contemporary organizations, but practitioners continue to dispute its value. This article analyzes debate over computer security risk management in the 1970s and 1980s United States, using this debate to enhance our understanding of the value of computer security metrics more generally. Regulators placed a high value on risk analysis and measurement because of their association with objectivity, control, and efficiency. However, practitioners disputed the value of risk analysis, questioning the final measurement of risk. The author argues that computer security risk management was most valuable not because it provided an accurate measure of risk, but because the process of accounting for risks could contribute to organizational learning. Unfortunately, however, organizations were sorely tempted to go through the motions of risk management without engaging in the more difficult process of learning.","General and reference:0.2,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.3,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.2,Security and privacy:0.9,Human-centered computing:0.4,Computing methodologies:0.2,Applied computing:0.3,Social and professional topics:0.1",Security and privacy,"Security and privacy is highly relevant as the paper analyzes computer security risk management, a core aspect of cybersecurity. Other fields like Software and its engineering or Applied computing are less central since the focus is on risk management processes rather than technical implementation or specific applications.","Cryptography:0.1,Database and storage security:0.0,Formal methods and theory of security:0.0,Human and societal aspects of security and privacy:0.9,Intrusion/anomaly detection and malware mitigation:0.0,Network security:0.1,Security in hardware:0.0,Security services:0.0,Software and application security:0.0,Systems security:0.0",Human and societal aspects of security and privacy,Human and societal aspects of security and privacy: The paper analyzes organizational learning in security risk management and debates the social value of security metrics. Other categories like Cryptography (0.1) are less central to the paper's focus on societal and institutional factors.,"Economics of security and privacy:1.0,Privacy protections:0.4,Social aspects of security and privacy:0.6,Usability in security and privacy:0.3",Economics of security and privacy,Economics of security and privacy: The paper analyzes the value of risk metrics and organizational learning in cybersecurity economics. Other categories like 'Privacy protections' are not explicitly discussed.
866,An XPath-based preference language for P3P,"The Platform for Privacy Preferences (P3P) is the most significant effort currently underway to enable web users to gain control over their private information. The designers of P3P simultaneously designed a preference language called APPEL to allow users to express their privacy preferences, thus enabling automatic matching of privacy preferences against P3P policies. Unfortunately subtle interactions between P3P and APPEL result in serious problems when using APPEL: Users can only directly specify what is unacceptable in a policy, not what is acceptable; simple preferences are hard to express; and writing APPEL preferences is error prone. We show that these problems follow from a fundamental design choice made by APPEL, and cannot be solved without completely redesigning the language. Therefore we explore alternatives to APPEL that can overcome these problems. In particular, we show that XPath serves quite nicely as a preference language and solves all the above problems. We identify the minimal subset of XPath that is needed, thus allowing matching programs to potentially use a smaller memory footprint. We also give an APPEL to XPath translator that shows that XPath is as expressive as APPEL.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.9,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Security and privacy,"Security and privacy is highly relevant as the paper addresses privacy preference languages for P3P. Computing methodologies is moderately relevant for XPath implementation, but the core focus is on privacy policy modeling.","Formal methods and theory of security:0.85,Software and application security:0.8,Cryptography:0.3,Security services:0.2,Human and societal aspects of security and privacy:0.6,Database and storage security:0.1,Intrusion/anomaly detection and malware mitigation:0.1,Network security:0.2,Security in hardware:0.1","Formal methods and theory of security,Software and application security,Human and societal aspects of security and privacy","Formal methods and theory of security: The paper analyzes APPEL's design flaws and proposes XPath as a formal alternative. Software and application security: The focus is on privacy preference languages for web applications. Human and societal aspects of security and privacy: The study addresses user control over privacy policies. Other children (e.g., Cryptography, Network security) are irrelevant as the work is about policy representation rather than cryptographic protocols.","Domain-specific security and privacy architectures:0.4,Economics of security and privacy:0.1,Formal security models:0.3,Logic and verification:0.2,Privacy protections:1.0,Security requirements:0.3,Social aspects of security and privacy:0.2,Social network security and privacy:0.1,Software reverse engineering:0.1,Software security engineering:0.3,Trust frameworks:0.2,Usability in security and privacy:0.5,Web application security:0.6","Privacy protections,Web application security",Privacy protections is directly relevant as the paper focuses on privacy preference languages. Web application security is relevant since the work involves P3P (a web privacy protocol). Formal models and social aspects are less central to the paper's contribution.
1394,Measuring Information Security Awareness - A West Africa Gold Mining Environment Case,"AngloGold Ashanti is an international gold mining company that has recently implemented an information security awareness program worldwide at all of their operations. Following the implementation, there was a normal business need to evaluate and measure the success and effectiveness of the program. A measuring tool that can be applied globally and that addressed AngloGold Ashanti’s unique requirements was developed and applied at the mining sites located in the West Africa region. The objective of this paper is, firstly, to give a brief overview on the measuring tool developed and, secondly to report on the application and results in the West Africa region.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:1.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Security and privacy,Security and privacy is directly relevant for evaluating information security awareness programs. Other categories lack connection to security policy evaluation or organizational implementation.,"Cryptography:0.05,Database and storage security:0.05,Formal methods and theory of security:0.05,Human and societal aspects of security and privacy:0.85,Intrusion/anomaly detection and malware mitigation:0.05,Network security:0.05,Security in hardware:0.05,Security services:0.05,Software and application security:0.05,Systems security:0.05",Human and societal aspects of security and privacy,The paper evaluates a security awareness program's societal impact. Other categories like network security are not addressed.,"Economics of security and privacy:0,Privacy protections:0,Social aspects of security and privacy:1,Usability in security and privacy:1","Social aspects of security and privacy,Usability in security and privacy",Social aspects of security and privacy is relevant as the paper evaluates a security awareness program in a real-world environment. Usability in security and privacy is relevant because the tool's effectiveness depends on user engagement and adoption.
5371,Cyber Threat Trend Analysis Model Using HMM,"Prevention is normally recognized as one of the best defense strategy against malicious hackers or attackers. The desire of deploying better prevention mechanisms has motivated many security researchers and practitioners, who are studies threat trend analysis models. However, threat trend is not directly revealed from the time-series data because the trend is implicit in its nature. Besides, traditional time-series analysis, which predicts the future trend pattern by relying exclusively on the past trend pattern, is not appropriate for predicting a trend pattern in dynamic network environments (e.g., the Internet). Thus, supplemental environmental information is required to uncover a trend pattern from the implicit (or hidden) raw data. In this paper, we propose cyber threat trend analysis model using hidden Markov model (HMM) by incorporating the supplemental environmental information into the trend analysis.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.9,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Security and privacy,Security and privacy is highly relevant as the paper proposes a threat trend analysis model for cyber security. Other categories like Mathematics of computing are less relevant as the focus is on security application rather than mathematical theory.,"Cryptography:0.1,Database and storage security:0.1,Formal methods and theory of security:0.1,Human and societal aspects of security and privacy:0.1,Intrusion/anomaly detection and malware mitigation:0.7,Network security:0.6,Security in hardware:0.1,Security services:0.1,Software and application security:0.2,Systems security:0.3","Intrusion/anomaly detection and malware mitigation,Network security",Intrusion/anomaly detection is relevant for threat trend analysis. Network security is relevant for HMM-based cyber threat modeling.,"Intrusion detection systems:1.0,Malware and its mitigation:0.6,Security protocols:0.4,Denial-of-service attacks:0.3,Firewalls:0.2,Mobile and wireless security:0.5,Social engineering attacks:0.1,Web protocol security:0.2","Intrusion detection systems,Malware and its mitigation",Intrusion detection systems is directly relevant as the paper uses HMM for threat trend analysis. Malware mitigation is relevant due to the focus on threat prevention. Other categories like Firewalls or Web protocol security are rejected as the work does not address specific security mechanisms or protocols.
2196,Enabling Data Hiding for Resource Sharing in Cloud Computing Environments Based on DNA Sequences,"The main target of this paper is to propose an algorithm to implement data hiding in DNA sequences to increase the confidentiality and complexity by using software point of view in cloud computing environments. By utilizing some interesting features of DNA sequences, the implementation of a data hiding is applied in cloud. The algorithm which has been proposed here is based on binary coding and complementary pair rules. Therefore, DNA reference sequence is chosen and a secret data M is hidden into it as well. As result of applying some steps, M´´´ is come out to upload to cloud environments. The process of identifying and extracting the original data M, hidden in DNA reference sequence begins once clients decide to use data. Furthermore, security issues are demonstrated to inspect the complexity of the algorithm.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:1.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Security and privacy,"Security and privacy: The paper introduces a data-hiding technique for cloud computing using DNA sequences, which is a direct contribution to cryptographic and secure resource-sharing methods. Other categories like Applied computing are less relevant as the focus is on security mechanisms.","Cryptography:0.8,Database and storage security:0.2,Formal methods and theory of security:0.3,Human and societal aspects of security and privacy:0.1,Intrusion/anomaly detection and malware mitigation:0.1,Network security:0.2,Security in hardware:0.1,Security services:0.6,Software and application security:0.4,Systems security:0.3","Cryptography,Security services",Cryptography is highly relevant as the paper presents a novel data hiding algorithm. Security services receives moderate relevance for the cloud computing context. Other security categories are less relevant as the paper focuses on data hiding rather than network security or application security.,"Access control:0.5,Authentication:0,Authorization:0.5,Cryptanalysis and other attacks:0,Digital rights management:0,Information-theoretic techniques:0.5,Key management:0,Mathematical foundations of cryptography:0,Privacy-preserving protocols:0,Pseudonymity, anonymity and untraceability:0,Public key (asymmetric) techniques:0,Symmetric cryptography and hash functions:1","Symmetric cryptography and hash functions,Access control,Authorization",Symmetric cryptography is used for data hiding in DNA sequences. Access control and authorization are secondary concerns for cloud confidentiality. Other cryptographic categories are not directly applied.
2851,A Non-Blind Watermarking Scheme for Gray Scale Images in Discrete Wavelet Transform Domain using Two Subbands,"Digital watermarking is the process to hide digital pattern directly into a digital content. Digital watermarking techniques are used to address digital rights management, protect information and conceal secrets. An invisible non-blind watermarking approach for gray scale images is proposed in this paper. The host image is decomposed into 3-levels using Discrete Wavelet Transform. Based on the parent-child relationship between the wavelet coefficients the Set Partitioning in Hierarchical Trees (SPIHT) compression algorithm is performed on the LH3, LH2, HL3 and HL2 subbands to find out the significant coefficients. The most significant coefficients of LH2 and HL2 bands are selected to embed a binary watermark image. The selected significant coefficients are modulated using Noise Visibility Function, which is considered as the best strength to ensure better imperceptibility. The approach is tested against various image processing attacks such as addition of noise, filtering, cropping, JPEG compression, histogram equalization and contrast adjustment. The experimental results reveal the high effectiveness of the method.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:1.0,Human-centered computing:0.0,Computing methodologies:0.25,Applied computing:0.0,Social and professional topics:0.0",Security and privacy,Security and privacy: The paper presents a digital watermarking scheme for content protection. Computing methodologies has limited relevance to DWT techniques.,"Cryptography:0.5,Database and storage security:0.1,Formal methods and theory of security:0.1,Human and societal aspects of security and privacy:0.1,Intrusion/anomaly detection and malware mitigation:0.1,Network security:0.1,Security in hardware:0.1,Security services:0.9,Software and application security:0.1,Systems security:0.8","Security services,Systems security",Security services (0.9) is relevant as the paper introduces a non-blind watermarking scheme for protecting digital content. Systems security (0.8) applies because the method ensures data integrity and robustness against attacks. Other fields like Cryptography are less relevant as the focus is on watermarking techniques rather than encryption.,"Access control:0,Authentication:0,Authorization:0,Browser security:0,Denial-of-service attacks:0,Digital rights management:1,Distributed systems security:0,File system security:0,Firewalls:0,Information flow control:0,Operating systems security:0,Privacy-preserving protocols:0,Pseudonymity, anonymity and untraceability:0,Vulnerability management:0",Digital rights management,Digital rights management is directly relevant as the paper presents a watermarking technique for content protection. Other security domains like access control or operating system security are not discussed.
4629,A template approach for group key distribution in dynamic ad-hoc groups,"The ubiquitous wireless communication medium poses serious threats to the confidentiality and integrity of communication in constrained networks like Internet-of-Things (IoT), sensor nodes or ad-hoc groups of soldiers on a battlefield. Data encryption is essential for ensuring the confidentiality and integrity of the wireless network communication. We focus on the problem of group key distribution in dynamically formed groups of network nodes. One major challenge is that the neighborhood of a node is not known prior to deployment and therefore, the group key establishment protocol needs to be independent of the physical network topology. In this paper, we describe an efficient template based approach for group key establishment using only shared symmetric secrets. Our template is a logical shared secret distribution built on the network nodes prior to deployment, which ensures that any node shares a distinct set of secrets with any dynamic group of nodes and thus, is able to establish a secure group key. We illustrate our template using two secret distribution protocols: sub-set and dual one-way hash chain distributions. Compared to existing algorithms, our template approach gives flexibility to the network administrator to choose from different secret distribution protocols for fine-grained control over the security levels and performance.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.3,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.9,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Security and privacy,Security and privacy: Focuses on secure group key distribution for dynamic ad-hoc networks. 'Networks' is secondary to the security protocol's primary contribution.,"Cryptography:0.9,Database and storage security:0.1,Formal methods and theory of security:0.2,Human and societal aspects of security and privacy:0.1,Intrusion/anomaly detection and malware mitigation:0.1,Network security:0.3,Security in hardware:0.1,Security services:0.2,Software and application security:0.1,Systems security:0.1",Cryptography,Cryptography is directly relevant for the group key distribution protocol. Network security is less relevant as the focus is on cryptographic methods rather than network infrastructure security.,"Cryptanalysis and other attacks:0,Information-theoretic techniques:0,Key management:0.9,Mathematical foundations of cryptography:0,Public key (asymmetric) techniques:0,Symmetric cryptography and hash functions:0.7","Key management,Symmetric cryptography and hash functions",Key management is relevant for the group key distribution problem. Symmetric cryptography and hash functions is relevant for the secret distribution protocols discussed. Other categories are less relevant as the focus is on key establishment rather than attacks or mathematical foundations.
3472,Secure distributed DNS,"A correctly working domain name system (DNS) is essential for the Internet. Due to its significance and because of deficiencies in its current design, the DNS is vulnerable to a wide range of attacks. This paper presents the design and implementation of a secure distributed name service on the level of a DNS zone. Our service is able to provide fault tolerance and security even in the presence of a fraction of corrupted name servers, avoiding any single point of failure. It further solves the problem of storing zone secrets online without leaking them to a corrupted server, while still supporting secure dynamic updates. Our service uses state-machine replication and threshold cryptography. We present results from experiments performed using a prototype implementation on the Internet in realistic setups. The results show that our design achieves the required assurances while servicing the most frequent requests in reasonable time.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.9,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Security and privacy,Security and privacy is relevant for the secure DNS design using state-machine replication. Other categories like Networks are not the primary focus.,"Cryptography:0.8,Database and storage security:0.1,Formal methods and theory of security:0.2,Human and societal aspects of security and privacy:0.1,Intrusion/anomaly detection and malware mitigation:0.1,Network security:0.85,Security in hardware:0.1,Security services:0.2,Software and application security:0.1,Systems security:0.1","Cryptography,Network security",Cryptography: Use of threshold cryptography for secure dynamic updates. Network security: Design of a secure distributed DNS service.,"Cryptanalysis and other attacks:0,Denial-of-service attacks:0,Firewalls:0,Information-theoretic techniques:0,Key management:0,Mathematical foundations of cryptography:0.5,Mobile and wireless security:0,Public key (asymmetric) techniques:0.5,Security protocols:1,Symmetric cryptography and hash functions:0.3,Web protocol security:1","Security protocols,Web protocol security",Security protocols are central to the distributed DNS design. Web protocol security applies to DNS service. Other categories like Public key techniques are secondary.
1153,Rationale for the RBAC96 family of access control models,"A family of role-based access control (RBAC) models, referred to here as the RBAC96 models, was recently published by the author and his colleagues. This paper gives our rationale for the major decisions in developing these models and discusses alternatives that were considered.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.2,Security and privacy:0.8,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Security and privacy,Security and privacy is highly relevant as the paper discusses access control models (RBAC96) which are fundamental to information security. Other categories like Software engineering are not the primary focus.,"Cryptography:0.2,Database and storage security:0.3,Formal methods and theory of security:0.8,Human and societal aspects of security and privacy:0.1,Intrusion/anomaly detection and malware mitigation:0.2,Network security:0.3,Security in hardware:0.2,Security services:0.7,Software and application security:0.3,Systems security:0.4","Formal methods and theory of security,Security services",Formal methods and theory of security is relevant as the paper presents theoretical RBAC models. Security services is relevant because access control is a fundamental security service. Other options like Network security were rejected because the paper focuses on access control theory rather than network-specific security.,"Access control:1,Authentication:0.1,Authorization:1,Digital rights management:0.2,Formal security models:0.3,Logic and verification:0.2,Privacy-preserving protocols:0.1,Pseudonymity, anonymity and untraceability:0.1,Security requirements:0.2,Trust frameworks:0.3","Access control,Authorization",Access control: The paper directly addresses role-based access control (RBAC) models. Authorization: RBAC inherently involves authorization mechanisms. Other categories like formal security models or privacy-preserving protocols are less relevant as the paper focuses on model rationale rather than formal verification or privacy.
4046,User-Controllable Security and Privacy for Pervasive Computing,"We describe our current work in developing novel mechanisms for managing security and privacy in pervasive computing environments. More specifically, we have developed and evaluated three different applications, including a contextual instant messenger, a people finder application, and a phone-based application for access control. We also draw out some themes we have learned thus far for user-controllable security and privacy.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:1.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Security and privacy,Security and privacy is directly relevant because the paper presents novel security and privacy mechanisms for pervasive computing environments. Other categories like Human-centered computing are less relevant as the focus is on technical security solutions rather than user interface or experience design.,"Cryptography:0.1,Database and storage security:0.1,Formal methods and theory of security:0.1,Human and societal aspects of security and privacy:0.9,Intrusion/anomaly detection and malware mitigation:0.1,Network security:0.1,Security in hardware:0.1,Security services:0.1,Software and application security:0.2,Systems security:0.1",Human and societal aspects of security and privacy,Human and societal aspects of security and privacy is directly relevant as the paper focuses on user-controllable privacy mechanisms in pervasive computing. Other security categories like Network security are less central to the user-centric focus.,"Privacy protections:1.0,Usability in security and privacy:0.8,Social aspects of security and privacy:0.3,Economics of security and privacy:0.2","Privacy protections,Usability in security and privacy","Privacy protections: The paper introduces novel user-controllable privacy mechanisms. Usability in security and privacy: Applications emphasize user control, implying usability considerations. Social aspects are less central than technical mechanisms."
1159,Logic based authorization program and its implementation,"In this paper, we describe a logic based authorization program and its implementation. Weighted Delegatable Authorization Program (WDAP) is a logic based framework supporting weighted authorizations and weighted administrative privilege delegations in a decentralised access control system. In this paper we describe various aspects about WDAP and show how it can be used to specify complex security policies. The access control policy is also given. The program Smodels is a widely used system that implements the answer set semantics for extended logic programs. We show how to use Smodels to implement WDAP.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.3,Networks:0.2,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.85,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Security and privacy,Security and privacy is the most relevant category as the paper introduces a logic-based authorization framework. Other categories like Software and its engineering are less relevant as the focus is on security policy modeling rather than software development practices.,"Cryptography:0.1,Database and storage security:0.1,Formal methods and theory of security:0.9,Human and societal aspects of security and privacy:0.1,Intrusion/anomaly detection and malware mitigation:0.1,Network security:0.1,Security in hardware:0.1,Security services:0.1,Software and application security:0.8,Systems security:0.1","Formal methods and theory of security,Software and application security",Formal methods and theory of security is relevant for the logic-based framework. Software and application security is relevant for implementing authorization policies. Other categories like cryptography or network security are less central.,"Domain-specific security and privacy architectures:0.8,Formal security models:0.9,Logic and verification:0.9,Security requirements:0.3,Social network security and privacy:0.1,Software reverse engineering:0.2,Software security engineering:0.5,Trust frameworks:0.4,Web application security:0.3","Formal security models,Logic and verification,Domain-specific security and privacy architectures",Formal security models: The paper presents a logic-based authorization framework. Logic and verification: Uses logic programming (Smodels) for implementation. Domain-specific security and privacy architectures: Discusses a specific authorization framework. Other categories: Less directly relevant as they focus on different aspects of security.
2702,Smart cheaters do prosper: defeating trust and reputation systems,"Traders in electronic marketplaces may behave dishonestly, cheating other agents. A multitude of trust and reputation systems have been proposed to try to cope with the problem of cheating. These systems are often evaluated by measuring their performance against simple agents that cheat randomly. Unfortunately, these systems are not often evaluated from the perspective of security---can a motivated attacker defeat the protection? Previously, it was argued that existing systems may suffer from vulnerabilities that permit effective, profitable cheating despite the use of the system. In this work, we experimentally substantiate the presence of these vulnerabilities by successfully implementing and testing a number of such 'attacks', which consist only of sequences of sales (honest and dishonest) that can be executed in the system. This investigation also reveals two new, previously-unnoted cheating techniques. Our success in executing these attacks compellingly makes a key point: security must be a central design goal for developers of trust and reputation systems.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.9,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Security and privacy,Security and privacy is directly relevant for analyzing vulnerabilities in trust systems. Other categories lack focus on security threats.,"Cryptography:0.2,Database and storage security:0.1,Formal methods and theory of security:0.1,Human and societal aspects of security and privacy:0.8,Intrusion/anomaly detection and malware mitigation:0.1,Network security:0.1,Security in hardware:0.1,Security services:0.9,Software and application security:0.7,Systems security:0.1","Security services,Software and application security","Security services: The paper analyzes vulnerabilities in trust systems, which is a core security service challenge. Software and application security: The attacks target software-based reputation systems. Other categories like Cryptography are less relevant as the focus is on system design flaws rather than cryptographic techniques.","Access control:0,Authentication:0,Authorization:0,Digital rights management:0,Domain-specific security and privacy architectures:0.3,Privacy-preserving protocols:0,Pseudonymity, anonymity and untraceability:0,Social network security and privacy:1,Software reverse engineering:0,Software security engineering:0.5,Web application security:1","Social network security and privacy,Web application security","Social network security and privacy is relevant because the paper analyzes vulnerabilities in trust/reputation systems (a core security mechanism in social/e-commerce systems). Web application security is relevant since the attacks target web-based marketplace systems. Domain-specific security architectures (0.3) is partially relevant as the paper discusses system design flaws, but it's secondary. Authorization/access control are irrelevant as the paper doesn't focus on access management."
214,A Node-failure-resilient Anonymous Communication Protocol through Commutative Path Hopping,"With rising concerns on user privacy over the Internet, anonymous communication systems that hide the identity of a participant from its partner or third parties are highly desired. Existing approaches either rely on a relative small set of pre-selected relay servers to redirect the messages, or use structured peer-to-peer systems to multicast messages among a set of relay groups. The pre-selection approaches provide good anonymity, but suffer from node failures and scalability problem. The peer-to-peer approaches are subject to node churns and high maintenance overhead, which are the intrinsic problems of P2P systems. In this paper, we present CAT, a node-failure-resilient anonymous communication protocol. In this protocol, relay servers are randomly assigned to relay groups. The initiator of a connection selects a set of relay groups instead of relay servers to set up anonymous paths. A valid path consists of relay servers, one from each selected relay group. The initiator explores valid anonymous paths via a probing process. Since the relative positions of relay servers in the path are commutative, there exist multiple anonymous yet commutative paths, which form an anonymous tunnel. When a connection encounters a node failure, it quickly switches to a nearest backup path in the tunnel through ``path hopping'''', without tampering the initiator or renegotiating the keys. Hence, the protocol is resilient to node failures. We also show that the protocol provides good anonymity even when facing types of active and passive attacks. Finally, the operating cost of CAT is analyzed and shown to be similar to other node-based anonymous communication protocols.","General and reference:0.1,Hardware:0.2,Computer systems organization:0.3,Networks:0.6,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.2,Information systems:0.1,Security and privacy:0.9,Human-centered computing:0.1,Computing methodologies:0.3,Applied computing:0.2,Social and professional topics:0.1",Security and privacy,"Security and privacy is highly relevant as the paper introduces an anonymous communication protocol with node-failure resilience, addressing anonymity and security against active/passive attacks. Other categories like Networks (0.6) are only peripherally relevant for the communication infrastructure, while others like Computer systems organization or Hardware are not core to the protocol's design.","Cryptography:0.6,Database and storage security:0.1,Formal methods and theory of security:0.2,Human and societal aspects of security and privacy:0.5,Intrusion/anomaly detection and malware mitigation:0.1,Network security:0.9,Security in hardware:0.1,Security services:0.3,Software and application security:0.1,Systems security:0.1","Network security,Human and societal aspects of security and privacy,Cryptography",Network security is highly relevant as the paper presents an anonymous communication protocol. Human and societal aspects of security and privacy are relevant due to the focus on user privacy and anonymity. Cryptography is relevant for the security mechanisms described. Other categories like database security or malware mitigation are not central to the protocol's core contribution.,"Security protocols:1,Privacy protections:1,Cryptanalysis and other attacks:0.3,Denial-of-service attacks:0.2,Economics of security and privacy:0.1,Firewalls:0.1,Information-theoretic techniques:0.2,Key management:0.2,Mathematical foundations of cryptography:0.3,Mobile and wireless security:0.4,Public key (asymmetric) techniques:0.2,Social aspects of security and privacy:0.2,Symmetric cryptography and hash functions:0.2,Usability in security and privacy:0.3,Web protocol security:0.3","Security protocols,Privacy protections",Security protocols: CAT is an anonymous communication protocol designed for node failure resilience. Privacy protections: The protocol ensures anonymity by hiding participant identities. Other categories like Cryptanalysis are not discussed in the paper.
4170,A secure and robust digital signature scheme for JPEG2000 image authentication,"In this paper, we present a secure and robust content-based digital signature scheme for verifying the authenticity of JPEG2000 images quantitatively, in terms of a unique concept named lowest authenticable bit rates (LABR). Given a LABR, the authenticity of the watermarked JPEG2000 image will be protected as long as its final transcoded bit rate is not less than the LABR. The whole scheme, which is extended from the crypto data-based digital signature scheme, mainly comprises signature generation/verification, error correction coding (ECC) and watermark embedding/extracting. The invariant features, which are generated from fractionalized bit planes during the procedure of embedded block coding with optimized truncation in JPEG2000, are coded and signed by the sender's private key to generate one crypto signature (hundreds of bits only) per image, regardless of the image size. ECC is employed to tame the perturbations of extracted features caused by processes such as transcoding. Watermarking only serves to store the check information of ECC. The proposed solution can be efficiently incorporated into the JPEG2000 codec (Part 1) and is also compatible with Public Key Infrastructure. After detailing the proposed solution, system performance on security as well as robustness will be evaluated.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.9,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Security and privacy,"Security and privacy is highly relevant as the paper proposes a digital signature scheme for image authentication. Other categories are rejected because the core contribution is cryptographic security, not systems or algorithms.","Cryptography:1.0,Security services:0.8,Database and storage security:0.6,Other options:0.0",Cryptography,Cryptography is core to the digital signature scheme. Security services relevance is secondary. Database security is not a primary focus.,"Cryptanalysis and other attacks:0.5,Information-theoretic techniques:0,Key management:0,Mathematical foundations of cryptography:0.5,Public key (asymmetric) techniques:1,Symmetric cryptography and hash functions:0","Public key (asymmetric) techniques,Mathematical foundations of cryptography",Public key (asymmetric) techniques is relevant due to the private key-based digital signature scheme. Mathematical foundations of cryptography is relevant for the crypto data-based signature framework. Cryptanalysis and other attacks has limited relevance as the paper focuses on design rather than attacks.
4472,Next-Generation Access Control for Distributed Control Systems,"With the rapid integration of wired and wireless internetworking technologies, distributed control systems (DCS) are increasingly susceptible to cyberattacks. A well-designed access control framework could potentially contain and mitigate the impact of cyberattacks. However, existing solutions often fail to cover and protect all connected devices, leaving holes that are sufficient to undermine the security and safety of a plant. Further, in current DCS environments, it's hard to adhere to the least-privilege principle because access control policies are distributed among many heterogeneous systems. In this article, the authors identify key challenges in moving toward a more complete and manageable access control framework for DCS, and present a model architecture that can be adapted by the industrial control system community to ensure that every access is checked against policies that adhere to the least-privilege principle. Their proposed architecture facilitates centralized (plant-wide) policy management and protection of every connected field device.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.2,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.9,Human-centered computing:0.2,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Security and privacy,Security and privacy is the most relevant as the paper addresses access control in distributed systems. Other categories like Applied computing are less central to the core security contribution.,"Cryptography:0.1,Database and storage security:0.1,Formal methods and theory of security:0.3,Human and societal aspects of security and privacy:0.1,Intrusion/anomaly detection and malware mitigation:0.1,Network security:0.2,Security in hardware:0.1,Security services:0.7,Software and application security:0.2,Systems security:1.0","Systems security,Security services",Systems security: The paper discusses security of distributed control systems. Security services: The paper presents a new access control framework. Other options are less relevant as they don't directly relate to the core contribution of access control for systems.,"Access control:1,Authorization:1,Distributed systems security:1,Authentication:0.1,Denial-of-service attacks:0.1,Digital rights management:0.1,File system security:0.1,Firewalls:0.1,Information flow control:0.1,Operating systems security:0.1,Privacy-preserving protocols:0.1,Pseudonymity, anonymity and untraceability:0.1,Vulnerability management:0.1","Access control,Authorization,Distributed systems security",Access control is directly addressed as the core solution. Authorization is relevant due to policy management discussions. Distributed systems security applies to the DCS context. Other options are unrelated to the paper's focus on access control frameworks and least-privilege policies.
1553,Secure computation in a bidirectional relay,"Bidirectional relaying, where a relay helps two user nodes to exchange equal length binary messages, has been an active area of recent research. A popular strategy involves a modified Gaussian MAC, where the relay decodes the XOR of the two messages using the naturally-occurring sum of symbols simultaneously transmitted by user nodes. In this work, we consider the Gaussian MAC in bidirectional relaying with an additional secrecy constraint for protection against a honest but curious relay. The constraint is that, while the relay should decode the XOR, it should be fully ignorant of the individual messages of the users. We exploit the symbol addition that occurs in a Gaussian MAC to design explicit strategies that achieve perfect independence between the received symbols and individual transmitted messages. Our results actually hold for a more general scenario where the messages at the two user nodes come from a finite Abelian group G, and the relay must decode the sum within G of the two messages. We provide a lattice coding strategy and study optimal rate versus average power trade-offs for asymptotically large dimensions.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.85,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Security and privacy,Security and privacy: The paper focuses on secure computation in relays with secrecy constraints. Networks could be considered but is less relevant than the primary focus on cryptographic security.,"Cryptography:0.9,Database and storage security:0.1,Formal methods and theory of security:0.7,Human and societal aspects of security and privacy:0.1,Intrusion/anomaly detection and malware mitigation:0.1,Network security:0.1,Security in hardware:0.1,Security services:0.1,Software and application security:0.1,Systems security:0.1",Cryptography,"Cryptography: The paper designs cryptographic strategies to ensure secrecy in bidirectional relay communication. Formal methods would apply if they used mathematical proofs, but the focus is on practical cryptographic protocol design. Other categories like 'Network security' are less relevant as the threat model is specific to relay confidentiality.","Cryptanalysis and other attacks:0.2,Information-theoretic techniques:1.0,Key management:0.4,Mathematical foundations of cryptography:0.6,Public key (asymmetric) techniques:0.2,Symmetric cryptography and hash functions:0.6",Information-theoretic techniques,Information-theoretic techniques is highly relevant for secure computation using entropy and independence. Other cryptography categories are less central as the paper focuses on information-theoretic security rather than cryptographic primitives.
3470,A chip-based watermarking framework for color image authentication,"In today's scenario authentication of multimedia contents have received a sharp attention as a result of epidemic development of cyber crime. Illegal replication, misappropriation of digital content has achieved an enigmatic growth. In this regard we've already proposed different frameworks for invisible fragile watermarking for color image authentication. In present context we are going to propose a chip-based framework for color image authentication in accordance with our proposed LSB scheme. In this watermarking framework, a secret key will be given to the user and with the help of a particular hash function the user can authenticate the ownership of an image. The security issue is guaranteed with the secret key and the hash function. If a forger tries to perform watermark extraction with an inappropriate key and a wrong hash function he'll get an image which resembles noise. We named our proposed chip as BLIND CHIP, because we've used blind method for watermark extraction, i.e., neither the host image nor the watermark image is needed at the extraction end.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.9,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Security and privacy,Security and privacy is relevant for the watermarking framework and authentication. Other categories like Software or Information systems are not the primary focus.,"Cryptography:0.85,Database and storage security:0.1,Formal methods and theory of security:0.2,Human and societal aspects of security and privacy:0.1,Intrusion/anomaly detection and malware mitigation:0.1,Network security:0.2,Security in hardware:0.1,Security services:0.75,Software and application security:0.1,Systems security:0.1","Cryptography,Security services",Cryptography: Use of secret keys and hash functions for authentication. Security services: Implementation of a chip-based watermarking framework for ownership verification.,"Access control:0,Authentication:1,Authorization:0,Cryptanalysis and other attacks:0,Digital rights management:1,Information-theoretic techniques:0,Key management:0.5,Mathematical foundations of cryptography:0.3,Privacy-preserving protocols:0,Pseudonymity, anonymity and untraceability:0,Public key (asymmetric) techniques:0.5,Symmetric cryptography and hash functions:1","Authentication,Digital rights management,Symmetric cryptography and hash functions",Authentication is central to the watermarking framework. Digital rights management applies to content ownership. Symmetric cryptography is used for the hash function. Other categories like Public key techniques are secondary.
4028,Visualizing Bitcoin Flows of Ransomware: WannaCry One Week Later,"Because of its pseudo-anonimity and decentralisation characteristics, bitcoin payments are often a tool utilised by ransomware: this kind of malware infects a victim computer by encrypting some/all its data and/or denying the access to it. Then, the victim has to pay a given amount of bitcoins to see all the blocked functionalities restored. The goal of this paper is to visualise these bitcoin transactions, and in particular we focus on the effects of one of such ransomware, i.e., WannaCry, one/two weeks after its diffusion. We exploit BlockChainVis, a tool for visualising flows of bitcoins through the use of Visual Analytics.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.2,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.2,Security and privacy:0.9,Human-centered computing:0.1,Computing methodologies:0.3,Applied computing:0.1,Social and professional topics:0.1",Security and privacy,Security and privacy: The paper analyzes Bitcoin ransomware transactions for security monitoring. Other categories like Networks/Information systems are secondary as they focus on the tool (BlockChainVis) rather than the core security analysis.,"Cryptography:0.3,Database and storage security:0,Formal methods and theory of security:0,Human and societal aspects of security and privacy:0.2,Intrusion/anomaly detection and malware mitigation:0.8,Network security:0.6,Security in hardware:0,Security services:0.4,Software and application security:0.2,Systems security:0.3",Intrusion/anomaly detection and malware mitigation,Intrusion/anomaly detection is directly relevant due to the focus on ransomware Bitcoin tracking. Network security is secondary as the paper analyzes transaction flows but does not address network-level security mechanisms.,"Intrusion detection systems:0.3,Malware and its mitigation:0.8,Social engineering attacks:0.1",Malware and its mitigation,"Malware and its mitigation: The paper focuses on visualizing Bitcoin transactions for ransomware analysis, a key malware mitigation concern. Intrusion detection systems was given lower relevance as the focus is on payment tracking rather than detection algorithms."
3490,Construction of Extended Multivariate Public Key Cryptosystems,"Based on the ideas: ＂invertible cycle＂, ＂tame transformation＂ and ＂special oil and vinegar＂, three different nonlinear invertible transformations were constructed separately. Then making use of the idea of the extended multivariate public key cryptosystem, and combining the nonlinear invertible transformations above with Matsumoto-Imai (MI) scheme, three methods of designing extended multivariate public key cryptosystem were proposed. Next, the corresponding encryption and signature algorithms were given. Analysis results demonstrate that the new extended cryptosystems inherit the merit of MI scheme, i.e., efficient computation. Meanwhile, the new extended cryptosystems can also resist the linearization attack, differential attack and algebraic attack.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.3,Information systems:0.1,Security and privacy:0.9,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Security and privacy,Security and privacy is highly relevant as the paper constructs cryptographic systems for multivariate public key encryption. Mathematics of computing is marginally relevant but secondary to the cryptographic application.,"Cryptography:1.0,Database and storage security:0.2,Formal methods and theory of security:0.3,Human and societal aspects of security and privacy:0.1,Intrusion/anomaly detection and malware mitigation:0.1,Network security:0.1,Security in hardware:0.1,Security services:0.1,Software and application security:0.1,Systems security:0.1",Cryptography,Cryptography is highly relevant as the paper introduces extended multivariate public key cryptosystems using nonlinear transformations and analyzes their resistance to attacks. Other categories like Database security or Network security are not discussed in the core contribution.,"Cryptanalysis and other attacks:1,Information-theoretic techniques:0,Key management:0,Mathematical foundations of cryptography:0,Public key (asymmetric) techniques:1,Symmetric cryptography and hash functions:0","Public key (asymmetric) techniques,Cryptanalysis and other attacks","Public key (asymmetric) techniques: The paper explicitly discusses multivariate public key cryptosystem construction. Cryptanalysis and other attacks: The paper analyzes resistance to linearization, differential, and algebraic attacks. Other categories are irrelevant as the focus is not on symmetric cryptography or key management."
821,Visual cryptography using image pixel transparency with cover image,"Transparency of pixels of the shares (the two or more encrypted images) can be used to reveal the secret image. The pixels of shares can be generated randomly or the cover image can be used to generate the first share. The encrypted shares generated using cover image seem to be visually less similar to those generated without using cover image. Also, on the basis of image similarity between shares and original image, further encryption of encrypted shares can be performed. Comparison of the outcomes when cover image is used and when not used to generate shares, shows that the latter is more sensitive to the transparency factor (alpha) than the former encouraging the use of properly selected cover image for visual cryptography using pixel transparency.","General and reference:0,Hardware:0,Computer systems organization:0,Networks:0,Software and its engineering:0,Theory of computation:0,Mathematics of computing:0,Information systems:0,Security and privacy:1.0,Human-centered computing:0,Computing methodologies:0,Applied computing:0,Social and professional topics:0",Security and privacy,Security and privacy is highly relevant as the paper presents a visual cryptography method for securing image data. Other categories like Computing methodologies are less relevant as the focus is on cryptographic techniques rather than general algorithmic contributions.,"Cryptography:1.0,Database and storage security:0.2,Formal methods and theory of security:0.3,Human and societal aspects of security and privacy:0.1,Intrusion/anomaly detection and malware mitigation:0.1,Network security:0.2,Security in hardware:0.1,Security services:0.3,Software and application security:0.2,Systems security:0.1",Cryptography,"Cryptography: The paper introduces a visual cryptography method using pixel transparency, which is the primary focus. Other options like Security services are less relevant as the contribution is specific to cryptographic techniques rather than broader security frameworks.","Cryptanalysis and other attacks:0.1,Information-theoretic techniques:1.0,Key management:0.2,Mathematical foundations of cryptography:0.6,Public key (asymmetric) techniques:0.1,Symmetric cryptography and hash functions:0.9","Information-theoretic techniques,Symmetric cryptography and hash functions",Information-theoretic techniques: Transparency-based cryptography is inherently information-theoretic. Symmetric cryptography and hash functions: Visual cryptography is a symmetric method. Other categories are less relevant.
461,Evaluation of Computer and Network Security Strategies: A Case Study of Nigerian Banks,"It has been identified that financial institutions are one of the major users of Information Technology hence the need to evaluate effective use of the computer security strategies. This study aimed at evaluating the effectiveness of computer and network security strategies employed in Nigerian banks. This study was led by the following objectives; to determine and assess the security strategies that are put in place by Nigerian banks, to suggest solutions that would improve the effectiveness of the evaluated security strategies, to identify the effect of the various security strategies on the integrated banking system. This study employed the use of online questionnaires as a means of data collection. The study majorly focused on computer security strategies employed by the banks, and how effective the implemented security strategies have been. The strategies include passwords, antivirus, firewalls, encryption, intrusion detection systems and intrusion prevention systems, and it was mentioned that the integrated banking systems used by Nigerian banks has a certain security level thereby aiding the computer systems security in general, the study also revealed that Nigerian banks rarely experience malicious attacks of any form on their systems. The study findings revealed that Nigerian banks are implementing at least five computer security strategies CCS Concepts • General and reference ➝ Cross-computing tools and techniques ➝ Evaluation","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.2,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:1.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Security and privacy,"Security and privacy: The paper evaluates computer/network security strategies in banks, directly addressing security measures. Networks is weakly relevant as infrastructure context but not primary focus.","Cryptography:0.0,Database and storage security:0.0,Formal methods and theory of security:0.0,Human and societal aspects of security and privacy:0.0,Intrusion/anomaly detection and malware mitigation:0.7,Network security:1.0,Security in hardware:0.0,Security services:0.2,Software and application security:0.5,Systems security:0.3","Network security,Intrusion/anomaly detection and malware mitigation","Network security is relevant as the paper evaluates security strategies (firewalls, intrusion detection) in banks. Intrusion detection is directly mentioned as a strategy. Software and application security receive a lower score as the focus is broader network-level strategies.","Denial-of-service attacks:0.2,Firewalls:0.9,Intrusion detection systems:0.9,Malware and its mitigation:0.3,Mobile and wireless security:0.2,Security protocols:0.4,Social engineering attacks:0.1,Web protocol security:0.3","Firewalls,Intrusion detection systems",Firewalls and Intrusion detection systems are directly relevant as the paper evaluates these specific security technologies. Other security categories like Denial-of-service attacks or Malware mitigation are less relevant to the primary focus on existing security strategy evaluation.
5698,Multi-aspect profiling of kernel rootkit behavior,"Kernel rootkits, malicious software designed to compromise a running operating system kernel, are difficult to analyze and profile due to their elusive nature, the variety and complexity of their behavior, and the privilege level at which they run. However, a comprehensive kernel rootkit profile that reveals key aspects of the rootkit's behavior is helpful in aiding a detailed manual analysis by a human expert. In this paper we present PoKeR, a kernel rootkit profiler capable of producing multi-aspect rootkit profiles which include the revelation of rootkit hooking behavior, the exposure of targeted kernel objects (both static and dynamic), assessment of user-level impacts, as well as the extraction of kernel rootkit code. The system is designed to be deployed in scenarios which can tolerate high overheads, such as honeypots. Our evaluation results with a number of real-world kernel rootkits show that PoKeR is able to accurately profile a variety of rootkits ranging from traditional ones with system call hooking to more advanced ones with direct kernel object manipulation. The obtained profiles lead to unique insights into the rootkits' characteristics and demonstrate PoKeR's usefulness as a tool for rootkit investigators.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.2,Networks:0.2,Software and its engineering:0.3,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.9,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Security and privacy,"Security and privacy: The paper analyzes kernel rootkits, a core cybersecurity topic. Software engineering aspects are secondary to the security focus.","Cryptography:0.1,Database and storage security:0.1,Formal methods and theory of security:0.1,Human and societal aspects of security and privacy:0.1,Intrusion/anomaly detection and malware mitigation:0.9,Network security:0.1,Security in hardware:0.1,Security services:0.1,Software and application security:0.1,Systems security:0.8","Intrusion/anomaly detection and malware mitigation,Systems security",Intrusion/anomaly detection and malware mitigation: PoKeR is a profiling tool for kernel rootkits. Systems security: Focuses on kernel-level security analysis. Other categories are irrelevant as the paper does not address cryptography or formal methods.,"Browser security:0.1,Denial-of-service attacks:0.1,Distributed systems security:0.2,File system security:0.1,Firewalls:0.1,Information flow control:0.1,Intrusion detection systems:0.2,Malware and its mitigation:0.8,Operating systems security:0.7,Social engineering attacks:0.1,Vulnerability management:0.3","Malware and its mitigation,Operating systems security",Malware and its mitigation: The paper profiles kernel rootkits (malware). Operating systems security: Rootkits target the OS kernel. Other categories like intrusion detection are secondary.
468,Lattice Forward-Secure Identity Based Encryption Scheme,"Protecting secret keys is crucial for cryptography. There are some relatively insecure devices (smart cards, mobile phones etc.) which have threat of key exposure. The goal of the forward security is to protect security of past uses of key even if the current secret key is exposed. In this paper we propose lattice based forward-secure identity based encryption scheme based on LWE assumption in random oracle model. We also propose lattice based forward-secure identity based encryption scheme in the standard model.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.9,Human-centered computing:0.2,Computing methodologies:0.3,Applied computing:0.2,Social and professional topics:0.1",Security and privacy,Security and privacy is relevant for the cryptographic scheme design. Other fields like Computing methodologies are secondary to the cryptographic contribution.,"Cryptography:0.95,Database and storage security:0.1,Formal methods and theory of security:0.1,Security services:0.2,Systems security:0.1",Cryptography,Cryptography is highly relevant as the paper presents a lattice-based encryption scheme with forward security. Other security categories are not directly related to the cryptographic protocol design.,"Cryptanalysis and other attacks:0,Information-theoretic techniques:0,Key management:1,Mathematical foundations of cryptography:1,Public key (asymmetric) techniques:1,Symmetric cryptography and hash functions:0","Public key (asymmetric) techniques,Mathematical foundations of cryptography,Key management","Public key (asymmetric) techniques: The paper presents a lattice-based IBE scheme, a public-key method. Mathematical foundations of cryptography: Lattice-based cryptography is a core mathematical foundation. Key management: Forward security addresses key exposure threats. Other options like Symmetric cryptography are not discussed."
5943,Transport Layer Security (TLS) Renegotiation Indication Extension,"SSL and TLS renegotiation are vulnerable to an attack in which the
attacker forms a TLS connection with the target server, injects
content of his choice, and then splices in a new TLS connection from a
client. The server treats the client's initial TLS handshake as a
renegotiation and thus believes that the initial data transmitted by
the attacker is from the same entity as the subsequent client data.
This draft defines a TLS extension to cryptographically tie
renegotiations to the TLS connections they are being performed over,
thus preventing this attack.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.9,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Security and privacy,Security and privacy is highly relevant as the paper addresses a TLS renegotiation vulnerability and proposes a cryptographic extension to mitigate it. Other categories like Networks are less relevant since the focus is on protocol security rather than network architecture.,"Cryptography:0.2,Database and storage security:0.1,Formal methods and theory of security:0.1,Human and societal aspects of security and privacy:0.1,Intrusion/anomaly detection and malware mitigation:0.1,Network security:0.85,Security in hardware:0.1,Security services:0.75,Software and application security:0.1,Systems security:0.2","Network security,Security services",Network security is directly relevant as the paper addresses TLS protocol vulnerabilities and mitigation. Security services are relevant because the extension provides cryptographic binding to secure renegotiation. Other categories like Cryptography and Systems security are less central since the focus is on protocol-level security rather than cryptographic algorithms or system-level protections.,"Access control:0.1,Authentication:0.2,Authorization:0.1,Denial-of-service attacks:0.1,Digital rights management:0.1,Firewalls:0.1,Mobile and wireless security:0.1,Privacy-preserving protocols:0.1,Pseudonymity, anonymity and untraceability:0.1,Security protocols:1.0,Web protocol security:0.8","Security protocols,Web protocol security",Security protocols is directly relevant as the paper addresses a TLS protocol vulnerability. Web protocol security is relevant because TLS is a web protocol. Other categories like Authentication or Firewalls are not central to the proposed cryptographic fix.
2012,A cross-domain privacy-preserving protocol for cooperative firewall optimization,"Firewalls have been widely deployed on the Internet for securing private networks. A firewall checks each incoming or outgoing packet to decide whether to accept or discard the packet based on its policy. Optimizing firewall policies is crucial for improving network performance. Prior work on firewall optimization focuses on either intra-firewall or inter-firewall optimization within one administrative domain where the privacy of firewall policies is not a concern. This paper explores inter-firewall optimization across administrative domains for the first time. The key technical challenge is that firewall policies cannot be shared across domains because a firewall policy contains confidential information and even potential security holes, which can be exploited by attackers. In this paper, we propose the first cross-domain privacy-preserving cooperative firewall policy optimization protocol. Specifically, for any two adjacent firewalls belonging to two different administrative domains, our protocol can identify in each firewall the rules that can be removed because of the other firewall. The optimization process involves cooperative computation between the two firewalls without any party disclosing its policy to the other. We implemented our protocol and conducted extensive experiments. The results on real firewall policies show that our protocol can remove as many as 49% of the rules in a firewall whereas the average is 19.4%. The communication cost is less than a few hundred KBs. Our protocol incurs no extra online packet processing overhead and the offline processing time is less than a few hundred seconds.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:1.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Security and privacy,"Security and privacy: The paper presents a privacy-preserving protocol for cross-domain firewall optimization, which directly addresses security and privacy concerns in network systems.","Cryptography:0.0,Database and storage security:0.0,Formal methods and theory of security:0.0,Human and societal aspects of security and privacy:0.0,Intrusion/anomaly detection and malware mitigation:0.0,Network security:1.0,Security in hardware:0.0,Security services:0.75,Software and application security:0.0,Systems security:0.0","Network security,Security services",Network security is highly relevant as the paper introduces a privacy-preserving protocol for cross-domain firewall optimization. Security services is relevant due to the protocol's role in secure inter-domain collaboration. Other categories like Cryptography are not central to the method described.,"Access control:0.1,Authentication:0.1,Authorization:0.1,Denial-of-service attacks:0.1,Digital rights management:0.1,Firewalls:1.0,Mobile and wireless security:0.2,Privacy-preserving protocols:1.0,Pseudonymity, anonymity and untraceability:0.2,Security protocols:0.9,Web protocol security:0.1","Firewalls,Privacy-preserving protocols,Security protocols",Firewalls is directly relevant to the domain. Privacy-preserving protocols is central to the cross-domain optimization approach. Security protocols is also relevant as the solution is implemented as a protocol. Other categories like Access control are less relevant to the core contributions.
2853,Security Issues and Challenges for Cyber Physical System,"In this paper, we investigate the security challenges and issues of cyber-physical systems. (1)We abstract the general workflow of cyber physical systems, (2)identify the possible vulnerabilities, attack issues, adversaries characteristics and a set of challenges that need to be addressed, (3)then we also propose a context-aware security framework for general cyber-physical systems and suggest some potential research areas and problems.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.9,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.2,Social and professional topics:0.1",Security and privacy,Security and privacy is highly relevant for the cyber-physical system security framework. Applied computing is marginally relevant for the system application context. Other children are irrelevant as the paper focuses on security challenges and solutions.,"Cryptography:0.1,Database and storage security:0.1,Formal methods and theory of security:0.3,Human and societal aspects of security and privacy:0.1,Intrusion/anomaly detection and malware mitigation:0.1,Network security:0.8,Security in hardware:0.1,Security services:0.1,Software and application security:0.1,Systems security:0.9","Systems security,Network security",Systems security (0.9) is relevant as the paper addresses vulnerabilities in cyber-physical systems. Network security (0.8) applies because the framework involves secure communication and attack mitigation. Other categories like Cryptography are less directly related to the discussed challenges.,"Browser security:0,Denial-of-service attacks:0,Distributed systems security:1,File system security:0,Firewalls:0,Information flow control:0,Mobile and wireless security:1,Operating systems security:0,Security protocols:0,Vulnerability management:0,Web protocol security:0","Distributed systems security,Mobile and wireless security",Distributed systems security is relevant as the paper addresses vulnerabilities in interconnected CPS. Mobile and wireless security is relevant due to the focus on wireless communication in CPS. Other categories like browser or web security are not discussed.
3221,Identifying and Analyzing Pointer Misuses for Sophisticated Memory-corruption Exploit Diagnosis,"Software exploits are one of the major threats to the Internet security. A large family of exploits works by corrupting memory of the victim process to execute malicious code. To quickly respond to these attacks, it is critical to automatically diagnose such exploits to find out how they circumvent existing defense mechanisms. Because of the complexity of the victim programs and sophistication of recent exploits, existing analysis techniques fall short: they either miss important attack steps or report too much irrelevant information. In this paper, based on the observation that the key steps in memory corruption exploits often involve pointer misuses, we propose a novel solution, PointerScope, to use type inference on binary execution to detect the pointer misuses induced by an exploit. These pointer misuses highlight the important attack steps of the exploit, and therefore convey valuable information about the exploit mechanisms. Our approach complements dependency-based solutions to perform more comprehensive diagnosis of sophisticated memory exploits. We prototyped PointerScope and evaluated it using real-world exploit samples and demonstrated that PointerScope can successfully capture the key attack steps, which significantly facilitates attack response.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.85,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Security and privacy,Security and privacy is highly relevant for exploit diagnosis techniques. Other categories are irrelevant as the paper focuses on security analysis of memory corruption attacks.,"Cryptography:0.1,Database and storage security:0.1,Formal methods and theory of security:0.2,Human and societal aspects of security and privacy:0.1,Intrusion/anomaly detection and malware mitigation:0.8,Network security:0.3,Security in hardware:0.1,Security services:0.2,Software and application security:0.9,Systems security:0.3","Software and application security,Intrusion/anomaly detection and malware mitigation",Software and application security: The paper introduces PointerScope for diagnosing memory corruption exploits. Intrusion/anomaly detection and malware mitigation: The system detects pointer misuses to identify exploit mechanisms. Other fields like Network security are less relevant as the focus is on software-level exploits rather than network attacks.,"Domain-specific security and privacy architectures:0.3,Intrusion detection systems:0.4,Malware and its mitigation:1.0,Social engineering attacks:0.1,Social network security and privacy:0.1,Software reverse engineering:0.5,Software security engineering:1.0,Web application security:0.2","Malware and its mitigation,Software security engineering",Malware mitigation is relevant for diagnosing memory corruption exploits. Software security engineering is relevant for developing PointerScope to detect pointer misuses.
2027,Malicious Shellcode Detection with Virtual Memory Snapshots,"Malicious shellcodes are segments of binary code disguised as normal input data. Such shellcodes can be injected into a target process's virtual memory. They overwrite the process's return addresses and hijack control flow. Detecting and filtering out such shellcodes is vital to prevent damage. In this paper, we propose a new malicious shellcode detection methodology in which we take snapshots of the process's virtual memory before input data are consumed, and feed the snapshots to a malicious shellcode detector. These snapshots are used to instantiate a runtime environment that emulates the target process's input data consumption to monitor shellcodes' behaviors. The snapshots can also be used to examine the system calls that shellcodes invoke, these system call parameters, and the process's execution flow. We implement a prototype system in Debian Linux with kernel version 2.6.26. Our extensive experiments with real traces and thousands of malicious shellcodes illustrate our system's performance with low overhead and few false negatives and few false positives.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.95,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Security and privacy,"Security and privacy: The paper presents a novel shellcode detection method using virtual memory analysis, directly addressing malware detection and system security. Other categories (e.g., Networks) are irrelevant as the focus is on application-layer security rather than network protocols.","Cryptography:0.1,Database and storage security:0.2,Formal methods and theory of security:0.1,Human and societal aspects of security and privacy:0.1,Intrusion/anomaly detection and malware mitigation:0.9,Network security:0.2,Security in hardware:0.1,Security services:0.3,Software and application security:0.7,Systems security:0.8","Intrusion/anomaly detection and malware mitigation,Software and application security",Intrusion/anomaly detection and malware mitigation: The paper detects malicious shellcodes. Software and application security: The system operates at the application level. Systems security is secondary as the focus is on runtime detection.,"Domain-specific security and privacy architectures:0.3,Intrusion detection systems:0.8,Malware and its mitigation:1.0,Social engineering attacks:0.0,Social network security and privacy:0.0,Software reverse engineering:0.0,Software security engineering:0.6,Web application security:0.0","Malware and its mitigation,Intrusion detection systems",Malware and its mitigation: The paper focuses on detecting malicious shellcodes. Intrusion detection systems: The snapshot-based monitoring aligns with intrusion detection techniques. Software security engineering is secondary to the core detection mechanism.
37,A Method for Verifying Privacy-Type Properties: The Unbounded Case,"In this paper, we consider the problem of verifying anonymity and unlinkability in the symbolic model, where protocols are represented as processes in a variant of the applied pi calculus notably used in the ProVerif tool. Existing tools and techniques do not allow one to verify directly these properties, expressed as behavioral equivalences. We propose a different approach: we design two conditions on protocols which are sufficient to ensure anonymity and unlinkability, and which can then be effectively checked automatically using ProVerif. Our two conditions correspond to two broad classes of attacks on unlinkability, corresponding to data and control-flow leaks. This theoretical result is general enough to apply to a wide class of protocols. In particular, we apply our techniques to provide the first formal security proof of the BAC protocol (e-passport). Our work has also lead to the discovery of new attacks, including one on the LAK protocol (RFID authentication) which was previously claimed to be unlinkable (in a weak sense) and one on the PACE protocol (e-passport).","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.2,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:1.0,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Security and privacy,"Security and privacy: The paper introduces formal methods for verifying anonymity/unlinkability in protocols using ProVerif, directly addressing security properties. Other categories are irrelevant as the focus is not on hardware, networks, or general computing theory.","Cryptography:0.2,Database and storage security:0.1,Formal methods and theory of security:0.9,Human and societal aspects of security and privacy:0.2,Intrusion/anomaly detection and malware mitigation:0.1,Network security:0.1,Security in hardware:0.1,Security services:0.1,Software and application security:0.1,Systems security:0.1",Formal methods and theory of security,Formal methods and theory of security is directly relevant as the paper introduces formal verification techniques for privacy properties. Other security categories like cryptography are secondary to the formal methods focus.,"Formal security models:0.9,Logic and verification:0.8,Security requirements:0.2,Trust frameworks:0.3","Formal security models,Logic and verification",Formal security models is directly relevant for verifying anonymity/unlinkability. Logic and verification is relevant for the symbolic model and behavioral equivalence analysis. Other categories like Trust frameworks are less specific.
494,Reliability engineering approach to digital watermark evaluation,"Robust watermarks are evaluated in terms of image fidelity and robustness. We extend this framework and apply reliability testing to robust watermark evaluation. Reliability is the probability that a watermarking algorithm will correctly detect or decode a watermark for a specified fidelity requirement under a given set of attacks and images. In reliability testing, a system is evaluated in terms of quality, load, capacity and performance. To measure quality that corresponds to image fidelity, we compensate for attacks to measure the fidelity of attacked watermarked images. We use the conditional mean of pixel values to compensate for valumetric attacks such as gamma correction and histogram equalization. To compensate for geometrical attacks, we use error concealment and perfect motion estimation assumption. We define capacity to be the maximum embedding strength parameter and the maximum data payload. Load is then defined to be the actual embedding strength and data payload of a watermark. To measure performance, we use bit error rate (BER) and receiver operating characteristics (ROC) and area under the curve (AUC) of the ROC curve of a watermarking algorithm for different attacks and images. We evaluate robust watermarks for various quality, loads, attacks, and images.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.8,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.2,Social and professional topics:0.1",Security and privacy,Security and privacy is highly relevant as the paper evaluates digital watermark robustness as a security mechanism. Applied computing is secondary due to the multimedia application context. Other fields like Information systems are less relevant as the focus is on security evaluation.,"Security services:1.0,Systems security:0.75,Cryptography:0.2,Database and storage security:0.1,Formal methods and theory of security:0.1,Human and societal aspects of security and privacy:0.1,Intrusion/anomaly detection and malware mitigation:0.1,Network security:0.1,Security in hardware:0.1,Software and application security:0.1","Security services,Systems security",Security services is directly relevant as the paper evaluates watermarking reliability as a security mechanism. Systems security is relevant due to the focus on system-level watermarking robustness. Other fields like Cryptography are not central to the core contribution.,"Access control:0.1,Authentication:0.1,Authorization:0.1,Browser security:0.1,Denial-of-service attacks:0.1,Digital rights management:1.0,Distributed systems security:0.1,File system security:0.1,Firewalls:0.1,Information flow control:0.1,Operating systems security:0.1,Privacy-preserving protocols:0.1,Pseudonymity, anonymity and untraceability:0.1,Vulnerability management:0.1",Digital rights management,"Digital rights management is directly relevant as the paper evaluates watermark robustness, a key aspect of DRM. Other security categories are not discussed."
1790,A framework of adaptive steganography resisting JPEG compression and detection,"Current typical adaptive steganography algorithms take the detection resistant capability into account adequately but usually cannot extract the embedded secret messages correctly when stego images suffer from compression attack. In order to solve this problem, a framework of adaptive steganography resisting JPEG compression and detection is proposed. Utilizing the relationship between Discrete Cosine Transformation DCT coefficients, the domain of messages embedding is determined; for the maximum of the JPEG compression resistant ability, the modifying magnitude of different DCT coefficients caused by messages embedding can be determined; in order to ensure the completely correct extraction of embedded messages after JPEG compression, error correct codes are used to encode the messages to be embedded; on the basis of the current distortion functions, the distortion value of DCT coefficients corresponding to the modifying magnitude in the embedding domain can be calculated; to improve the detection resistant ability of the stego images and realize the minimum distortion embedding, syndrome-trellis codes are used to embed the encoded messages into the DCT coefficients that have a smaller distortion value. Based on the proposed framework, an adaptive steganography algorithm resisting JPEG compression and detection is designed, which utilizes the relationship between coefficients in a DCT block and the means of that in three adjacent DCT blocks. The experimental results that demonstrate the proposed algorithm not only has a good JPEG compression resistant ability but also has a strong detection resistant performance. Comparing with current J-UNIWARD steganography under quality factor 85 of JPEG compression, the extraction error rates without pre-compression decrease from about 50% to nearly 0, while the stego images remain a good detection resistant ability comparing with a typical robust watermarking algorithm, which shows the validity of the proposed framework. Copyright © 2016 John Wiley & Sons, Ltd.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:1.0,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Security and privacy,"Security and privacy: The paper introduces an adaptive steganography framework that addresses both JPEG compression resistance and detection evasion, which are core challenges in secure communication and data hiding. The use of error correction codes and syndrome-trellis codes aligns with cryptographic techniques for secure data transmission. Other categories are irrelevant as the paper focuses on security mechanisms rather than general computing or hardware.","Cryptography:1,Security services:1,Formal methods and theory of security:0.5,Database and storage security:0.3,Intrusion/anomaly detection and malware mitigation:0.4,Network security:0.3,Systems security:0.4,Software and application security:0.3,Human and societal aspects of security and privacy:0.2","Cryptography,Security services",Cryptography and security services are central to steganography and data protection. Other categories are less aligned with the technical focus on embedding and detection resistance.,"Access control:0,Authentication:0,Authorization:0,Cryptanalysis and other attacks:0.2,Digital rights management:0,Information-theoretic techniques:1,Key management:0,Mathematical foundations of cryptography:0.3,Privacy-preserving protocols:0,Pseudonymity, anonymity and untraceability:0,Public key (asymmetric) techniques:0,Symmetric cryptography and hash functions:1","Information-theoretic techniques,Symmetric cryptography and hash functions","Information-theoretic techniques are relevant due to the use of DCT coefficient relationships and error correction codes for steganography. Symmetric cryptography is relevant as the framework involves secure message embedding. Cryptanalysis and mathematical foundations are secondary because the paper focuses on defense mechanisms, not attacks or theoretical proofs."
6014,Security-aware Modeling and Analysis for HW/SW Partitioning,"The rising wave of attacks on communicating embedded systems has exposed their users to risks of informa- tion theft, monetary damage, and personal injury. Through improved modeling and analysis of security, we propose that these flaws could be mitigated. Since HW/SW partitioning, one of the first phases, impacts future integration of security into the system, this phase would benefit from supporting modeling security abstrac- tions and security properties, providing designers with useful partitioning feedback obtained from a security formal analyzer.
In this paper, we present how our toolkit supports security modeling, automated security integration, and formal analysis during the HW/SW partitioning phase for secure communications in embedded systems. We introduce “Cryptographic Configurations”, an abstract representation of security that allows us to verify security formally. Our toolkit further assists designers by automatically adding these security representations based on a mapping and security requirements.","General and reference:0.1,Hardware:0.2,Computer systems organization:0.3,Networks:0.1,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:1.0,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Security and privacy,Security and privacy is highly relevant as the paper addresses security modeling and analysis in embedded systems. Other categories like Hardware are less directly relevant to the core contribution.,"Cryptography:0.75,Database and storage security:0.0,Formal methods and theory of security:1.0,Human and societal aspects of security and privacy:0.0,Intrusion/anomaly detection and malware mitigation:0.0,Network security:0.25,Security in hardware:0.75,Security services:0.0,Software and application security:0.0,Systems security:0.0","Formal methods and theory of security,Cryptography,Security in hardware",Formal methods and theory of security is highly relevant as the paper introduces a formal analysis framework for security modeling. Cryptography is relevant due to the mention of cryptographic configurations. Security in hardware is relevant as the work focuses on secure communications in embedded systems. Other categories like Network security are less relevant as the paper emphasizes formal methods rather than network-specific threats.,"Cryptanalysis and other attacks:0,Embedded systems security:1,Formal security models:1,Hardware attacks and countermeasures:0,Hardware reverse engineering:0,Hardware security implementation:0.5,Information-theoretic techniques:0,Key management:0,Logic and verification:0.5,Mathematical foundations of cryptography:0.5,Public key (asymmetric) techniques:0.5,Security requirements:0.5,Symmetric cryptography and hash functions:0,Tamper-proof and tamper-resistant designs:0,Trust frameworks:0","Embedded systems security,Formal security models",Embedded systems security is relevant as the paper focuses on securing embedded systems through partitioning. Formal security models is relevant due to the use of formal analysis for security verification. Other categories like Hardware security implementation and Logic and verification are partially relevant but less central.
978,Inferring Accurate Histories of Malware Evolution from Structural Evidence,"An important problem in malware forensics is generating a partial ordering of a collection of variants of a malware program, reflecting a history of the malware’s evolution as it is adapted by the original or new authors. Frequently the only temporal clue to which variants were developed earlier is the date on which they were first observed in the wild. In the absence of reliable temporal clues, our approach leverages heuristic evidence based on common trends in the evolution of software structure over time. We extract structural features from each variant binary executable and generate from them three different forms of evidence that one variant is a likely ancestor of another. We then combine this evidence using a truth maintenance system to create a family tree of malware variants.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.8,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Security and privacy,Security and privacy is highly relevant as the paper addresses malware forensics and evolution analysis. 'Computing methodologies' is less relevant as the focus is on application to security rather than algorithmic innovation.,"Cryptography:0.0,Database and storage security:0.0,Formal methods and theory of security:0.5,Human and societal aspects of security and privacy:0.0,Intrusion/anomaly detection and malware mitigation:0.9,Network security:0.0,Security in hardware:0.0,Security services:0.0,Software and application security:0.8,Systems security:0.0","Intrusion/anomaly detection and malware mitigation,Software and application security",Intrusion/anomaly detection and malware mitigation is relevant for the malware evolution analysis. Software and application security is relevant for the binary analysis techniques. Formal methods has some relevance but is less central than the security aspects.,"Domain-specific security and privacy architectures:0.1,Intrusion detection systems:0.1,Malware and its mitigation:0.9,Social engineering attacks:0.1,Social network security and privacy:0.1,Software reverse engineering:0.8,Software security engineering:0.2,Web application security:0.1","Malware and its mitigation,Software reverse engineering",Malware and its mitigation is directly relevant to analyzing malware evolution. Software reverse engineering is crucial for extracting structural features from binaries. Other categories like Intrusion detection are less aligned with the paper's focus on evolutionary analysis.
2759,Trust and obfuscation principles for quality of information in emerging pervasive environments,"The emergence of large scale, distributed, sensor-enabled, machine-to-machine pervasive applications necessitates engaging with providers of information on demand to collect the information, of varying quality levels, to be used to infer about the state of the world and decide actions in response. In these highly fluid operational environments, involving information providers and consumers of various degrees of trust and intentions, obfuscation of information is used to protect providers from misuses of the information they share, while still providing benefits to their information consumers. In this paper, we develop the initial principles for relating to trust and obfuscation within the context of this emerging breed of applications. We start by extending the definitions of trust and obfuscation into this emerging application space. We, then, highlight their role as we move from tightly-coupled to loosely-coupled sensory-inference systems. Finally, we present the interplay between trust and obfuscation as well as the implications for reasoning under obfuscation.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.9,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Security and privacy,"Security and privacy: The paper addresses trust and obfuscation principles in pervasive systems, directly tackling information security challenges. Other categories like Networks or Applied computing are secondary to the core focus on securing data sharing in distributed environments.","Cryptography:0.0,Database and storage security:0.0,Formal methods and theory of security:1.0,Human and societal aspects of security and privacy:1.0,Intrusion/anomaly detection and malware mitigation:0.0,Network security:0.0,Security in hardware:0.0,Security services:0.0,Software and application security:0.0,Systems security:0.0","Formal methods and theory of security,Human and societal aspects of security and privacy",Formal methods and theory of security are relevant for defining trust and obfuscation principles. Human aspects are relevant for analyzing stakeholder interactions. Other security categories are unrelated to the theoretical focus.,"Economics of security and privacy:0,Formal security models:0,Logic and verification:0,Privacy protections:1,Security requirements:0,Social aspects of security and privacy:0.5,Trust frameworks:1,Usability in security and privacy:0","Privacy protections,Trust frameworks",Privacy protections and Trust frameworks are central to the paper's discussion on obfuscation and trust in pervasive environments. Social aspects of security is moderately relevant but not as primary.
5213,Provisions: Privacy-preserving Proofs of Solvency for Bitcoin Exchanges,"Bitcoin exchanges function like banks, securely holding customers' bitcoins on their behalf. Several exchanges have suffered catastrophic losses with customers permanently losing their savings. A proof of solvency demonstrates cryptographically that the exchange controls sufficient reserves to settle each customer's account. We introduce Provisions, a privacy-preserving proof of solvency whereby an exchange does not have to disclose its Bitcoin addresses; total holdings or liabilities; or any information about its customers. We also propose an extension which prevents exchanges from colluding to cover for each other's losses. We have implemented Provisions and it offers practical computation times and proof sizes even for a large Bitcoin exchange with millions of customers.","General and reference:0.2,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.9,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Security and privacy,"Security and privacy is highly relevant as the paper introduces privacy-preserving cryptographic proofs for Bitcoin exchange solvency. Other categories are rejected because the paper does not focus on hardware, networks, software engineering, or other listed fields.","Cryptography:1.0,Security services:0.8,Database and storage security:0.0,Formal methods and theory of security:0.3,Human and societal aspects of security and privacy:0.0,Intrusion/anomaly detection and malware mitigation:0.0,Network security:0.0,Security in hardware:0.0,Software and application security:0.0,Systems security:0.0","Cryptography,Security services",Cryptography is highly relevant as the paper introduces a privacy-preserving cryptographic proof of solvency. Security services are relevant for ensuring transaction integrity. Other categories like Database and storage security are irrelevant as the focus is on cryptographic protocols.,"Access control:0.1,Authentication:0.3,Authorization:0.2,Cryptanalysis and other attacks:0.1,Digital rights management:0.1,Information-theoretic techniques:0.2,Key management:0.2,Mathematical foundations of cryptography:0.3,Privacy-preserving protocols:0.9,Pseudonymity, anonymity and untraceability:0.7,Public key (asymmetric) techniques:0.6,Symmetric cryptography and hash functions:0.2","Privacy-preserving protocols,Pseudonymity, anonymity and untraceability",Privacy-preserving protocols is primary as the paper introduces cryptographic proofs of solvency without revealing customer data. Pseudonymity/anonymity is secondary as the protocol preserves privacy through cryptographic anonymity. Public key techniques are relevant but not selected due to lower relevance compared to privacy-focused categories.
3882,PHY layer authentication via drifting oscillators,"PHY layer authentication of a wireless sender has gained much interest recently. In this paper, we consider the famous Alice, Bob and Eve model and investigate (for the first time) the feasibility of using time-varying clock offsets for sender-node-authentication at Bob. Specifically, we exploit the fact (and de-facto problem) that clock offset between every node pair is unique; moreover, the two clock offsets between any two node pairs drift independently and randomly over time. Therefore, an explicit mechanism is needed to track the time-varying clock offsets. To this end, we model oscillator drift as brownian motion frequency and phase drift, and present a novel framework which is based on interplay between a hypothesis testing device and a bank of two Kaiman filters; one KF (KFh0) tracks Alice's clock while other KF (KFh1) tracks Eve's clock. Building on aforementioned framework, we then propose a novel sender-node-authentication method (so-called MHF method) by means of which Bob can automatically accept (reject) a received packet if it is sent by Alice (Eve). Finally, simulation results are presented which corroborate the efficiency of the proposed method.","General and reference:0.1,Hardware:0.2,Computer systems organization:0.1,Networks:0.3,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.4,Information systems:0.1,Security and privacy:0.9,Human-centered computing:0.1,Computing methodologies:0.5,Applied computing:0.2,Social and professional topics:0.1",Security and privacy,Security and privacy is directly relevant because the paper proposes a novel PHY-layer authentication method using oscillator drift. Other fields like Networks are less relevant as the focus is on security mechanisms rather than communication protocols.,"Cryptography:0.5,Database and storage security:0.1,Formal methods and theory of security:0.2,Human and societal aspects of security and privacy:0.1,Intrusion/anomaly detection and malware mitigation:0.1,Network security:1.0,Security in hardware:0.3,Security services:0.2,Software and application security:0.1,Systems security:0.4",Network security,Network security is highly relevant as the paper introduces a novel physical-layer authentication method for wireless communication. Cryptography is secondary if the method involves cryptographic principles (not explicitly stated).,"Denial-of-service attacks:0.1,Firewalls:0.1,Mobile and wireless security:1.0,Security protocols:0.9,Web protocol security:0.1","Mobile and wireless security,Security protocols",Mobile and wireless security: Focuses on wireless sender authentication at the PHY layer. Security protocols: Proposes a novel authentication framework using Kalman filters. Other options like Firewalls/DoS attacks are unrelated to the paper's focus on oscillator-based authentication.
710,HTTPOS: Sealing Information Leaks with Browser-side Obfuscation of Encrypted Flows,"Leakage of private information from web applications— even when the traffic is encrypted—is a major security threat to many applications that use HTTP for data delivery. This paper considers the problem of inferring from encrypted HTTP traffic the web sites or web pages visited by a user. Existing browser-side approaches to this problem cannot defend against more advanced attacks, and serverside approaches usually require modifications to web entities, such as browsers, servers, or web objects. In this paper, we propose a novel browser-side system, namely HTTPOS, to prevent information leaks and offer much better scalability and flexibility. HTTPOS provides a comprehensive and configurable suite of traffic transformation techniques for a browser to defeat traffic analysis without requiring any server-side modifications. Extensive evaluation of HTTPOS on live web traffic shows that it can successfully prevent the state-of-the-art attacks from inferring private information from encrypted HTTP flows.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.9,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Security and privacy,Security and privacy is highly relevant as the paper addresses encrypted traffic analysis and browser-side obfuscation. Other fields are irrelevant as the focus is on information leak prevention.,"Network security:0.9,Software and application security:0.85,Cryptography:0.4,Database and storage security:0.3,Formal methods and theory of security:0.3,Human and societal aspects of security and privacy:0.3,Intrusion/anomaly detection and malware mitigation:0.4,Security in hardware:0.2,Security services:0.3,Systems security:0.4","Network security,Software and application security",Network security is relevant as HTTPOS addresses encrypted HTTP traffic leaks. Software and application security is relevant due to browser-side obfuscation techniques. Cryptography and intrusion detection are less central as the focus is on traffic analysis mitigation rather than cryptographic primitives or malware.,"Denial-of-service attacks:0.1,Domain-specific security and privacy architectures:0.4,Firewalls:0.2,Mobile and wireless security:0.3,Security protocols:0.7,Social network security and privacy:0.1,Software reverse engineering:0.1,Software security engineering:0.3,Web application security:1.0,Web protocol security:1.0","Web application security,Web protocol security",Web application security and Web protocol security are directly relevant to the paper's focus on protecting HTTP traffic. Other fields like Security protocols are secondary.
3209,Securely combining public-key cryptosystems,"It is a maxim of sound computer-security practice that a cryptographic key should have only a single use. For example, an RSA key pair should be used only for public-key encryption or only for digital signatures, and not for both.In this paper we show that in many cases, the simultaneous use of related keys for two cryptosystems, e.g. for a public-key encryption system and for a public-key signature system, does not compromise their security. We demonstrate this for a variety of public-key encryption schemes that are secure against chosen-ciphertext attacks, and for a variety of digital signature schemes that are secure against forgery under chosen-message attacks. The precise form of the statement of security that we are able to prove depends on the particular cryptographic schemes in question and on the cryptographic assumptions needed for their proofs of security; but in every case, our proof of security does not require any additional cryptographic assumptions.Among the cryptosystems that we analyze in this manner are the public-key encryption schemes of Cramer and Shoup, Naor and Yung, and Dolev, Dwork, and Naor, which are all defined in them standard model, while in the random-oracle model we analyze plaintext-aware encryption schemes (as defined by Bellare and Rogaway) and in particular the OAEP+ cryptosystem. Among public-key signature schemes, we analyze those of Cramer and Shoup and of Gennaro, Halevi, and Rabin in the standard model, while in the random-oracle model we analyze the RSA PSS scheme as well as variants of the El Gamal and Schnorr schemes. (See references within.)","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.2,Mathematics of computing:0.3,Information systems:0.1,Security and privacy:0.9,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Security and privacy,Security and privacy is directly relevant for the cryptographic key analysis and security proofs in public-key systems. Mathematics of computing (0.3) relates to the theoretical foundations but is secondary. Theory of computation (0.2) is tangential to the proofs but not primary.,"Cryptography:0.95,Database and storage security:0.1,Formal methods and theory of security:0.85,Human and societal aspects of security and privacy:0.1,Intrusion/anomaly detection and malware mitigation:0.2,Network security:0.1,Security in hardware:0.1,Security services:0.2,Software and application security:0.1,Systems security:0.1","Cryptography,Formal methods and theory of security",Cryptography is highly relevant as the paper analyzes public-key cryptosystems. Formal methods and theory of security is relevant for the theoretical proofs of security. Other categories like Network security or Hardware security are not directly addressed.,"Cryptanalysis and other attacks:0.0,Formal security models:0.7,Information-theoretic techniques:0.0,Key management:0.3,Logic and verification:0.0,Mathematical foundations of cryptography:0.2,Public key (asymmetric) techniques:0.9,Security requirements:0.0,Symmetric cryptography and hash functions:0.0,Trust frameworks:0.0","Public key (asymmetric) techniques,Formal security models",Public key (asymmetric) techniques: The paper directly addresses public-key cryptosystems and their secure combination. Formal security models: The analysis is grounded in formal proofs of security for cryptographic schemes. Key management is secondary as the focus is on combining cryptosystems rather than managing keys.
959,Online privacy as a collective phenomenon,"The problem of online privacy is often reduced to individual decisions to hide or reveal personal information in online social networks (OSNs). However, with the increasing use of OSNs, it becomes more important to understand the role of the social network in disclosing personal information that a user has not revealed voluntarily: How much of our private information do our friends disclose about us, and how much of our privacy is lost simply because of online social interaction? Without strong technical effort, an OSN may be able to exploit the assortativity of human private features, this way constructing shadow profiles with information that users chose not to share. Furthermore, because many users share their phone and email contact lists, this allows an OSN to create full shadow profiles for people who do not even have an account for this OSN.
 We empirically test the feasibility of constructing shadow profiles of sexual orientation for users and non-users, using data from more than 3 Million accounts of a single OSN. We quantify a lower bound for the predictive power derived from the social network of a user, to demonstrate how the predictability of sexual orientation increases with the size of this network and the tendency to share personal information. This allows us to define a privacy leak factor that links individual privacy loss with the decision of other individuals to disclose information. Our statistical analysis reveals that some individuals are at a higher risk of privacy loss, as prediction accuracy increases for users with a larger and more homogeneous first- and second-order neighborhood of their social network. While we do not provide evidence that shadow profiles exist at all, our results show that disclosing of private information is not restricted to an individual choice, but becomes a collective decision that has implications for policy and privacy regulation.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.2,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.9,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Security and privacy,Security and privacy is directly relevant as the paper studies privacy loss in online social networks through shadow profiles. Other fields like Networks or Social and Professional Topics are not the primary focus.,"Cryptography:0.1,Database and storage security:0.1,Formal methods and theory of security:0.1,Human and societal aspects of security and privacy:0.95,Intrusion/anomaly detection and malware mitigation:0.1,Network security:0.2,Security in hardware:0.1,Security services:0.1,Software and application security:0.1,Systems security:0.1",Human and societal aspects of security and privacy,Human and societal aspects of security and privacy is directly relevant as the paper analyzes collective privacy loss in social networks and quantifies privacy risks from social interactions. Other categories like Network security or Cryptography are not central to this study.,"Economics of security and privacy:0,Privacy protections:1,Social aspects of security and privacy:1,Usability in security and privacy:0","Privacy protections,Social aspects of security and privacy","Privacy protections: The paper discusses the technical feasibility of constructing shadow profiles, which relates to protecting user privacy. Social aspects of security and privacy: The study focuses on how social network interactions inherently influence privacy loss, emphasizing collective privacy dynamics. Other categories lack direct relevance."
1904,Evaluating Firewall Models for Hybrid Clouds,"Cloud computing offers flexible IT solutions that suites the growingly dynamic organizational demands. Different types of cloud deployment models offer distinctive solutions based on the security requirements of the organization. Hybrid clouds are considered the prominent deployment model that offers the extended flexibility of public cloud resources and the security of private clouds. This paper aims at modelling and evaluating three firewall deployment models for hybrid clouds: no firewall, regular firewalls and special firewalls that block web traffic. Experimental results illustrated that having a regular firewall would be sufficient for small hybrid clouds (≤150 nodes) to give high point-to-point utilization without compromising the response time. However, for larger networks (up to 300 nodes), a firewall that blocks web traffic is more efficient.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:1.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Security and privacy,Security and privacy receives 1.0 as the paper's core contribution is evaluating firewall models for hybrid cloud security. No other categories address the primary domain of network security.,"Cryptography:0.1,Database and storage security:0.1,Formal methods and theory of security:0.2,Human and societal aspects of security and privacy:0.1,Intrusion/anomaly detection and malware mitigation:0.1,Network security:0.9,Security in hardware:0.1,Security services:0.3,Software and application security:0.1,Systems security:0.8","Network security,Systems security",Network security is directly relevant due to firewall deployment evaluation in hybrid clouds. Systems security is relevant for assessing security models in cloud infrastructure. Other categories like Cryptography or Database security are not discussed.,"Browser security:0,Denial-of-service attacks:0,Distributed systems security:1,File system security:0,Firewalls:1,Information flow control:0,Mobile and wireless security:0,Operating systems security:0,Security protocols:0,Vulnerability management:0,Web protocol security:0","Firewalls,Distributed systems security",Firewalls is directly relevant to the firewall deployment models discussed. Distributed systems security is relevant for hybrid cloud environments. Other security topics like DOS attacks are not addressed.
2720,Integrating grid with intrusion detection,"In recent years, distributed denial-of-service (DDoS) and denial-of-service (DoS) are the most dreadful network threats. Single-node IDS often suffers from losing its detection effectiveness and capability when processing enormous network traffic. To solve the drawbacks, we propose grid-based IDS, called grid intrusion detection system (GIDS), which uses grid computing resources to detect intrusion packets. For balancing detection load, score subtraction approach (SSA) and score addition approach (SAA) are deployed. Furthermore, to effectively detect intrusions, a two-phase packet detection process is proposed. The first phase detects logical and momentary attacks. Chronic attacks are detected in the second phase. Experiments are also performed and the results show that GIDS is truly an outstanding system in detecting attacks.","General and reference:0,Hardware:0,Computer systems organization:0,Networks:0.4,Software and its engineering:0,Theory of computation:0,Mathematics of computing:0,Information systems:0,Security and privacy:1,Human-centered computing:0,Computing methodologies:0,Applied computing:0,Social and professional topics:0",Security and privacy,"Security and privacy is relevant as the paper addresses intrusion detection systems (IDS) for DDoS attacks. Networks is marginally relevant due to distributed computing, but the primary domain is security.","Cryptography:0.1,Database and storage security:0.1,Formal methods and theory of security:0.1,Human and societal aspects of security and privacy:0.1,Intrusion/anomaly detection and malware mitigation:0.8,Network security:0.3,Security in hardware:0.1,Security services:0.1,Software and application security:0.1,Systems security:0.3",Intrusion/anomaly detection and malware mitigation,Intrusion/anomaly detection and malware mitigation is highly relevant due to the paper's focus on distributed intrusion detection using grid computing. Network security receives partial relevance for system-wide protection considerations.,"Intrusion detection systems:1,Malware and its mitigation:0,Social engineering attacks:0",Intrusion detection systems,The paper proposes a grid-based intrusion detection system (GIDS) for DDoS mitigation. Other options like 'Malware' are unrelated to the focus on intrusion detection.
4056,Verifiable Distributed Oblivious Transfer and Mobile Agent Security,"The mobile agent is a fundamental building block of the mobile computing paradigm. In mobile agent security, oblivious transfer (OT) from a trusted party can be used to protect the agent’s privacy and the hosts’ privacy. In this paper, we introduce a new cryptographic primitive called Verifiable Distributed Oblivious Transfer (VDOT), which allows us to replace a single trusted party with a group of threshold trusted servers. The design of VDOT uses a novel technique called consistency verification of encrypted secret shares. VDOT protects the privacy of both the sender and the receiver against malicious attacks of the servers. We also show the design of a system to apply VDOT to protect the privacy of mobile agents. Our design partitions an agent into the general portion and the security-sensitive portion. We also implement the key components of our system. As far as we know, this is the first effort to implement a system that protects the privacy of mobile agents. Our preliminary evaluation shows that protecting mobile agents not only is possible, but also can be implemented efficiently.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.2,Networks:0.2,Software and its engineering:0.3,Theory of computation:0.2,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.9,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Security and privacy,Security and privacy is highly relevant for the cryptographic protocol (VDOT) and mobile agent security. Other categories like Computer systems organization are less central to the core contribution.,"Cryptography:0.9,Database and storage security:0.1,Formal methods and theory of security:0.7,Human and societal aspects of security and privacy:0.1,Intrusion/anomaly detection and malware mitigation:0.1,Network security:0.1,Security in hardware:0.1,Security services:0.2,Software and application security:0.1,Systems security:0.1","Cryptography,Formal methods and theory of security",Cryptography is highly relevant as the paper introduces a cryptographic primitive (VDOT). Formal methods and theory of security is relevant for the security analysis. Other security fields are not central to the core contribution.,"Cryptanalysis and other attacks:0.3,Formal security models:1,Information-theoretic techniques:0.3,Key management:0.3,Logic and verification:0.3,Mathematical foundations of cryptography:1,Public key (asymmetric) techniques:1,Security requirements:0.3,Symmetric cryptography and hash functions:0.3,Trust frameworks:0.8","Formal security models,Mathematical foundations of cryptography,Public key (asymmetric) techniques",Formal security models is relevant for VDOT's security analysis. Mathematical foundations of cryptography and Public key techniques are central to the cryptographic protocol design. Trust frameworks is secondary for distributed trust.
1386,New Notions of Soundness and Simultaneous Resettability in the Public-Key Model,"In this paper, some new notions of soundness in public-key model are presented. We clarify the relationships among our new notions of soundness and the original 4 soundness notions presented by Micali and Reyzin. Our new soundness notions also characterize a new model for ZK protocols in public key model: weak soundness model. By ``weak” we mean for each common input x selected by a malicious prover on the fly, x is used by the malicious prover at most a-priori bounded polynomial times. The weak soundness model just lies in between BPK model and UPK model. Namely, it is weaker than BPK model but stronger than UPK model. In the weak soundness model (also in the UPK model, since weak soundness model implies UPK model), we get a 3-round black-box rZK arguments with weak resettable soundness for NP. Note that simultaneous resettability is an important open problem in the field of ZK protocols. And Reyzin has proven that there are no ZK protocols with resettable soundness in the BPK model. It means that to achieve simultaneous resettability one needs to augment the BPK model in a reasonable fashion. Although Barak et al. [BGGL01] have proven that any language which has a black-box ZK arguments with resettable soundness is in BPP. It is the weak soundness that makes us to get simultaneous resettability. More interestingly, our protocols work in a somewhat ``parallel repetition” manner to reduce the error probability and the verifier indeed has secret information with respect to historical transcripts. Note that in general the error probability of such protocols can not be reduced by parallel repetition. [BIN97] At last, we give a 3-round non-black-box rZK arguments system with resettable soundness for NP in the preprocessing model in which a trusted third party is assumed. Our construction for such protocol is quite simple. Note that although the preprocessing model is quite imposing but it is still quite reasonable as indicated in [CGGM00]. For example, in many e-commerce setting a trusted third party is often assumed. The critical tools used in this paper are: verifiable pseudorandom functions, zap and complexity leveraging. To our knowledge, our protocols are also the second application of verifiable pseudorandom functions. The first application is the 3round rZK arguments with one-time soundness for NP in the public-key model as indicated by Micali and Reyzin [MR01a].","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:1.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Security and privacy,Security and privacy is highly relevant because the paper introduces new cryptographic concepts (soundness and resettability) in zero-knowledge protocols. Other categories lack direct relevance to the security-focused contributions.,"Cryptography:1.0,Database and storage security:0.05,Formal methods and theory of security:0.85,Human and societal aspects of security and privacy:0.05,Intrusion/anomaly detection and malware mitigation:0.05,Network security:0.05,Security in hardware:0.05,Security services:0.1,Software and application security:0.05,Systems security:0.05","Cryptography,Formal methods and theory of security",Cryptography is central to the zero-knowledge protocols. Formal methods and theory of security are relevant for the theoretical soundness notions. Other categories like Network security are not directly connected.,"Formal security models:1,Public key (asymmetric) techniques:1,Cryptanalysis and other attacks:0.3","Formal security models,Public key (asymmetric) techniques","Formal security models are central as the paper introduces new soundness notions. Public key techniques apply to the public-key model framework. Cryptanalysis is less relevant as the focus is on protocol design, not attacks."
2569,Blind 3D Video Watermarking Based on 3D-HEVC Encoder Using Depth,"Due to enormous advancement of internet technology and display devices, 3D video becomes popular in recent times. To ensure secure media transmission, efficient authentication scheme for such 3D video sequence is a requirement. In recent past, watermarking is being regarded as a popular DRM tool for video authentication. It has been observed that video watermarking becomes a challenging task in the presence of advanced auto-stereoscopic display devices and MVD (Multi-view Video plus Depth) based encoding technique in case of 3D video. In this paper, depth image based rendering technique is proposed for blind 3D video watermarking. In this scheme, rendering technique is used to find the Z-axis of the stereo videos (left and right video). The connected regions of the Z-axis of a Group of Picture (GOP) have been filtered using the motion prediction of the video. Block DCT coefficients are used to embed the watermark signal with the selected Z-axis regions of the each video (left and right separately). A comprehensive set of experiments have been done to justify the robustness of the proposed scheme over existing schemes with respect to compression of the 3D-HEVC video codec.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.2,Networks:0.1,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:1.0,Human-centered computing:0.1,Computing methodologies:0.3,Applied computing:0.1,Social and professional topics:0.1",Security and privacy,"Security and privacy is highly relevant as the paper introduces a 3D video watermarking technique for secure transmission. Computing methodologies is less central because the focus is on the application of algorithms to security, not algorithmic theory.","Security services:1.0,Software and application security:0.9,Cryptography:0.2,Database and storage security:0.1,Formal methods and theory of security:0.1,Human and societal aspects of security and privacy:0.1,Intrusion/anomaly detection and malware mitigation:0.3,Network security:0.2,Security in hardware:0.1,Systems security:0.4","Security services,Software and application security",Security services is directly relevant for the watermarking as a security mechanism. Software security is relevant for protecting 3D video applications. Other categories like cryptography are less central to this specific security implementation.,"Access control:0,Authentication:1,Authorization:0,Digital rights management:1,Domain-specific security and privacy architectures:0.5,Privacy-preserving protocols:0,Pseudonymity, anonymity and untraceability:0,Social network security and privacy:0,Software reverse engineering:0,Software security engineering:0.3,Web application security:0","Digital rights management,Authentication",Digital rights management is relevant as watermarking is a DRM tool. Authentication is relevant for verifying video authenticity. Domain-specific security receives a moderate score as the paper addresses 3D video security but not broader domain-specific architectures.
5778,"Keeping Authorities Honest or Bust with Decentralized Witness Cosigning""","The secret keys of critical network authorities -- such as time, name, certificate, and software update services -- represent high-value targets for hackers, criminals, and spy agencies wishing to use these keys secretly to compromise other hosts. To protect authorities and their clients proactively from undetected exploits and misuse, we introduce CoSi, a scalable witness cosigning protocol ensuring that every authoritative statement is validated and publicly logged by a diverse group of witnesses before any client will accept it. A statement S collectively signed by W witnesses assures clients that S has been seen, and not immediately found erroneous, by those W observers. Even if S is compromised in a fashion not readily detectable by the witnesses, CoSi still guarantees S's exposure to public scrutiny, forcing secrecy-minded attackers to risk that the compromise will soon be detected by one of the W witnesses. Because clients can verify collective signatures efficiently without communication, CoSi protects clients' privacy, and offers the first transparency mechanism effective against persistent man-in-the-middle attackers who control a victim's Internet access, the authority's secret key, and several witnesses' secret keys. CoSi builds on existing cryptographic multisignature methods, scaling them to support thousands of witnesses via signature aggregation over efficient communication trees. A working prototype demonstrates CoSi in the context of timestamping and logging authorities, enabling groups of over 8,000 distributed witnesses to cosign authoritative statements in under two seconds.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.2,Networks:0.3,Software and its engineering:0.2,Theory of computation:0.2,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.9,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Security and privacy,Security and privacy: The paper presents a cryptographic protocol (CoSi) for protecting network authorities against key compromise attacks. Networks is secondary as the focus is on security mechanisms rather than network protocols.,"Cryptography:0.95,Database and storage security:0.2,Formal methods and theory of security:0.3,Human and societal aspects of security and privacy:0.1,Intrusion/anomaly detection and malware mitigation:0.1,Network security:0.8,Security in hardware:0.1,Security services:0.1,Software and application security:0.1,Systems security:0.1","Cryptography,Network security",Cryptography is highly relevant as the paper introduces a witness cosigning protocol using cryptographic multisignatures. Network security is relevant for the application to network authorities. Other fields like formal methods or intrusion detection are less central to the protocol's design.,"Cryptanalysis and other attacks:0.3,Denial-of-service attacks:0.2,Firewalls:0.2,Information-theoretic techniques:0.4,Key management:0.7,Mathematical foundations of cryptography:0.5,Mobile and wireless security:0.3,Public key (asymmetric) techniques:0.8,Security protocols:1,Symmetric cryptography and hash functions:0.4,Web protocol security:0.3","Security protocols,Public key (asymmetric) techniques","Security protocols is highly relevant as the paper introduces CoSi, a witness cosigning protocol. Public key techniques are relevant due to the use of cryptographic multisignatures. Other categories like Key management are secondary but still moderately relevant."
429,Security in Outsourcing of Association Rule Mining,"Outsourcing association rule mining to an outside service provider brings several important benefits to the data owner. These include (i) relief from the high mining cost, (ii) minimization of demands in resources, and (iii) effective centralized mining for multiple distributed owners. On the other hand, security is an issue; the service provider should be prevented from accessing the actual data since (i) the data may be associated with private information, (ii) the frequency analysis is meant to be used solely by the owner. This paper proposes substitution cipher techniques in the encryption of transactional data for outsourcing association rule mining. After identifying the non-trivial threats to a straightforward one-to-one item mapping substitution cipher, we propose a more secure encryption scheme based on a one-to-n item mapping that transforms transactions non-deterministically, yet guarantees correct decryption. We develop an effective and efficient encryption algorithm based on this method. Our algorithm performs a single pass over the database and thus is suitable for applications in which data owners send streams of transactions to the service provider. A comprehensive cryptanalysis study is carried out. The results show that our technique is highly secure with a low data transformation cost.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.9,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Security and privacy,Security and privacy is directly relevant as the paper focuses on cryptographic techniques to protect data during outsourcing. Other categories are not central to the core contribution of secure data encryption.,"Cryptography:1.0,Database and storage security:0.7,Formal methods and theory of security:0.3,Human and societal aspects of security and privacy:0.2,Intrusion/anomaly detection and malware mitigation:0.1,Network security:0.2,Security in hardware:0.1,Security services:0.3,Software and application security:0.4,Systems security:0.3","Cryptography,Database and storage security",Cryptography is directly relevant as the paper proposes substitution cipher techniques for secure association rule mining. Database and storage security is relevant since the work addresses securing data during outsourcing to third-party services. Other options like Network security are less relevant as the focus is on data encryption rather than network-specific threats.,"Cryptanalysis and other attacks:0.7,Data anonymization and sanitization:0.6,Database activity monitoring:0.2,Information accountability and usage control:0.3,Information-theoretic techniques:0.3,Key management:0.2,Management and querying of encrypted data:0.4,Mathematical foundations of cryptography:0.3,Public key (asymmetric) techniques:0.2,Symmetric cryptography and hash functions:0.8","Symmetric cryptography and hash functions,Cryptanalysis and other attacks","Symmetric cryptography and hash functions: The paper presents a substitution cipher technique for data encryption, which is a symmetric cryptographic method. Cryptanalysis and other attacks: The paper identifies threats to the cipher and analyzes its security, aligning with cryptanalysis. Other categories like Data anonymization were rejected as the focus is on encryption rather than data anonymization techniques."
1441,Secret sharing on trees: problem solved,"We determine the worst case information rate for all secret sharing schemes based on trees. It is the inverse of 2 − 1/c, where c is the size of the maximal core in the tree. A core is a connected subset of the vertices so that every vertex in the core has a neighbor outside the core. The upper bound comes from an application of the entropy method [2, 3], while the lower bound is achieved by a construction using Stinson’s decomposition theorem [7]. It is shown that 2 − 1/c is also the fractional cover number of the tree where the edges of the tree are covered by stars, and the vertex cover should be minimized, cf [5]. We also give an O(n) algorithm which finds an optimal cover on any tree, and thus a perfect secret sharing scheme with optimal rate.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.3,Information systems:0.1,Security and privacy:1.0,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Security and privacy,"Security and privacy: The paper presents a novel secret sharing scheme for trees, a core cryptographic problem. Other fields like Mathematics of Computing are secondary to the security application.","Cryptography:0.9,Database and storage security:0.1,Formal methods and theory of security:0.6,Human and societal aspects of security and privacy:0.1,Intrusion/anomaly detection and malware mitigation:0.1,Network security:0.1,Security in hardware:0.1,Security services:0.2,Software and application security:0.1,Systems security:0.3","Cryptography,Formal methods and theory of security",Cryptography: Core focus on secret sharing schemes. Formal methods and theory of security: Uses entropy methods and decomposition theorems. Other categories are less relevant to the cryptographic protocol analysis.,"Cryptanalysis and other attacks:0,Formal security models:0.5,Information-theoretic techniques:1,Key management:0,Logic and verification:0,Mathematical foundations of cryptography:1,Public key (asymmetric) techniques:0,Security requirements:0,Symmetric cryptography and hash functions:0,Trust frameworks:0","Information-theoretic techniques,Mathematical foundations of cryptography","Information-theoretic techniques: The paper uses entropy methods and fractional cover numbers, core to information-theoretic cryptography. Mathematical foundations of cryptography: The work builds on Stinson’s decomposition theorem, a foundational technique. Formal security models was rejected as the focus is on information-theoretic bounds, not model definitions."
2731,Cryptanalysis of the Improved Cellular Message Encryption Algorithm,This paper analyzes the Improved Cellular Message Encryption Algorithm (CMEA-I) which is an improved version of the Telecommunication Industry Association’s Cellular Message Encryption Algorithm (CMEA). We present a chosen-plaintext attack of CMEA-I which requires less than 850 plaintexts in its adaptive version. This demonstrates that the improvements made over CMEA are ineffective to thwart such attacks and confirms that the security of CMEA and its variants must be reconsidered from the beginning.,"General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:1.0,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Security and privacy,Security and privacy is highly relevant as the paper presents a cryptanalytic attack on an encryption algorithm. Other categories like Theory of computation are less central to the paper's focus on security vulnerabilities.,"Cryptography:0.9,Database and storage security:0.2,Formal methods and theory of security:0.3,Human and societal aspects of security and privacy:0.1,Intrusion/anomaly detection and malware mitigation:0.1,Network security:0.4,Security in hardware:0.2,Security services:0.3,Software and application security:0.2,Systems security:0.2",Cryptography,Cryptography is directly relevant as the paper analyzes and breaks a specific encryption algorithm (CMEA-I). Network security is secondary as the attack demonstrates vulnerabilities in a communication protocol.,"Cryptanalysis and other attacks:1,Symmetric cryptography and hash functions:0.8,Mathematical foundations of cryptography:0.5,Public key (asymmetric) techniques:0.1,Key management:0.2,Information-theoretic techniques:0.1","Cryptanalysis and other attacks,Symmetric cryptography and hash functions",Core is chosen-plaintext attack on symmetric algorithm. Other areas are not discussed.
1025,The design and implementation of a secure CAPTCHA against man-in-the-middle attacks,"In this paper, we propose a novel security protocol for the implementation of CAPTCHA tests that feature advance mechanisms against man-in-the-middle MITM, for short attacks. This type of attack is fulfilled by a malicious entity, the MITM, that leverages on unaware users to mass-solve CAPTCHA tests shielding the access to a service. The protocol that we propose uses collision-resistant hash functions modeled as random oracles to guarantee that the solution to a CAPTCHA test solved by an end user is valid only for the server to which the user is connected to. This will prevent MITM attacks because the user is not directly connected to the server. We developed a reference implementation for our protocol that has a low impact and is easy to use, featuring a software plug-in running in the Firefox web browser, on the client side, and a Java servlet-based application, on the server side. Copyright © 2013 John Wiley & Sons, Ltd.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.3,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.9,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.2,Social and professional topics:0.1",Security and privacy,"Security and privacy: The paper introduces a security protocol to prevent MITM attacks on CAPTCHA systems. Other categories like Software and its engineering or Networks are less relevant as the core contribution is a security mechanism, not software design or communication systems.","Cryptography:0.8,Database and storage security:0.1,Formal methods and theory of security:0.1,Human and societal aspects of security and privacy:0.1,Intrusion/anomaly detection and malware mitigation:0.1,Network security:0.8,Security in hardware:0.1,Security services:0.1,Software and application security:0.1,Systems security:0.1","Cryptography,Network security",Cryptography is relevant due to the use of hash functions for secure CAPTCHA protocols. Network security is relevant as the paper addresses MITM attacks in networked systems. Other categories do not directly align with the paper's focus.,"Cryptanalysis and other attacks:0.1,Denial-of-service attacks:0.1,Firewalls:0.1,Information-theoretic techniques:0.1,Key management:0.1,Mathematical foundations of cryptography:0.1,Mobile and wireless security:0.2,Public key (asymmetric) techniques:0.1,Security protocols:0.9,Symmetric cryptography and hash functions:0.8,Web protocol security:0.1","Security protocols,Symmetric cryptography and hash functions",Security protocols is relevant because the paper introduces a protocol to prevent MITM attacks. Symmetric cryptography and hash functions are relevant due to the use of collision-resistant hash functions as random oracles. Other categories like Cryptanalysis or Web protocol security are not central to the paper's contribution.
5441,SMM-Based Hypervisor Integrity Measurement,"Hypervisors play an important role in the virtualised environment and consequently are a prime target for attacks. Different kinds of attacks have been reported and a great deal of research has been done to address vulnerabilities in hypervisors. Recently, after successful defeat of integrity measurement tools, a new class of measurement tools have been developed capitalising on the SMM to measure the integrity of hypervisors and other system components. Although those new tools are successful in their tasks, they do not take full advantage of the main benefits of SMM: isolation and stealth. We argue that this is due to the architecture those tools employ. Thus, in this paper, we establish a set of requirements and propose a generic architecture to build and deploy an SMM-based hypervisor integrity measurement tool. We believe that such an architecture might be applied to any SMM-based tool.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.8,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Security and privacy,"Security and privacy: Focuses on hypervisor integrity measurement. Other categories (e.g., Networks) are irrelevant as the paper centers on system security.","Cryptography:0.2,Database and storage security:0.1,Formal methods and theory of security:0.3,Human and societal aspects of security and privacy:0.1,Intrusion/anomaly detection and malware mitigation:0.1,Network security:0.1,Security in hardware:0.8,Security services:0.4,Software and application security:0.3,Systems security:0.9","Security in hardware,Systems security",Security in hardware is highly relevant as the paper proposes an SMM-based solution for hypervisor integrity measurement. Systems security is also highly relevant as the paper focuses on system-level security through hardware features. Other security categories are less relevant as the paper specifically addresses hardware-based security at the system level.,"Browser security:0.1,Denial-of-service attacks:0.1,Distributed systems security:0.3,Embedded systems security:0.2,File system security:0.1,Firewalls:0.1,Hardware attacks and countermeasures:0.6,Hardware reverse engineering:0.4,Hardware security implementation:1.0,Information flow control:0.2,Operating systems security:0.8,Tamper-proof and tamper-resistant designs:0.7,Vulnerability management:0.3","Hardware security implementation,Operating systems security",Hardware security implementation is highly relevant due to SMM-based security. Operating systems security is relevant for hypervisor protection. Other fields like firewalls are not directly related to the paper's focus.
1203,Managing Integrity for Data Exchanged on the Web,"The World Wide Web is a medium for publishing data used by collaborating groups and communities of shared interest. This paper proposes mechanisms to support the accuracy and authenticity of published data. In our framework, publishers annotate data with virtually unforgeable evidence of authorship. Intermediaries may query, restructure, and integrate this data while propagating the annotations. Final recipients of the data may then derive useful conclusions about the authenticity of the data they receive.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.8,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Security and privacy,Security and privacy is relevant due to the focus on data authenticity and integrity mechanisms. Other categories like 'Information systems' are less directly relevant because the paper emphasizes security protocols rather than general data systems.,"Database and storage security:1.0,Security services:0.9,Cryptography:0.7,Formal methods and theory of security:0.5,Human and societal aspects of security and privacy:0.3,Intrusion/anomaly detection and malware mitigation:0.2,Network security:0.2,Security in hardware:0.2,Software and application security:0.2,Systems security:0.2","Database and storage security,Security services",Database and storage security is primary for data authenticity mechanisms. Security services is relevant for the proposed annotation framework. Cryptography is partially relevant but not the core focus.,"Access control:0.1,Authentication:1,Authorization:0.2,Data anonymization and sanitization:0.1,Database activity monitoring:0.1,Digital rights management:0.1,Information accountability and usage control:0.2,Management and querying of encrypted data:0.1,Privacy-preserving protocols:0.3,Pseudonymity, anonymity and untraceability:0.1",Authentication,Authentication is relevant as the paper proposes mechanisms for verifying data authorship. Other categories like authorization or privacy-preserving protocols are not central to the work.
4176,Performance Improvements and a Baseline Parameter Generation Algorithm for NTRUSign,"The NTRUSign signature scheme was introduced in [8]. The original presentation gave a theoretical description of the scheme and an analysis of its security, along with several parameter choices which claimed to yield an 80 bit security level. The paper [8] did not give a general recipe for generating parameter sets to a specific level of security. In line with recent research on NTRUEncrypt [9], this paper presents an outline of such a recipe for NTRUSign. NTRUSign has many more implementation options than NTRUEncrypt, and research is ongoing to improve the efficiency of NTRUSign operations at a given security level. This paper is therefore not intended to be the last word on parameter generation for NTRUSign, but to provide a specific parameter generation algorithm whose output has, we believe, the stated security properties. We also present certain technical advances upon which we intend to build in subsequent papers. In addition to outlining a parameter generation algorithm for NTRUSign, this paper makes the following four important contributions.","General and reference:0.1,Hardware:0.2,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.2,Information systems:0.1,Security and privacy:0.9,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Security and privacy,Security and privacy is relevant because the paper addresses cryptographic parameter generation for NTRUSign. Other categories are less applicable.,"Cryptography:0.9,Database and storage security:0.1,Formal methods and theory of security:0.1,Human and societal aspects of security and privacy:0.1,Intrusion/anomaly detection and malware mitigation:0.1,Network security:0.1,Security in hardware:0.1,Security services:0.2,Software and application security:0.1,Systems security:0.1",Cryptography,Cryptography is directly relevant as the paper focuses on parameter generation for the NTRUSign signature scheme. Security services is only marginally relevant as the focus is on cryptographic algorithms rather than service implementation.,"Cryptanalysis and other attacks:0.5,Information-theoretic techniques:0,Key management:0,Mathematical foundations of cryptography:1,Public key (asymmetric) techniques:1,Symmetric cryptography and hash functions:0","Public key (asymmetric) techniques,Mathematical foundations of cryptography",Public key (asymmetric) techniques is relevant for the signature scheme. Mathematical foundations of cryptography is relevant for lattice-based NTRU math. Cryptanalysis has limited relevance as the paper focuses on design.
6011,An operating system security method for integrity and privacy protection in consumer electronics,"Linux is an open source operating system that is rapidly gaining popularity in consumer electronic devices. It has started being deployed in mobile phones and digital audio and video devices. However, with its success in the market, there is a greater need for security. The traditional Discretionary Access Control (DAC) model does not provide adequate security support. Mandatory Access Control (MAC) models can provide additional security in addition to the DAC scheme. However, some of the existing MAC schemes are too complex to be used in embedded devices, while others are simple yet do not provide fine-grained access control. In this paper, we present a MAC model that grants subjects permissions based on their past behavior. It is simple but it also provides a fine-grained access control to the operating system. With its small footprint, it can be applied to various systems, including embedded devices.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.2,Networks:0.1,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:1.0,Human-centered computing:0.2,Computing methodologies:0.3,Applied computing:0.2,Social and professional topics:0.1",Security and privacy,Security and privacy is highly relevant as the paper focuses on operating system security mechanisms like MAC and DAC. Other categories like Computer systems organization are less directly relevant as the core contribution is security-focused.,"Cryptography:0.1,Database and storage security:0.1,Formal methods and theory of security:0.7,Human and societal aspects of security and privacy:0.3,Intrusion/anomaly detection and malware mitigation:0.1,Network security:0.1,Security in hardware:0.1,Security services:0.2,Software and application security:0.8,Systems security:0.9","Systems security,Software and application security,Formal methods and theory of security",Systems security is core for the operating system-level MAC model. Software and application security applies to implementation in Linux. Formal methods and theory of security are relevant for the access control design. Other categories like Network security are less relevant.,"Browser security:0.1,Denial-of-service attacks:0.2,Distributed systems security:0.4,Domain-specific security and privacy architectures:0.3,File system security:0.2,Firewalls:0.1,Formal security models:1.0,Information flow control:0.3,Logic and verification:0.2,Operating systems security:1.0,Security requirements:0.3,Social network security and privacy:0.1,Software reverse engineering:0.2,Software security engineering:0.4,Trust frameworks:0.3,Vulnerability management:0.3,Web application security:0.2","Operating systems security,Formal security models",The paper introduces a MAC model for OS security. Formal security models are relevant for the theoretical framework. Other categories like 'Web security' are irrelevant.
2147,User-level secure deletion on log-structured file systems,"Deleting a file from a storage medium serves two purposes: it reclaims storage resources and ensures that any sensitive information contained in the file becomes inaccessible. When done for the latter purpose, it is critical that the file is securely deleted, meaning that its content does not persist on the storage medium after deletion. Secure deletion is the act of deleting data from a storage medium such that the data is afterwards irrecoverable from the storage medium. The time between deleting data and it becoming irrecoverable is called the deletion latency.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.1,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:1.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Security and privacy,Security and privacy is highly relevant because the paper addresses secure deletion mechanisms to prevent data recovery. Other categories like Software and its engineering received low scores as the focus is on security guarantees rather than software design.,"Database and storage security:1.0,Security in hardware:0.75,Cryptography:0.25,Formal methods and theory of security:0.25,Human and societal aspects of security and privacy:0.25,Intrusion/anomaly detection and malware mitigation:0.25,Network security:0.25,Security services:0.25,Software and application security:0.25,Systems security:0.25","Database and storage security,Security in hardware","Database and storage security is highly relevant as the paper focuses on secure deletion in storage systems. Security in hardware is partially relevant due to hardware-level considerations. Other options are irrelevant as the paper does not address cryptography, formal methods, or privacy.","Data anonymization and sanitization:1,Database activity monitoring:0.2,Embedded systems security:0.3,Hardware attacks and countermeasures:0.1,Hardware reverse engineering:0.1,Hardware security implementation:0.2,Information accountability and usage control:0.4,Management and querying of encrypted data:0.3,Tamper-proof and tamper-resistant designs:1","Data anonymization and sanitization,Tamper-proof and tamper-resistant designs","Data anonymization and sanitization: Secure deletion ensures data irrecoverability. Tamper-proof and tamper-resistant designs: Prevent unauthorized access to deleted data. 'Database activity monitoring' is irrelevant as the focus is on deletion, not monitoring."
253,Privacy Challenges in Third-Party Location Services,"The concern for location privacy in mobile applications is commonly motivated by a scenario in which a mobile device communicates personal location data, i.e. the device holder location, to a third party e.g. LBS provider, in exchange for some information service. We argue that this scenario offers a partial view of the actual risks for privacy, because in reality the information How can be more complex. For example, more and more often location is computed by a third party, the location provider, e.g. Google Location Service. Location providers are in the position of collecting huge amounts of location data from the users of diverse applications (e.g. Facebook and Foursquare to cite a few). This raises novel privacy concerns. In this paper, we discuss two issues related to the protection from location providers. The first focuses on the compliance of emerging location services standards with European data protection norms; the latter focuses on hard privacy solutions protecting from untrusted location providers.","General and reference:0.05,Hardware:0.05,Computer systems organization:0.05,Networks:0.05,Software and its engineering:0.1,Theory of computation:0.05,Mathematics of computing:0.05,Information systems:0.15,Security and privacy:0.9,Human-centered computing:0.05,Computing methodologies:0.2,Applied computing:0.05,Social and professional topics:0.1",Security and privacy,Security and privacy is highly relevant as the paper focuses on location privacy risks and solutions for third-party services. Information systems is secondary as it mentions data systems but not the primary focus.,"Cryptography:0.1,Database and storage security:0.2,Formal methods and theory of security:0.1,Human and societal aspects of security and privacy:0.8,Intrusion/anomaly detection and malware mitigation:0.1,Network security:0.1,Security in hardware:0.1,Security services:0.75,Software and application security:0.1,Systems security:0.1","Human and societal aspects of security and privacy,Security services",Human and societal aspects of security and privacy: The paper analyzes privacy risks in location services from a societal perspective. Security services: It proposes solutions for protecting against untrusted location providers. Database and storage security is less relevant as the focus is on service-level privacy.,"Access control:0.3,Authentication:0.3,Authorization:0.3,Digital rights management:0.2,Economics of security and privacy:0.6,Privacy protections:1.0,Privacy-preserving protocols:0.8,Pseudonymity, anonymity and untraceability:0.3,Social aspects of security and privacy:0.4,Usability in security and privacy:0.3",Privacy protections,Privacy protections is highly relevant as the paper directly addresses privacy in location services. Privacy-preserving protocols is moderately relevant as the paper discusses solutions for protecting location data. Other options are less relevant as they don't directly match the focus.
2807,Simulation of Computer Network Attacks,"In this work we present a prototype for simulating computer network attacks. Our objective is to simulate large networks (thousands of hosts, with applications and vulnerabilities) while remaining realistic from the attacker's point of view. The foundation for the simulator is a model of computer intrusions, based on the analysis of real world attacks. In particular we show how to interpret vulnerabilities and exploits as communication channels. This conceptual model gives a tool to describe the theater of operations, targets, actions and assets involved in multistep network attacks. We conclude with applications of the attack simulator.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.5,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:1.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Security and privacy,Security and privacy is relevant because the paper focuses on simulating network attacks and vulnerabilities. Networks is secondary as the work involves network modeling but not core networking protocols.,"Cryptography:0.1,Database and storage security:0.1,Formal methods and theory of security:0.3,Human and societal aspects of security and privacy:0.1,Intrusion/anomaly detection and malware mitigation:0.85,Network security:0.9,Security in hardware:0.1,Security services:0.2,Software and application security:0.1,Systems security:0.2","Network security,Intrusion/anomaly detection and malware mitigation",Network security is directly relevant as the paper models network attacks. Intrusion/anomaly detection is relevant due to the analysis of vulnerabilities and exploits. Other categories like Systems security are less specific.,"Denial-of-service attacks:0.8,Firewalls:0,Intrusion detection systems:0.7,Malware and its mitigation:1,Mobile and wireless security:0,Security protocols:0,Social engineering attacks:0,Web protocol security:0","Malware and its mitigation,Denial-of-service attacks",Malware and its mitigation is directly relevant as the paper discusses simulating network attacks involving vulnerabilities and exploits. Denial-of-service attacks is secondary as the framework could model DDoS scenarios. Other categories like Firewalls or Web protocol security are not discussed in the context of the attack simulation framework.
2939,Automatic secure fingerprint verification system based on fuzzy vault scheme,"We construct an automatic secure fingerprint verification system based on the fuzzy vault scheme to address a major security hole currently existing in most biometric authentication systems. The construction of the fuzzy vault during the enrollment phase is automated by aligning the most reliable reference points between different templates, based on which the converted features are used to form the lock set. The size of the fuzzy vault, the degree of the underlying polynomial, as well as the number of templates needed for reaching the reliable reference point are investigated. This results in a high unlocking complexity for attackers with an acceptable unlocking accuracy for legal users.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.9,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Security and privacy,Security and privacy is directly relevant as the paper presents a secure biometric authentication system using the fuzzy vault scheme. Computing methodologies (0.2) has limited relevance to the implementation details but is secondary to the security mechanism. Other categories like Information systems are irrelevant to this cryptographic work.,"Cryptography:1.0,Database and storage security:0.2,Formal methods and theory of security:0.3,Human and societal aspects of security and privacy:0.1,Intrusion/anomaly detection and malware mitigation:0.1,Network security:0.4,Security in hardware:0.1,Security services:0.8,Software and application security:0.3,Systems security:0.2","Cryptography,Security services",Cryptography is central to the fuzzy vault scheme for secure biometric storage. Security services applies to the authentication system. Network security is less directly relevant to the core contribution.,"Access control:0.3,Authentication:1,Authorization:0.2,Cryptanalysis and other attacks:0.1,Digital rights management:0.1,Information-theoretic techniques:0.4,Key management:0.2,Mathematical foundations of cryptography:0.6,Privacy-preserving protocols:0.5,Pseudonymity, anonymity and untraceability:0.3,Public key (asymmetric) techniques:0.4,Symmetric cryptography and hash functions:0.3","Authentication,Privacy-preserving protocols",Authentication: The system implements biometric authentication. Privacy-preserving protocols: Uses fuzzy vault techniques to protect biometric data. Public key cryptography is less relevant as the focus is on fuzzy vault schemes rather than asymmetric cryptography.
1565,ANASTASIA: ANdroid mAlware detection using STatic analySIs of Applications,"The number of malware applications targeting the Android operating system has significantly increased in recent years. Malicious applications pose a significant threat to Android platform security. We propose ANASTASIA, a system to detect malicious Android applications through statically analyzing applications' behaviors. ANASTASIA provides a more complete coverage of security behaviors when compared to state-of-the-art solutions. We utilize a large number of statically extracted features from various security behavioral characteristics of an application. We built a Machine Learning-based detection framework with high performance detection and acceptable false positive rate. The significance of our work is to develop a lightweight malware detection system for Android-powered smartphones that leverages robust, effective, and efficient features. Besides, in order to assess our solution, we used a reliable, large-scale, and updated malware data-set in terms of diversity and number of malware applications. We evaluated the performance of our proposal on large-scale malware data-set (including 18,677 malware and 11,187 benign apps). Our experimental results show a true positive rate of 97.3% and a false negative rate of 2.7%. These results are better than what are reported by state-of-the-art Android malware detection methods.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:1.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Security and privacy,Security and privacy is directly relevant as the paper introduces an Android malware detection system. Other fields are irrelevant as the focus is on security analysis.,"Cryptography:0.0,Database and storage security:0.0,Formal methods and theory of security:0.0,Human and societal aspects of security and privacy:0.0,Intrusion/anomaly detection and malware mitigation:0.75,Network security:0.0,Security in hardware:0.0,Security services:0.0,Software and application security:0.75,Systems security:0.0","Intrusion/anomaly detection and malware mitigation,Software and application security",Intrusion detection is relevant as the paper focuses on malware detection. Software and application security is relevant for Android application security. Categories like Network security are less relevant as the focus is on application-level security.,"Domain-specific security and privacy architectures:0.3,Intrusion detection systems:0.6,Malware and its mitigation:1.0,Social engineering attacks:0.1,Social network security and privacy:0.1,Software reverse engineering:0.2,Software security engineering:0.4,Web application security:0.1","Malware and its mitigation,Intrusion detection systems",Malware and its mitigation: The paper proposes a system for Android malware detection. Intrusion detection systems: The static analysis approach aligns with intrusion detection principles. Other categories like Web application security are less relevant.
2044,Third-Party Online Payment System Based on Campus Card,"This paper discusses the possibility of Third-Party online payment system based on campus card, then analyses the security of online payment based on campus card and finally provides the scheme of realization. In the realization, virtual account is introduced. According to the characteristic of campus commerce, this paper originates the escrow without third party logistics. KeywordsOnline payment; Campus card; Third-Party online payment platform; Escrow; SSL","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.2,Software and its engineering:0.3,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.9,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Security and privacy,"Security and privacy: The paper focuses on the security of a third-party online payment system, analyzing risks and proposing secure implementation strategies. Other fields like Networks or Software Engineering are only tangentially related as the core contribution is security analysis.","Cryptography:0.05,Database and storage security:0.1,Formal methods and theory of security:0.05,Human and societal aspects of security and privacy:0.05,Intrusion/anomaly detection and malware mitigation:0.05,Network security:0.8,Security in hardware:0.05,Security services:0.85,Software and application security:0.1,Systems security:0.05","Network security,Security services",Network security: The paper discusses secure online payment systems and uses SSL. Security services: The research focuses on payment platform security and escrow mechanisms. Other children rejected: The paper doesn't address cryptography in depth or malware detection.,"Access control:0.3,Authentication:0.3,Authorization:0.3,Denial-of-service attacks:0.1,Digital rights management:0.2,Firewalls:0.1,Mobile and wireless security:0.2,Privacy-preserving protocols:0.2,Pseudonymity, anonymity and untraceability:0.1,Security protocols:0.8,Web protocol security:0.7","Security protocols,Web protocol security",Security protocols is directly relevant for payment system design. Web protocol security applies to SSL implementation and online transaction security.
1518,Enhancing the Scalability and Memory Usage of Hashsieve on Multi-core CPUs,"The Shortest Vector Problem (SVP) is a key problem in lattice-based cryptography and cryptanalysis. While the cryptography community has accumulated a vast knowledge of SVP-solvers from a theoretical standpoint, the practical performance of these algorithms is commonly not well understood. This gap in knowledge poses many challenges to cryptographers, who are oftentimes confronted with algorithms that perform worse in practice then expected from theory. This is a problem because the asymptotic complexity of the best algorithms plays a key role in the construction of cryptosystems, but only practically appealing, validated algorithms are accounted for in this process. Thus, if one cannot extract the full potential of theoretically strong algorithms in practice, efficient algorithms might be ruled out and wrong assumptions are made when constructing cryptosystems. In this paper, we take a step forward to fill this gap, by providing a computational analysis of HashSieve, the most practical sieving SVP-solver to date, and showing how its performance can be enhanced in practice. To this end, we revisit the parallel generation of random numbers, memory allocation and memory access patterns. Employing scalable random sampling, object memory pools, scalable memory allocators and aggressive memory prefetching, we were able to improve the best current implementation of HashSieve by factors of 3x and 4x, depending on the lattice dimension, and set new records for the HashSieve algorithm, thereby shrinking the gap between its theoretical complexity and its performance in practice.","General and reference:0.0,Hardware:0.2,Computer systems organization:0.2,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.2,Information systems:0.0,Security and privacy:0.9,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Security and privacy,Security and privacy is central to the cryptographic SVP-solving work. Hardware and Mathematics of computing are secondary. Other categories are irrelevant.,"Cryptography:0.9,Database and storage security:0.2,Formal methods and theory of security:0.3,Human and societal aspects of security and privacy:0.1,Intrusion/anomaly detection and malware mitigation:0.1,Network security:0.2,Security in hardware:0.3,Security services:0.2,Software and application security:0.2,Systems security:0.3",Cryptography,Cryptography is highly relevant as the paper focuses on improving the practical performance of a lattice-based cryptographic algorithm. Other security categories are not central to the core contribution of optimizing the SVP-solver.,"Cryptanalysis and other attacks:1,Information-theoretic techniques:0,Key management:0,Mathematical foundations of cryptography:0.5,Public key (asymmetric) techniques:0,Symmetric cryptography and hash functions:0","Cryptanalysis and other attacks,Mathematical foundations of cryptography","Cryptanalysis and other attacks: The paper focuses on improving a cryptanalytic algorithm (Hashsieve) for solving the Shortest Vector Problem, which is central to cryptanalysis. Mathematical foundations of cryptography: The SVP is a mathematical problem in lattice-based cryptography, though the paper's focus is on practical performance rather than theoretical aspects. Other categories are not relevant as the paper doesn't discuss key management, symmetric cryptography, or information-theoretic techniques."
776,Protecting Web Browser Extensions from JavaScript Injection Attacks,"Vulnerable web browser extensions can be used by an attacker to steal users' credentials and lure users into leaking sensitive information to unauthorized parties. Current browser security models and existing JavaScript security solutions are inadequate for preventing JavaScript injection attacks that can exploit such vulnerable extensions. In this paper, we present a runtime protection mechanism based on a code randomization technique coupled with a static analysis technique to protect browser extensions from JavaScript injection attacks. The protection is enforced at runtime by distinguishing malicious code from the randomized extension code. We implemented our protection mechanism for the Mozilla Firefox browser and evaluated it on a set of vulnerable and non-vulnerable Firefox extensions. The evaluation results indicate that our approach can be a viable solution for preventing attacks on JavaScript-based browser extensions. In designing and implementing our approach, we were also able to reduce false positives and achieve maximum backward compatibility with existing extensions.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.95,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Security and privacy,Security and privacy is highly relevant as the paper addresses JavaScript injection attack protection for web browser extensions. The security mechanisms and runtime protection focus clearly place it in this category.,"Cryptography:0.0,Database and storage security:0.0,Formal methods and theory of security:0.25,Human and societal aspects of security and privacy:0.0,Intrusion/anomaly detection and malware mitigation:0.5,Network security:0.0,Security in hardware:0.0,Security services:0.0,Software and application security:1.0,Systems security:0.5",Software and application security,Software and application security is directly relevant as the paper addresses browser extension security. Intrusion detection is partially relevant for runtime protection. Systems security is marginally relevant but not the primary focus. Other categories are not applicable.,"Domain-specific security and privacy architectures:0.1,Social network security and privacy:0.1,Software reverse engineering:0.1,Software security engineering:1.0,Web application security:1.0","Software security engineering,Web application security",Software security engineering is relevant for the runtime protection mechanism design. Web application security is relevant as the focus is on browser extensions and JavaScript attacks. Other children are unrelated to browser-specific security challenges.
5863,Proactive Containment of Malice in Survivable Distributed Systems,"The uncontrolled propagation of faults due to malicious intrusion can severely decrease system performance and survivability. Our goal is to employ available information about known or suspected faults in order to provide collusionavoidance and epidemic-avoidance. We proactively make use of knowledge of faults to notify potentially damaged areas of the system, in order to contain the tainted parts. Our objective is to lessen the impact of an intrusion, by spreading the performance cost of recovery over a controlled period of time.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.2,Networks:0.1,Software and its engineering:0.3,Theory of computation:0.1,Mathematics of computing:0.2,Information systems:0.2,Security and privacy:0.9,Human-centered computing:0.1,Computing methodologies:0.4,Applied computing:0.3,Social and professional topics:0.1",Security and privacy,Security and privacy is highly relevant as the paper addresses malicious intrusion containment. Other categories like Computer systems organization are less directly related to the security-focused contribution.,"Cryptography:0.2,Database and storage security:0.3,Formal methods and theory of security:0.4,Human and societal aspects of security and privacy:0.2,Intrusion/anomaly detection and malware mitigation:0.9,Network security:0.3,Security in hardware:0.1,Security services:0.5,Software and application security:0.6,Systems security:0.8","Intrusion/anomaly detection and malware mitigation,Systems security",Intrusion detection is central to the paper's focus on containing malicious intrusions. Systems security is relevant as the paper addresses system-level survivability. Network security receives lower scores as the focus is on distributed system containment rather than network-specific threats.,"Browser security:0.1,Denial-of-service attacks:0.2,Distributed systems security:1.0,File system security:0.1,Firewalls:0.2,Information flow control:0.3,Intrusion detection systems:0.8,Malware and its mitigation:0.3,Operating systems security:0.2,Social engineering attacks:0.1,Vulnerability management:0.2","Distributed systems security,Intrusion detection systems",Distributed systems security is central to the paper's focus on containment in distributed environments. Intrusion detection systems are relevant to the proactive fault monitoring. Other categories like malware mitigation are less specific to the system-level containment approach.
1493,Privacy Shielding by Design — A Strategies Case for Near-Compliance,"Changes to the EU-US agreements on transatlantic data transmission are accepted. With the updates leading to an adequacy decision for the Privacy Shield, the European Commission further advances US adherence to the General Data Protection Regulation. The regulation comes with increasing territorial scope for the processing of personal data of persons in the EU, and includes the risk of substantial fines. Soon, a Privacy Shield self-certification will be necessary for US organizations which process EU data. Compliance with these requirements may be assisted by privacy by design. In particular, a recent approach to this uses privacy design strategies. Our paper takes this approach and applies it to the Privacy Shield and its suggested changes. It then explores a case study within scope of the Privacy Shield to demonstrate how to apply privacy by design using strategies.","General and reference:0.1,Hardware:0.05,Computer systems organization:0.05,Networks:0.1,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.2,Security and privacy:0.9,Human-centered computing:0.2,Computing methodologies:0.2,Applied computing:0.2,Social and professional topics:0.1",Security and privacy,Security and privacy is highly relevant as the paper addresses privacy shielding and compliance strategies under the Privacy Shield. Other categories like Social and professional topics are less directly related to the core focus on privacy mechanisms.,"Human and societal aspects of security and privacy:0.9,Database and storage security:0.7,Security services:0.7,Formal methods and theory of security:0.3,Intrusion/anomaly detection and malware mitigation:0.2","Human and societal aspects of security and privacy,Database and storage security,Security services",Human and societal aspects is relevant for privacy strategies. Database and storage security is relevant for data protection. Security services is relevant for compliance. Other categories are less relevant as the focus is not on formal methods or intrusion detection.,"Access control:0.3,Authentication:0.3,Authorization:0.3,Data anonymization and sanitization:0.4,Database activity monitoring:0.2,Digital rights management:0.3,Economics of security and privacy:0.4,Information accountability and usage control:0.5,Management and querying of encrypted data:0.3,Privacy protections:1.0,Privacy-preserving protocols:0.6,Pseudonymity, anonymity and untraceability:0.4,Social aspects of security and privacy:0.5,Usability in security and privacy:0.6",Privacy protections,Privacy protections: Focuses on privacy design strategies for compliance. Other options: Specific technical controls not primary focus.
3586,Generic Danger Detection for Mission Continuity,"Mobile Ad-hoc Networks (MANETs) have become the environment of choice for providing edge connectivity to mobile forces. In particular, next-generation military systems leverage MANET technology to provide information assets to troops. However, MANETs face a number of serious security exposures, which are a superset of traditional networks. In prior work, we have described BITSI, the Biologically-Inspired Tactical Security Infrastructure, which attempts to address these challenges. BITSI uses a variety of techniques inspired by biological systems to provide effect-based security that is centered upon mission enablement. One of these techniques is the application of Danger Theory to mission continuity. In this paper, we explore different ways of implementing danger detection within BITSI, and show how generic approaches that are low-cost both computationally and in terms of implementation can provide acceptable results.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.2,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:1.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Security and privacy,Security and privacy is directly relevant as the paper addresses security in MANETs using danger theory. Networks receives a low score because the focus is on security mechanisms rather than network protocols.,"Network security:1,Intrusion/anomaly detection and malware mitigation:1,Cryptography:0.3,Database and storage security:0.2,Formal methods and theory of security:0.3,Human and societal aspects of security and privacy:0.2,Security in hardware:0.3,Security services:0.2,Software and application security:0.3,Systems security:0.3","Network security,Intrusion/anomaly detection and malware mitigation",Network security is relevant due to the focus on MANET security. Intrusion detection is relevant as the paper discusses danger detection. Other security categories are less directly addressed.,"Denial-of-service attacks:0,Firewalls:0,Intrusion detection systems:1,Malware and its mitigation:0,Mobile and wireless security:1,Security protocols:1,Social engineering attacks:0,Web protocol security:0","Intrusion detection systems,Mobile and wireless security,Security protocols",Intrusion detection systems is relevant because the paper discusses danger detection for security. Mobile and wireless security applies due to the focus on MANETs. Security protocols is relevant as the paper proposes a biologically inspired security infrastructure. Other categories are irrelevant as they do not address mission continuity or MANET-specific threats.
395,Key establishment protocols for secure communication in clustered sensor networks,"Clustering in wireless sensor networks WSN is a technique used to improve the lifetime of the network. Heinzelman et al. proposed a clustering method called LEACH which rotates the role of CH among the nodes in the network. However, it does not provide any security. In this paper, we propose two deterministic key establishment protocols to counter the vulnerabilities of LEACH protocol. These protocols are called polynomial-based key establishment protocol PBKEP and hash-based key establishment protocol HBKEP. These protocols ensure that it is always possible to establish a secret key with the current CH in every round of LEACH clustering protocol. The protocols also ensure that a malicious node cannot join the cluster at any round. The clusters are formed only among the legitimate nodes. Also any malicious node pretending as CH will not succeed as no other legitimate node will choose that as CH.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.95,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Security and privacy,Security and privacy is highly relevant as the paper introduces cryptographic key establishment protocols for sensor networks. Other categories like Networks are secondary to the security focus.,"Cryptography:1.0,Network security:0.9,Security services:0.7,Formal methods and theory of security:0.6,Intrusion/anomaly detection and malware mitigation:0.3,Database and storage security:0.2,Human and societal aspects of security and privacy:0.1,Security in hardware:0.2,Software and application security:0.3,Systems security:0.4","Cryptography,Network security",Cryptography is directly relevant as the paper introduces key establishment protocols. Network security is also relevant due to the focus on securing clustered sensor networks. Other categories like intrusion detection are less central as the paper does not address malicious attacks directly.,"Cryptanalysis and other attacks:0.1,Denial-of-service attacks:0.1,Firewalls:0.1,Information-theoretic techniques:0.1,Key management:1.0,Mathematical foundations of cryptography:0.2,Mobile and wireless security:0.9,Public key (asymmetric) techniques:0.1,Security protocols:0.8,Symmetric cryptography and hash functions:0.1,Web protocol security:0.1","Key management,Mobile and wireless security,Security protocols",Key management is central to the protocols. Mobile and wireless security applies to WSNs. Security protocols are explicitly proposed. Other categories like cryptanalysis are not discussed.
1850,Location sharing privacy preference: analysis and personalized recommendation,"Location-based systems are becoming more popular with the explosive growth in popularity of smart phones. However, the user adoption of these systems is hindered by growing user concerns about privacy. To design better location-based systems that attract more user adoption and protect users from information under/overexposure, it is highly desirable to understand users' location sharing and privacy preferences. This paper makes two main contributions. First, by studying users' location sharing privacy preferences with three groups of people (i.e., Family, Friend and Colleague) in different contexts, including check-in time, companion and emotion, we reveal that location sharing behaviors are highly dynamic, context-aware, audience-aware and personal. In particular, we find that emotion and companion are good contextual predictors of privacy preferences. Moreover, we find that there are strong similarities or correlations among contexts and groups. Our second contribution is to show, in light of the user study, that despite the dynamic and context-dependent nature of location sharing, it is still possible to predict a user's in-situ sharing preference in various contexts. More specifically, we explore whether it is possible to give users a personalized recommendation of the sharing setting they are most likely to prefer, based on context similarity, group correlation and collective check-in preference. PPRec, the proposed recommendation algorithm that incorporates the above three elements, delivers personalized recommendations that could be helpful to reduce both user's burden and privacy risk. It also provides additional insights into the relative usefulness of different personal and contextual factors in predicting users' sharing behavior.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.9,Human-centered computing:0.2,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Security and privacy,Security and privacy: The paper focuses on privacy-preserving location sharing mechanisms and personalized recommendation systems. Human-centered computing (0.2) is secondary as the focus is on privacy mechanisms rather than human interaction design.,"Human and societal aspects of security and privacy:1.0,Security services:0.8,Cryptography:0.2,Database and storage security:0.1,Formal methods and theory of security:0.3,Intrusion/anomaly detection and malware mitigation:0.2,Network security:0.3,Security in hardware:0.1,Software and application security:0.2,Systems security:0.2","Human and societal aspects of security and privacy,Security services",Human and societal aspects... is directly relevant for analyzing privacy preferences. Security services is relevant for the recommendation system as a privacy protection mechanism. Cryptography and Network security are less central to the study of user behavior.,"Access control:0.2,Authentication:0.1,Authorization:0.1,Digital rights management:0.1,Economics of security and privacy:0.2,Privacy protections:1,Privacy-preserving protocols:0.5,Pseudonymity, anonymity and untraceability:0.3,Social aspects of security and privacy:1,Usability in security and privacy:0.6","Privacy protections,Social aspects of security and privacy",Privacy protections is central to the user preference analysis. Social aspects are relevant due to context-aware sharing with different groups. Usability is secondary but still pertinent.
3208,SPAM trends and techniques used on Exchange Online system used for racunarstvo.hr domain,"This paper will give an overview of the technologies and techniques employed in SPAM protection on University College Algebra (UCA). UCA uses Exchange Online system included in Office 365 service that is free of charge for every educational institution covering majority of countries around the world. In today's world of electronic communication, e-mail is still primary system for exchanging messages and data. Globally, average mail user daily sends and receive around 115 e-mail messages and from that number around 19% or 13 of received e-mail messages are considered as SPAM. Exchange Online system is based on the Microsoft Exchange 2013 server and it is located in the multi-tenancy datacentres around the world, serving corporate, governmental and educational users. That system has built in anti-SPAM protection and reporting services that will be used for conducting results in this paper. Those results will be than compared with other publicly available researches to achieve comparison reports that is presented in this paper.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.2,Software and its engineering:0.3,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.9,Human-centered computing:0.2,Computing methodologies:0.1,Applied computing:0.3,Social and professional topics:0.1",Security and privacy,Security and privacy is highly relevant as the paper focuses on SPAM protection systems and email security analysis. Other categories like Applied computing (0.3) relate to system implementation but are secondary to the security focus. Software and its engineering (0.3) and Networks (0.2) are tangentially related but not primary domains.,"Cryptography:0.1,Database and storage security:0.2,Formal methods and theory of security:0.1,Human and societal aspects of security and privacy:0.1,Intrusion/anomaly detection and malware mitigation:0.85,Network security:0.7,Security in hardware:0.1,Security services:0.65,Software and application security:0.1,Systems security:0.2","Intrusion/anomaly detection and malware mitigation,Network security,Security services",Intrusion/anomaly detection and malware mitigation is relevant as the paper analyzes SPAM protection techniques. Network security is relevant due to the focus on email communication systems. Security services is relevant as it discusses built-in protection mechanisms. Other categories like Cryptography or Database security are not directly addressed.,"Access control:0.0,Authentication:0.0,Authorization:0.0,Denial-of-service attacks:0.0,Digital rights management:0.0,Firewalls:0.1,Intrusion detection systems:0.0,Malware and its mitigation:0.9,Mobile and wireless security:0.0,Privacy-preserving protocols:0.0,Pseudonymity, anonymity and untraceability:0.0,Security protocols:0.2,Social engineering attacks:0.0,Web protocol security:0.0",Malware and its mitigation,"Malware and its mitigation: The paper focuses on SPAM protection, which is a form of malware mitigation. Firewalls and Security protocols are only tangentially relevant (mentioned as part of the system but not the core contribution). Other categories like Social engineering attacks are not discussed."
4599,DEcryption Contract ENforcement Tool (DECENT): A Practical Alternative to Government Decryption Backdoors,A cryptographic contract and enforcement technology would guarantee release of a data decryption key to an authorized party if and only if predetermined contract requirements are satisfied. Threshold secret sharing can be used to eliminate the need for access to the hidden key under normal circumstances. It can also eliminate the liability and burden normally carried by device manufacturers or service providers when they store the decryption keys of their customers. Blockchain technology provides a mechanism for a public audit trail of the creation and release of the hidden key. The use of peer-to-peer mix-net network overlay technology can be added to insure that the blockchain audit trail documents the release of the key even if an all-powerful entity forces actors to act under duress.,"General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:1.0,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Security and privacy,"Security and privacy is relevant because the paper addresses cryptographic contracts, decryption enforcement, and blockchain-based audit trails for secure key management. Other categories like Software or Networks are less relevant as the focus is on security protocols and privacy mechanisms.","Cryptography:1.0,Security services:0.8,Systems security:0.6,Database and storage security:0.2,Formal methods and theory of security:0.4,Human and societal aspects of security and privacy:0.1,Intrusion/anomaly detection and malware mitigation:0.1,Network security:0.3,Software and application security:0.2","Cryptography,Security services",Cryptography is directly relevant due to threshold secret sharing and blockchain-based key management. Security services are relevant for audit trails and access control. Other categories like Systems security and Database and storage security are less central as the paper focuses on cryptographic protocols rather than system-level security or data storage.,"Access control:0.3,Authentication:0.3,Authorization:0.3,Cryptanalysis and other attacks:0.2,Digital rights management:0.5,Information-theoretic techniques:0.2,Key management:1,Mathematical foundations of cryptography:0.3,Privacy-preserving protocols:1,Pseudonymity, anonymity and untraceability:0.4,Public key (asymmetric) techniques:0.3,Symmetric cryptography and hash functions:0.3","Key management,Privacy-preserving protocols",Key management: The paper focuses on secure key management using threshold secret sharing. Privacy-preserving protocols: The blockchain-based audit trail and mix-net network overlay ensure privacy-preserving enforcement. Other options like Access control are secondary to the primary cryptographic focus.
5024,An Identity based Node Authentication and Session Key Management Algorithm using Elliptic Curve Cryptography,This paper has been withdrawn for modification.,"General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.9,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Security and privacy,Security and privacy: The title indicates cryptographic authentication research. Other categories are irrelevant due to missing content from the withdrawn abstract.,"Cryptography:1.0,Database and storage security:0.0,Formal methods and theory of security:0.0,Human and societal aspects of security and privacy:0.0,Intrusion/anomaly detection and malware mitigation:0.0,Network security:0.0,Security in hardware:0.0,Security services:0.0,Software and application security:0.0,Systems security:0.0",Cryptography,"Cryptography is highly relevant as the title indicates an identity-based authentication algorithm using ECC. The paper was withdrawn, but the title clearly points to cryptography.","Cryptanalysis and other attacks:0,Information-theoretic techniques:0,Key management:1,Mathematical foundations of cryptography:0,Public key (asymmetric) techniques:1,Symmetric cryptography and hash functions:0","Key management,Public key (asymmetric) techniques",Key management is relevant for session key management. Public key (asymmetric) techniques are relevant for elliptic curve cryptography. Other categories are not mentioned in the withdrawn paper.
405,An introduction to multilevel secure relational database management systems,"Multilevel Security (MLS) is a capability that allows information with different classifications to be available in an information system, with users having different security clearances and authorizations, while preventing users from accessing information for which they are not cleared or authorized. It is a security policy that has grown out of research and development efforts funded mostly by the U.S. Department of Defense (DoD) to address some of the drawbacks of the single level mode of operation that was used at the DoD. The goal was to build and deploy an MLS-compliant environment (e.g., Networks, Operating Systems, Database Systems) that would provide a much needed efficiency in processing and distributing classified information by providing security through computer security, communications security, and trusted system techniques instead of using physical controls, administrative procedures, and personnel security. As Relational Database Management Systems (RDBMS) are at the heart of the DoD's information system, significant research and development efforts have been put into building multilevel secure RDBMS, which have led to the emergence of a number of multilevel secure RDBMS solutions, including commercial ones. Over the past few years and with the increase of security concerns, MLS compliance has become a major requirement from a number U.S. Federal Government agencies that appear to have grown beyond the traditional agencies that require such type and level of security. This paper introduces MLS, and outlines the challenges and complexities of building a multilevel secure RDBMS. The paper also gives concrete examples of both research and commercial multilevel secure RDBMS and describes how they met the above challenges and complexities.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.3,Security and privacy:1.0,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Security and privacy,"Security and privacy is highly relevant because the paper introduces multilevel security (MLS) for relational databases, a core security policy. 'Information systems' is marginally relevant due to database systems but not the primary focus. Other categories like 'Hardware' or 'Networks' are irrelevant as the paper focuses on database security policies.","Cryptography:0.2,Database and storage security:1.0,Formal methods and theory of security:0.2,Human and societal aspects of security and privacy:0.2,Intrusion/anomaly detection and malware mitigation:0.2,Network security:0.2,Security in hardware:0.2,Security services:0.9,Software and application security:0.8,Systems security:0.2","Database and storage security,Security services,Software and application security",Database and storage security is directly relevant for securing RDBMS. Security services is relevant for security policy implementation. Software and application security is relevant for secure software design. Other categories are rejected as they do not align with the paper's focus on MLS in databases.,"Access control:1.0,Authentication:0.0,Authorization:1.0,Data anonymization and sanitization:0.0,Database activity monitoring:0.5,Digital rights management:0.0,Domain-specific security and privacy architectures:0.0,Information accountability and usage control:0.0,Management and querying of encrypted data:0.0,Privacy-preserving protocols:0.0,Pseudonymity, anonymity and untraceability:0.0,Social network security and privacy:0.0,Software reverse engineering:0.0,Software security engineering:0.0,Web application security:0.0","Access control,Authorization",Access control and Authorization are directly relevant as the paper introduces multilevel security systems that enforce access rules based on clearance levels. Database activity monitoring is less relevant as the focus is on system design rather than monitoring. Other categories like Web application security are irrelevant as the paper focuses on database security rather than web-specific threats.
548,Privacy-intimacy tradeoff in self-disclosure,"In this paper, we introduce a self-disclosure decision-making mechanism based on information-theoretic measures. This decision-making mechanism uses an intimacy measure between agents and the privacy loss that a particular disclosure may cause.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.2,Information systems:0.1,Security and privacy:0.9,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Security and privacy,Security and privacy is relevant due to the focus on privacy-intimacy tradeoff in self-disclosure mechanisms. Other categories like Mathematics of computing are secondary to the technical privacy framework.,"Human and societal aspects of security and privacy:0.9,Formal methods and theory of security:0.3,Security services:0.2,Network security:0.2,Cryptography:0.15,Database and storage security:0.15,Intrusion/anomaly detection and malware mitigation:0.1,Software and application security:0.1,Systems security:0.1",Human and societal aspects of security and privacy,The paper directly addresses human factors in privacy decisions. Other security categories are not the primary focus.,"Economics of security and privacy:0,Privacy protections:1,Social aspects of security and privacy:1,Usability in security and privacy:0","Privacy protections,Social aspects of security and privacy",Privacy protections and Social aspects are both central to the paper's analysis of self-disclosure tradeoffs. Economics is not a focus here.
4039,Locale-based access control: placing collaborative authorization decisions in context,"Collaboration systems require an appropriate authorization model to specify and maintain policies that not only facilitate group activities but also enforce restrictions and accountability. Existing models fail to incorporate adequately into authorization decisions the rich notion of context that is inherent to any collaborative setting. In this paper we present the locale-based access control (locale-BAC) model for collaborative systems, a model whose design is based upon the application of Fitzpatrick's locale framework for collaboration to the problem of access control. This model encapsulates the notion of context using locales, allowing for a natural representation of collaborative authorization decisions.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.2,Networks:0.3,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.2,Information systems:0.3,Security and privacy:0.8,Human-centered computing:0.3,Computing methodologies:0.4,Applied computing:0.2,Social and professional topics:0.1",Security and privacy,Security and privacy is directly relevant as the paper introduces a locale-based access control model for collaborative systems. Other categories like Networks (0.3) are less relevant as the focus is on authorization rather than network infrastructure.,"Cryptography:0.1,Database and storage security:0.2,Formal methods and theory of security:1.0,Human and societal aspects of security and privacy:0.5,Intrusion/anomaly detection and malware mitigation:0.1,Network security:0.3,Security in hardware:0.1,Security services:0.75,Software and application security:0.2,Systems security:0.2","Formal methods and theory of security,Security services",Formal methods and theory of security are directly relevant as the paper introduces a new access control model with formal foundations. Security services is relevant for policy enforcement. Other categories like cryptography or network security are not central to the paper's focus.,"Access control:1.0,Authentication:0.0,Authorization:1.0,Digital rights management:0.0,Formal security models:0.0,Logic and verification:0.0,Privacy-preserving protocols:0.0,Pseudonymity, anonymity and untraceability:0.0,Security requirements:0.0,Trust frameworks:0.0","Access control,Authorization",Access control is relevant as the paper introduces a locale-based model for collaborative systems. Authorization is relevant due to the focus on collaborative authorization decisions. Other categories like Digital rights management are unrelated to the authorization framework.
160,A Public-key Asymmetric Robust Watermarking Algorithm Based on Signal with Special Correlation Characteristic,"A public-key asymmetric robust watermarking algorithm based on signal with special correlation characteristic is proposed in this paper. This algorithm is designed to permit pubic watermark detection while preventing the watermark from being removed without the private keys. A signal, which is pseudorandom sequence with special auto-correlation characteristic, is used as watermark, and the signal’s characteristic is the foundation of this algorithm. Therefore we first construct the method of generating the pseudorandom sequence and prove its auto-correlation characteristic. Then we describe the algorithm in detail. Experiment results show that the algorithm is valid and robust to common signal distortions and malicious attacks.","General and reference:0.1,Hardware:0.2,Computer systems organization:0.3,Networks:0.2,Software and its engineering:0.3,Theory of computation:0.2,Mathematics of computing:0.4,Information systems:0.2,Security and privacy:0.9,Human-centered computing:0.1,Computing methodologies:0.5,Applied computing:0.3,Social and professional topics:0.1",Security and privacy,Security and privacy is relevant for the public-key watermarking algorithm. Other categories like Mathematics of computing are secondary as the focus is on security application rather than mathematical theory.,"Cryptography:1.0,Database and storage security:0.0,Formal methods and theory of security:0.0,Human and societal aspects of security and privacy:0.0,Intrusion/anomaly detection and malware mitigation:0.0,Network security:0.0,Security in hardware:0.0,Security services:0.75,Software and application security:0.0,Systems security:0.0","Cryptography,Security services",Cryptography is central to the public-key watermarking algorithm. Security services applies to robust watermarking as a security mechanism. Other categories lack direct relevance to cryptographic methods.,"Access control:0.0,Authentication:0.0,Authorization:0.0,Cryptanalysis and other attacks:0.8,Digital rights management:0.7,Information-theoretic techniques:0.1,Key management:0.1,Mathematical foundations of cryptography:0.1,Privacy-preserving protocols:0.0,Pseudonymity, anonymity and untraceability:0.0,Public key (asymmetric) techniques:0.9,Symmetric cryptography and hash functions:0.0","Public key (asymmetric) techniques,Digital rights management,Cryptanalysis and other attacks",Public key (asymmetric) techniques: Central to the algorithm's design using public-key watermarking. Digital rights management: Watermarking is a DRM technique for content protection. Cryptanalysis and other attacks: The algorithm is robust against malicious attacks. Other children are not central to the paper's focus.
814,Tracking based product authentication: Catching intruders in the supply chain,"Product counterfeiting is a growing problem worldwide, threatening the health of consumers and reducing company profits. By detecting and intercepting counterfeits before they reach the customer, the problem can be mitigated. In this paper, an approach to detect counterfeit items based on their claimed history is presented. The necessary data is provided by tracking infrastructures that enable the recording and retrieval of movements of individual items in the supply chain based on unique identifiers assigned to products. If the movement history of an item deviates from the movements of genuine items that have been learned before, a warning about a potential counterfeit is issued. Counterfeiter activities that are possible in a tracking enabled environment are modelled and the capability of the proposed approach to detect these strategies is assessed.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:1.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.5,Social and professional topics:0.0",Security and privacy,Security and privacy is highly relevant as the paper addresses product authentication and counterfeit detection in supply chains. Applied computing is partially relevant for the tracking system but secondary to the security focus.,"Cryptography:0.1,Database and storage security:0.4,Formal methods and theory of security:0.2,Human and societal aspects of security and privacy:0.3,Intrusion/anomaly detection and malware mitigation:0.8,Network security:0.5,Security in hardware:0.1,Security services:0.3,Software and application security:0.2,Systems security:0.2",Intrusion/anomaly detection and malware mitigation,Intrusion/anomaly detection and malware mitigation: The paper detects counterfeit items via anomalous supply chain tracking. Other categories like Database security are secondary.,"Intrusion detection systems:1.0,Malware and its mitigation:0.0,Social engineering attacks:0.0",Intrusion detection systems,"Intrusion detection systems: The paper proposes a system to detect counterfeit items by analyzing supply chain movement anomalies, which aligns with intrusion detection principles. Other categories are irrelevant as the paper does not discuss malware or social engineering."
1560,Security Components in a One-Stop-Shop Border Control System,"Each year the number of passengers travelling around the world is steadily increasing. Hence, the efficient handling of border crossings while maintaining a high security is a demanding challenge for the future. In this work we present the key security components for a novel proposed one-stop-shop (OSS) border control system, which tries to achieve greatest throughput of travelers while applying highest security measurements. We collect the main stakeholder's requirements for an OSS system and assembly the necessary technological solutions so that the proposed OSS system can be operated at all kinds of borders. Thereby, the selected technologies are evaluated and current limitations and constraints described.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.9,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Security and privacy,"Security and privacy: The paper designs security components for a border control system, emphasizing high security and throughput. Other categories like Applied computing are secondary as the focus is on security mechanisms.","Cryptography:0.0,Database and storage security:0.0,Formal methods and theory of security:0.25,Human and societal aspects of security and privacy:0.25,Intrusion/anomaly detection and malware mitigation:0.0,Network security:0.0,Security in hardware:0.0,Security services:0.75,Software and application security:0.75,Systems security:0.75","Security services,Software and application security,Systems security",Security services is relevant as the paper discusses security components for a border control system. Software and application security is relevant for the technological solutions implemented. Systems security is relevant for the overall system design. Categories like Cryptography or Network security are less relevant as the focus is on system-level security rather than specific protocols or networks.,"Access control:1.0,Authentication:1.0,Authorization:0.3,Browser security:0.1,Denial-of-service attacks:0.1,Digital rights management:0.1,Distributed systems security:0.2,Domain-specific security and privacy architectures:0.4,File system security:0.1,Firewalls:0.1,Information flow control:0.1,Operating systems security:0.2,Privacy-preserving protocols:0.2,Pseudonymity, anonymity and untraceability:0.1,Social network security and privacy:0.1,Software reverse engineering:0.1,Software security engineering:0.3,Vulnerability management:0.2,Web application security:0.1","Access control,Authentication",Access control: The paper discusses security components for border control. Authentication: The system requires high security through authentication measures. Authorization is less central as the focus is on access and identity verification.
2937,Canal: scaling social network-based Sybil tolerance schemes,"There has been a flurry of research on leveraging social networks to defend against multiple identity, or Sybil, attacks. A series of recent works does not try to explicitly identify Sybil identities and, instead, bounds the impact that Sybil identities can have. We call these approaches Sybil tolerance; they have shown to be effective in applications including reputation systems, spam protection, online auctions, and content rating systems. All of these approaches use a social network as a credit network, rendering multiple identities ineffective to an attacker without a commensurate increase in social links to honest users (which are assumed to be hard to obtain). Unfortunately, a hurdle to practical adoption is that Sybil tolerance relies on computationally expensive network analysis, thereby limiting widespread deployment.
 To address this problem, we first demonstrate that despite their differences, all proposed Sybil tolerance systems work by conducting payments over credit networks. These payments require max flow computations on a social network graph, and lead to poor scalability. We then present Canal, a system that uses landmark routing-based techniques to efficiently approximate credit payments over large networks. Through an evaluation on real-world data, we show that Canal provides up to a three-order-of-magnitude speedup while maintaining safety and accuracy, even when applied to social networks with millions of nodes and hundreds of millions of edges. Finally, we demonstrate that Canal can be easily plugged into existing Sybil tolerance schemes, enabling them to be deployed in an online fashion in real-world systems.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.2,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.9,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Security and privacy,"Security and privacy is directly relevant as the paper addresses Sybil attacks in social networks, presenting the Canal system for secure credit network analysis. Theory of computation (0.2) has limited relevance to the graph algorithms used but is secondary to the security focus. Other categories like Networks are irrelevant to the security mechanism.","Cryptography:0.3,Database and storage security:0.1,Formal methods and theory of security:0.1,Human and societal aspects of security and privacy:0.1,Intrusion/anomaly detection and malware mitigation:0.1,Network security:1.0,Security in hardware:0.1,Security services:0.5,Software and application security:0.2,Systems security:0.3","Network security,Security services",Network security is central to addressing Sybil attacks in social networks. Security services applies to the credit-based authentication mechanism. Other categories like cryptography or systems security are not primary.,"Access control:0.4,Authentication:1,Authorization:0.3,Denial-of-service attacks:0.2,Digital rights management:0.1,Firewalls:0.1,Mobile and wireless security:0.2,Privacy-preserving protocols:0.6,Pseudonymity, anonymity and untraceability:0.4,Security protocols:1,Web protocol security:0.2","Security protocols,Authentication",Security protocols: The paper introduces a novel Sybil tolerance protocol (Canal). Authentication: The system's security relies on authentication mechanisms. Web protocol security is irrelevant as the focus is on social network-based security rather than web protocols.
4514,CAES Cryptosystem: Advanced Security Tests and Results,"A robust and secure cryptosystem is an encrypting system that resists against all practical cryptanalysis methods such as statistical attacks, differential cryptanalysis and linear cryptanalysis. To prove the resistance against these attacks, the cryptosystem designer must carry out a list of robustness tests. Considering these constraints, we present in the current paper results of robustness and security tests conducted on the CAES (Cellular automata Encryption System) cryptosystem published in a previous article. The presented tests focus on randomness tests and on differential cryptanalysis. As results of these tests, we concluded that the cryptosystem CAES gives a pseudo-random output regardless the input. Also the differential attack needs huge number of chosen plaintexts which make it impractical.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:1.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Security and privacy,"Security and privacy: The paper evaluates a cryptosystem against statistical and differential attacks, which is a core topic in security research.","Cryptography:0.9,Database and storage security:0.1,Formal methods and theory of security:0.6,Human and societal aspects of security and privacy:0.1,Intrusion/anomaly detection and malware mitigation:0.1,Network security:0.1,Security in hardware:0.1,Security services:0.1,Software and application security:0.1,Systems security:0.1","Cryptography,Formal methods and theory of security",Cryptography is directly relevant as the paper analyzes a cryptosystem's security properties. Formal methods and theory of security receives moderate relevance due to rigorous testing and mathematical analysis of resistance to cryptanalysis. Other categories are irrelevant as the focus is on cryptographic algorithm evaluation rather than application security or network security.,"Cryptanalysis and other attacks:1.0,Formal security models:0.2,Information-theoretic techniques:0.0,Key management:0.0,Logic and verification:0.0,Mathematical foundations of cryptography:0.0,Public key (asymmetric) techniques:0.0,Security requirements:0.0,Symmetric cryptography and hash functions:1.0,Trust frameworks:0.0","Cryptanalysis and other attacks,Symmetric cryptography and hash functions",Cryptanalysis and other attacks: The paper evaluates CAES against cryptanalysis methods like differential cryptanalysis. Symmetric cryptography and hash functions: The paper describes a symmetric cryptosystem using cellular automata.
340,Anonymität in digitalen Münzsystemen mit Wechselgeld,"Beim praktischen Einsatz von digitalen Münzsystemen steht man vor dem Problem, dass die Kunden sämtliche Beträge entweder passend bezahlen müssen oder eine Möglichkeit zur Herausgabe von Wechselgeld geschaffen werden muss. Der naive Ansatz führt dazu, dass das Geldsystem seine wichtigste Eigenschaft einbüßt: die Anonymität. Wir diskutieren in diesem Papier die bis heute bekannten Verfahren zum Umgang mit Wechselgeld und ihre Konsequenzen für die Kundenanonymität. Wir können zeigen, dass unter den bisher bekannten Lösungsverfahren nur die Vermeidung von Wechselgeld die perfekte Anonymität der Kunden gewährleistet. Die Integration von Zusatzeigenschaften wie Übertragbarkeit oder Teilbarkeit hingegen schwächt die Anonymität.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:1.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Security and privacy,Security and privacy: The paper analyzes anonymity in digital currency systems. Other categories are irrelevant.,"Cryptography:0.9,Database and storage security:0.1,Formal methods and theory of security:0.3,Human and societal aspects of security and privacy:0.7,Intrusion/anomaly detection and malware mitigation:0.1,Network security:0.2,Security in hardware:0.1,Security services:0.8,Software and application security:0.1,Systems security:0.1","Cryptography,Security services",Cryptography is central to maintaining anonymity in digital currency systems. Security services is relevant because the paper evaluates anonymity as a core security service. Human aspects of security/privacy is partially relevant but secondary to the cryptographic analysis of anonymity mechanisms.,"Access control:0.1,Authentication:0.1,Authorization:0.1,Cryptanalysis and other attacks:0.1,Digital rights management:0.1,Information-theoretic techniques:0.2,Key management:0.1,Mathematical foundations of cryptography:0.2,Privacy-preserving protocols:1.0,Pseudonymity, anonymity and untraceability:1.0,Public key (asymmetric) techniques:0.2,Symmetric cryptography and hash functions:0.2","Privacy-preserving protocols,Pseudonymity, anonymity and untraceability","The paper directly analyzes anonymity in digital currency systems, making 'Privacy-preserving protocols' and 'Pseudonymity' highly relevant. Other cryptographic categories are tangential."
3486,Protection scheme for secure MPEG-2 streaming,"In this paper, we propose a protection scheme for MPEG-2 streaming media. To protect MPEG-2 media more effectively and securely, an encryption process should be considered on encoding phase of uncompressed stream and also a streaming server should be designed to support the protection scheme. However this way cannot support existing streaming systems and requires pre-processing such as demultiplexing and decoding. Our approach is to design a protection scheme independent to the streaming server. So, we propose an encryption method which can be applied to compressed MPEG-2 transport stream (TS). Proposed scheme can protect attacks over networks since streaming server streams pre-encrypted content and enables the client to decrypt the streamed data and playback in real time. Even though streamed content is stored in local system users who do not have available rights cannot use the encrypted content","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.3,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.9,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Security and privacy,Security and privacy is highly relevant as the paper focuses on encryption schemes for secure media streaming. Networks is less relevant as the primary contribution is cryptographic protection rather than network protocols.,"Cryptography:1.0,Database and storage security:0.0,Formal methods and theory of security:0.0,Human and societal aspects of security and privacy:0.0,Intrusion/anomaly detection and malware mitigation:0.0,Network security:1.0,Security in hardware:0.0,Security services:0.0,Software and application security:0.8,Systems security:0.0","Cryptography,Network security",Cryptography: The paper introduces an encryption method for secure streaming. Network security: The scheme protects against network-based attacks. Software and application security is secondary as the focus is on encryption and streaming.,"Cryptanalysis and other attacks:0.0,Denial-of-service attacks:0.0,Firewalls:0.0,Information-theoretic techniques:0.0,Key management:1.0,Mathematical foundations of cryptography:0.0,Mobile and wireless security:0.0,Public key (asymmetric) techniques:0.0,Security protocols:1.0,Symmetric cryptography and hash functions:1.0,Web protocol security:0.0","Security protocols,Symmetric cryptography and hash functions,Key management",Security protocols: The paper designs a protocol for secure MPEG-2 streaming. Symmetric cryptography and hash functions: Encryption is applied to the transport stream. Key management: Secure key handling is implied for access control.
5132,Hulk: Eliciting Malicious Behavior in Browser Extensions,"We present Hulk, a dynamic analysis system that detects malicious behavior in browser extensions by monitoring their execution and corresponding network activity. Hulk elicits malicious behavior in extensions in two ways. First, Hulk leverages HoneyPages, which are dynamic pages that adapt to an extension's expectations in web page structure and content. Second, Hulk employs a fuzzer to drive the numerous event handlers that modern extensions heavily rely upon. We analyzed 48K extensions from the Chrome Web store, driving each with over 1M URLs. We identify a number of malicious extensions, including one with 5.5 million affected users, stressing the risks that extensions pose for today's web security ecosystem, and the need to further strengthen browser security to protect user data and privacy.","General and reference:0.2,Hardware:0.1,Computer systems organization:0.1,Networks:0.3,Software and its engineering:0.5,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:1.0,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Security and privacy,Security and privacy is highly relevant as the paper focuses on detecting malicious browser extensions and web security risks. Other categories like Software and its engineering or Networks are less central to the core contribution.,"Cryptography:0.0,Database and storage security:0.0,Formal methods and theory of security:0.0,Human and societal aspects of security and privacy:0.0,Intrusion/anomaly detection and malware mitigation:0.75,Network security:0.0,Security in hardware:0.0,Security services:0.0,Software and application security:1.0,Systems security:0.0","Software and application security,Intrusion/anomaly detection and malware mitigation","Software and application security is directly relevant as the paper introduces Hulk, a system for detecting malicious browser extensions. Intrusion/anomaly detection and malware mitigation is relevant due to the focus on dynamic analysis to identify malicious behavior. Other fields like network security or database security are not central to the paper's core contribution.","Domain-specific security and privacy architectures:0,Intrusion detection systems:0,Malware and its mitigation:1,Social engineering attacks:0,Social network security and privacy:0,Software reverse engineering:0,Software security engineering:0,Web application security:1","Malware and its mitigation,Web application security",Malware and its mitigation is central to the paper's goal of detecting malicious browser extensions. Web application security is relevant due to the focus on browser extension security within the web ecosystem.
5271,Keeping data secret under full compromise using porter devices,"We address the problem of confidentiality in scenarios where the attacker is not only able to observe the communication between principals, but can also fully compromise the communicating parties (their devices, not only their long term secrets) after the confidential data has been exchanged. We formalize this problem and explore solutions that provide confidentiality after the full compromise of devices and user passwords. We propose two new solutions that use explicit key deletion and forward-secret protocols combined with key storage on porter devices. Our solutions provide the users with control over their privacy. We analyze the proposed solutions using an automatic verification tool. We also implement a prototype using a mobile phone as a porter device to illustrate how the solution can be realized on modern platforms.","General and reference:0.1,Hardware:0.2,Computer systems organization:0.2,Networks:0.1,Software and its engineering:0.3,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.9,Human-centered computing:0.2,Computing methodologies:0.4,Applied computing:0.3,Social and professional topics:0.1",Security and privacy,"Security and privacy is highly relevant as the paper addresses post-compromise confidentiality using porter devices. Applied computing receives moderate relevance for the mobile phone implementation, while Computing methodologies is less central as the focus is on security protocols rather than algorithmic methods.","Cryptography:0.9,Security services:0.85,Formal methods and theory of security:0.75,Database and storage security:0.2,Human and societal aspects of security and privacy:0.3,Intrusion/anomaly detection and malware mitigation:0.2,Network security:0.3,Security in hardware:0.4,Software and application security:0.3,Systems security:0.4","Cryptography,Security services",Cryptography is relevant for key deletion and forward secrecy. Security services is relevant for confidentiality post-compromise. Formal methods is secondary for verification but less central than the cryptographic protocols.,"Access control:0.1,Authentication:0.2,Authorization:0.1,Cryptanalysis and other attacks:0.1,Digital rights management:0.1,Information-theoretic techniques:0.1,Key management:0.8,Mathematical foundations of cryptography:0.1,Privacy-preserving protocols:0.7,Pseudonymity, anonymity and untraceability:0.2,Public key (asymmetric) techniques:0.3,Symmetric cryptography and hash functions:0.1","Key management,Privacy-preserving protocols",Key management is relevant because the paper proposes solutions using key deletion and porter devices. Privacy-preserving protocols are relevant due to the focus on confidentiality post-compromise. Other categories like Authentication or Public key techniques are less central to the core contribution.
2161,An analysis of signature overlaps in Intrusion Detection Systems,"An Intrusion Detection System (IDS) protects computer networks against attacks and intrusions, in combination with firewalls and anti-virus systems. One class of IDS is called signature-based network IDSs, as they monitor network traffic, looking for evidence of malicious behaviour as specified in attack descriptions (referred to as signatures).","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.8,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Security and privacy,"Security and privacy: The paper analyzes signature overlaps in IDS, which directly relates to intrusion detection and security mechanisms. Other categories like Networks are irrelevant as the focus is on security policy evaluation.","Cryptography:0.0,Database and storage security:0.0,Formal methods and theory of security:0.0,Human and societal aspects of security and privacy:0.0,Intrusion/anomaly detection and malware mitigation:1.0,Network security:0.5,Security in hardware:0.0,Security services:0.0,Software and application security:0.0,Systems security:0.0",Intrusion/anomaly detection and malware mitigation,Intrusion/anomaly detection and malware mitigation: The paper directly analyzes signature overlaps in IDS systems. Network security: IDS is a subset of network security but the paper's focus is more specific.,"Intrusion detection systems:1.0,Malware and its mitigation:0.0,Social engineering attacks:0.0",Intrusion detection systems,Intrusion detection systems is directly relevant as the paper analyzes signature overlaps in IDS. Other categories like Malware or Social engineering are not discussed.
2087,Using Kolmogorov complexity for understanding some limitations on steganography,"Perfectly secure steganographic systems have been recently described for a wide class of sources of covertexts. The speed of transmission of secret information for these stegosystems is proportional to the length of the covertext. In this work we show that there are sources of covertexts for which such stegosystems do not exist. The key observation is that if the set of possible covertexts has a maximal Kolmogorov complexity, then a high-speed perfect stegosystem has to have complexity of the same order.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.2,Mathematics of computing:0.3,Information systems:0.1,Security and privacy:0.9,Human-centered computing:0.1,Computing methodologies:0.4,Applied computing:0.1,Social and professional topics:0.1",Security and privacy,Security and privacy is directly relevant for steganography analysis. Mathematics of computing is less relevant as the focus is on security implications rather than pure mathematics.,"Cryptography:0.8,Database and storage security:0.0,Formal methods and theory of security:0.85,Human and societal aspects of security and privacy:0.0,Intrusion/anomaly detection and malware mitigation:0.0,Network security:0.0,Security in hardware:0.0,Security services:0.0,Software and application security:0.0,Systems security:0.0","Cryptography,Formal methods and theory of security",Cryptography is relevant due to the paper's focus on steganography. Formal methods and theory of security are relevant for the theoretical analysis of steganographic limitations. Other categories like Network security or Security in hardware are not directly related to the paper's core contribution.,"Cryptanalysis and other attacks:0,Formal security models:1,Information-theoretic techniques:1,Key management:0,Logic and verification:0,Mathematical foundations of cryptography:0,Public key (asymmetric) techniques:0,Security requirements:0,Symmetric cryptography and hash functions:0,Trust frameworks:0","Formal security models,Information-theoretic techniques","Formal security models: The paper analyzes theoretical limitations of steganographic systems using formal models. Information-theoretic techniques: Kolmogorov complexity is a core information-theoretic method applied to steganography. Other children: The work does not discuss attacks, key management, or specific cryptographic algorithms."
5608,Security analysis of a NTRU-based mutual authentication scheme,"NFC-based mobile transaction has come into limelight in recent years thanks to the rapid development of NFC and mobile technologies. In these applications, the NFC-chip is in the card emulation mode to simulate a credit card. Because many sensitive information is exchanged during the communication of the mobile transaction, mutual authentication is required in order to verify the legality of each communicating party. Recently, Part and Lee introduced an anonymous authentication scheme based on NTRU. It is aimed to protect user information in NFC mobile payment systems without directly using private financial information of users. However, we found a security flaw in their new scheme. In this paper, we show that their scheme is insecure against an eavesdropping attack. An attacker, without any secret information, can impersonate the user against a service provider and pass the authentication procedure. This may result in a serious problem in which an attacker can enjoy a service such as an on-line shopping on behalf of the real user without the permission of the real user. An improved scheme will be left as our future work.","General and reference:0.05,Hardware:0.1,Computer systems organization:0.05,Networks:0.1,Software and its engineering:0.05,Theory of computation:0.05,Mathematics of computing:0.05,Information systems:0.05,Security and privacy:0.9,Human-centered computing:0.05,Computing methodologies:0.1,Applied computing:0.05,Social and professional topics:0.05",Security and privacy,Security and privacy: Analyzes a vulnerability in an NTRU-based authentication scheme. Other categories are irrelevant as the focus is on cryptographic security flaws.,"Cryptography:1.0,Security services:0.9,Network security:0.3,Database and storage security:0.1,Formal methods and theory of security:0.2,Human and societal aspects of security and privacy:0.1,Intrusion/anomaly detection and malware mitigation:0.1,Security in hardware:0.2,Software and application security:0.1,Systems security:0.1","Cryptography,Security services",Cryptography is directly relevant as the paper analyzes a NTRU-based authentication scheme. Security services is relevant as the paper evaluates authentication protocols. Network security is less relevant since the focus is on cryptographic protocol vulnerabilities rather than network-specific threats. Other categories like hardware security or formal methods are not central to the paper's core contribution.,"Access control:0.3,Authentication:1,Authorization:0.2,Cryptanalysis and other attacks:1,Digital rights management:0.1,Information-theoretic techniques:0.2,Key management:0.4,Mathematical foundations of cryptography:0.6,Privacy-preserving protocols:0.5,Pseudonymity, anonymity and untraceability:0.3,Public key (asymmetric) techniques:1,Symmetric cryptography and hash functions:0.2","Authentication,Cryptanalysis and other attacks,Public key (asymmetric) techniques",Authentication is the core focus of the scheme analysis. Cryptanalysis is relevant due to the identified eavesdropping attack. Public key techniques are fundamental to the NTRU-based protocol. Other options like Access control are peripheral.
2023,CloudFilter: practical control of sensitive data propagation to the cloud,"A major obstacle for the adoption of cloud services in enterprises is the potential loss of control over sensitive data. Companies often have to safeguard a subset of their data because it is crucial to their business or they are required to do so by law. In contrast, cloud service providers handle enterprise data without providing guarantees and may put confidentiality at risk. In order to maintain control over their sensitive data, companies typically block all access to a wide range of cloud services at the network level. Such restrictions significantly reduce employee productivity while offering limited practical protection in the presence of malicious employees.
 In this paper, we suggest a practical mechanism to ensure that an enterprise maintains control of its sensitive data while employees are allowed to use cloud services. We observe that most cloud services use HTTP as a transport protocol. Since HTTP offers well-defined methods to transfer files, inspecting HTTP messages allows the propagation of data between the enterprise and cloud services to be monitored independently of the implementation of specific cloud services. Our system, CloudFilter, intercepts file transfers to cloud services, performs logging and enforces data propagation policies. CloudFilter controls where files propagate after they have been uploaded to the cloud and ensures that only authorised users may gain access. We show that CloudFilter can be applied to control data propagation to Dropbox and GSS, describing the realistic data propagation policies that it can enforce.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.2,Software and its engineering:0.3,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.95,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Security and privacy,"Security and privacy: The paper addresses cloud data leakage prevention through access control and policy enforcement, which is directly within the domain of information security. Other categories (e.g., Networks) are secondary as the focus is on security mechanisms rather than network protocols.","Cryptography:0.2,Database and storage security:0.8,Formal methods and theory of security:0.3,Human and societal aspects of security and privacy:0.1,Intrusion/anomaly detection and malware mitigation:0.1,Network security:0.2,Security in hardware:0.1,Security services:0.7,Software and application security:0.3,Systems security:0.9","Database and storage security,Systems security",Database and storage security: The paper focuses on controlling data propagation policies. Systems security: CloudFilter implements system-level security mechanisms. Software and application security is rejected as the focus is infrastructure-level.,"Browser security:0.2,Data anonymization and sanitization:0.3,Database activity monitoring:0.5,Denial-of-service attacks:0.0,Distributed systems security:0.7,File system security:0.4,Firewalls:0.6,Information accountability and usage control:1.0,Information flow control:1.0,Management and querying of encrypted data:0.1,Operating systems security:0.3,Vulnerability management:0.0","Information flow control,Information accountability and usage control",Information flow control: CloudFilter monitors data propagation and enforces access policies. Information accountability: The system ensures data accountability through logging and authorization checks. Firewalls and File system security are less relevant as the paper focuses on application-layer control rather than network or storage security.
4816,Account Reachability: A Measure of Privacy Risk for Exposure of a User's Multiple SNS Accounts,"With the increased worldwide popularity of social networking services (SNSs), the leakage of a user's private information is becoming a serious problem. An increased number of users now have multiple accounts on various social networks and they tend to use each account to write different user experiments. Aggregating information from different accounts leads to the unintended leakage of personal information. Therefore, we argue that SNS users should be vigilant in protecting the relationship between multiple accounts.
 In this paper, we propose the use of Account Reachability, a measure of privacy risk which demonstrates the possibility of a stranger finding a user's private account based on information in their public account. In addition, we present ARChecker, a tool to calculate the value of account reachability. ARChecker also provides advice on how to modify the user's profiles and messages to decrease the privacy risk. By checking the privacy measure and modifying the profiles and messages of their SNS accounts, users can protect their multiple accounts from the risk of an unintended leakage of personal information.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.9,Human-centered computing:0.2,Computing methodologies:0.3,Applied computing:0.3,Social and professional topics:0.1",Security and privacy,Security and privacy is highly relevant as the paper addresses privacy risks in social networks. Applied computing is secondary to the privacy-specific focus.,"Cryptography:0.2,Database and storage security:0.1,Formal methods and theory of security:0.1,Human and societal aspects of security and privacy:0.9,Intrusion/anomaly detection and malware mitigation:0.1,Network security:0.1,Security in hardware:0.1,Security services:0.1,Software and application security:0.3,Systems security:0.1","Human and societal aspects of security and privacy,Software and application security",Human and societal aspects of security and privacy: The paper addresses privacy risks in social networks and user behavior. Software and application security: ARChecker is a tool for assessing privacy risks in SNS applications. Other fields like cryptography are not central to the privacy risk analysis framework.,"Domain-specific security and privacy architectures:0.2,Economics of security and privacy:0.1,Privacy protections:1.0,Social aspects of security and privacy:0.3,Social network security and privacy:1.0,Software reverse engineering:0.1,Software security engineering:0.2,Usability in security and privacy:0.4,Web application security:0.1","Privacy protections,Social network security and privacy",Privacy protections is relevant because the paper introduces a privacy risk metric and mitigation tool. Social network security and privacy is directly relevant due to the focus on SNS account relationships. Other categories like economics or web security are unrelated.
2567,Modeling Attack Behaviors in Rating Systems,"Online feedback-based rating systems are gaining popularity. Dealing with unfair ratings in such systems has been recognized as an important problem and many unfair rating detection approaches have been developed. Currently, these approaches are evaluated against simple attack models, but complicated attacking strategies can be used by attackers in the real world. The lack of unfair rating data from real human users and realistic attack behavior models has become an obstacle toward developing reliable rating systems. To solve this problem, we design and launch a rating challenge to collect unfair rating data from real human users. In order to broaden the scope of the data collection, we also develop a comprehensive signal-based unfair rating detection system. Based on the analysis of real attack data, we discover important features in unfair ratings, build attack models, and develop an unfair rating generator. The models and generator developed in this paper can be directly used to evaluate current rating aggregation systems, as well as to assist the design of future rating systems.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.2,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:1.0,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.2,Social and professional topics:0.1",Security and privacy,Security and privacy is highly relevant as the paper addresses attack modeling in rating systems. Applied computing is less central because the focus is on security mechanisms rather than application-specific computing.,"Intrusion/anomaly detection and malware mitigation:1.0,Software and application security:0.9,Database and storage security:0.3,Cryptography:0.1,Formal methods and theory of security:0.2,Human and societal aspects of security and privacy:0.2,Network security:0.2,Security in hardware:0.1,Security services:0.7,Systems security:0.4","Intrusion/anomaly detection and malware mitigation,Software and application security",Intrusion/anomaly detection is directly relevant for modeling attack behaviors. Software security is relevant for securing rating systems. Other categories like cryptography or network security are less relevant to this specific application domain.,"Domain-specific security and privacy architectures:1,Intrusion detection systems:1,Malware and its mitigation:0,Social engineering attacks:0,Social network security and privacy:0,Software reverse engineering:0,Software security engineering:0.3,Web application security:0","Domain-specific security and privacy architectures,Intrusion detection systems",Domain-specific security and privacy architectures are relevant because the paper focuses on rating system security. Intrusion detection systems are relevant as the framework detects unfair ratings. Software security engineering receives a low score as the paper emphasizes attack modeling over general software security practices.
5950,Attribute-Based Access Control with Hidden Policies and Hidden Credentials,"In an open environment such as the Internet, the decision to collaborate with a stranger (e.g., by granting access to a resource) is often based on the characteristics (rather than the identity) of the requester, via digital credentials: access is granted if Alice's credentials satisfy Bob's access policy. The literature contains many scenarios in which it is desirable to carry out such trust negotiations in a privacy-preserving manner, i.e., so as minimize the disclosure of credentials and/or of access policies. Elegant solutions were proposed for achieving various degrees of privacy-preservation through minimal disclosure. In this paper, we present protocols that protect both sensitive credentials and sensitive policies. That is, Alice gets the resource only if she satisfies the policy, Bob does not learn anything about Alice's credentials (not even whether Alice got access), and Alice learns neither Bob's policy structure nor which credentials caused her to gain access. Our protocols are efficient in terms of communication and in rounds of interaction","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.9,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Security and privacy,Security and privacy is central to the access control protocols. No other categories address the focus on privacy-preserving authentication mechanisms.,"Cryptography:1.0,Formal methods and theory of security:0.8,Database and storage security:0.2,Human and societal aspects of security and privacy:0.3,Intrusion/anomaly detection and malware mitigation:0.1,Network security:0.1,Security in hardware:0.1,Security services:0.2,Software and application security:0.2,Systems security:0.1","Cryptography,Formal methods and theory of security","Cryptography is highly relevant as the paper presents protocols for protecting sensitive credentials and policies. Formal methods and theory of security are relevant for verifying protocol correctness. Database, network, and hardware security are less relevant since the focus is on access control logic rather than specific systems.","Cryptanalysis and other attacks:0.1,Formal security models:0.9,Information-theoretic techniques:0.2,Key management:0.1,Logic and verification:0.3,Mathematical foundations of cryptography:0.7,Public key (asymmetric) techniques:0.8,Security requirements:0.2,Symmetric cryptography and hash functions:0.3,Trust frameworks:0.6","Formal security models,Public key (asymmetric) techniques,Trust frameworks","Formal security models: The paper introduces protocols with formal privacy guarantees. Public key (asymmetric) techniques: Uses cryptographic methods for secure access. Trust frameworks: Addresses policy enforcement in access control systems. Other options are either tangential (e.g., symmetric crypto) or not central to the work."
3443,Toward a novel classification-based attack detection and response architecture,"Attacks on information systems have increased tremendously and have become more diverse and complex. Evolving in an unpredictable manner and having devastating outcomes, the detection and the selection of appropriate countermeasures has become a priority for security analysts. This paper introduces a classification-based Attack Detection system which provides a framework to evaluate, identify, classify and defend against sophisticated attacks. Our approach helps simplify complex rules' expression and alert handling, thanks to a modular architecture and an intuitive rules defining with a high power of expression language. The proposed system is flexible and takes into account several attack properties in order to simplify attack handling and aggregate defense mechanisms.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.9,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Security and privacy,Security and privacy is relevant for the attack detection and response architecture. Other categories like Software Engineering are not the focus.,"Cryptography:0.0,Database and storage security:0.0,Formal methods and theory of security:0.0,Human and societal aspects of security and privacy:0.0,Intrusion/anomaly detection and malware mitigation:1.0,Network security:0.0,Security in hardware:0.0,Security services:1.0,Software and application security:0.0,Systems security:0.0","Intrusion/anomaly detection and malware mitigation,Security services",Intrusion detection is directly addressed as the paper introduces a classification-based attack detection system. Security services is relevant as the system provides a framework for security responses. Other fields are not central to the paper's focus.,"Access control:0,Authentication:0,Authorization:0,Digital rights management:0,Intrusion detection systems:1,Malware and its mitigation:0,Privacy-preserving protocols:0,Pseudonymity, anonymity and untraceability:0,Social engineering attacks:0",Intrusion detection systems,"The paper introduces a classification-based attack detection system, which aligns with Intrusion Detection Systems. Other categories like Malware mitigation are not explicitly addressed."
5716,An Improved Remote User Authentication Scheme with Smart Cards using Bilinear Pairings,"Manik et al. [3] proposed a novel remote user authentication scheme using bilinear pairings. Recently, Fang et al [15] analyzed this scheme and pointed out that the proposed scheme is insecure. They proposed an improvement over this scheme. Further, Giri and Srivastava [16] observed that the improved scheme of Fang et al is still insecure against off-line attack and they suggested a new improved scheme. However, the improved scheme is still insecure. In this paper, we show some attacks on this scheme and propose an improved protocol that provides the better security as compared to the schemes previously discussed.","General and reference:0.1,Hardware:0.2,Computer systems organization:0.2,Networks:0.3,Software and its engineering:0.8,Theory of computation:0.2,Mathematics of computing:0.3,Information systems:0.1,Security and privacy:1.0,Human-centered computing:0.1,Computing methodologies:0.3,Applied computing:0.2,Social and professional topics:0.1",Security and privacy,Security and privacy is highly relevant for the authentication scheme. Software and its engineering is partially relevant for implementation.,"Cryptography:0.9,Database and storage security:0.3,Formal methods and theory of security:0.6,Human and societal aspects of security and privacy:0.2,Intrusion/anomaly detection and malware mitigation:0.2,Network security:0.8,Security in hardware:0.3,Security services:0.6,Software and application security:0.4,Systems security:0.5","Cryptography,Network security",Cryptography is central as the paper discusses authentication schemes using bilinear pairings. Network security is relevant because it's about remote user authentication. Formal methods gets a moderate score due to the security analysis. Other security categories like Systems security are less directly relevant.,"Cryptanalysis and other attacks:0.6,Denial-of-service attacks:0.0,Firewalls:0.0,Information-theoretic techniques:0.0,Key management:0.0,Mathematical foundations of cryptography:0.0,Mobile and wireless security:0.0,Public key (asymmetric) techniques:1.0,Security protocols:0.8,Symmetric cryptography and hash functions:0.0,Web protocol security:0.0","Public key (asymmetric) techniques,Security protocols","Public key (asymmetric) techniques: The paper uses bilinear pairings, a key asymmetric technique. Security protocols: The scheme is an authentication protocol. Other options are less relevant as they do not directly relate to the paper's focus on public-key authentication."
3899,TOCTTOU vulnerabilities in UNIX-style file systems: an anatomical study,"Due to their non-deterministic nature, Time of Check To Time of Use (TOCTTOU) vulnerabilities in Unix-style file systems (e.g., Linux) are difficult to find and prevent. We describe a comprehensive model of TOCTTOU vulnerabilities, enumerating 224 file system call pairs that may lead to successful TOCTTOU attacks. Based on this model, we built kernel monitoring tools that confirmed known vulnerabilities and discovered new ones (in often-used system utilities such as rpm, vi, and emacs). We evaluated the probability of successfully exploiting these newly discovered vulnerabilities and analyzed in detail the system events during such attacks. Our performance evaluation shows that the dynamic monitoring of system calls introduces non-negligible overhead in microbenchmark of those file system calls, but their impact on application benchmarks such as Andrew and PostMark is only a few percent.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.9,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Security and privacy,Security and privacy is highly relevant as the paper focuses on TOCTTOU vulnerabilities in Unix systems. Other categories like Hardware or Networks are not central to the vulnerability analysis and mitigation strategies.,"Cryptography:0.1,Database and storage security:0.1,Formal methods and theory of security:0.1,Human and societal aspects of security and privacy:0.1,Intrusion/anomaly detection and malware mitigation:0.7,Network security:0.1,Security in hardware:0.1,Security services:0.1,Software and application security:0.1,Systems security:0.85","Systems security,Intrusion/anomaly detection and malware mitigation",Systems security is directly relevant for file system vulnerabilities. Intrusion detection applies to monitoring techniques. Other categories like Network security are not primary focus.,"File system security:1.0,Vulnerability management:1.0,Browser security:0.0,Denial-of-service attacks:0.0,Distributed systems security:0.0,Firewalls:0.0,Information flow control:0.0,Intrusion detection systems:0.3,Malware and its mitigation:0.0,Operating systems security:0.7,Social engineering attacks:0.0","File system security,Vulnerability management",File system security is directly relevant as the paper analyzes TOCTTOU vulnerabilities in Unix file systems. Vulnerability management is relevant due to the discovery and analysis of new vulnerabilities. Intrusion detection systems is low relevance as the focus is not on intrusion detection.
1770,Analysis of network address shuffling as a moving target defense,"Address shuffling is a type of moving target defense that prevents an attacker from reliably contacting a system by periodically remapping network addresses. Although limited testing has demonstrated it to be effective, little research has been conducted to examine the theoretical limits of address shuffling. As a result, it is difficult to understand how effective shuffling is and under what circumstances it is a viable moving target defense. This paper introduces probabilistic models that can provide insight into the performance of address shuffling. These models quantify the probability of attacker success in terms of network size, quantity of addresses scanned, quantity of vulnerable systems, and the frequency of shuffling. Theoretical analysis shows that shuffling is an acceptable defense if there is a small population of vulnerable systems within a large network address space, however shuffling has a cost for legitimate users. These results will also be shown empirically using simulation and actual traffic traces.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.5,Information systems:0.0,Security and privacy:1.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Security and privacy,Security and privacy is highly relevant as the paper analyzes a moving target defense technique. Mathematics of computing is partially relevant for probabilistic models but not central. Other categories are not relevant.,"Cryptography:0.1,Database and storage security:0.1,Formal methods and theory of security:0.1,Human and societal aspects of security and privacy:0.1,Intrusion/anomaly detection and malware mitigation:0.1,Network security:0.9,Security in hardware:0.1,Security services:0.1,Software and application security:0.1,Systems security:0.3",Network security,"Network security is highly relevant as the paper introduces probabilistic models to evaluate address shuffling as a network-level moving target defense. Systems security receives a moderate score due to general system defense implications, but the primary focus is on network-specific mechanisms. Other categories like Cryptography or Database security are not addressed.","Denial-of-service attacks:0.4,Firewalls:0.1,Mobile and wireless security:0.2,Security protocols:0.7,Web protocol security:0.3",Security protocols,Security protocols: The paper analyzes a network defense technique through protocol-level behavior modeling. Denial-of-service attacks has partial relevance but is not the primary focus as the analysis is broader than just DoS scenarios.
2330,On the concrete hardness of Learning with Errors,"Abstract The learning with errors (LWE) problem has become a central building block of modern cryptographic constructions. This work collects and presents hardness results for concrete instances of LWE. In particular, we discuss algorithms proposed in the literature and give the expected resources required to run them. We consider both generic instances of LWE as well as small secret variants. Since for several methods of solving LWE we require a lattice reduction step, we also review lattice reduction algorithms and use a refined model for estimating their running times. We also give concrete estimates for various families of LWE instances, provide a Sage module for computing these estimates and highlight gaps in the knowledge about algorithms for solving the LWE problem.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.5,Mathematics of computing:0.75,Information systems:0.0,Security and privacy:1.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Security and privacy,"Security and privacy is highly relevant because the paper focuses on cryptographic hardness and LWE problems, which are fundamental to cryptographic security. Other categories like Theory of computation (0.5) relate to algorithms, but the primary contribution is in security/cryptographic analysis. Mathematics of computing (0.75) is relevant for the mathematical models used but is secondary to the security focus.","Cryptography:0.9,Database and storage security:0.1,Formal methods and theory of security:0.4,Human and societal aspects of security and privacy:0.1,Intrusion/anomaly detection and malware mitigation:0.1,Network security:0.1,Security in hardware:0.1,Security services:0.1,Software and application security:0.1,Systems security:0.1",Cryptography,Cryptography is directly relevant to the LWE problem analysis. Formal methods receive a moderate score for theoretical contributions but are secondary to the core cryptographic focus.,"Cryptanalysis and other attacks:1,Information-theoretic techniques:0,Key management:0,Mathematical foundations of cryptography:1,Public key (asymmetric) techniques:0,Symmetric cryptography and hash functions:0","Cryptanalysis and other attacks,Mathematical foundations of cryptography","Cryptanalysis and other attacks: The paper analyzes algorithms for solving LWE, a foundational cryptographic problem. Mathematical foundations of cryptography: The work discusses the theoretical hardness of LWE and lattice reduction techniques. Other children are irrelevant as the focus is on foundational analysis, not key management or specific cryptographic techniques."
3888,Asymmetric fingerprinting for larger collusions,"Fingerprinting schemes deter people from illegally redistributing digital data by enabling the original merchant of the data to identify the original buyer of a redistributed copy. So-called traitor-tracing schemes have the same goal for keys that can be used to decrypt information that is broadcast in encrypted form. Recently, asymmetric fingerprinting and traitor-tracing schemes were introduced. Here, only the buyer knows the fingerprinted copy after a sale, and if the merchant finds this copy somewhere, he obtains a proof that he found the copy of this particular buyer. First constructions showed the validity of the concept. However, essentially all these constructions use so-called memory-less symmetric schemes as building blocks, whereas the better ones among the known symmetric schemes are not memory-less in this sense. Consequently, the previous asymmetric constructions did not reach the same level of tolerance against collusions as symmetric schemes. We now show asymmetric constructions without this restriction.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.2,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.9,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Security and privacy,Security and privacy is highly relevant as the paper introduces asymmetric fingerprinting schemes to prevent traitor tracing and collusion attacks. Other categories like 'Software and its engineering' are less central to the core security contribution.,"Cryptography:0.9,Database and storage security:0.2,Formal methods and theory of security:0.7,Human and societal aspects of security and privacy:0.1,Intrusion/anomaly detection and malware mitigation:0.1,Network security:0.2,Security in hardware:0.1,Security services:0.3,Software and application security:0.1,Systems security:0.3","Cryptography,Formal methods and theory of security",Cryptography is central as the paper introduces asymmetric fingerprinting schemes. Formal methods and theory of security are relevant for the theoretical analysis of collusion resistance. Other fields like database security or network security are not directly addressed in the paper's core contribution.,"Cryptanalysis and other attacks:0.1,Formal security models:0.9,Information-theoretic techniques:0.2,Key management:0.2,Logic and verification:0.1,Mathematical foundations of cryptography:0.8,Public key (asymmetric) techniques:1.0,Security requirements:0.2,Symmetric cryptography and hash functions:0.1,Trust frameworks:0.1","Public key (asymmetric) techniques,Formal security models,Mathematical foundations of cryptography",Public key (asymmetric) techniques: Core to asymmetric fingerprinting. Formal security models: Analyzes security proofs and collusion resistance. Mathematical foundations of cryptography: Underlying theory for the schemes. Symmetric cryptography is not the focus here.
2039,Resistance analysis of scalable video fingerprinting systems under fair collusion attacks,"Digital fingerprinting is an important tool in multimedia forensics to trace traitors and protect multimedia content after decryption. This paper addresses the enforcement of digital rights when distributing multimedia over heterogeneous networks and studies the scalable multimedia fingerprinting systems in which users receive copies of different quality. We investigate the traitor tracing capability of such scalable fingerprinting systems, in particular, the robustness of the embedded fingerprints against multi-user collusion attacks. Under the fairness constraints on collusion that all attackers share the same risk of being captured, we analyze the maximum number of colluders that the fingerprinting systems can withstand, and our results show that multimedia fingerprints can survive collusion attacks by a few dozen colluders.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.2,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.3,Information systems:0.1,Security and privacy:0.9,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Security and privacy,"Security and privacy is highly relevant because the paper studies multimedia fingerprinting resistance to collusion attacks, a core security mechanism. Mathematics of computing is less central as the focus is on system robustness rather than mathematical theory.","Cryptography:0.8,Database and storage security:0.05,Formal methods and theory of security:0.1,Human and societal aspects of security and privacy:0.05,Intrusion/anomaly detection and malware mitigation:0.05,Network security:0.1,Security in hardware:0.05,Security services:0.85,Software and application security:0.1,Systems security:0.05","Cryptography,Security services","Cryptography: The paper analyzes digital fingerprinting systems for multimedia content protection. Security services: The research focuses on traitor tracing and collusion resistance in digital rights management. Other children rejected: The paper doesn't address network security, hardware security, or malware.","Access control:0.2,Authentication:0.2,Authorization:0.2,Cryptanalysis and other attacks:0.7,Digital rights management:0.8,Information-theoretic techniques:0.3,Key management:0.2,Mathematical foundations of cryptography:0.6,Privacy-preserving protocols:0.2,Pseudonymity, anonymity and untraceability:0.2,Public key (asymmetric) techniques:0.1,Symmetric cryptography and hash functions:0.1","Digital rights management,Cryptanalysis and other attacks,Mathematical foundations of cryptography",Digital rights management is directly relevant for fingerprinting and traitor tracing. Cryptanalysis and other attacks apply to collusion resistance analysis. Mathematical foundations of cryptography is relevant for the probabilistic analysis of collusion bounds.
3761,A framework for merging inconsistent beliefs in security protocol analysis,"This paper proposes a framework for merging inconsistent beliefs in the analysis of security protocols. The merge application is a procedure of computing the inferred beliefs of message sources and resolving the conflicts among the sources. Some security properties of secure messages are used to ensure the correctness of authentication of messages. Several instances are presented, and demonstrate our method is useful in resolving inconsistent beliefs in secure messages.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:1.0,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Security and privacy,Security and privacy is highly relevant as the paper addresses security protocol analysis and belief merging for authentication. Other fields are irrelevant as they do not focus on security mechanisms or cryptographic protocols.,"Formal methods and theory of security:0.98,Network security:0.85,Cryptography:0.15,Database and storage security:0.05,Human and societal aspects of security and privacy:0.1,Intrusion/anomaly detection and malware mitigation:0.05,Security in hardware:0.05,Security services:0.1,Software and application security:0.05,Systems security:0.1","Formal methods and theory of security,Network security",Formal methods and theory of security: The paper introduces a formal framework for resolving belief conflicts in security protocols. Network security: The application context involves secure message authentication. Other areas like cryptography or hardware security are not the primary focus.,"Denial-of-service attacks:0,Firewalls:0,Formal security models:1,Logic and verification:1,Mobile and wireless security:0,Security protocols:1,Security requirements:0,Trust frameworks:0,Web protocol security:0","Security protocols,Formal security models,Logic and verification",Security protocols: The paper directly addresses security protocol analysis. Formal security models: The framework involves formal methods for resolving inconsistencies. Logic and verification: The conflict resolution procedure aligns with logical verification techniques.
5066,A framework for empirical evaluation of malware detection resilience against behavior obfuscation,"Program obfuscation is increasingly popular among malware creators. Objectively comparing different malware detection approaches with respect to their resilience against obfuscation is challenging. To the best of our knowledge, there is no common empirical framework for evaluating the resilience of malware detection approaches w.r.t. behavior obfuscation. We propose and implement such a framework, called FEEBO that obfuscates the observable behavior of malware binaries targeting Microsoft Windows operating systems. To assess the framework's utility, we use it to obfuscate known malware binaries and then investigate the impact on detection effectiveness of different popular behavior based malware detection approaches. We find that the obfuscation transformations employed by FEEBO can affect the accuracy of such detection approaches significantly. FEEBO is therefore an effective and fair way to test the degree of resilience of behavior-based malware detection approaches against behavior obfuscation.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:1.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.0",Security and privacy,Security and privacy is highly relevant as the paper evaluates malware detection resilience against obfuscation attacks. Other categories like Networks are irrelevant as the focus is on security analysis rather than communication protocols.,"Cryptography:0.1,Database and storage security:0.1,Formal methods and theory of security:0.2,Human and societal aspects of security and privacy:0.1,Intrusion/anomaly detection and malware mitigation:1.0,Network security:0.3,Security in hardware:0.1,Security services:0.2,Software and application security:0.3,Systems security:0.4",Intrusion/anomaly detection and malware mitigation,Intrusion/anomaly detection and malware mitigation is directly relevant as the paper evaluates malware detection resilience against obfuscation. Other security categories are less specific to the behavior-based detection focus of the study.,"Intrusion detection systems:0,Malware and its mitigation:1,Social engineering attacks:0",Malware and its mitigation,Malware and its mitigation: The paper evaluates malware detection resilience against obfuscation. Other children are irrelevant as they focus on intrusion detection or social engineering.
2243,Botnet Traffic Analysis Using Flow Graphs,"The botnet which is a group of interconnected computers communicating autonomously for malicious purposes. These networked computers communicate and coordinate with similar machines and are controlled by Command and Control (C2). The use of botnet is on rise, growing day by day and a huge number of bot-masters are switching to HTTP-based C2 infrastructure, which allow them to blend in with benign web traffic. Fast-flux DNS is another way to make it tough to track down the control servers, because it may hop from DNS domain to DNS domain using domain generation algorithms. Botnets are becoming more serious problem on the internet due to its rapid evaluation and adoption of new strategies in their structure, protocols and attacks. In this paper, the method proposed is a graph based analysis mechanism in order to detect bots activities and capture sequence of events. The proposed approach has been tested on CTU dataset and evaluated its effectiveness.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.9,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.1",Security and privacy,Security and privacy is highly relevant as the paper addresses botnet traffic analysis for malicious detection. Other categories like Networks or Computing methodologies are secondary to the core security focus.,"Cryptography:0.1,Database and storage security:0.1,Formal methods and theory of security:0.1,Human and societal aspects of security and privacy:0.1,Intrusion/anomaly detection and malware mitigation:0.85,Network security:0.8,Security in hardware:0.1,Security services:0.1,Software and application security:0.1,Systems security:0.1","Network security,Intrusion/anomaly detection and malware mitigation",Network security is relevant for botnet detection in networked systems. Intrusion/anomaly detection applies to the traffic analysis methodology. Other security categories are less relevant as the paper focuses specifically on network-based botnet detection.,"Malware and its mitigation:1,Intrusion detection systems:0.8,Web protocol security:0.6,Denial-of-service attacks:0.4,Firewalls:0.2,Security protocols:0.5,Social engineering attacks:0.1","Malware and its mitigation,Intrusion detection systems",Malware mitigation is central for botnet detection. Intrusion detection is relevant for the graph-based analysis. Web security is secondary due to HTTP C2 focus. Other categories are less directly related.
3011,Establishing Mutually Trusted Channels for Remote Sensing Devices with Trusted Execution Environments,"Remote and largely unattended sensing devices are being deployed rapidly in sensitive environments, such as healthcare, in the home, and on corporate premises. A major challenge, however, is trusting data from such devices to inform critical decision-making using standardised trust mechanisms. Previous attempts have focused heavily on Trusted Platform Modules (TPMs) as a root of trust, but these forgo desirable features of recent developments, namely Trusted Execution Environments (TEEs), such as Intel SGX and the GlobalPlatform TEE. In this paper, we contrast the application of TEEs in trusted sensing devices with TPMs, and raise the challenge of secure TEE-to-TEE communication between remote devices with mutual trust assurances. To this end, we present a novel secure and trusted channel protocol that performs mutual remote attestation in a single run for small-scale devices with TEEs. This is evaluated on two ARM development boards hosting GlobalPlatform-compliant TEEs, yielding approximately four-times overhead versus untrusted world TLS and SSH. Our work provides strong resilience to integrity and confidentiality attacks from untrusted world adversaries, facilitates TEE interoperability, and is subjected to mechanical formal analysis using Scyther.","General and reference:0.1,Hardware:0.2,Computer systems organization:0.2,Networks:0.3,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:1.0,Human-centered computing:0.2,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.1",Security and privacy,Security and privacy is highly relevant as the paper focuses on secure TEE-to-TEE communication and mutual trust mechanisms. Hardware and Networks are less central to the core security contribution.,"Cryptography:0.3,Database and storage security:0.1,Formal methods and theory of security:0.8,Human and societal aspects of security and privacy:0.1,Intrusion/anomaly detection and malware mitigation:0.1,Network security:0.2,Security in hardware:0.9,Security services:0.5,Software and application security:0.3,Systems security:0.7","Security in hardware,Formal methods and theory of security,Systems security",Security in hardware is central for TEE-based communication. Formal methods are relevant for protocol analysis. Systems security addresses trust assurances. Cryptography is secondary to TEEs.,"Browser security:0.1,Denial-of-service attacks:0.1,Distributed systems security:0.3,Embedded systems security:0.6,File system security:0.1,Firewalls:0.1,Formal security models:0.5,Hardware attacks and countermeasures:0.3,Hardware reverse engineering:0.1,Hardware security implementation:0.4,Information flow control:0.1,Logic and verification:0.2,Operating systems security:0.7,Security requirements:0.2,Tamper-proof and tamper-resistant designs:0.4,Trust frameworks:0.8,Vulnerability management:0.1","Trust frameworks,Operating systems security",Trust frameworks: The paper introduces a TEE-based trusted channel protocol. Operating systems security: TEEs are part of OS-level security. 'Embedded systems security' is relevant but secondary. 'Hardware security implementation' is less directly addressed as the focus is on communication protocols.
584,The Increased Need For FCC Merger Review In A Networked World,"Recently, the FCC announced a new standard for review in mergers. Under the new standard, mass media mergers that comply with existing rules will automaticly receive approval, while those that do not will receive a more searching review. Common carrier mergers, however, will continue to receive the 4=part test established in Bell Atlantic/Nynex. The new standard fails to take into account the complexities of the emerging, converged networked world, and is essentially obsolete on arrival. Looking to those areas where Congress has required an additional public interest review of mergers, a pattern emerges. The emergence of vast, vertically integrated networks of content and conduit fit the historic pattern of areas requiring piublic interest review and re-enforce the need for increased, rather than decreased merger review.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.9",Social and professional topics,"Social and professional topics: The paper discusses FCC merger review policies, a regulatory and policy issue. Other fields like Security or Networks are irrelevant.","Computing / technology policy:1.0,Professional topics:0.2,User characteristics:0.1",Computing / technology policy,Computing / technology policy: The paper directly addresses regulatory policy for networked technology mergers. Professional topics and User characteristics are not discussed.,"Government technology policy:0.9,Network access control:0.4,Commerce policy:0.2,Privacy policies:0.1,Censorship:0.1,Intellectual property:0.1,Computer crime:0.1,Surveillance:0.1,Medical information policy:0.1",Government technology policy,Government technology policy: The paper analyzes FCC merger review policies in networked contexts. Other categories like network access control have limited relevance to the policy discussion.
2549,Information ethics in the design and use of metadata,"Just as information contains the explicit and implicit values of its creators, so does the metadata that describes, summarizes, or represents that information. Consequently, issues arise in the planned design of metadata; in the creation of surrogates that represent data, text, or objects; as well as in the use and ownership of the metadata content in electronic environments. The intent here is to be illustrative, rather than prescriptive, when discussing issues raised.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.2,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.3,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.9",Social and professional topics,Social and professional topics is directly relevant as the paper discusses ethical issues in metadata design. Information systems is less central as the focus is on ethics rather than technical metadata management.,"Computing / technology policy:0.8,Professional topics:0.2,User characteristics:0.1",Computing / technology policy,"Computing / technology policy: The paper discusses ethical design of metadata, which falls under technology policy. Other children are irrelevant as the paper does not address professional ethics or user characteristics.","Censorship:0,Commerce policy:0,Computer crime:0,Government technology policy:0,Intellectual property:1,Medical information policy:0,Network access control:0,Privacy policies:1,Surveillance:0","Intellectual property,Privacy policies","Intellectual property: The paper discusses ownership of metadata. Privacy policies: Ethical issues in metadata design relate to privacy. Other options (e.g., Censorship) are not addressed."
840,Organizational Conditions as Catalysts for Successful People-Focused Knowledge Sharing Initiatives: An Empirical Study,"This paper analyzes the impact of different organizational enablers on the degree of success of different people-focused knowledge sharing initiatives. Considering company size and technology intensity as two of the most relevant contingent variables in terms of organizational conditions (Mintzberg, 1979), the moderator role of these variables will be examined. For these relationships to be tested, an empirical study has been carried out among Spanish manufacturing firms with more than 50 employees which carry out R&D activities. Structural equation modelling (SEM) based on partial least squares (PLS) has been applied in order to test the main hypotheses of the research. The results obtained show that organizational design and organizational culture play a substantial role when it comes to explaining the degree of success of implementation of people-focused knowledge sharing initiatives. Conversely, the influence of information and communication technologies (ICT) is less relevant. Some interesting differences arise depending on technology intensity and company size.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.9",Social and professional topics,"Social and professional topics: The study examines organizational factors influencing knowledge sharing initiatives, which aligns with social/professional computing research. Other categories are irrelevant as the focus is not on technical systems.","Computing / technology policy:0.2,Professional topics:0.9,User characteristics:0.3",Professional topics,Professional topics is highly relevant as the paper examines organizational design and culture's impact on knowledge sharing initiatives. Computing/technology policy and User characteristics are less relevant since the study focuses on organizational structures rather than policy or user demographics.,"Computing and business:1,Management of computing and information systems:1,Computing education:0.2","Computing and business,Management of computing and information systems",Computing and business is directly relevant as the paper examines organizational conditions in R&D. Management of computing systems is relevant due to the focus on knowledge sharing management. Other categories like Computing education are not addressed.
107,Servitization Strategies and Product-Service-Systems,"This paper attempts to explain the concept of servitization in order to declare and emphasize its capabilities in being an influential strategy that provides competitive advantage in manufacturing industries. This is conducted by a detailed elaboration of its notion and origins in the literature as well as the characteristics and processes regarding value co-creation, which in turn is consolidated as fundamentals of servitization. Furthermore, the driving characteristics separated by general environmental trends, financial-, strategic-, as well as marketing-drivers are discussed in detail. The final part of the analysis refers to the classification of servitization and the actual options and alternatives of implementation.","General and reference:0.2,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.2,Social and professional topics:0.7",Social and professional topics,Social and professional topics is relevant for governance analysis in open content creation. Other categories lack direct technical alignment with the paper's focus on social mechanisms.,"Computing / technology policy:0.2,Professional topics:0.8,User characteristics:0.3",Professional topics,Professional topics (0.8) is relevant as the paper discusses business strategies and organizational policies in manufacturing. Computing / technology policy and User characteristics are less relevant as the paper focuses on organizational strategies rather than technology policy or user behavior.,"Computing and business:1,Computing education:0,Computing industry:0,Computing profession:0,History of computing:0,Management of computing and information systems:0",Computing and business,"Computing and business is relevant as the paper discusses servitization as a business strategy in manufacturing. Other categories are irrelevant since the paper does not address education, industry practices, or computing history."
3266,Network Structure and Social Outcomes: Network Analysis for Social Science,"Human behavior is characterized by connections to others. We define ourselves by these connections: our families, our friends, our neighbors, our co-workers all form a social geography. Social scientists who study networks serve as cartographers for these social plains, identifying actors who influence others. In their overview of the study of political networks, McClurg and Young (2011) state, “We would probably all agree that one primary tie among political scientists is our emphasis on power, and understanding how and why power is used. We are all inherently interested in the exercise of power between and among individuals and groups and the implications that this exercise holds for social outcomes. We contend that this unifying concept is, at its very core, relational.” Social scientists have an interest in relational social science, with roles as either researchers directly focusing on relationships between actors or else as scholars accounting for interdependence among actors and institutions in their analyses. Additionally, we have seen an explosion in the availability of networked data. With the rise of social media, the relationships between ordinary citizens and political elites, among ordinary citizens, and even among political elites is more easily quantified. When once scholars of Congress had to “soak and poke” to understand a legislator’s relationship with her constituents (Fenno 1978), now it is possible to directly observe the connections that legislators establish with their constituents over Twitter, as well as the connections between the constituents themselves (Barbera 2015), the donations made to legislators and","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.2,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.9",Social and professional topics,"Social and professional topics: The paper discusses social network analysis and its implications for social outcomes, which aligns with social and professional topics. Human-centered computing is secondary as the focus is on network theory rather than user behavior.","Computing / technology policy:0.2,Professional topics:0.1,User characteristics:0.6",User characteristics,User characteristics is relevant as the paper examines how network structures influence social outcomes through human behavior analysis. Computing/technology policy receives a low score since the focus is on social science analysis rather than policy implications. Professional topics is not directly related to the study's content.,"Age:0,Cultural characteristics:0,Gender:0,Geographic characteristics:0,People with disabilities:0,Race and ethnicity:0,Religious orientation:0,Sexual orientation:0",,"The paper focuses on social network analysis and political power dynamics, which are unrelated to demographic categories like age, gender, or race. None of the provided options align with the paper's core contributions in social science network analysis."
3640,Practitioner engineers perceptions for a successful early employment career,"The aim of this research is to examine the transition of Lebanese engineering students from collegiate life to working professional as a case study in the Middle East region. Practicing engineers (n=217) graduated from universities in Lebanon and working domestically and abroad in different social and multicultural settings were surveyed. The survey identified their current employment and their attitudes toward their academic preparation level. The results showed that locating a job is not a major concern for graduates due to the high demand of engineers in the prosperous Arabic Gulf States. However, novice engineers confront critical issues during the transition process. While Internships and graduation projects appear to be essentially helpful for participants, it was observed the lacking role of career centers in such critical stage. An Exploratory Factor Analysis revealed three main challenges facing engineering graduates: communication, responsibility, and self-confidence. Participants' answers to open-ended questions suggest a strong need for collaboration between the engineering industries and the academic institutions to facilitate a smoother transition.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.95",Social and professional topics,Social and professional topics is highly relevant as the paper examines the transition of engineering students to professionals. Other categories are irrelevant as the focus is on career challenges and academic-industry collaboration.,"Computing / technology policy:0.0,Professional topics:1.0,User characteristics:0.75","Professional topics,User characteristics",Professional topics is relevant for career transition challenges. User characteristics is relevant for analyzing engineer perceptions. Computing/technology policy is not directly relevant to the study.,"Computing education:1,Computing profession:0.9,Computing and business:0.2,Computing industry:0.3,Cultural characteristics:0.1,Gender:0.1,Geographic characteristics:0.4,History of computing:0.1,Management of computing and information systems:0.1,People with disabilities:0.1,Race and ethnicity:0.1,Religious orientation:0.1,Sexual orientation:0.1","Computing education,Computing profession",Computing education is directly relevant as the paper examines academic preparation and transition for engineering students. Computing profession is relevant due to the focus on early career challenges for engineers. Other fields like Cultural characteristics are secondary but not primary here.
4855,No Strategy is an Island to Itself: China First-Mover and Other FDI Strategies' Interaction Effects,"The first-mover strategy for foreign investment is examined to determine under what conditions a significant effect exists when it is combined with other foreign investment strategies like partner selection, geographical market focus, joint-venture control, and resource commitment strategies. Using official audited data and survey data from Sichuan, the results reveal that there are significant interaction effects. The interaction effects can eliminate first-mover advantage, create a first-mover effect that previously didnâ€™t exist, or change the direction of the effect. Consequently, the author argues that it is better to analyze strategies as a set that is formed by a series of strategic decisions made by managers as they establish foreign joint ventures and wholly owned subsidiaries.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.2,Social and professional topics:0.8",Social and professional topics,"Social and professional topics is highly relevant because the paper examines business strategies in foreign investment, focusing on organizational decision-making. Applied computing (0.2) is less relevant as the paper does not focus on technical systems.","Computing / technology policy:0.1,Professional topics:0.8,User characteristics:0.1",Professional topics,Professional topics: The paper examines strategic interactions in foreign investment. Other categories like Computing / technology policy are irrelevant to the business strategy analysis.,"Computing and business:1,Computing education:0,Computing industry:0,Computing profession:0,History of computing:0,Management of computing and information systems:0.3",Computing and business,Computing and business is directly relevant to analyzing FDI strategies. Management of computing systems is tangential.
3856,Public library Facebook use: established positions renegotiated?,In this research project public library Facebook use is explored and analyzed in relation to Henry Jenkins's work on participatory cultures and literacy.,"General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.9",Social and professional topics,"Social and professional topics: The paper analyzes Facebook use in public libraries through the lens of participatory culture and literacy, which aligns with social computing research. Other categories are irrelevant as the work focuses on social behavior rather than technical systems.","Computing / technology policy:0.3,Professional topics:0.4,User characteristics:1.0",User characteristics,User characteristics is highly relevant as the study examines Facebook use in public libraries through a participatory culture lens. Other categories do not address user behavior or literacy directly.,"Age:0.1,Cultural characteristics:0.9,Gender:0.2,Geographic characteristics:0.3,People with disabilities:0.1,Race and ethnicity:0.1,Religious orientation:0.1,Sexual orientation:0.1",Cultural characteristics,Cultural characteristics (0.9): The paper analyzes Facebook use in public libraries through the lens of participatory cultures. Other demographic categories are not explicitly discussed.
3738,The Origins of Information Science and the International Institute of Bibliography/International Federation for Information and Documentation (FID),"This article suggests that the ideas and practices embraced by the term “documentation,” introduced by Paul Otlet and his colleagues to describe the work of the International Institute of Bibliography (later FID) that they set up in Brussels in 1895, constituted a new “discursive formation,” to echo Foucault. While today's special terminology of information science was not then in use, this should not obscure the fact that key concepts for information science as we now understand this field of study and research—and the technical systems and professional activities in which it is anchored—were implicit in and operationalized by what was created within the International Institute of Bibliography in 1895 and the decades that followed. The ideas and practices to be discussed would today be rubricated as information technology, information retrieval, search strategies, information centers, fee-based information services, linked data bases, database management software, scholarly communication networks, multimedia and hypertext, even the modern, diffuse notion of “information” itself. The article argues that important aspects of the origins of information science, as we now know it in the U.S. and elsewhere in the English-speaking world, were contained within or became an extension of the discursive formation that we have labeled “documentation.” — Author's Abstract","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.8",Social and professional topics,"Social and professional topics is highly relevant because the paper discusses the historical origins of information science as a discursive formation. Information systems is less relevant as the focus is on conceptual history, not system design.","Computing / technology policy:0.4,Professional topics:1.0,User characteristics:0.3",Professional topics,Professional topics (1.0) is directly relevant for the historical analysis of information science. Computing/technology policy (0.4) has limited relevance to the historical context. User characteristics (0.3) is not relevant to the paper's focus.,"Computing and business:0.0,Computing education:0.0,Computing industry:0.0,Computing profession:1.0,History of computing:1.0,Management of computing and information systems:0.0","History of computing,Computing profession",History of computing is relevant because the paper discusses the historical origins of information science. Computing profession is relevant as it examines the formation of professional practices in the field. Other options like 'Computing and business' are not directly addressed.
2186,Newsworthiness and Network Gatekeeping on Twitter: The Role of Social Deviance,"Publishers of news information are keen to amplify the reach of their content by making it as re-sharable as possible on social media. In this work we study the relationship between the concept of social deviance and the re-sharing of news headlines by network gatekeepers on Twitter. Do network gatekeepers have the same predilection for selecting socially deviant news items as professionals? Through a study of 8,000 news items across 8 major news outlets in the U.S. we predominately find that network gatekeepers re-share news items more often when they reference socially deviant events. At the same time we find and discuss exceptions for two outlets, suggesting a more complex picture where newsworthiness for networked gatekeepers may be moderated by other effects such as topicality or varying motivations and relationships with their audience.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:1.0",Social and professional topics,Social and professional topics is highly relevant for the study of social media behavior and gatekeeping. Other categories like Human-centered computing are less focused on the social dynamics analyzed.,"Computing / technology policy:0.1,Professional topics:0.1,User characteristics:0.9",User characteristics,User characteristics is relevant as the paper studies user behavior in re-sharing news on social media. Other categories like Computing / technology policy are less central to the analysis of social deviance and network gatekeeping.,"Age:0.1,Cultural characteristics:0.1,Gender:0.1,Geographic characteristics:0.1,People with disabilities:0.1,Race and ethnicity:0.1,Religious orientation:0.1,Sexual orientation:0.1",,None of the provided demographic categories directly address the paper's focus on social deviance and network gatekeeping behavior in news sharing. The study examines information diffusion patterns rather than specific demographic characteristics.
5314,Empowering Local Village Community Enterprises in Thailand: A Possible Model for China and South - East Asia,"This paper describes a process that has been developed over the years to empower local village communities in Thailand to create sustainable community enterprises principally based on local knowledge and resources. The paper, which is written for an international audience and therefore first describes some characteristics of local communities in Thailand, then discusses research methodology and approach leading to a community enterprise learning model, refers to case study work carried out in the Central Region of Thailand, and finally discusses the community learning process. The work may be more generically applicable, and can now form the basis for a broader co-operation with similar groups conducting studies in other countries. With this in mind the research has been presented and discussed in December 2011 at The Faculty of Economics1, Jinan University, Guangzhou, China and in March 2012 with the Chinese Academy of Social Sciences2 in Beijing as a basis for possible future co-operation. Further dissemination of the research into other ASEAN countries is envisaged.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:1.0",Social and professional topics,Social and professional topics is relevant for the community development study. Other categories lack connection to the social science focus on village enterprise models.,"Computing / technology policy:0.1,Professional topics:0.1,User characteristics:0.6",User characteristics,User characteristics is relevant as the paper studies community behavior patterns in enterprise development. Other categories like Computing/technology policy are not directly related to the social science focus.,"Age:0,Cultural characteristics:1,Gender:0,Geographic characteristics:1,People with disabilities:0,Race and ethnicity:0,Religious orientation:0,Sexual orientation:0","Cultural characteristics,Geographic characteristics",Cultural characteristics is relevant for community learning models in Thailand. Geographic characteristics is relevant for regional case studies. Other demographic categories are not discussed in the abstract.
5518,Creating a shared understanding of testing culture on a social coding site,"Many software development projects struggle with creating and communicating a testing culture that is appropriate for the project's needs. This may degrade software quality by leaving defects undiscovered. Previous research suggests that social coding sites such as GitHub provide a collaborative environment with a high degree of social transparency. This makes developers' actions and interactions more visible and traceable. We conducted interviews with 33 active users of GitHub to investigate how the increased transparency found on GitHub influences developers' testing behaviors. Subsequently, we validated our findings with an online questionnaire that was answered by 569 members of GitHub. We found several strategies that software developers and managers can use to positively influence the testing behavior in their projects. However, project owners on GitHub may not be aware of them. We report on the challenges and risks caused by this and suggest guidelines for promoting a sustainable testing culture in software development projects.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.3,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.8",Social and professional topics,Social and professional topics is highly relevant as the paper analyzes testing culture in software development communities. Other categories are less relevant since the focus is on social dynamics rather than technical methodologies or systems.,"Computing / technology policy:0.2,Professional topics:0.9,User characteristics:0.7","Professional topics,User characteristics",Professional topics is highly relevant as the study focuses on software development practices and testing culture in collaborative environments. User characteristics is relevant due to the analysis of developer behaviors and social dynamics on GitHub. Computing/technology policy is less directly connected as the focus is not on policy frameworks but on cultural and behavioral aspects.,"Age:0.1,Computing and business:0.1,Computing education:0.6,Computing industry:0.1,Computing profession:1.0,Cultural characteristics:0.8,Gender:0.1,Geographic characteristics:0.1,History of computing:0.1,Management of computing and information systems:0.1,People with disabilities:0.1,Race and ethnicity:0.1,Religious orientation:0.1,Sexual orientation:0.1","Computing profession,Cultural characteristics,Computing education",Computing profession is relevant as the paper studies professional software development practices. Cultural characteristics is relevant due to the focus on testing culture. Computing education is relevant as the paper provides guidelines for promoting testing culture. Other categories like Gender are not addressed.
1415,Digital India and Women: Bridging the Digital Gender Divide,"The purpose of this study is to find out whether digital technologies can reduce the existing gender inequality in Indian society. The current debate around digitalization is limited to the ones who have access and the ones who haven't. But this paper takes the debate beyond access by suggesting a three-tier framework for measuring the impact of digital technologies on women empowerment. The Digital India programme run by Government of India is taken as a case study by the authors. Rather than depending on traditional methodologies to bring gender equality, can we consider Digital India as a potential solution? This paper explores the answer to the above question from a gendered perspective with the help of primary data analysis of a few selected Digital India products and services.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.9",Social and professional topics,"Social and professional topics is highly relevant because the paper analyzes the societal impact of digital technologies on gender inequality in India, focusing on social frameworks and policy implications. Other categories like Applied computing are less relevant as the focus is on sociological analysis rather than technical implementation.","Computing / technology policy:0.9,Professional topics:0.3,User characteristics:0.7","Computing / technology policy,User characteristics",Computing / technology policy is relevant because the paper examines the policy implications of digital technologies for gender equality. User characteristics is relevant as it analyzes the impact of technology on women. Professional topics is rejected because the focus is not on professional practices but on societal impact.,"Age:0,Censorship:0,Commerce policy:0,Computer crime:0,Cultural characteristics:0,Gender:1,Geographic characteristics:0.2,Government technology policy:1,Intellectual property:0,Medical information policy:0,Network access control:0,People with disabilities:0,Privacy policies:0,Race and ethnicity:0,Religious orientation:0,Sexual orientation:0,Surveillance:0","Gender,Government technology policy",Gender is directly addressed as the study focuses on digital gender divide. Government technology policy is relevant as Digital India is a government initiative. Geographic characteristics is moderately relevant but not central.
995,How to break an API: cost negotiation and community values in three software ecosystems,"Change introduces conflict into software ecosystems: breaking changes may ripple through the ecosystem and trigger rework for users of a package, but often developers can invest additional effort or accept opportunity costs to alleviate or delay downstream costs. We performed a multiple case study of three software ecosystems with different tooling and philosophies toward change, Eclipse, R/CRAN, and Node.js/npm, to understand how developers make decisions about change and change-related costs and what practices, tooling, and policies are used. We found that all three ecosystems differ substantially in their practices and expectations toward change and that those differences can be explained largely by different community values in each ecosystem. Our results illustrate that there is a large design space in how to build an ecosystem, its policies and its supporting infrastructure; and there is value in making community values and accepted tradeoffs explicit and transparent in order to resolve conflicts and negotiate change-related costs.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.85",Social and professional topics,"Social and professional topics is relevant as the paper discusses community values and governance in software ecosystems. Other categories are not relevant as the core contribution is in software engineering sociology, not technical computing.","Computing / technology policy:1.0,Professional topics:0.75,User characteristics:0.1","Computing / technology policy,Professional topics","Computing / technology policy is highly relevant as the paper examines API change policies and community values in software ecosystems. Professional topics is relevant due to the discussion of developer practices and ecosystem management. User characteristics is not applicable as the focus is on developers, not end-users.","Censorship:0.1,Commerce policy:0.2,Computer crime:0.1,Computing and business:0.5,Computing education:0.2,Computing industry:0.7,Computing profession:0.3,Government technology policy:0.2,History of computing:0.1,Intellectual property:0.3,Management of computing and information systems:0.6,Medical information policy:0.1,Network access control:0.1,Privacy policies:0.1,Surveillance:0.1","Computing industry,Management of computing and information systems",Computing industry is relevant as the paper studies software ecosystems and API changes. Management of computing and information systems is relevant due to the focus on community values and cost negotiation in software development. Other categories like Commerce policy are less directly related to the study's focus on ecosystem dynamics.
4196,Kafka in the academy: a note on ethics in IA education,"In studying how to protect the United States' critical infrastructure, a presidential commission divided it into several sectors: information and communications, banking and finance, energy, physical distribution, and vital human services. Given that all sectors are strongly interconnected, the vulnerability of one represents dangers for the others. For example, a failure in the communications infrastructure would quickly have consequences in the finance and physical distribution sectors, which rely on it for coordination. Disruption of finance and transportation would quickly spill over into the energy and human services sectors. The communications and information infrastructures' self-evident long-term dependence on energy and finance completes the cycle","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.9",Social and professional topics,Social and professional topics is highly relevant for the infrastructure sector analysis and ethics discussion. Other fields like Computing methodologies are not core to this policy-oriented analysis.,"Computing / technology policy:0.2,Professional topics:1.0,User characteristics:0.0",Professional topics,"Professional topics is relevant as the paper discusses ethics in education, a key concern in academic and professional computing environments. Computing / technology policy is only peripherally related to infrastructure interdependencies.","Computing and business:1,Computing education:0,Computing industry:0,Computing profession:0,History of computing:0,Management of computing and information systems:1","Computing and business,Management of computing and information systems","Computing and business applies to infrastructure sector interdependencies. Management of computing is relevant for coordinating critical systems. Computing education is irrelevant as the focus is on infrastructure, not pedagogy."
2460,ICT pollution and liability,"To a large extent liability for ICT perils is still a grey area, even though an increasing number of information security researchers adopt economic approaches to highlight market mechanisms and externalities. That is why this article focuses on the need for increased awareness of externalities and liability among ICT professionals and their customers. This is critical to achieve in order to promote appropriate ICT technologies and services with comprehensible privacy and security protection.
 What is needed is a better understanding of the consequences of externalities from ICT perils. This would benefit customers and increase trust in ICT products and services which in turn may even increase suppliers' profit margin; customers would be prepared to buy more services and products if contracts and service level agreements would accept more liability. Moreover, suppliers could sell guaranties and insurances on top of their products and services. The result would be a win-win situation and would benefit society at large.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.8",Social and professional topics,Social and professional topics is highly relevant for ICT liability and externalities. Other categories like Security and privacy are not central to the economic and legal analysis focus.,"Computing / technology policy:1.0,Professional topics:0.2,User characteristics:0.2",Computing / technology policy,Computing/technology policy is directly relevant for ICT liability analysis. Other options are not core to the paper's focus.,"Government technology policy:1,Privacy policies:1,Computer crime:0.2,Commerce policy:0.3,Intellectual property:0.2,Medical information policy:0.1,Network access control:0.1,Surveillance:0.1","Government technology policy,Privacy policies",Government technology policy is relevant for discussing ICT externalities. Privacy policies align with the paper's focus on trust and security in ICT. Other fields like Computer crime are not directly discussed.
777,Technical opinion: The emperor with no clothes,"The true problem with software ishardware. We have been seducedby the promise of more and moreand have become entranced underthe spell of Moore’s Law. Contin-ued progress in hardware is not afriend, but our nemesis. We havebeen shielded by hardwareadvances from confronting ourown incompetence as software pro-fessionals and our immaturity asan engineering profession.—","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.95",Social and professional topics,Social and professional topics is the most relevant category as the paper presents a critical opinion piece on software engineering practices rather than a technical contribution to computing methodologies or systems.,"Computing / technology policy:0.25,Professional topics:0.75,User characteristics:0.0",Professional topics,Professional topics is relevant as the paper critiques software engineering practices. Computing/technology policy is marginally relevant to hardware/progress discussions. User characteristics is not related to the core argument.,"Computing and business:0.1,Computing education:0.1,Computing industry:0.7,Computing profession:1.0,History of computing:0.1,Management of computing and information systems:0.1","Computing profession,Computing industry",Computing profession is relevant for discussing the maturity of software engineering. Computing industry is relevant for hardware/software trade-offs in practice. Other children like Management or Education are not central to the critique presented.
613,E-learning model for Polish libraries: BIBWEB,"– This article aims to describe the important role of e‐learning in Poland and to cover experiences in transferring and localising the course, know‐how about how to find strategic partners and establishing a sound infrastructure for e‐learning, and guidelines as to how to get and use feedback from learners to continuously improve the quality of the course., – The article describes the situation in the Polish library world and the process of implementation and conducting the BIBWEB online course for librarians. It also relates to how the course improves information literacy., – Libraries in Poland can considerably strengthen their role as an element of the information and knowledge society by expanding their offers regarding internet access and e‐learning. The situation in Germany in 1999 was comparable with the current situation in Poland. As a reaction to this, the Bertelsmann Foundation initiated an online course offer comprising three modules: “BIBWEB – internet training for libraries” (see www.bibweb.de), which met with great acceptance in Germany. For this reason, it made sense to translate the existing successful course product and adapt it to the special requirements of libraries in Poland. The Bertelsmann Foundation and its two project partners, the Warsaw University Library and the “Elektroniczna Biblioteka” (EBIB) library information service, have jointly developed such a Polish‐language online course (see www.bibweb.pl) and have offered it on the internet to the specialist target group since 2003., – This paper shows the benefits of an international educational project. Polish experiences concerning e‐learning may be useful for other nations.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.3,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.9",Social and professional topics,Social and professional topics: Focuses on educational initiatives in libraries. Other categories rejected as paper focuses on e-learning implementation and educational outcomes.,"Computing / technology policy:0.0,Professional topics:1.0,User characteristics:0.0",Professional topics,Professional topics is highly relevant as the paper discusses e-learning implementation for librarians. No other options align with the educational and professional development focus.,"Computing and business:0.2,Computing education:1,Computing industry:0,Computing profession:0,History of computing:0,Management of computing and information systems:0.3",Computing education,Computing education is the primary domain as the paper describes an e-learning initiative for librarians. Other categories like 'Management of computing and information systems' are only peripherally relevant.
2515,Uncertainties in software projects management,"The organizational environment becomes increasingly focused on projects, it is time to unleash the power and energy embedded in projects. The disturbingly poor project success rate makes it imperative that the organization pays more attention to their project activity, their potential, and the competitive advantage that they can bring. The goal of this article is to understand what gives rise to uncertainty; to recognize its symptoms and explore strategies for controlling it. This article offers a classification to source of uncertainty in software projects, a guide to manage software projects in phases of uncertainty and our aim is to assist project managers and staff to reduce uncertainty and thereby contribute to commercial project success.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:1.0",Social and professional topics,Social and professional topics: The paper discusses project management practices in software development. Other categories are irrelevant as the focus is on professional practices rather than technical computer science concepts.,"Computing / technology policy:0.0,Professional topics:0.9,User characteristics:0.0",Professional topics,Professional topics is relevant for software project management practices. Other categories like Computing/Technology Policy are not addressed in this uncertainty analysis focused on project execution.,"Computing and business:0.5,Computing education:0.0,Computing industry:0.0,Computing profession:0.0,History of computing:0.0,Management of computing and information systems:1.0",Management of computing and information systems,Management of computing and information systems is relevant for software project management strategies. Computing and business is secondary due to the project's business implications. Other options are irrelevant.
1799,The regulation of online extreme pornography: purposive teleology (in)action,"The Internet has necessitated some changes in the way pornography is regulated. In the UK, there has been a demonstrable shift of focus in the regulation of the end user. The traditional regulatory model, focussing on the producer and distributor of content, has altered with the introduction of simple possession offences for pornography that makes the end user also liable for illegal content. This article analyses the issue by considering the general teleological reasoning for the regulation of pornography, and applies this to the legislation dealing with extreme adult pornography. The article points out that a sound teleological basis is imperative for pornography regulation, and that legislative action that overlooks this is fundamentally flawed and will ultimately fail in providing efficacious protection. The article argues that in order to restore confidence in the regulatory system there should be a coherent teleology behind the regulation of pornography, and also that the focus of legal regulation for extreme pornography should remain with producers and distributors rather than content recipients.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.9",Social and professional topics,Social and professional topics is highly relevant as the paper discusses legal and societal implications of pornography regulation. Other categories like 'Information systems' or 'Security and privacy' are irrelevant as the focus is not on technical systems or data protection.,"Computing / technology policy:1.0,Professional topics:0.2,User characteristics:0.3",Computing / technology policy,Computing policy: Focuses on legal regulation of online content. Other categories are not discussed.,"Censorship:1,Commerce policy:0.2,Computer crime:0.3,Government technology policy:1,Intellectual property:0.1,Medical information policy:0.1,Network access control:0.2,Privacy policies:0.4,Surveillance:0.2","Censorship,Government technology policy","Censorship: The paper directly addresses online pornography regulation, a form of content censorship. Government technology policy: The analysis of UK legislation and regulatory shifts falls under government policy. Other categories like 'Privacy policies' are only peripherally related as the focus is on legal frameworks rather than user privacy."
2332,Listening to Customers: How EBSCO Plans Enhancements and Product Acquisitions Based on Customer Feedback,"The importance of customer feedback and the value that it adds to product development and enhancements cannot be overstated. Customer feedback also plays a key role in the development of online resources. But how do you merge customer feedback and the development of new proposals for product design? The balance between customer feedback and strategic direction is something EBSCO strives for in every build of new product enhancements. Many of the company's most important products and interface enhancements can trace their origins back (at least in part) to customer feedback or customer‐based advisory boards. By keeping an open channel of communication between customers and developers, EBSCO has positioned itself as a global leader in matching ongoing product development to current customer need. In doing so, the company has also laid the foundation for ascertaining and responding to the future needs of customers.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.5,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:1.0",Social and professional topics,"Social and professional topics is highly relevant because the paper discusses customer feedback strategies in product development. Human-centered computing (0.5) relates to user interaction but is secondary to the focus on organizational processes. Other categories are irrelevant as the paper is about business practices, not technical systems.","Computing / technology policy:0.3,Professional topics:0.7,User characteristics:0.2",Professional topics,"Professional topics is relevant for discussing business strategies in product development. User characteristics is less relevant as the focus is on organizational practices, not user demographics.","Computing and business:1,Computing education:0,Computing industry:0,Computing profession:0,History of computing:0,Management of computing and information systems:1","Computing and business,Management of computing and information systems","Computing and business: The paper discusses how a company (EBSCO) integrates customer feedback into product development, a business practice. Management of computing and information systems: The focus on product planning and strategic direction aligns with management practices. Other children are irrelevant as the paper does not discuss education, history, or the computing profession."
2982,The consortium in the evolving information industry,"■ Before setting up X/Open in the late ’80s, I was not a fan of the consortium as a means of doing business. I considered it, as I am sure many of you did, to be an unnecessary interference in the competitive workings of the marketplace. At that time, long ago, we had all seen consortia in many industries come and go as companies jostled for supremacy, and it was not clear to me how much difference a consortium would really make in the market.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.9",Social and professional topics,"Social and professional topics: The paper discusses the role of industry consortia in business practices and market dynamics. No technical computing contributions are made, making this the only relevant category.","Computing / technology policy:0.8,Professional topics:0.7,User characteristics:0.2","Computing / technology policy,Professional topics","Computing/technology policy is relevant for analyzing consortia's role in tech industries. Professional topics addresses organizational dynamics. User characteristics is irrelevant as the paper discusses business structures, not user demographics.","Censorship:0.0,Commerce policy:0.1,Computer crime:0.0,Computing and business:0.6,Computing education:0.0,Computing industry:0.8,Computing profession:0.2,Government technology policy:0.0,History of computing:0.0,Intellectual property:0.0,Management of computing and information systems:0.0,Medical information policy:0.0,Network access control:0.0,Privacy policies:0.0,Surveillance:0.0","Computing industry,Computing and business","Computing industry: The paper discusses the role of consortia in the information industry, aligning with this category. Computing and business: The abstract addresses business implications of consortia in computing. Other categories like Commerce policy or Privacy policies are unrelated to the paper's focus on consortium dynamics in the industry."
2877,Informatics minor for non-computer students,"The Rochester Institute of Technology's School of Informatics has developed a minor in Applied Informatics that allows non-computing students from throughout the university to learn problem solving, data retrieval, and information processing and presentation skills so that they can be productive knowledge workers in the 21st century. The minor is strongly problem-oriented with students being taught how to apply deductive, inductive, and abductive reasoning, as well as fundamental information technology skills, to problems in their specific domains. It is the coursework's relevance and applicability to the students' majors that eases the acquisition of these skills.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.2,Social and professional topics:0.9",Social and professional topics,"Social and professional topics is highly relevant as the paper discusses educational programs for non-computing students. Applied computing (0.2) receives minor relevance for the technical skills taught, but the primary contribution is educational curriculum design.","Computing / technology policy:0.3,Professional topics:1.0,User characteristics:0.7",Professional topics,Professional topics is relevant as the paper discusses an informatics curriculum for non-computer students. User characteristics is secondary. Computing/technology policy is not the focus.,"Computing and business:0.1,Computing education:0.9,Computing industry:0.1,Computing profession:0.1,History of computing:0.1,Management of computing and information systems:0.1",Computing education,Computing education is the primary category as the paper describes an informatics minor for non-computing students. Other categories like Computing and business are not directly relevant to the educational focus.
1282,A Strategic Roadmap for Navigating Academic-Industry Collaborations in Information Systems Research: Avoiding Rigor Mortis,"Research collaboration between industry and academia remains challenging despite progress being made on a number of fronts. We are not implying that all information systems researchers should be engaging in industry collaborations, nor should the value of critique and work exclusively addressing academic and other audiences be viewed as less valuable than those engaging industry. The intent is instead to encourage more industry collaboration, and for those considering such initiatives to roadmap the activities strategically giving consideration to the full range of activities required so reasonable tradeoffs can be made in the context of building longer-term sustainable relationships.
 We develop a roadmap from literature that organizes the building blocks (component activities) needed to plan and implement strategic academic-industry collaborations over longer time horizons. Specifically, we identify the key themes or layers of (1) Strategic Business Problem(s), (2) Governance, (3) Funding Criteria, (4) Privacy, Security and Ethics, (5) Intellectual Property, (6) Research Design and (7) Recognized Outputs, which need to be in place if you are to succeed with academic-industry research.
 The roadmap framework highlights the need to consider the implications of the components across each of the layers at a particular point in time (vertical slice), the relationship between layers (dependencies) and the need for aligning the various activities in a coordinated manner; otherwise success at one layer is often undermined by a lack of awareness or failure at another layer.
 Our hope is that these themes (layers) facilitate organizing this collaboration in a systematic and coordinated manner to produce academic-industry collaborations that progressively improve over time.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.9",Social and professional topics,"Social and professional topics: The paper discusses strategic frameworks for academic-industry collaborations, a topic centered on professional and organizational practices. No technical computing systems are the primary focus.","Computing / technology policy:0.1,Professional topics:0.9,User characteristics:0.1",Professional topics,"Professional topics: The paper addresses strategic frameworks for academic-industry collaborations. Computing/technology policy is only relevant in the sense of research governance, not the primary focus on collaboration methodologies.","Computing and business:1.0,Computing education:0.3,Computing industry:0.6,Computing profession:0.2,History of computing:0.1,Management of computing and information systems:0.8","Computing and business,Management of computing and information systems",Computing and business is highly relevant as the paper focuses on academic-industry collaborations. Management of computing and information systems is relevant as the paper discusses governance and management aspects. Computing industry is also relevant but slightly less so.
4702,Programming languages and gender,Comparing differences and similarities in programming language usage according to programmer gender.,"General and reference:0.1,Hardware:0.1,Computer systems organization:0.2,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.9",Social and professional topics,"Social and professional topics (0.9): The paper examines gender differences in programming language usage, a social/educational study. Other categories like Human-centered computing (0.1) are less relevant as the focus is on demographic analysis rather than user interface design.","Computing / technology policy:0.1,Professional topics:0.3,User characteristics:0.9",User characteristics,User characteristics: Study analyzes gender-based differences in programming language usage. Other categories are irrelevant as the focus is purely on demographic analysis of programmers.,"Age:0.1,Cultural characteristics:0.1,Gender:0.9,Geographic characteristics:0.1,People with disabilities:0.1,Race and ethnicity:0.1,Religious orientation:0.1,Sexual orientation:0.1",Gender,Gender: Paper directly investigates gender differences in programming language usage.
3930,Who Gets Started on Kickstarter? Demographic Variations in Fundraising Success,"Crowdfunding platforms like Kickstarter are expected to “democratize” funding by increasing the availability of capital to traditionally underrepresented groups, but there is conflicting evidence about racial disparities in success rates. This paper contributes to the information systems literature on crowdfunding by examining the racial dynamics in the crowdfunding platform Kickstarter. The race of subjects in project and user photos are determined with facial recognition software for 138,778 fundraising projects, and matched sample techniques are used to control for observable differences in project categories among racial groups. Even controlling for these observable differences, this study finds that projects with African-American photo subjects achieve lower success rates. African-American fundraisers also achieve significantly lower success than other groups, and this effect is larger than the effect from project photos. This study has practical implications for individuals seeking capital in these markets as well as design implications for the platforms themselves.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.3,Information systems:0.5,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.2,Social and professional topics:0.9",Social and professional topics,"Social and professional topics: The study examines racial disparities in crowdfunding success, which is a social and professional issue. Other categories like Information systems are less directly related as the focus is on demographic analysis rather than system design.","Computing / technology policy:0.3,Professional topics:0.1,User characteristics:0.85",User characteristics,User characteristics: The paper analyzes racial disparities in crowdfunding success. Other categories like Policy are less relevant as the focus is on user demographics rather than technology policy.,"Age:0,Cultural characteristics:0,Gender:0.1,Geographic characteristics:0,People with disabilities:0,Race and ethnicity:1,Religious orientation:0,Sexual orientation:0",Race and ethnicity,Race and ethnicity is directly addressed as the paper analyzes racial disparities in Kickstarter success rates. Gender and other categories are only tangentially related and not the primary focus.
2393,Vitruvius in Cyberspace,"Nowadays, new forms of virtual architecture are appearing as virtual worlds based on the Metaverse concept are becoming increasingly more popular, holding all kind of virtual versions of human activities. This paper aims to make a reflection on the features of these new architectures compared with their equivalents in the real world by means of an analysis through the prism of the three fundamental conditions of the classic architecture: Firmitas, Utilitas and Venustas. The text describes how these principles once stated by Vitruvius are now affected, modulated or changed in the digital realm by the introduction of the Virtualitas concept.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.85",Social and professional topics,Social and professional topics is relevant for the analysis of virtual architecture's societal implications and reflection on Vitruvius' principles. Other categories like 'Human-centered computing' are less relevant as the focus is on theoretical reflection rather than user interaction design.,"Computing / technology policy:0.0,Professional topics:1.0,User characteristics:0.0",Professional topics,Professional topics is relevant for discussing the impact of virtual architecture on design principles. Other fields like User characteristics are not discussed.,"Computing and business:0.0,Computing education:0.0,Computing industry:0.0,Computing profession:0.0,History of computing:0.5,Management of computing and information systems:0.0",History of computing,History of computing is moderately relevant as the paper reflects on architectural principles in virtual environments. Other categories like Computing and business are not discussed.
675,Emergent and Immergent Effects in Complex Social Systems,"In this paper the notion of emergence in complex social systems is rediscussed as a necessary instrument for a theory of the macro-micro link. Referring to Schelling’s model of segregation, emergent effects are defined as effects generated by (inter)acting micro-social entities, and implemented upon, but not incorporated into, their rules. In the successive section, the way back from macro to micro, i.e. downward causation, is examined. Simple and complex loops are distinguished, with reference to concrete examples drawn from the social scientific and the computational literature. Next, how a given macro-effect is implemented on the lower levels is shown, and two specific mechanisms of implementation, 2 order emergence and immergence, are discussed.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:1.0",Social and professional topics,"The paper discusses theoretical models of emergence in social systems, which aligns with social science and professional topics in ACM-CCS.","User characteristics:0.75,Computing / technology policy:0.2,Professional topics:0.1",User characteristics,"User characteristics is relevant for analyzing online identity roles ('Givers', 'Takers') in OPCs. Computing policy and professional topics are less aligned with the social behavior focus.","Age:0,Cultural characteristics:0,Gender:0,Geographic characteristics:0,People with disabilities:0,Race and ethnicity:0,Religious orientation:0,Sexual orientation:0",,"None of the children are relevant because the paper discusses theoretical social system dynamics, not demographic or identity-based categories."
4530,Research on the Necessity of College Teachers to Establish the Information System Education Concept,"This paper argues that the education of Information system is a kind of modern ideological emancipation movement with scientific thinking as its subject, a natural extension and a higher stage of the computer education, and at the same time, a concrete manifestation of information quality education. We describe the origin, definition and the education pattern of information system literacy, and suggest that information system literacy is one of the college teacher's basic qualities. It is necessary and urgent to establish the concept of information system literacy education, for college teachers are entrusted with the responsibility of information system education, and they should put themselves at the forward position of the ideological emancipation movement.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:1.0",Social and professional topics,Social and professional topics is highly relevant as the paper discusses education and teaching practices for information systems literacy. Other fields like Information Systems or Applied Computing are not central to the core educational focus.,"Computing / technology policy:0.8,Professional topics:0.6,User characteristics:0.0",Computing / technology policy,Computing / technology policy: The paper discusses the necessity of integrating information system literacy into teacher education as a policy-level educational reform. Professional topics receives a lower score because it focuses on teacher qualities rather than the broader educational policy aspect.,"Censorship:0.1,Commerce policy:0.1,Computer crime:0.1,Government technology policy:0.1,Intellectual property:0.1,Medical information policy:0.1,Network access control:0.1,Privacy policies:0.1,Surveillance:0.1",,"All options are irrelevant as the paper discusses educational concepts for information systems, not policy, security, or legal issues."
3248,Implications Regarding Computer Technology Attitudes of New Employees,"Abstract Employee attitudes toward computer technology change over time. As computer technology becomes increasingly more prevalent throughout society and throughout the educational process, appreciative and critical attitudes toward the technology change. Understanding these attitudes can help organizations develop appropriate strategies to improve organizational effectiveness. Entry-level employees often present the greatest challenge for assimilation into the organization. Because today's student becomes tomorrow's entry-level employee, an examination of today's students to determine the appreciative and critical attitudes of future employees can prove beneficial in making modifications to organizational policies and procedures.","General and reference:0.2,Hardware:0.2,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.6,Computing methodologies:0.1,Applied computing:0.3,Social and professional topics:0.85",Social and professional topics,Social and professional topics: 0.85 - The paper examines employee attitudes toward computer technology. Human-centered computing: 0.6 - Involves human factors but not the primary focus. Other categories are irrelevant as the paper focuses on human attitudes in organizational contexts.,"Computing / technology policy:0.8,Professional topics:0.2,User characteristics:1.0","User characteristics,Computing / technology policy","User characteristics: The paper examines attitudes of new employees toward technology, directly aligning with user characteristics. Computing / technology policy: The study's goal to inform organizational strategies ties to technology policy. Professional topics: Less relevant as the focus is on user attitudes rather than professional development topics.","Age:0.7,Censorship:0.0,Commerce policy:0.0,Computer crime:0.0,Cultural characteristics:0.3,Gender:0.0,Geographic characteristics:0.0,Government technology policy:0.0,Intellectual property:0.0,Medical information policy:0.0,Network access control:0.0,People with disabilities:0.0,Privacy policies:0.0,Race and ethnicity:0.0,Religious orientation:0.0,Sexual orientation:0.0,Surveillance:0.0",Age,Age is the most relevant as the paper focuses on entry-level employees' attitudes. Cultural characteristics has low relevance as the study doesn't explicitly address cultural factors. All other options are unrelated to the demographic analysis of employee attitudes.
5602,Exploring the Role of Structural Holes in Learning - an Empirical Study of Swedish pharmacies,"Purpose: There is a lack of studies investigating the role of the structural configuration of social capital - more specifically, structural holes - for employees' individual learning. The objectiv ...","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:1.0",Social and professional topics,Social and professional topics is relevant for the empirical study of structural holes in social capital and learning. Other categories like Networks or Theory of computation are not discussed.,"Computing / technology policy:0.0,Professional topics:1.0,User characteristics:0.0",Professional topics,Professional topics is relevant as the study focuses on organizational learning and social capital in pharmacies. Computing/technology policy and User characteristics are unrelated to the social science focus.,"Computing and business:1.0,Computing education:1.0,Computing industry:0.2,Computing profession:0.2,History of computing:0.2,Management of computing and information systems:0.3","Computing education,Computing and business",Computing education: Focus on learning in pharmacies. Computing and business: Structural holes in organizational contexts. Others like History are irrelevant.
3710,The digital tipping point,"You can’t buy love, and you can't buy freedom, but you can share both. And free open source software is gonna help you do both. The crew of the Digital Tipping Point traveled to Brazil, Spain, German, Scotland and (gasp) Oregon to find out how free open source software and the Internet, like the printing press, is going to change how human beings express themselves, how they organize themselves, and how they govern themselves. Here at OOPSLA, we are presenting a raw, 9 minute clip of interviews with some of the thought leaders as they weigh in on how open source is succeeding on the desktop and elsewhere, and what's at stake in the battle for freedom in cyberspace.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.2,Software and its engineering:0.3,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.2,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.9",Social and professional topics,"Social and professional topics: The paper discusses open source software's societal impact and freedom in cyberspace, aligning with social/professional implications. Rejected: No direct relevance to computing methodologies, hardware, or software engineering.","Computing / technology policy:1.0,Professional topics:0.2,User characteristics:0.1",Computing / technology policy,Computing/technology policy is directly relevant as the paper discusses the societal impact of open source software and internet freedom. Other categories are rejected as the focus is on policy implications rather than user demographics or professional practices.,"Censorship:0,Commerce policy:0,Computer crime:0,Government technology policy:0.5,Intellectual property:1,Medical information policy:0,Network access control:0,Privacy policies:0,Surveillance:0","Intellectual property,Government technology policy","Intellectual property: The paper discusses open source software and freedom, which relates to IP. Government technology policy: It touches on policy implications of open source adoption. Other options like Privacy policies or Surveillance are not addressed."
5952,The management of high seas fisheries,"The intergovernmental United Nations Conference on Highly Migratory and Straddling Stocks, initiated in 1993 and finished in 1995, addressed the conservation and management of fishery resources located both within the coastal state 200 mile Exclusive Economic Zone (EEZ) and the adjacent high seas. These types of marine resources continue to be a source for international conflicts and debates. The original United Nations Law of the Sea of 1982 failed to address transboundary fisheries in a proper way. In particular, the agreement did not recognize the emergence of the complicated “straddling stock” issue. In the new United Nations Law of the Sea agreement of 1995, a consensus was reached that the management of the straddling and highly migratory fish stocks should be carried out through regional fisheries management organizations. We present a review of the straddling stock issues in the international agreement emerging from the negotiations within the United Nations. The review is contrasted with and clarified by game theoretic analyses. We also discuss one international fishery exemplifying the case, the Norwegian spring‐spawning herring. The main conclusion is that the local problems, faced during the stage of setting up regional fisheries organizations for the management of straddling and highly migratory fish stocks, are expected to be much more complicated and difficult to solve as compared to the cases of “shared fish stocks”. In the current paper, we present two reasons for this increased complexity. The first is the larger number of players as compared to the case of “shared fish stocks” and the second is the possibility of new members entering the regional fisheries organizations.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.9",Social and professional topics,Social and professional topics is relevant due to international fisheries policy analysis. No other categories address the focus on organizational and policy challenges in resource management.,"Computing / technology policy:0.8,Professional topics:0.3,User characteristics:0.2",Computing / technology policy,Computing/technology policy applies to fisheries management agreements. Professional topics and user characteristics are too generic and not directly related to the paper's focus on policy mechanics and game theory.,"Censorship:0.1,Commerce policy:0.8,Computer crime:0.1,Government technology policy:0.3,Intellectual property:0.1,Medical information policy:0.1,Network access control:0.1,Privacy policies:0.2,Surveillance:0.1",Commerce policy,Commerce policy: Addresses international fisheries governance and resource allocation through policy frameworks. Other categories like privacy policies or government technology policy are not directly relevant to the paper's focus on economic and regulatory aspects.
1514,The Role of Relational Familiarity When Interpreting Family Business Communication,"Research problem: This study investigates the difference in perceptions within the family-owned businesses of messages received from family members and outsiders to assess the role that relational familiarity (the amount of prior experience two people share communicating with each other) plays in internal business communication in these settings. Previous research has shown that this relational familiarity-along with experience with the communicative medium, the message topic, and the business context-lead to channel expansion (the ability to reduce uncertainty from a message with limited communicative cues) for business communicators. But the impact of relational familiarity on communication competence and personal biases between family members-which could impact the ability of relational familiarity to achieve its intended goals-has yet to be fully explored. Research questions: (1) Does increased relational familiarity between communicators lead to increased perceptions of message effectiveness and appropriateness in lean media settings? (2) In family business settings where familial biases may be prevalent, is increased relational familiarity linked to perceptions of increased sender likeability and lowered message negativity? Literature review: Channel Expansion Theory, which proposes that communicators can overcome the limitations of a medium by using their personal experience, guides this study. Previous research found that with personal experience with the communicator sending the message, a message receiver can enhance the limited capacity of a medium to convey cues that aid in understanding the message. Four perceptual variables-message effectiveness, message appropriateness, message negativity, sender likeability-result in benefits accrued from channel expansion and relational familiarity, such as reduced uncertainty and equivocality about the message topic. Methodology: A quasiexperimental setting was designed to elicit messages from business co-workers using email. Message senders were instructed to describe the same business problem in order for the recipient to understand the problem and decide how to proceed. Eighty-three family business owners, managers, and employees received one message from a family member belonging to the same business and one message assigned at random from a nonfamily member working elsewhere (a stranger). The 83 message receivers were asked to complete instruments measuring their perceptions of the messages. Results and conclusions: Results suggested that the relational familiarity between communicators played a significant role in improving message effectiveness and message appropriateness. Sender likeability was also enhanced when family members correctly identified the source of the message. The findings suggest that family members can benefit from channel expansion, though not as many members as expected, and that complex problem and task descriptions were communicated using lean media (communication channels that inhibit sending messages infused with communicative cues) and are perceived as most effective when described by familiar others.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.2,Computing methodologies:0.0,Applied computing:0.2,Social and professional topics:0.9",Social and professional topics,Social and professional topics align with the study of communication dynamics in family businesses. Other categories like Human-centered computing are less relevant as the focus is on organizational behavior.,"Computing / technology policy:0.1,Professional topics:0.2,User characteristics:0.6",User characteristics,"User characteristics is relevant because the study examines how relational familiarity (a human factor) influences communication effectiveness. The other categories are not relevant as the paper focuses on organizational communication theory, not computing technology or professional practice in computing.","Age:0.0,Cultural characteristics:0.0,Gender:0.0,Geographic characteristics:0.0,People with disabilities:0.0,Race and ethnicity:0.0,Religious orientation:0.0,Sexual orientation:0.0",,"The paper focuses on communication dynamics in family businesses, not demographic categories."
4674,How Customer Participation can Drive Repurchase Intent,"The purpose of this paper is to examine the impact of customer participation in the service delivery process by designing and testing an empirical model with the customers’ point of view in mind. Data are collected in the context of professional financial insurance services. The proposed model is analyzed with partial least squares (PLS) path modeling in SmartPLS 2.0 software. The results of the study show that customer participation produces positive effects on customer satisfaction and affective commitment through the customer relational value. Affective commitment is a strong predictor of repurchase intent, but no relationship between customer satisfaction and repurchase intent was found. This study suggests that customer participation can be a win-win situation for customers and the service firm. Customers who create relational value with their service providers effectively enjoy their services more and are more likely to build and maintain long-term relationships with their service firm. Our findings highlight the roles of the customer and indicate the heuristic value of viewing customer satisfaction and affective commitment as consequences of customer participation. This can enhance the understanding of how encounters should be designed in order to support employees and improve the co-creation of value.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.9",Social and professional topics,Social and professional topics is relevant for business/customer behavior analysis. Other fields like Information systems are not applicable as the paper is not technical.,"Computing / technology policy:0.1,Professional topics:0.1,User characteristics:0.9",User characteristics,"User characteristics: The paper studies how customer participation affects satisfaction and repurchase intent, directly relating to user behavior modeling. Other categories are not relevant to this service science study.","Age:0.1,Cultural characteristics:0.1,Gender:0.1,Geographic characteristics:0.1,People with disabilities:0.1,Race and ethnicity:0.1,Religious orientation:0.1,Sexual orientation:0.1",,None of the demographic categories are directly relevant to the study of customer participation in service delivery. The paper focuses on service delivery models and behavioral outcomes rather than demographic characteristics.
2015,Adaption-Innovation Theory and Cognitive Diversity: The Impact on Knowledge Use within Organizations,"The usefulness of data within organizations is proposed to be partially dependent upon the characteristics of those organization members who actually use it. Certain types of data are more likely to be accessed and utilized by organization members when they prefer working with it. The value and usefulness of data is therefore partially determined by the desire of individuals to use it. Because critical data may not be in a preferred format, it can be overlooked or ignored by some organizational members. Moreover, organizational members may also rely too heavily on their favored types of data. Thus, inappropriate access and use of data can occur within organizational decision making. This can result in ineffective decision-making and poor organizational performance. The purpose of this study is to propose relationships between the users of both tacit and explicit knowledge and their preferred cognitive style and to test these proposed relationships using empirical data.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.1,Information systems:0.3,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:0.6",Social and professional topics,Social and professional topics is the only relevant category as the paper discusses organizational decision-making and cognitive styles. Other categories are unrelated to organizational theory or cognitive science.,"Computing / technology policy:0.1,Professional topics:0.2,User characteristics:0.9",User characteristics,"User characteristics: The study examines how individual cognitive styles affect data usage, aligning directly with user characteristics. 'Professional topics' (0.2) and 'Computing / technology policy' (0.1) are less directly related to the core behavioral analysis.","Age:0.2,Cultural characteristics:0.3,Gender:0.2,Geographic characteristics:0.1,People with disabilities:0.1,Race and ethnicity:0.1,Religious orientation:0.1,Sexual orientation:0.1",,None of the options directly address cognitive styles or knowledge use patterns discussed in the paper. The focus is on cognitive diversity rather than demographic characteristics listed in the options.
3055,Responder Feelings in a Three-Player Three-Option Ultimatum Game: Affective Determinants of Rejection Behavior,"This paper addresses the role of affect and emotions in shaping the behavior of responders in the ultimatum game. A huge amount of research shows that players do not behave in an economically rational way in the ultimatum game, and emotional mechanisms have been proposed as a possible explanation. In particular, feelings of fairness, anger and envy are likely candidates as affective determinants. We introduce a three-player ultimatum game with three-options, which permits the responder to either penalize the proposer or to penalize a third party by rejecting offers. This allows for partially distinguishing rejections due to a retaliation motive driven by anger towards the proposer from rejections due to inequity aversion driven by feelings of envy towards a third party. Results from two experiments suggest that responders experience feelings of dissatisfaction and unfairness if their share is small in comparison to the proposer’s share; anger, then, may trigger rejections towards the proposer. Responders also experience dissatisfaction and envy when third party shares exceed their own shares; however, in contrast to anger, envy does not trigger rejections and is dissociated from the decision to accept or reject an offer. We conclude that acting upon anger is socially acceptable, whereas envy is not acceptable as a reason for action. Furthermore, we find that responders generally feel better after rejections, suggesting that rejections serve to regulate one’s affective state.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.5,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:1.0",Social and professional topics,"Social and professional topics is highly relevant as the paper focuses on behavioral economics and social decision-making. Other categories like Human-centered computing (0.5) receive moderate scores due to tangential relevance to human behavior but lack core technical contributions. Remaining categories are irrelevant as the paper does not address computational systems, algorithms, or applied computing.","Computing / technology policy:0.1,Professional topics:0.1,User characteristics:0.8",User characteristics,"The study is about user behavior in games, though it's more social science.","Age:0,Cultural characteristics:0,Gender:0,Geographic characteristics:0,People with disabilities:0,Race and ethnicity:0,Religious orientation:0,Sexual orientation:0",,"None of the options are relevant to the study of affective determinants in ultimatum game behavior. The paper focuses on emotions (fairness, anger, envy) rather than demographic categories like age, gender, or ethnicity."
1087,Reading strategies and student comprehension in an Internet ethics course,"Our students' reading comprehension skills underpin their ability to learn the concepts and competencies in computer science. However, there has been little attention given to evaluating how well our students comprehend what they read and the effectiveness of strategies to improve their reading comprehension. In this paper we focus on the case of a non-major course on internet ethics and the reading of the original writings of key figures in ethical theory. Due to the observed weakness of the reading comprehension skills of the students, we have been teaching our students strategies to improve their skills in reading comprehension and meta-comprehension within the framework of an in-class group reading. The reading strategies we have been using are described, evaluated, and placed in the context of the reading comprehension literature. Increased awareness of the limitations of the reading comprehension skills of our students and of ways to teach the students strategies to improve those skills should prove useful in helping instructors ensure that our students have the foundation they need to be successful in their computer science studies.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.2,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.8",Social and professional topics,Social and professional topics: The paper discusses educational methods for computer science students. Other categories are rejected because the focus is on pedagogy rather than technical computing aspects.,"Computing / technology policy:0.1,Professional topics:0.75,User characteristics:0.85","User characteristics,Professional topics",User characteristics: The paper studies student reading comprehension skills as a user characteristic. Professional topics: The focus on teaching strategies in a computing course aligns with professional education topics. Computing/technology policy is not relevant to the study's focus on pedagogical methods.,"Age:0.0,Computing and business:0.0,Computing education:1.0,Computing industry:0.0,Computing profession:0.0,Cultural characteristics:0.0,Gender:0.0,Geographic characteristics:0.0,History of computing:0.0,Management of computing and information systems:0.0,People with disabilities:0.0,Race and ethnicity:0.0,Religious orientation:0.0,Sexual orientation:0.0",Computing education,"Computing education is relevant because the paper focuses on improving student reading comprehension in a computer science course through pedagogical strategies. Other categories like Age, Gender, or Race are demographic factors not central to the study's focus on educational methods."
3964,Education and Training in ComSoc: Recent Achievements,"This article describes the recent achievements of the IEEE Communications Society in the area of education and training, and provides some background and description of how the Education and Training Board has been engaged in several activities that have led to results that position ComSoc at the forefront of educational activities and services within IEEE.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.0,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:1.0",Social and professional topics,Social and professional topics is relevant as the paper discusses educational activities. Other categories like Applied computing are not discussed.,"Computing / technology policy:0.1,Professional topics:0.9,User characteristics:0.2",Professional topics,"Professional topics: The paper discusses education and training initiatives by IEEE ComSoc, which aligns with professional development and organizational activities. Computing/technology policy and User characteristics are irrelevant as the paper does not address policy issues or user demographics.","Computing and business:0,Computing education:1,Computing industry:0,Computing profession:0.3,History of computing:0,Management of computing and information systems:0",Computing education,Computing education (1): The paper focuses on educational initiatives by IEEE Communications Society. Computing profession (0.3): Marginal relevance through professional development context.
496,Misconceptions about computer science,Common misconceptions about computer science hinder professional growth and harm the identity of computing.,"General and reference:0,Hardware:0,Computer systems organization:0,Networks:0,Software and its engineering:0,Theory of computation:0,Mathematics of computing:0,Information systems:0,Security and privacy:0,Human-centered computing:0,Computing methodologies:0,Applied computing:0,Social and professional topics:1",Social and professional topics,"The paper discusses misconceptions about computer science and their impact on professional growth and identity, which directly aligns with 'Social and professional topics'. No other categories are relevant as the paper does not address technical or methodological contributions.","Computing / technology policy:1.0,Professional topics:0.2,User characteristics:0.1",Computing / technology policy,Computing / technology policy is directly relevant as the paper addresses policy issues in computer science education. Other fields like Professional topics are not central to the core contribution of policy analysis.,"Censorship:0.1,Commerce policy:0.1,Computer crime:0.1,Government technology policy:0.2,Intellectual property:0.1,Medical information policy:0.1,Network access control:0.1,Privacy policies:0.1,Surveillance:0.1",,"None of the options directly address misconceptions about computer science as discussed in the abstract. The paper focuses on societal/educational aspects, which are not covered by the provided policy or security-related categories."
1320,Persevere or Exit: What is the Right Strategy?,": The teaching case describes India’s telecommunications market as it went through a major transformation due to its adopting mobile telephony. The country’s huge market size and low teledensity have provided an attractive opportunity for foreign multinationals. Telenor entered the Indian market through a joint venture with Unitech Wireless under the brand name of Uninor and targeted the value-conscious segment of mobile customers with its attractive pricing schemes. After a few years of rapid growth, Uninor faced a huge business risk when the Supreme Court o f India ruled that its purchased spectrum was illegal. The case describes the aftermath of the 2G spectrum scam and how it adversely affected Uninor’s future. At this critical juncture, Telenor faced a strategic decision dilemma. Should it continue its operations in the ever-growing Indian mobile market or should it cut its losses and exit before receiving further damage to its global brand? This teaching case imparts important lessons about doing telecommunications-related business in an emerging economy with high returns and immense business risks.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.9",Social and professional topics,Social and professional topics is highly relevant as the paper is a business case study. Other categories are rejected as the focus is not on computing techniques but on strategic business decisions.,"Computing / technology policy:0.8,Professional topics:0.7,User characteristics:0.1","Computing / technology policy,Professional topics",Computing/technology policy is relevant to the telecommunications market analysis. Professional topics applies to the strategic decision-making context. User characteristics is not directly related to the core business case study.,"Censorship:0,Commerce policy:1,Computer crime:0,Computing and business:1,Computing education:0,Computing industry:0,Computing profession:0,Government technology policy:0,History of computing:0,Intellectual property:0,Management of computing and information systems:0,Medical information policy:0,Network access control:0,Privacy policies:0,Surveillance:0","Commerce policy,Computing and business",Commerce policy is relevant due to the focus on telecommunications market strategies. Computing and business is relevant as the paper discusses business decisions in an emerging economy. Other fields like Privacy policies are not discussed.
2848,Standards for Language Codes: developing ISO 639,"The international community, including the International Organization for Standardization (ISO), is currently seeking more granular systems of language identifiers than the widely used tags of ISO 639 parts 1 and 2. There is growing need for the more precise identification and annotation of language-based resources. This paper presents a key response to this need, in which the Linguasphere Register of the World's Languages and Speech Communities would provide the referential framework for a future part 6 of ISO 639. The paper discusses the proposed evolution of ISO 639-6 and its relationship to other parts of ISO 639, including its relevance to the definition of meta-data categories in ISO 12620.","General and reference:0.0,Hardware:0.0,Computer systems organization:0.0,Networks:0.0,Software and its engineering:0.0,Theory of computation:0.0,Mathematics of computing:0.0,Information systems:0.25,Security and privacy:0.0,Human-centered computing:0.0,Computing methodologies:0.0,Applied computing:0.0,Social and professional topics:1.0",Social and professional topics,Social and professional topics: The paper addresses language code standards development. Information systems has minimal relevance to metadata references.,"Computing / technology policy:1.0,Professional topics:0.75,User characteristics:0.2",Computing / technology policy,Computing / technology policy is highly relevant as the paper addresses the development of ISO 639 language standards. Professional topics is secondary as it relates to standardization processes. User characteristics are not discussed.,"Censorship:0,Commerce policy:0,Computer crime:0,Government technology policy:0,Intellectual property:0,Medical information policy:0,Network access control:0,Privacy policies:0,Surveillance:0",,None of the provided categories directly relate to the paper's focus on language code standardization and metadata frameworks. The paper addresses technical standardization rather than policy or security domains.
691,Using poetry as a teaching tool for the humanities to learn computer science concepts,"The emerging fields of digital humanities and humanities computing require new approaches in the area of their education by computer scientists. Currently, the role of educators in computer science has been to ready computer science students to obtain employment and research within the sciences, however, this must be changed if these two new fields are to be adequately served. We look at four areas where poetry can be used as an introduction to computer science: the analysis of poetry which uses the computer as its media, the analysis of concrete poems, the creation of poetry using a computer or using the computer as an aid to its creation, and professional development for instructors.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.9",Social and professional topics,Social and professional topics is relevant as the paper discusses educational approaches and interdisciplinary collaboration in digital humanities. Other categories are irrelevant since the paper focuses on pedagogical methods rather than technical computing systems.,"Computing / technology policy:0.2,Professional topics:0.8,User characteristics:0.2",Professional topics,Professional topics is relevant as the paper discusses educational methods for computer science. Other categories are irrelevant as the focus is not on policy or user demographics.,"Computing and business:0.2,Computing education:1,Computing industry:0.2,Computing profession:0.2,History of computing:0.2,Management of computing and information systems:0.2",Computing education,Computing education: Uses poetry to teach CS concepts in humanities. Other options do not address educational methodologies.
4461,The cybernetics of organising: management and leadership,Purpose – The purpose of this paper is to argue that in a strategic context organising is a cybernetic process that corresponds leadership and management. The paper reflects on the obverse condition where the lack of correspondence may facilitate failure.Design/methodology/approach – The paper applies Stafford Beer's viable systems model to consider management and leadership's relationship in the organisational context and draws on the practice of leadership and management to support the theoretical assertions.Findings – That management and leadership are key processes in organising that need to be in mutual correspondence in order to sustain the viability of the organisation.Research limitations/implications – The paper explores management and leadership from a systems perspective and so further practical work could be initiated to consider both successful organising and failure.Practical implications – The paper is attempting to demonstrate that organisations may need to develop leadership and managemen...,"General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.9",Social and professional topics,"Social and professional topics: The paper focuses on organizational theory, management, and leadership from a systems perspective, which aligns with social and professional topics. Other categories like Hardware or Networks are irrelevant as the paper discusses theoretical organizational concepts rather than technical computing domains.","Professional topics:0.9,Computing / technology policy:0.1,User characteristics:0.1",Professional topics,Professional topics is relevant as the paper discusses organizational management from a systems perspective. The other categories are unrelated to management theory.,"Computing and business:0.5,Computing education:0.1,Computing industry:0.1,Computing profession:0.1,History of computing:0.1,Management of computing and information systems:1","Management of computing and information systems,Computing and business",Management of computing and information systems: Focuses on organizational cybernetics. Computing and business: Applies computing concepts to business management. Other children are unrelated to the systems theory focus.
626,Effects of knowledge management on unit performance: examining the moderating role of tacitness and learnability,"Purpose 
 
 
 
 
This paper aims to examine and test the moderating influence of the type of knowledge underlying work – known as the knowledge in practice (KIP) perspective – on the relationship between knowledge management (KM) activities and unit performance. KIP proposes that the knowledge underlying work varies according to two dimensions: tacitness and learnability. This theory proposes that aligning KM activities with tacitness and learnability results in increased performance. However, to the authors’ knowledge, there exists no direct empirical tests of these propositions outlined in KIP theory. This study examines the empirical support for the theoretical predictions outlined by KIP. 
 
 
 
 
Design/methodology/approach 
 
 
 
 
The study uses a multiple survey, multiple respondent survey design to measure KM activity sets, the tacitness and learnability involved in work contexts and unit performance. Regression analysis is used to test the hypotheses. 
 
 
 
 
Findings 
 
 
 
 
In line with previous research, the authors find support for a direct relationship between some KM activity sets and unit performance. Surprisingly, the authors did not find support for the predictions offered by KIP theory. Specifically, the degree of tacitness or learnability did not moderate the relationship between KM activity sets and unit performance. 
 
 
 
 
Research limitations/implications 
 
 
 
 
The lack of findings to support the moderating effects of tacitness and learnability on the relationship between KM activity sets and unit performance challenges the adequacy of existing formulations of KIP theory. The authors discuss several important future research directions to examine this puzzling finding. 
 
 
 
 
Practical implications 
 
 
 
 
This paper reinforces the suggestion that managers at all levels of organizations should engage in KM activities to increase performance. These findings also suggest that considering the type of knowledge underlying a unit’s work should not be a consideration in implementing KM activities. 
 
 
 
 
Originality/value 
 
 
 
 
This is the first study to empirically test a KIP perspective. That is, how the type of knowledge involved in work moderates the relationships between KM activity sets and unit performance.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.9",Social and professional topics,Social and professional topics is relevant for organizational knowledge management studies. Other categories lack management theory or organizational focus.,"Computing / technology policy:0.1,Professional topics:0.8,User characteristics:0.2",Professional topics,Professional topics is relevant for examining knowledge management in organizational settings. User characteristics is marginally relevant but not central to the study's focus on organizational performance.,"Computing and business:1.0,Computing education:0.3,Computing industry:0.4,Computing profession:0.2,History of computing:0.1,Management of computing and information systems:1.0","Computing and business,Management of computing and information systems",Computing and business: The study examines KM activities in organizational contexts. Management of computing: The paper directly addresses knowledge management strategies in organizational performance. Other categories like 'Computing education' are irrelevant as the focus is on business applications.
2738,The Effects of Network Diversity and Social Norms on Social Structuring: Empirical Evidence from Online Social Networks,"Behavior in social groups follows social norms defining what is acceptable and what is not. Prior research has found strong tendencies toward informational isomorphism in online social networks, as social peers seem to establish a shared understanding of what behavior is acceptable. Due to the additive nature of these social norms, individual' s social context gets more restrictive as network diversity increases. As a consequence, individuals organize their contacts into groups, which is referred to as social structuring behavior, to create a less restrictive environment and to avoid violating social norms. While drawing on diversity literature, this study examines the relationship between age, social embeddedness, and nationality diversity on social structuring behavior. For that, a matched dataset from Facebook is used. Findings support the important role social norms play in OSN, and that increases in social embeddedness lead to increases in structuring behavior. Implications for IS theory are discussed.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.2,Security and privacy:0.1,Human-centered computing:0.2,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.9",Social and professional topics,Social and professional topics is relevant as the paper empirically studies social norms and structuring behavior in online networks. Other categories like Networks focus on infrastructure rather than social dynamics.,"Computing / technology policy:0.7,Professional topics:0.1,User characteristics:0.9","User characteristics,Computing / technology policy",User characteristics is highly relevant for studying social structuring behavior. Computing / technology policy is relevant for understanding social norms in OSNs.,"Age:0.5,Censorship:0.1,Commerce policy:0.1,Computer crime:0.1,Cultural characteristics:0.8,Gender:0.1,Geographic characteristics:0.1,Government technology policy:0.1,Intellectual property:0.1,Medical information policy:0.1,Network access control:0.1,People with disabilities:0.1,Privacy policies:0.1,Race and ethnicity:0.1,Religious orientation:0.1,Sexual orientation:0.1,Surveillance:0.1",Cultural characteristics,"Cultural characteristics relates to social norms and diversity effects. Other demographic categories (age, gender) are factors in the study but not core themes. Network access control is irrelevant."
5442,The Role of Body Image in the Relationship Between Internet Use and Bulimic Symptoms: Three Theoretical Frameworks,"Exposure to traditional media has been associated with bulimic symptoms. However, to date, little is known regarding the effects of Internet exposure. The aim of this study was to explore the relationships between Internet use and bulimic symptoms within the competing frameworks of sociocultural, impression management, and self-objectification theory. A sample of 289 French women aged 18-25 years completed an online questionnaire assessing bulimic symptoms, body dissatisfaction, body image avoidance, self-surveillance, body shame, and weekly Internet use. Bootstrapping analyses revealed that body shame and body image avoidance mediated the effect of weekly Internet use on bulimic symptoms. Furthermore, when entered into a multiple mediation analysis, these two variables provided independent mediation pathways of equal magnitude. The findings support the usefulness of both the self-objectification and impression management frameworks for investigating the relationship between Internet use and bulimic symptoms. Longitudinal research would help to clarify these pathways further.","General and reference:0.1,Hardware:0.05,Computer systems organization:0.05,Networks:0.05,Software and its engineering:0.05,Theory of computation:0.05,Mathematics of computing:0.05,Information systems:0.05,Security and privacy:0.05,Human-centered computing:0.1,Computing methodologies:0.05,Applied computing:0.15,Social and professional topics:0.2",Social and professional topics,"The paper analyzes the social and psychological impact of internet use on bulimic symptoms, which aligns with social and professional topics. Other categories like human-centered computing or information systems are not directly relevant as the focus is on behavioral studies rather than computational systems.","Computing / technology policy:0.2,Professional topics:0.1,User characteristics:0.7",User characteristics,User characteristics is highly relevant as the paper studies how Internet use affects human behavior related to bulimic symptoms. The other categories are less relevant as the paper is not primarily about computing policy or professional topics.,"Age:0.2,Cultural characteristics:0.3,Gender:1.0,Geographic characteristics:0.8,People with disabilities:0.1,Race and ethnicity:0.1,Religious orientation:0.1,Sexual orientation:0.1","Gender,Geographic characteristics",Gender is highly relevant as the study focuses on French women. Geographic characteristics are relevant due to the French sample. Other categories like age or race are not central to the study.
4095,Have Faith,"Faith is often described as being sure of what we hope for and certain of what we don't see. As embedded devices become ever smaller and computers continue to fade into the background or “disappear,” it's interesting to reflect on the levels of faith or trust we, as users, might place in such future systems.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.9",Social and professional topics,Social and professional topics is relevant for the philosophical discussion on user trust in embedded systems. Other categories lack the socio-technical focus.,"Computing / technology policy:0.7,Professional topics:0.2,User characteristics:0.6","Computing / technology policy,User characteristics",Computing / technology policy is relevant for the discussion on trust in embedded systems. User characteristics applies to the analysis of user faith in future systems. Professional topics is less central as the paper is conceptual rather than career-focused.,"Age:0.1,Censorship:0.1,Commerce policy:0.1,Computer crime:0.1,Cultural characteristics:0.2,Gender:0.1,Geographic characteristics:0.1,Government technology policy:0.1,Intellectual property:0.1,Medical information policy:0.1,Network access control:0.1,People with disabilities:0.1,Privacy policies:0.6,Race and ethnicity:0.1,Religious orientation:0.1,Sexual orientation:0.1,Surveillance:0.5","Privacy policies,Surveillance","Privacy policies and surveillance are moderately relevant as the paper discusses trust in systems, but the connection to technical fields is weak. Most other categories are irrelevant as the paper is philosophical rather than technical."
3500,"Service systems, service scientists, SSME, and innovation","Computer scientists work with formal models of algorithms and computation, and someday service scientists may work with formal models of service systems. The four examples here document some of the early efforts to establish a new academic discipline and new profession.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.1,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.1,Human-centered computing:0.1,Computing methodologies:0.1,Applied computing:0.1,Social and professional topics:0.9",Social and professional topics,Social and professional topics is relevant as the paper discusses the establishment of a new academic discipline and profession (service science). Other categories lack direct alignment with the focus on academic and professional structure.,"Computing / technology policy:0.1,Professional topics:1.0,User characteristics:0.1",Professional topics,Professional topics is relevant as the paper discusses establishing a new academic discipline and profession for service systems. Other categories are unrelated to academic/professional development.,"Computing and business:1.0,Computing education:0.3,Computing industry:0.1,Computing profession:0.5,History of computing:0.1,Management of computing and information systems:0.2","Computing and business,Computing profession",Computing and business: The paper discusses the development of a service systems discipline for business applications. Computing profession: It addresses the emergence of service scientists as a new professional field. Education and management aspects are secondary to the core focus on disciplinary development.
5866,The Right to Digital Self-Defense,"Although the right to physical self-defense is legally well established, it’s unclear how this principle translates to security in today’s digital world. Approaches to cyber self-defense are discussed, as are arguments for and against allowing organizations to actively respond to cyberattacks.","General and reference:0.1,Hardware:0.1,Computer systems organization:0.1,Networks:0.1,Software and its engineering:0.2,Theory of computation:0.1,Mathematics of computing:0.1,Information systems:0.1,Security and privacy:0.2,Human-centered computing:0.1,Computing methodologies:0.2,Applied computing:0.1,Social and professional topics:0.9",Social and professional topics,Social and professional topics is highly relevant as the paper discusses legal and policy aspects of digital self-defense. Other categories like Security and privacy are secondary as the focus is on societal implications rather than technical mechanisms.,"Computing / technology policy:0.95,Professional topics:0.1,User characteristics:0.1",Computing / technology policy,Computing policy is directly relevant as the paper examines legal principles for digital self-defense. Other categories are irrelevant as the paper doesn't discuss professional issues or user behavior.,"Censorship:0.1,Commerce policy:0.1,Computer crime:1.0,Government technology policy:0.6,Intellectual property:0.2,Medical information policy:0.3,Network access control:0.1,Privacy policies:0.3,Surveillance:0.2",Computer crime,"The paper discusses legal aspects of cyber self-defense, aligning directly with computer crime policy considerations. Government technology policy is tangentially relevant but less specific to the core legal analysis."
